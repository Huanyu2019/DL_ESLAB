Files already downloaded and verified
Files already downloaded and verified
Training Epoch: 1 [256/50000]	Loss: 4.6090	LR: 0.000000
Training Epoch: 1 [512/50000]	Loss: 4.6289	LR: 0.005102
Training Epoch: 1 [768/50000]	Loss: 4.6683	LR: 0.010204
Training Epoch: 1 [1024/50000]	Loss: 4.6127	LR: 0.015306
Training Epoch: 1 [1280/50000]	Loss: 4.6213	LR: 0.020408
Training Epoch: 1 [1536/50000]	Loss: 4.6371	LR: 0.025510
Training Epoch: 1 [1792/50000]	Loss: 4.6111	LR: 0.030612
Training Epoch: 1 [2048/50000]	Loss: 4.6448	LR: 0.035714
Training Epoch: 1 [2304/50000]	Loss: 4.6657	LR: 0.040816
Training Epoch: 1 [2560/50000]	Loss: 4.6507	LR: 0.045918
Training Epoch: 1 [2816/50000]	Loss: 4.6405	LR: 0.051020
Training Epoch: 1 [3072/50000]	Loss: 4.6669	LR: 0.056122
Training Epoch: 1 [3328/50000]	Loss: 4.6444	LR: 0.061224
Training Epoch: 1 [3584/50000]	Loss: 4.6265	LR: 0.066327
Training Epoch: 1 [3840/50000]	Loss: 4.7109	LR: 0.071429
Training Epoch: 1 [4096/50000]	Loss: 4.6549	LR: 0.076531
Training Epoch: 1 [4352/50000]	Loss: 4.5786	LR: 0.081633
Training Epoch: 1 [4608/50000]	Loss: 4.6163	LR: 0.086735
Training Epoch: 1 [4864/50000]	Loss: 4.6523	LR: 0.091837
Training Epoch: 1 [5120/50000]	Loss: 4.6567	LR: 0.096939
Training Epoch: 1 [5376/50000]	Loss: 4.6232	LR: 0.102041
Training Epoch: 1 [5632/50000]	Loss: 4.5750	LR: 0.107143
Training Epoch: 1 [5888/50000]	Loss: 4.5940	LR: 0.112245
Training Epoch: 1 [6144/50000]	Loss: 4.7025	LR: 0.117347
Training Epoch: 1 [6400/50000]	Loss: 4.5905	LR: 0.122449
Training Epoch: 1 [6656/50000]	Loss: 4.6628	LR: 0.127551
Training Epoch: 1 [6912/50000]	Loss: 4.5366	LR: 0.132653
Training Epoch: 1 [7168/50000]	Loss: 4.5977	LR: 0.137755
Training Epoch: 1 [7424/50000]	Loss: 4.5530	LR: 0.142857
Training Epoch: 1 [7680/50000]	Loss: 4.5212	LR: 0.147959
Training Epoch: 1 [7936/50000]	Loss: 4.5463	LR: 0.153061
Training Epoch: 1 [8192/50000]	Loss: 4.5229	LR: 0.158163
Training Epoch: 1 [8448/50000]	Loss: 4.5574	LR: 0.163265
Training Epoch: 1 [8704/50000]	Loss: 4.5415	LR: 0.168367
Training Epoch: 1 [8960/50000]	Loss: 4.5912	LR: 0.173469
Training Epoch: 1 [9216/50000]	Loss: 4.4855	LR: 0.178571
Training Epoch: 1 [9472/50000]	Loss: 4.5611	LR: 0.183673
Training Epoch: 1 [9728/50000]	Loss: 4.5198	LR: 0.188776
Training Epoch: 1 [9984/50000]	Loss: 4.5536	LR: 0.193878
Training Epoch: 1 [10240/50000]	Loss: 4.4923	LR: 0.198980
Training Epoch: 1 [10496/50000]	Loss: 4.5130	LR: 0.204082
Training Epoch: 1 [10752/50000]	Loss: 4.5700	LR: 0.209184
Training Epoch: 1 [11008/50000]	Loss: 4.5414	LR: 0.214286
Training Epoch: 1 [11264/50000]	Loss: 4.4641	LR: 0.219388
Training Epoch: 1 [11520/50000]	Loss: 4.4553	LR: 0.224490
Training Epoch: 1 [11776/50000]	Loss: 4.5556	LR: 0.229592
Training Epoch: 1 [12032/50000]	Loss: 4.4654	LR: 0.234694
Training Epoch: 1 [12288/50000]	Loss: 4.4338	LR: 0.239796
Training Epoch: 1 [12544/50000]	Loss: 4.5179	LR: 0.244898
Training Epoch: 1 [12800/50000]	Loss: 4.4956	LR: 0.250000
Training Epoch: 1 [13056/50000]	Loss: 4.4483	LR: 0.255102
Training Epoch: 1 [13312/50000]	Loss: 4.4584	LR: 0.260204
Training Epoch: 1 [13568/50000]	Loss: 4.4494	LR: 0.265306
Training Epoch: 1 [13824/50000]	Loss: 4.4815	LR: 0.270408
Training Epoch: 1 [14080/50000]	Loss: 4.4864	LR: 0.275510
Training Epoch: 1 [14336/50000]	Loss: 4.3982	LR: 0.280612
Training Epoch: 1 [14592/50000]	Loss: 4.4518	LR: 0.285714
Training Epoch: 1 [14848/50000]	Loss: 4.4987	LR: 0.290816
Training Epoch: 1 [15104/50000]	Loss: 4.5172	LR: 0.295918
Training Epoch: 1 [15360/50000]	Loss: 4.4001	LR: 0.301020
Training Epoch: 1 [15616/50000]	Loss: 4.4800	LR: 0.306122
Training Epoch: 1 [15872/50000]	Loss: 4.4557	LR: 0.311224
Training Epoch: 1 [16128/50000]	Loss: 4.4146	LR: 0.316327
Training Epoch: 1 [16384/50000]	Loss: 4.5422	LR: 0.321429
Training Epoch: 1 [16640/50000]	Loss: 4.4383	LR: 0.326531
Training Epoch: 1 [16896/50000]	Loss: 4.4328	LR: 0.331633
Training Epoch: 1 [17152/50000]	Loss: 4.5166	LR: 0.336735
Training Epoch: 1 [17408/50000]	Loss: 4.4498	LR: 0.341837
Training Epoch: 1 [17664/50000]	Loss: 4.4621	LR: 0.346939
Training Epoch: 1 [17920/50000]	Loss: 4.5309	LR: 0.352041
Training Epoch: 1 [18176/50000]	Loss: 4.5504	LR: 0.357143
Training Epoch: 1 [18432/50000]	Loss: 4.4517	LR: 0.362245
Training Epoch: 1 [18688/50000]	Loss: 4.4898	LR: 0.367347
Training Epoch: 1 [18944/50000]	Loss: 4.4338	LR: 0.372449
Training Epoch: 1 [19200/50000]	Loss: 4.4861	LR: 0.377551
Training Epoch: 1 [19456/50000]	Loss: 4.4977	LR: 0.382653
Training Epoch: 1 [19712/50000]	Loss: 4.4837	LR: 0.387755
Training Epoch: 1 [19968/50000]	Loss: 4.3790	LR: 0.392857
Training Epoch: 1 [20224/50000]	Loss: 4.5116	LR: 0.397959
Training Epoch: 1 [20480/50000]	Loss: 4.4008	LR: 0.403061
Training Epoch: 1 [20736/50000]	Loss: 4.3806	LR: 0.408163
Training Epoch: 1 [20992/50000]	Loss: 4.3543	LR: 0.413265
Training Epoch: 1 [21248/50000]	Loss: 4.4454	LR: 0.418367
Training Epoch: 1 [21504/50000]	Loss: 4.3310	LR: 0.423469
Training Epoch: 1 [21760/50000]	Loss: 4.4533	LR: 0.428571
Training Epoch: 1 [22016/50000]	Loss: 4.5417	LR: 0.433673
Training Epoch: 1 [22272/50000]	Loss: 4.4441	LR: 0.438776
Training Epoch: 1 [22528/50000]	Loss: 4.4302	LR: 0.443878
Training Epoch: 1 [22784/50000]	Loss: 4.4145	LR: 0.448980
Training Epoch: 1 [23040/50000]	Loss: 4.5189	LR: 0.454082
Training Epoch: 1 [23296/50000]	Loss: 4.4639	LR: 0.459184
Training Epoch: 1 [23552/50000]	Loss: 4.4231	LR: 0.464286
Training Epoch: 1 [23808/50000]	Loss: 4.3921	LR: 0.469388
Training Epoch: 1 [24064/50000]	Loss: 4.4522	LR: 0.474490
Training Epoch: 1 [24320/50000]	Loss: 4.3633	LR: 0.479592
Training Epoch: 1 [24576/50000]	Loss: 4.4169	LR: 0.484694
Training Epoch: 1 [24832/50000]	Loss: 4.4021	LR: 0.489796
Training Epoch: 1 [25088/50000]	Loss: 4.4194	LR: 0.494898
Training Epoch: 1 [25344/50000]	Loss: 4.3981	LR: 0.500000
Training Epoch: 1 [25600/50000]	Loss: 4.3448	LR: 0.505102
Training Epoch: 1 [25856/50000]	Loss: 4.3949	LR: 0.510204
Training Epoch: 1 [26112/50000]	Loss: 4.3732	LR: 0.515306
Training Epoch: 1 [26368/50000]	Loss: 4.3185	LR: 0.520408
Training Epoch: 1 [26624/50000]	Loss: 4.3462	LR: 0.525510
Training Epoch: 1 [26880/50000]	Loss: 4.3371	LR: 0.530612
Training Epoch: 1 [27136/50000]	Loss: 4.4009	LR: 0.535714
Training Epoch: 1 [27392/50000]	Loss: 4.3238	LR: 0.540816
Training Epoch: 1 [27648/50000]	Loss: 4.4252	LR: 0.545918
Training Epoch: 1 [27904/50000]	Loss: 4.4072	LR: 0.551020
Training Epoch: 1 [28160/50000]	Loss: 4.4025	LR: 0.556122
Training Epoch: 1 [28416/50000]	Loss: 4.4176	LR: 0.561224
Training Epoch: 1 [28672/50000]	Loss: 4.3457	LR: 0.566327
Training Epoch: 1 [28928/50000]	Loss: 4.3349	LR: 0.571429
Training Epoch: 1 [29184/50000]	Loss: 4.3129	LR: 0.576531
Training Epoch: 1 [29440/50000]	Loss: 4.3311	LR: 0.581633
Training Epoch: 1 [29696/50000]	Loss: 4.3883	LR: 0.586735
Training Epoch: 1 [29952/50000]	Loss: 4.3981	LR: 0.591837
Training Epoch: 1 [30208/50000]	Loss: 4.4446	LR: 0.596939
Training Epoch: 1 [30464/50000]	Loss: 4.4601	LR: 0.602041
Training Epoch: 1 [30720/50000]	Loss: 4.3955	LR: 0.607143
Training Epoch: 1 [30976/50000]	Loss: 4.3790	LR: 0.612245
Training Epoch: 1 [31232/50000]	Loss: 4.4346	LR: 0.617347
Training Epoch: 1 [31488/50000]	Loss: 4.3587	LR: 0.622449
Training Epoch: 1 [31744/50000]	Loss: 4.3669	LR: 0.627551
Training Epoch: 1 [32000/50000]	Loss: 4.3720	LR: 0.632653
Training Epoch: 1 [32256/50000]	Loss: 4.3861	LR: 0.637755
Training Epoch: 1 [32512/50000]	Loss: 4.3862	LR: 0.642857
Training Epoch: 1 [32768/50000]	Loss: 4.2869	LR: 0.647959
Training Epoch: 1 [33024/50000]	Loss: 4.5153	LR: 0.653061
Training Epoch: 1 [33280/50000]	Loss: 4.4041	LR: 0.658163
Training Epoch: 1 [33536/50000]	Loss: 4.4155	LR: 0.663265
Training Epoch: 1 [33792/50000]	Loss: 4.3926	LR: 0.668367
Training Epoch: 1 [34048/50000]	Loss: 4.4090	LR: 0.673469
Training Epoch: 1 [34304/50000]	Loss: 4.3766	LR: 0.678571
Training Epoch: 1 [34560/50000]	Loss: 4.4023	LR: 0.683673
Training Epoch: 1 [34816/50000]	Loss: 4.4417	LR: 0.688776
Training Epoch: 1 [35072/50000]	Loss: 4.3701	LR: 0.693878
Training Epoch: 1 [35328/50000]	Loss: 4.4152	LR: 0.698980
Training Epoch: 1 [35584/50000]	Loss: 4.3723	LR: 0.704082
Training Epoch: 1 [35840/50000]	Loss: 4.4384	LR: 0.709184
Training Epoch: 1 [36096/50000]	Loss: 4.4064	LR: 0.714286
Training Epoch: 1 [36352/50000]	Loss: 4.3896	LR: 0.719388
Training Epoch: 1 [36608/50000]	Loss: 4.3512	LR: 0.724490
Training Epoch: 1 [36864/50000]	Loss: 4.4011	LR: 0.729592
Training Epoch: 1 [37120/50000]	Loss: 4.3717	LR: 0.734694
Training Epoch: 1 [37376/50000]	Loss: 4.3441	LR: 0.739796
Training Epoch: 1 [37632/50000]	Loss: 4.4092	LR: 0.744898
Training Epoch: 1 [37888/50000]	Loss: 4.3317	LR: 0.750000
Training Epoch: 1 [38144/50000]	Loss: 4.3453	LR: 0.755102
Training Epoch: 1 [38400/50000]	Loss: 4.3742	LR: 0.760204
Training Epoch: 1 [38656/50000]	Loss: 4.3022	LR: 0.765306
Training Epoch: 1 [38912/50000]	Loss: 4.3586	LR: 0.770408
Training Epoch: 1 [39168/50000]	Loss: 4.4661	LR: 0.775510
Training Epoch: 1 [39424/50000]	Loss: 4.4642	LR: 0.780612
Training Epoch: 1 [39680/50000]	Loss: 4.3955	LR: 0.785714
Training Epoch: 1 [39936/50000]	Loss: 4.3744	LR: 0.790816
Training Epoch: 1 [40192/50000]	Loss: 4.3096	LR: 0.795918
Training Epoch: 1 [40448/50000]	Loss: 4.3563	LR: 0.801020
Training Epoch: 1 [40704/50000]	Loss: 4.2913	LR: 0.806122
Training Epoch: 1 [40960/50000]	Loss: 4.3820	LR: 0.811224
Training Epoch: 1 [41216/50000]	Loss: 4.4234	LR: 0.816327
Training Epoch: 1 [41472/50000]	Loss: 4.3559	LR: 0.821429
Training Epoch: 1 [41728/50000]	Loss: 4.4108	LR: 0.826531
Training Epoch: 1 [41984/50000]	Loss: 4.3666	LR: 0.831633
Training Epoch: 1 [42240/50000]	Loss: 4.3697	LR: 0.836735
Training Epoch: 1 [42496/50000]	Loss: 4.4081	LR: 0.841837
Training Epoch: 1 [42752/50000]	Loss: 4.3739	LR: 0.846939
Training Epoch: 1 [43008/50000]	Loss: 4.4122	LR: 0.852041
Training Epoch: 1 [43264/50000]	Loss: 4.3136	LR: 0.857143
Training Epoch: 1 [43520/50000]	Loss: 4.3653	LR: 0.862245
Training Epoch: 1 [43776/50000]	Loss: 4.3861	LR: 0.867347
Training Epoch: 1 [44032/50000]	Loss: 4.3668	LR: 0.872449
Training Epoch: 1 [44288/50000]	Loss: 4.3367	LR: 0.877551
Training Epoch: 1 [44544/50000]	Loss: 4.4275	LR: 0.882653
Training Epoch: 1 [44800/50000]	Loss: 4.4029	LR: 0.887755
Training Epoch: 1 [45056/50000]	Loss: 4.3702	LR: 0.892857
Training Epoch: 1 [45312/50000]	Loss: 4.2931	LR: 0.897959
Training Epoch: 1 [45568/50000]	Loss: 4.3333	LR: 0.903061
Training Epoch: 1 [45824/50000]	Loss: 4.3018	LR: 0.908163
Training Epoch: 1 [46080/50000]	Loss: 4.3064	LR: 0.913265
Training Epoch: 1 [46336/50000]	Loss: 4.2939	LR: 0.918367
Training Epoch: 1 [46592/50000]	Loss: 4.3122	LR: 0.923469
Training Epoch: 1 [46848/50000]	Loss: 4.3992	LR: 0.928571
Training Epoch: 1 [47104/50000]	Loss: 4.3206	LR: 0.933673
Training Epoch: 1 [47360/50000]	Loss: 4.4673	LR: 0.938776
Training Epoch: 1 [47616/50000]	Loss: 4.4080	LR: 0.943878
Training Epoch: 1 [47872/50000]	Loss: 4.3171	LR: 0.948980
Training Epoch: 1 [48128/50000]	Loss: 4.2240	LR: 0.954082
Training Epoch: 1 [48384/50000]	Loss: 4.3659	LR: 0.959184
Training Epoch: 1 [48640/50000]	Loss: 4.3535	LR: 0.964286
Training Epoch: 1 [48896/50000]	Loss: 4.3011	LR: 0.969388
Training Epoch: 1 [49152/50000]	Loss: 4.3957	LR: 0.974490
Training Epoch: 1 [49408/50000]	Loss: 4.3050	LR: 0.979592
Training Epoch: 1 [49664/50000]	Loss: 4.3244	LR: 0.984694
Training Epoch: 1 [49920/50000]	Loss: 4.4062	LR: 0.989796
Training Epoch: 1 [50000/50000]	Loss: 4.3740	LR: 0.994898
epoch 1 training time consumed: 22.16s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |    1501 GB |    1500 GB |
|       from large pool |  400448 KB |    1770 MB |    1499 GB |    1499 GB |
|       from small pool |    3549 KB |       9 MB |       1 GB |       1 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |    1501 GB |    1500 GB |
|       from large pool |  400448 KB |    1770 MB |    1499 GB |    1499 GB |
|       from small pool |    3549 KB |       9 MB |       1 GB |       1 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247267 KB |  475717 KB |     925 GB |     924 GB |
|       from large pool |  244672 KB |  473024 KB |     923 GB |     923 GB |
|       from small pool |    2595 KB |    4076 KB |       1 GB |       1 GB |
|---------------------------------------------------------------------------|
| Allocations           |     221    |     291    |   64479    |   64258    |
|       from large pool |      36    |      77    |   31033    |   30997    |
|       from small pool |     185    |     223    |   33446    |   33261    |
|---------------------------------------------------------------------------|
| Active allocs         |     221    |     291    |   64479    |   64258    |
|       from large pool |      36    |      77    |   31033    |   30997    |
|       from small pool |     185    |     223    |   33446    |   33261    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      28    |   32146    |   32124    |
|       from large pool |      10    |      11    |   12855    |   12845    |
|       from small pool |      12    |      18    |   19291    |   19279    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 1, Average loss: 0.0174, Accuracy: 0.0238, Time consumed:1.45s

Training Epoch: 2 [256/50000]	Loss: 4.4082	LR: 1.000000
Training Epoch: 2 [512/50000]	Loss: 4.3356	LR: 1.000000
Training Epoch: 2 [768/50000]	Loss: 4.3581	LR: 1.000000
Training Epoch: 2 [1024/50000]	Loss: 4.3903	LR: 1.000000
Training Epoch: 2 [1280/50000]	Loss: 4.4196	LR: 1.000000
Training Epoch: 2 [1536/50000]	Loss: 4.3636	LR: 1.000000
Training Epoch: 2 [1792/50000]	Loss: 4.3381	LR: 1.000000
Training Epoch: 2 [2048/50000]	Loss: 4.3469	LR: 1.000000
Training Epoch: 2 [2304/50000]	Loss: 4.3016	LR: 1.000000
Training Epoch: 2 [2560/50000]	Loss: 4.4596	LR: 1.000000
Training Epoch: 2 [2816/50000]	Loss: 4.4018	LR: 1.000000
Training Epoch: 2 [3072/50000]	Loss: 4.3926	LR: 1.000000
Training Epoch: 2 [3328/50000]	Loss: 4.2945	LR: 1.000000
Training Epoch: 2 [3584/50000]	Loss: 4.3125	LR: 1.000000
Training Epoch: 2 [3840/50000]	Loss: 4.3784	LR: 1.000000
Training Epoch: 2 [4096/50000]	Loss: 4.4037	LR: 1.000000
Training Epoch: 2 [4352/50000]	Loss: 4.3708	LR: 1.000000
Training Epoch: 2 [4608/50000]	Loss: 4.4255	LR: 1.000000
Training Epoch: 2 [4864/50000]	Loss: 4.3962	LR: 1.000000
Training Epoch: 2 [5120/50000]	Loss: 4.3596	LR: 1.000000
Training Epoch: 2 [5376/50000]	Loss: 4.3565	LR: 1.000000
Training Epoch: 2 [5632/50000]	Loss: 4.3678	LR: 1.000000
Training Epoch: 2 [5888/50000]	Loss: 4.4834	LR: 1.000000
Training Epoch: 2 [6144/50000]	Loss: 4.4011	LR: 1.000000
Training Epoch: 2 [6400/50000]	Loss: 4.3980	LR: 1.000000
Training Epoch: 2 [6656/50000]	Loss: 4.3367	LR: 1.000000
Training Epoch: 2 [6912/50000]	Loss: 4.3530	LR: 1.000000
Training Epoch: 2 [7168/50000]	Loss: 4.3677	LR: 1.000000
Training Epoch: 2 [7424/50000]	Loss: 4.3892	LR: 1.000000
Training Epoch: 2 [7680/50000]	Loss: 4.3469	LR: 1.000000
Training Epoch: 2 [7936/50000]	Loss: 4.2457	LR: 1.000000
Training Epoch: 2 [8192/50000]	Loss: 4.3907	LR: 1.000000
Training Epoch: 2 [8448/50000]	Loss: 4.3108	LR: 1.000000
Training Epoch: 2 [8704/50000]	Loss: 4.2734	LR: 1.000000
Training Epoch: 2 [8960/50000]	Loss: 4.3602	LR: 1.000000
Training Epoch: 2 [9216/50000]	Loss: 4.3086	LR: 1.000000
Training Epoch: 2 [9472/50000]	Loss: 4.3518	LR: 1.000000
Training Epoch: 2 [9728/50000]	Loss: 4.2448	LR: 1.000000
Training Epoch: 2 [9984/50000]	Loss: 4.3566	LR: 1.000000
Training Epoch: 2 [10240/50000]	Loss: 4.3224	LR: 1.000000
Training Epoch: 2 [10496/50000]	Loss: 4.2499	LR: 1.000000
Training Epoch: 2 [10752/50000]	Loss: 4.3526	LR: 1.000000
Training Epoch: 2 [11008/50000]	Loss: 4.3291	LR: 1.000000
Training Epoch: 2 [11264/50000]	Loss: 4.2303	LR: 1.000000
Training Epoch: 2 [11520/50000]	Loss: 4.3136	LR: 1.000000
Training Epoch: 2 [11776/50000]	Loss: 4.4379	LR: 1.000000
Training Epoch: 2 [12032/50000]	Loss: 4.2760	LR: 1.000000
Training Epoch: 2 [12288/50000]	Loss: 4.2981	LR: 1.000000
Training Epoch: 2 [12544/50000]	Loss: 4.3772	LR: 1.000000
Training Epoch: 2 [12800/50000]	Loss: 4.3459	LR: 1.000000
Training Epoch: 2 [13056/50000]	Loss: 4.3672	LR: 1.000000
Training Epoch: 2 [13312/50000]	Loss: 4.3542	LR: 1.000000
Training Epoch: 2 [13568/50000]	Loss: 4.3672	LR: 1.000000
Training Epoch: 2 [13824/50000]	Loss: 4.3724	LR: 1.000000
Training Epoch: 2 [14080/50000]	Loss: 4.3212	LR: 1.000000
Training Epoch: 2 [14336/50000]	Loss: 4.4830	LR: 1.000000
Training Epoch: 2 [14592/50000]	Loss: 4.3304	LR: 1.000000
Training Epoch: 2 [14848/50000]	Loss: 4.3714	LR: 1.000000
Training Epoch: 2 [15104/50000]	Loss: 4.3174	LR: 1.000000
Training Epoch: 2 [15360/50000]	Loss: 4.3364	LR: 1.000000
Training Epoch: 2 [15616/50000]	Loss: 4.3806	LR: 1.000000
Training Epoch: 2 [15872/50000]	Loss: 4.3263	LR: 1.000000
Training Epoch: 2 [16128/50000]	Loss: 4.3454	LR: 1.000000
Training Epoch: 2 [16384/50000]	Loss: 4.2897	LR: 1.000000
Training Epoch: 2 [16640/50000]	Loss: 4.3721	LR: 1.000000
Training Epoch: 2 [16896/50000]	Loss: 4.4202	LR: 1.000000
Training Epoch: 2 [17152/50000]	Loss: 4.3992	LR: 1.000000
Training Epoch: 2 [17408/50000]	Loss: 4.4043	LR: 1.000000
Training Epoch: 2 [17664/50000]	Loss: 4.4002	LR: 1.000000
Training Epoch: 2 [17920/50000]	Loss: 4.3917	LR: 1.000000
Training Epoch: 2 [18176/50000]	Loss: 4.3824	LR: 1.000000
Training Epoch: 2 [18432/50000]	Loss: 4.2283	LR: 1.000000
Training Epoch: 2 [18688/50000]	Loss: 4.4062	LR: 1.000000
Training Epoch: 2 [18944/50000]	Loss: 4.3371	LR: 1.000000
Training Epoch: 2 [19200/50000]	Loss: 4.3367	LR: 1.000000
Training Epoch: 2 [19456/50000]	Loss: 4.2275	LR: 1.000000
Training Epoch: 2 [19712/50000]	Loss: 4.4335	LR: 1.000000
Training Epoch: 2 [19968/50000]	Loss: 4.2901	LR: 1.000000
Training Epoch: 2 [20224/50000]	Loss: 4.2921	LR: 1.000000
Training Epoch: 2 [20480/50000]	Loss: 4.3429	LR: 1.000000
Training Epoch: 2 [20736/50000]	Loss: 4.3614	LR: 1.000000
Training Epoch: 2 [20992/50000]	Loss: 4.2556	LR: 1.000000
Training Epoch: 2 [21248/50000]	Loss: 4.3424	LR: 1.000000
Training Epoch: 2 [21504/50000]	Loss: 4.3226	LR: 1.000000
Training Epoch: 2 [21760/50000]	Loss: 4.3154	LR: 1.000000
Training Epoch: 2 [22016/50000]	Loss: 4.3080	LR: 1.000000
Training Epoch: 2 [22272/50000]	Loss: 4.4114	LR: 1.000000
Training Epoch: 2 [22528/50000]	Loss: 4.3282	LR: 1.000000
Training Epoch: 2 [22784/50000]	Loss: 4.3851	LR: 1.000000
Training Epoch: 2 [23040/50000]	Loss: 4.3902	LR: 1.000000
Training Epoch: 2 [23296/50000]	Loss: 4.3196	LR: 1.000000
Training Epoch: 2 [23552/50000]	Loss: 4.3062	LR: 1.000000
Training Epoch: 2 [23808/50000]	Loss: 4.3831	LR: 1.000000
Training Epoch: 2 [24064/50000]	Loss: 4.4219	LR: 1.000000
Training Epoch: 2 [24320/50000]	Loss: 4.3239	LR: 1.000000
Training Epoch: 2 [24576/50000]	Loss: 4.3208	LR: 1.000000
Training Epoch: 2 [24832/50000]	Loss: 4.3675	LR: 1.000000
Training Epoch: 2 [25088/50000]	Loss: 4.4003	LR: 1.000000
Training Epoch: 2 [25344/50000]	Loss: 4.2772	LR: 1.000000
Training Epoch: 2 [25600/50000]	Loss: 4.3099	LR: 1.000000
Training Epoch: 2 [25856/50000]	Loss: 4.3523	LR: 1.000000
Training Epoch: 2 [26112/50000]	Loss: 4.3521	LR: 1.000000
Training Epoch: 2 [26368/50000]	Loss: 4.3327	LR: 1.000000
Training Epoch: 2 [26624/50000]	Loss: 4.2378	LR: 1.000000
Training Epoch: 2 [26880/50000]	Loss: 4.3538	LR: 1.000000
Training Epoch: 2 [27136/50000]	Loss: 4.2683	LR: 1.000000
Training Epoch: 2 [27392/50000]	Loss: 4.3084	LR: 1.000000
Training Epoch: 2 [27648/50000]	Loss: 4.3406	LR: 1.000000
Training Epoch: 2 [27904/50000]	Loss: 4.3679	LR: 1.000000
Training Epoch: 2 [28160/50000]	Loss: 4.3583	LR: 1.000000
Training Epoch: 2 [28416/50000]	Loss: 4.3231	LR: 1.000000
Training Epoch: 2 [28672/50000]	Loss: 4.2535	LR: 1.000000
Training Epoch: 2 [28928/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 2 [29184/50000]	Loss: 4.3738	LR: 1.000000
Training Epoch: 2 [29440/50000]	Loss: 4.3990	LR: 1.000000
Training Epoch: 2 [29696/50000]	Loss: 4.3464	LR: 1.000000
Training Epoch: 2 [29952/50000]	Loss: 4.3292	LR: 1.000000
Training Epoch: 2 [30208/50000]	Loss: 4.3130	LR: 1.000000
Training Epoch: 2 [30464/50000]	Loss: 4.3904	LR: 1.000000
Training Epoch: 2 [30720/50000]	Loss: 4.3365	LR: 1.000000
Training Epoch: 2 [30976/50000]	Loss: 4.3626	LR: 1.000000
Training Epoch: 2 [31232/50000]	Loss: 4.3949	LR: 1.000000
Training Epoch: 2 [31488/50000]	Loss: 4.3626	LR: 1.000000
Training Epoch: 2 [31744/50000]	Loss: 4.5035	LR: 1.000000
Training Epoch: 2 [32000/50000]	Loss: 4.3271	LR: 1.000000
Training Epoch: 2 [32256/50000]	Loss: 4.3067	LR: 1.000000
Training Epoch: 2 [32512/50000]	Loss: 4.3358	LR: 1.000000
Training Epoch: 2 [32768/50000]	Loss: 4.4018	LR: 1.000000
Training Epoch: 2 [33024/50000]	Loss: 4.3332	LR: 1.000000
Training Epoch: 2 [33280/50000]	Loss: 4.3500	LR: 1.000000
Training Epoch: 2 [33536/50000]	Loss: 4.3965	LR: 1.000000
Training Epoch: 2 [33792/50000]	Loss: 4.3604	LR: 1.000000
Training Epoch: 2 [34048/50000]	Loss: 4.2572	LR: 1.000000
Training Epoch: 2 [34304/50000]	Loss: 4.3617	LR: 1.000000
Training Epoch: 2 [34560/50000]	Loss: 4.3137	LR: 1.000000
Training Epoch: 2 [34816/50000]	Loss: 4.3594	LR: 1.000000
Training Epoch: 2 [35072/50000]	Loss: 4.2466	LR: 1.000000
Training Epoch: 2 [35328/50000]	Loss: 4.2583	LR: 1.000000
Training Epoch: 2 [35584/50000]	Loss: 4.2805	LR: 1.000000
Training Epoch: 2 [35840/50000]	Loss: 4.2509	LR: 1.000000
Training Epoch: 2 [36096/50000]	Loss: 4.2585	LR: 1.000000
Training Epoch: 2 [36352/50000]	Loss: 4.2501	LR: 1.000000
Training Epoch: 2 [36608/50000]	Loss: 4.2055	LR: 1.000000
Training Epoch: 2 [36864/50000]	Loss: 4.3124	LR: 1.000000
Training Epoch: 2 [37120/50000]	Loss: 4.3459	LR: 1.000000
Training Epoch: 2 [37376/50000]	Loss: 4.3239	LR: 1.000000
Training Epoch: 2 [37632/50000]	Loss: 4.2680	LR: 1.000000
Training Epoch: 2 [37888/50000]	Loss: 4.2447	LR: 1.000000
Training Epoch: 2 [38144/50000]	Loss: 4.4000	LR: 1.000000
Training Epoch: 2 [38400/50000]	Loss: 4.2690	LR: 1.000000
Training Epoch: 2 [38656/50000]	Loss: 4.3395	LR: 1.000000
Training Epoch: 2 [38912/50000]	Loss: 4.2520	LR: 1.000000
Training Epoch: 2 [39168/50000]	Loss: 4.2620	LR: 1.000000
Training Epoch: 2 [39424/50000]	Loss: 4.3400	LR: 1.000000
Training Epoch: 2 [39680/50000]	Loss: 4.3388	LR: 1.000000
Training Epoch: 2 [39936/50000]	Loss: 4.3006	LR: 1.000000
Training Epoch: 2 [40192/50000]	Loss: 4.4529	LR: 1.000000
Training Epoch: 2 [40448/50000]	Loss: 4.2943	LR: 1.000000
Training Epoch: 2 [40704/50000]	Loss: 4.3751	LR: 1.000000
Training Epoch: 2 [40960/50000]	Loss: 4.2984	LR: 1.000000
Training Epoch: 2 [41216/50000]	Loss: 4.3287	LR: 1.000000
Training Epoch: 2 [41472/50000]	Loss: 4.3692	LR: 1.000000
Training Epoch: 2 [41728/50000]	Loss: 4.3806	LR: 1.000000
Training Epoch: 2 [41984/50000]	Loss: 4.1920	LR: 1.000000
Training Epoch: 2 [42240/50000]	Loss: 4.2493	LR: 1.000000
Training Epoch: 2 [42496/50000]	Loss: 4.2281	LR: 1.000000
Training Epoch: 2 [42752/50000]	Loss: 4.2639	LR: 1.000000
Training Epoch: 2 [43008/50000]	Loss: 4.3622	LR: 1.000000
Training Epoch: 2 [43264/50000]	Loss: 4.3678	LR: 1.000000
Training Epoch: 2 [43520/50000]	Loss: 4.3098	LR: 1.000000
Training Epoch: 2 [43776/50000]	Loss: 4.2696	LR: 1.000000
Training Epoch: 2 [44032/50000]	Loss: 4.2821	LR: 1.000000
Training Epoch: 2 [44288/50000]	Loss: 4.2870	LR: 1.000000
Training Epoch: 2 [44544/50000]	Loss: 4.4203	LR: 1.000000
Training Epoch: 2 [44800/50000]	Loss: 4.3224	LR: 1.000000
Training Epoch: 2 [45056/50000]	Loss: 4.3492	LR: 1.000000
Training Epoch: 2 [45312/50000]	Loss: 4.3177	LR: 1.000000
Training Epoch: 2 [45568/50000]	Loss: 4.3163	LR: 1.000000
Training Epoch: 2 [45824/50000]	Loss: 4.3331	LR: 1.000000
Training Epoch: 2 [46080/50000]	Loss: 4.2689	LR: 1.000000
Training Epoch: 2 [46336/50000]	Loss: 4.3449	LR: 1.000000
Training Epoch: 2 [46592/50000]	Loss: 4.4328	LR: 1.000000
Training Epoch: 2 [46848/50000]	Loss: 4.3237	LR: 1.000000
Training Epoch: 2 [47104/50000]	Loss: 4.2749	LR: 1.000000
Training Epoch: 2 [47360/50000]	Loss: 4.3424	LR: 1.000000
Training Epoch: 2 [47616/50000]	Loss: 4.3629	LR: 1.000000
Training Epoch: 2 [47872/50000]	Loss: 4.3283	LR: 1.000000
Training Epoch: 2 [48128/50000]	Loss: 4.3023	LR: 1.000000
Training Epoch: 2 [48384/50000]	Loss: 4.3558	LR: 1.000000
Training Epoch: 2 [48640/50000]	Loss: 4.3537	LR: 1.000000
Training Epoch: 2 [48896/50000]	Loss: 4.3481	LR: 1.000000
Training Epoch: 2 [49152/50000]	Loss: 4.3654	LR: 1.000000
Training Epoch: 2 [49408/50000]	Loss: 4.3345	LR: 1.000000
Training Epoch: 2 [49664/50000]	Loss: 4.2677	LR: 1.000000
Training Epoch: 2 [49920/50000]	Loss: 4.3878	LR: 1.000000
Training Epoch: 2 [50000/50000]	Loss: 4.4776	LR: 1.000000
epoch 2 training time consumed: 22.18s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |    3001 GB |    3001 GB |
|       from large pool |  400448 KB |    1770 MB |    2998 GB |    2998 GB |
|       from small pool |    3549 KB |       9 MB |       2 GB |       2 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |    3001 GB |    3001 GB |
|       from large pool |  400448 KB |    1770 MB |    2998 GB |    2998 GB |
|       from small pool |    3549 KB |       9 MB |       2 GB |       2 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  475717 KB |    1848 GB |    1847 GB |
|       from large pool |  244672 KB |  473024 KB |    1845 GB |    1844 GB |
|       from small pool |    4642 KB |    4843 KB |       3 GB |       3 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  128503    |  128281    |
|       from large pool |      36    |      77    |   61992    |   61956    |
|       from small pool |     186    |     224    |   66511    |   66325    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  128503    |  128281    |
|       from large pool |      36    |      77    |   61992    |   61956    |
|       from small pool |     186    |     224    |   66511    |   66325    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      31    |   64878    |   64856    |
|       from large pool |      10    |      11    |   25678    |   25668    |
|       from small pool |      12    |      21    |   39200    |   39188    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 2, Average loss: 0.0173, Accuracy: 0.0354, Time consumed:1.46s

Training Epoch: 3 [256/50000]	Loss: 4.3265	LR: 1.000000
Training Epoch: 3 [512/50000]	Loss: 4.3597	LR: 1.000000
Training Epoch: 3 [768/50000]	Loss: 4.3593	LR: 1.000000
Training Epoch: 3 [1024/50000]	Loss: 4.3621	LR: 1.000000
Training Epoch: 3 [1280/50000]	Loss: 4.4057	LR: 1.000000
Training Epoch: 3 [1536/50000]	Loss: 4.4445	LR: 1.000000
Training Epoch: 3 [1792/50000]	Loss: 4.2888	LR: 1.000000
Training Epoch: 3 [2048/50000]	Loss: 4.3320	LR: 1.000000
Training Epoch: 3 [2304/50000]	Loss: 4.2341	LR: 1.000000
Training Epoch: 3 [2560/50000]	Loss: 4.2847	LR: 1.000000
Training Epoch: 3 [2816/50000]	Loss: 4.2379	LR: 1.000000
Training Epoch: 3 [3072/50000]	Loss: 4.2510	LR: 1.000000
Training Epoch: 3 [3328/50000]	Loss: 4.2617	LR: 1.000000
Training Epoch: 3 [3584/50000]	Loss: 4.4151	LR: 1.000000
Training Epoch: 3 [3840/50000]	Loss: 4.2460	LR: 1.000000
Training Epoch: 3 [4096/50000]	Loss: 4.3449	LR: 1.000000
Training Epoch: 3 [4352/50000]	Loss: 4.3768	LR: 1.000000
Training Epoch: 3 [4608/50000]	Loss: 4.2998	LR: 1.000000
Training Epoch: 3 [4864/50000]	Loss: 4.3207	LR: 1.000000
Training Epoch: 3 [5120/50000]	Loss: 4.3294	LR: 1.000000
Training Epoch: 3 [5376/50000]	Loss: 4.3068	LR: 1.000000
Training Epoch: 3 [5632/50000]	Loss: 4.2984	LR: 1.000000
Training Epoch: 3 [5888/50000]	Loss: 4.2491	LR: 1.000000
Training Epoch: 3 [6144/50000]	Loss: 4.2933	LR: 1.000000
Training Epoch: 3 [6400/50000]	Loss: 4.3792	LR: 1.000000
Training Epoch: 3 [6656/50000]	Loss: 4.2851	LR: 1.000000
Training Epoch: 3 [6912/50000]	Loss: 4.3509	LR: 1.000000
Training Epoch: 3 [7168/50000]	Loss: 4.2954	LR: 1.000000
Training Epoch: 3 [7424/50000]	Loss: 4.3352	LR: 1.000000
Training Epoch: 3 [7680/50000]	Loss: 4.2990	LR: 1.000000
Training Epoch: 3 [7936/50000]	Loss: 4.3570	LR: 1.000000
Training Epoch: 3 [8192/50000]	Loss: 4.2215	LR: 1.000000
Training Epoch: 3 [8448/50000]	Loss: 4.3326	LR: 1.000000
Training Epoch: 3 [8704/50000]	Loss: 4.2758	LR: 1.000000
Training Epoch: 3 [8960/50000]	Loss: 4.2946	LR: 1.000000
Training Epoch: 3 [9216/50000]	Loss: 4.2839	LR: 1.000000
Training Epoch: 3 [9472/50000]	Loss: 4.3006	LR: 1.000000
Training Epoch: 3 [9728/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 3 [9984/50000]	Loss: 4.2410	LR: 1.000000
Training Epoch: 3 [10240/50000]	Loss: 4.2819	LR: 1.000000
Training Epoch: 3 [10496/50000]	Loss: 4.4083	LR: 1.000000
Training Epoch: 3 [10752/50000]	Loss: 4.3949	LR: 1.000000
Training Epoch: 3 [11008/50000]	Loss: 4.3457	LR: 1.000000
Training Epoch: 3 [11264/50000]	Loss: 4.2370	LR: 1.000000
Training Epoch: 3 [11520/50000]	Loss: 4.3072	LR: 1.000000
Training Epoch: 3 [11776/50000]	Loss: 4.2795	LR: 1.000000
Training Epoch: 3 [12032/50000]	Loss: 4.2947	LR: 1.000000
Training Epoch: 3 [12288/50000]	Loss: 4.4969	LR: 1.000000
Training Epoch: 3 [12544/50000]	Loss: 4.3202	LR: 1.000000
Training Epoch: 3 [12800/50000]	Loss: 4.2536	LR: 1.000000
Training Epoch: 3 [13056/50000]	Loss: 4.2880	LR: 1.000000
Training Epoch: 3 [13312/50000]	Loss: 4.3466	LR: 1.000000
Training Epoch: 3 [13568/50000]	Loss: 4.2999	LR: 1.000000
Training Epoch: 3 [13824/50000]	Loss: 4.1928	LR: 1.000000
Training Epoch: 3 [14080/50000]	Loss: 4.3187	LR: 1.000000
Training Epoch: 3 [14336/50000]	Loss: 4.3450	LR: 1.000000
Training Epoch: 3 [14592/50000]	Loss: 4.2796	LR: 1.000000
Training Epoch: 3 [14848/50000]	Loss: 4.3316	LR: 1.000000
Training Epoch: 3 [15104/50000]	Loss: 4.3303	LR: 1.000000
Training Epoch: 3 [15360/50000]	Loss: 4.3185	LR: 1.000000
Training Epoch: 3 [15616/50000]	Loss: 4.2177	LR: 1.000000
Training Epoch: 3 [15872/50000]	Loss: 4.3587	LR: 1.000000
Training Epoch: 3 [16128/50000]	Loss: 4.3285	LR: 1.000000
Training Epoch: 3 [16384/50000]	Loss: 4.4097	LR: 1.000000
Training Epoch: 3 [16640/50000]	Loss: 4.3735	LR: 1.000000
Training Epoch: 3 [16896/50000]	Loss: 4.3382	LR: 1.000000
Training Epoch: 3 [17152/50000]	Loss: 4.3441	LR: 1.000000
Training Epoch: 3 [17408/50000]	Loss: 4.2939	LR: 1.000000
Training Epoch: 3 [17664/50000]	Loss: 4.2932	LR: 1.000000
Training Epoch: 3 [17920/50000]	Loss: 4.3565	LR: 1.000000
Training Epoch: 3 [18176/50000]	Loss: 4.4545	LR: 1.000000
Training Epoch: 3 [18432/50000]	Loss: 4.3749	LR: 1.000000
Training Epoch: 3 [18688/50000]	Loss: 4.3453	LR: 1.000000
Training Epoch: 3 [18944/50000]	Loss: 4.3924	LR: 1.000000
Training Epoch: 3 [19200/50000]	Loss: 4.3711	LR: 1.000000
Training Epoch: 3 [19456/50000]	Loss: 4.3334	LR: 1.000000
Training Epoch: 3 [19712/50000]	Loss: 4.3203	LR: 1.000000
Training Epoch: 3 [19968/50000]	Loss: 4.2250	LR: 1.000000
Training Epoch: 3 [20224/50000]	Loss: 4.3580	LR: 1.000000
Training Epoch: 3 [20480/50000]	Loss: 4.2752	LR: 1.000000
Training Epoch: 3 [20736/50000]	Loss: 4.3435	LR: 1.000000
Training Epoch: 3 [20992/50000]	Loss: 4.3112	LR: 1.000000
Training Epoch: 3 [21248/50000]	Loss: 4.3445	LR: 1.000000
Training Epoch: 3 [21504/50000]	Loss: 4.4066	LR: 1.000000
Training Epoch: 3 [21760/50000]	Loss: 4.3221	LR: 1.000000
Training Epoch: 3 [22016/50000]	Loss: 4.3051	LR: 1.000000
Training Epoch: 3 [22272/50000]	Loss: 4.2629	LR: 1.000000
Training Epoch: 3 [22528/50000]	Loss: 4.2266	LR: 1.000000
Training Epoch: 3 [22784/50000]	Loss: 4.3821	LR: 1.000000
Training Epoch: 3 [23040/50000]	Loss: 4.1702	LR: 1.000000
Training Epoch: 3 [23296/50000]	Loss: 4.4047	LR: 1.000000
Training Epoch: 3 [23552/50000]	Loss: 4.3287	LR: 1.000000
Training Epoch: 3 [23808/50000]	Loss: 4.3633	LR: 1.000000
Training Epoch: 3 [24064/50000]	Loss: 4.3479	LR: 1.000000
Training Epoch: 3 [24320/50000]	Loss: 4.3317	LR: 1.000000
Training Epoch: 3 [24576/50000]	Loss: 4.3587	LR: 1.000000
Training Epoch: 3 [24832/50000]	Loss: 4.3649	LR: 1.000000
Training Epoch: 3 [25088/50000]	Loss: 4.3144	LR: 1.000000
Training Epoch: 3 [25344/50000]	Loss: 4.2842	LR: 1.000000
Training Epoch: 3 [25600/50000]	Loss: 4.3050	LR: 1.000000
Training Epoch: 3 [25856/50000]	Loss: 4.2652	LR: 1.000000
Training Epoch: 3 [26112/50000]	Loss: 4.2136	LR: 1.000000
Training Epoch: 3 [26368/50000]	Loss: 4.3726	LR: 1.000000
Training Epoch: 3 [26624/50000]	Loss: 4.2865	LR: 1.000000
Training Epoch: 3 [26880/50000]	Loss: 4.3711	LR: 1.000000
Training Epoch: 3 [27136/50000]	Loss: 4.2421	LR: 1.000000
Training Epoch: 3 [27392/50000]	Loss: 4.2719	LR: 1.000000
Training Epoch: 3 [27648/50000]	Loss: 4.3291	LR: 1.000000
Training Epoch: 3 [27904/50000]	Loss: 4.3863	LR: 1.000000
Training Epoch: 3 [28160/50000]	Loss: 4.3557	LR: 1.000000
Training Epoch: 3 [28416/50000]	Loss: 4.2387	LR: 1.000000
Training Epoch: 3 [28672/50000]	Loss: 4.3269	LR: 1.000000
Training Epoch: 3 [28928/50000]	Loss: 4.3035	LR: 1.000000
Training Epoch: 3 [29184/50000]	Loss: 4.3064	LR: 1.000000
Training Epoch: 3 [29440/50000]	Loss: 4.2621	LR: 1.000000
Training Epoch: 3 [29696/50000]	Loss: 4.3519	LR: 1.000000
Training Epoch: 3 [29952/50000]	Loss: 4.2654	LR: 1.000000
Training Epoch: 3 [30208/50000]	Loss: 4.2523	LR: 1.000000
Training Epoch: 3 [30464/50000]	Loss: 4.2756	LR: 1.000000
Training Epoch: 3 [30720/50000]	Loss: 4.2621	LR: 1.000000
Training Epoch: 3 [30976/50000]	Loss: 4.3921	LR: 1.000000
Training Epoch: 3 [31232/50000]	Loss: 4.3052	LR: 1.000000
Training Epoch: 3 [31488/50000]	Loss: 4.3346	LR: 1.000000
Training Epoch: 3 [31744/50000]	Loss: 4.2871	LR: 1.000000
Training Epoch: 3 [32000/50000]	Loss: 4.2531	LR: 1.000000
Training Epoch: 3 [32256/50000]	Loss: 4.2810	LR: 1.000000
Training Epoch: 3 [32512/50000]	Loss: 4.3051	LR: 1.000000
Training Epoch: 3 [32768/50000]	Loss: 4.2568	LR: 1.000000
Training Epoch: 3 [33024/50000]	Loss: 4.2823	LR: 1.000000
Training Epoch: 3 [33280/50000]	Loss: 4.4491	LR: 1.000000
Training Epoch: 3 [33536/50000]	Loss: 4.4114	LR: 1.000000
Training Epoch: 3 [33792/50000]	Loss: 4.2632	LR: 1.000000
Training Epoch: 3 [34048/50000]	Loss: 4.2898	LR: 1.000000
Training Epoch: 3 [34304/50000]	Loss: 4.3295	LR: 1.000000
Training Epoch: 3 [34560/50000]	Loss: 4.2700	LR: 1.000000
Training Epoch: 3 [34816/50000]	Loss: 4.3752	LR: 1.000000
Training Epoch: 3 [35072/50000]	Loss: 4.3799	LR: 1.000000
Training Epoch: 3 [35328/50000]	Loss: 4.3431	LR: 1.000000
Training Epoch: 3 [35584/50000]	Loss: 4.2815	LR: 1.000000
Training Epoch: 3 [35840/50000]	Loss: 4.3524	LR: 1.000000
Training Epoch: 3 [36096/50000]	Loss: 4.3003	LR: 1.000000
Training Epoch: 3 [36352/50000]	Loss: 4.2454	LR: 1.000000
Training Epoch: 3 [36608/50000]	Loss: 4.3134	LR: 1.000000
Training Epoch: 3 [36864/50000]	Loss: 4.2212	LR: 1.000000
Training Epoch: 3 [37120/50000]	Loss: 4.3057	LR: 1.000000
Training Epoch: 3 [37376/50000]	Loss: 4.3683	LR: 1.000000
Training Epoch: 3 [37632/50000]	Loss: 4.3212	LR: 1.000000
Training Epoch: 3 [37888/50000]	Loss: 4.2909	LR: 1.000000
Training Epoch: 3 [38144/50000]	Loss: 4.3232	LR: 1.000000
Training Epoch: 3 [38400/50000]	Loss: 4.3702	LR: 1.000000
Training Epoch: 3 [38656/50000]	Loss: 4.3347	LR: 1.000000
Training Epoch: 3 [38912/50000]	Loss: 4.2459	LR: 1.000000
Training Epoch: 3 [39168/50000]	Loss: 4.3260	LR: 1.000000
Training Epoch: 3 [39424/50000]	Loss: 4.4023	LR: 1.000000
Training Epoch: 3 [39680/50000]	Loss: 4.3903	LR: 1.000000
Training Epoch: 3 [39936/50000]	Loss: 4.2665	LR: 1.000000
Training Epoch: 3 [40192/50000]	Loss: 4.3063	LR: 1.000000
Training Epoch: 3 [40448/50000]	Loss: 4.2970	LR: 1.000000
Training Epoch: 3 [40704/50000]	Loss: 4.3058	LR: 1.000000
Training Epoch: 3 [40960/50000]	Loss: 4.4283	LR: 1.000000
Training Epoch: 3 [41216/50000]	Loss: 4.3763	LR: 1.000000
Training Epoch: 3 [41472/50000]	Loss: 4.4526	LR: 1.000000
Training Epoch: 3 [41728/50000]	Loss: 4.4238	LR: 1.000000
Training Epoch: 3 [41984/50000]	Loss: 4.3464	LR: 1.000000
Training Epoch: 3 [42240/50000]	Loss: 4.3765	LR: 1.000000
Training Epoch: 3 [42496/50000]	Loss: 4.3667	LR: 1.000000
Training Epoch: 3 [42752/50000]	Loss: 4.3332	LR: 1.000000
Training Epoch: 3 [43008/50000]	Loss: 4.3007	LR: 1.000000
Training Epoch: 3 [43264/50000]	Loss: 4.2822	LR: 1.000000
Training Epoch: 3 [43520/50000]	Loss: 4.2377	LR: 1.000000
Training Epoch: 3 [43776/50000]	Loss: 4.3266	LR: 1.000000
Training Epoch: 3 [44032/50000]	Loss: 4.3081	LR: 1.000000
Training Epoch: 3 [44288/50000]	Loss: 4.3156	LR: 1.000000
Training Epoch: 3 [44544/50000]	Loss: 4.2838	LR: 1.000000
Training Epoch: 3 [44800/50000]	Loss: 4.1961	LR: 1.000000
Training Epoch: 3 [45056/50000]	Loss: 4.2827	LR: 1.000000
Training Epoch: 3 [45312/50000]	Loss: 4.3288	LR: 1.000000
Training Epoch: 3 [45568/50000]	Loss: 4.3609	LR: 1.000000
Training Epoch: 3 [45824/50000]	Loss: 4.3540	LR: 1.000000
Training Epoch: 3 [46080/50000]	Loss: 4.3207	LR: 1.000000
Training Epoch: 3 [46336/50000]	Loss: 4.3598	LR: 1.000000
Training Epoch: 3 [46592/50000]	Loss: 4.3930	LR: 1.000000
Training Epoch: 3 [46848/50000]	Loss: 4.2957	LR: 1.000000
Training Epoch: 3 [47104/50000]	Loss: 4.4042	LR: 1.000000
Training Epoch: 3 [47360/50000]	Loss: 4.4095	LR: 1.000000
Training Epoch: 3 [47616/50000]	Loss: 4.4043	LR: 1.000000
Training Epoch: 3 [47872/50000]	Loss: 4.2753	LR: 1.000000
Training Epoch: 3 [48128/50000]	Loss: 4.3144	LR: 1.000000
Training Epoch: 3 [48384/50000]	Loss: 4.3361	LR: 1.000000
Training Epoch: 3 [48640/50000]	Loss: 4.4326	LR: 1.000000
Training Epoch: 3 [48896/50000]	Loss: 4.3045	LR: 1.000000
Training Epoch: 3 [49152/50000]	Loss: 4.3657	LR: 1.000000
Training Epoch: 3 [49408/50000]	Loss: 4.3728	LR: 1.000000
Training Epoch: 3 [49664/50000]	Loss: 4.3497	LR: 1.000000
Training Epoch: 3 [49920/50000]	Loss: 4.3871	LR: 1.000000
Training Epoch: 3 [50000/50000]	Loss: 4.3950	LR: 1.000000
epoch 3 training time consumed: 22.12s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |    4501 GB |    4501 GB |
|       from large pool |  400448 KB |    1770 MB |    4497 GB |    4497 GB |
|       from small pool |    3549 KB |       9 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |    4501 GB |    4501 GB |
|       from large pool |  400448 KB |    1770 MB |    4497 GB |    4497 GB |
|       from small pool |    3549 KB |       9 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |    2771 GB |    2770 GB |
|       from large pool |  244672 KB |  473024 KB |    2766 GB |    2766 GB |
|       from small pool |    2594 KB |    4843 KB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  192527    |  192305    |
|       from large pool |      36    |      77    |   92951    |   92915    |
|       from small pool |     186    |     224    |   99576    |   99390    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  192527    |  192305    |
|       from large pool |      36    |      77    |   92951    |   92915    |
|       from small pool |     186    |     224    |   99576    |   99390    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      25    |      31    |   97781    |   97756    |
|       from large pool |      10    |      11    |   38501    |   38491    |
|       from small pool |      15    |      21    |   59280    |   59265    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 3, Average loss: 0.0171, Accuracy: 0.0338, Time consumed:1.45s

Training Epoch: 4 [256/50000]	Loss: 4.3441	LR: 1.000000
Training Epoch: 4 [512/50000]	Loss: 4.3104	LR: 1.000000
Training Epoch: 4 [768/50000]	Loss: 4.2641	LR: 1.000000
Training Epoch: 4 [1024/50000]	Loss: 4.3942	LR: 1.000000
Training Epoch: 4 [1280/50000]	Loss: 4.3276	LR: 1.000000
Training Epoch: 4 [1536/50000]	Loss: 4.4230	LR: 1.000000
Training Epoch: 4 [1792/50000]	Loss: 4.3712	LR: 1.000000
Training Epoch: 4 [2048/50000]	Loss: 4.3665	LR: 1.000000
Training Epoch: 4 [2304/50000]	Loss: 4.3271	LR: 1.000000
Training Epoch: 4 [2560/50000]	Loss: 4.2912	LR: 1.000000
Training Epoch: 4 [2816/50000]	Loss: 4.3106	LR: 1.000000
Training Epoch: 4 [3072/50000]	Loss: 4.2685	LR: 1.000000
Training Epoch: 4 [3328/50000]	Loss: 4.3439	LR: 1.000000
Training Epoch: 4 [3584/50000]	Loss: 4.2535	LR: 1.000000
Training Epoch: 4 [3840/50000]	Loss: 4.3756	LR: 1.000000
Training Epoch: 4 [4096/50000]	Loss: 4.3752	LR: 1.000000
Training Epoch: 4 [4352/50000]	Loss: 4.4073	LR: 1.000000
Training Epoch: 4 [4608/50000]	Loss: 4.3914	LR: 1.000000
Training Epoch: 4 [4864/50000]	Loss: 4.3804	LR: 1.000000
Training Epoch: 4 [5120/50000]	Loss: 4.3489	LR: 1.000000
Training Epoch: 4 [5376/50000]	Loss: 4.4842	LR: 1.000000
Training Epoch: 4 [5632/50000]	Loss: 4.3507	LR: 1.000000
Training Epoch: 4 [5888/50000]	Loss: 4.3728	LR: 1.000000
Training Epoch: 4 [6144/50000]	Loss: 4.4201	LR: 1.000000
Training Epoch: 4 [6400/50000]	Loss: 4.3012	LR: 1.000000
Training Epoch: 4 [6656/50000]	Loss: 4.2889	LR: 1.000000
Training Epoch: 4 [6912/50000]	Loss: 4.3762	LR: 1.000000
Training Epoch: 4 [7168/50000]	Loss: 4.3635	LR: 1.000000
Training Epoch: 4 [7424/50000]	Loss: 4.4050	LR: 1.000000
Training Epoch: 4 [7680/50000]	Loss: 4.4086	LR: 1.000000
Training Epoch: 4 [7936/50000]	Loss: 4.3752	LR: 1.000000
Training Epoch: 4 [8192/50000]	Loss: 4.4138	LR: 1.000000
Training Epoch: 4 [8448/50000]	Loss: 4.2846	LR: 1.000000
Training Epoch: 4 [8704/50000]	Loss: 4.4055	LR: 1.000000
Training Epoch: 4 [8960/50000]	Loss: 4.3709	LR: 1.000000
Training Epoch: 4 [9216/50000]	Loss: 4.3913	LR: 1.000000
Training Epoch: 4 [9472/50000]	Loss: 4.3344	LR: 1.000000
Training Epoch: 4 [9728/50000]	Loss: 4.2864	LR: 1.000000
Training Epoch: 4 [9984/50000]	Loss: 4.3456	LR: 1.000000
Training Epoch: 4 [10240/50000]	Loss: 4.3007	LR: 1.000000
Training Epoch: 4 [10496/50000]	Loss: 4.2798	LR: 1.000000
Training Epoch: 4 [10752/50000]	Loss: 4.3748	LR: 1.000000
Training Epoch: 4 [11008/50000]	Loss: 4.3340	LR: 1.000000
Training Epoch: 4 [11264/50000]	Loss: 4.2083	LR: 1.000000
Training Epoch: 4 [11520/50000]	Loss: 4.4213	LR: 1.000000
Training Epoch: 4 [11776/50000]	Loss: 4.3508	LR: 1.000000
Training Epoch: 4 [12032/50000]	Loss: 4.2288	LR: 1.000000
Training Epoch: 4 [12288/50000]	Loss: 4.3442	LR: 1.000000
Training Epoch: 4 [12544/50000]	Loss: 4.4066	LR: 1.000000
Training Epoch: 4 [12800/50000]	Loss: 4.3110	LR: 1.000000
Training Epoch: 4 [13056/50000]	Loss: 4.4886	LR: 1.000000
Training Epoch: 4 [13312/50000]	Loss: 4.3588	LR: 1.000000
Training Epoch: 4 [13568/50000]	Loss: 4.4074	LR: 1.000000
Training Epoch: 4 [13824/50000]	Loss: 4.3098	LR: 1.000000
Training Epoch: 4 [14080/50000]	Loss: 4.3939	LR: 1.000000
Training Epoch: 4 [14336/50000]	Loss: 4.3620	LR: 1.000000
Training Epoch: 4 [14592/50000]	Loss: 4.3952	LR: 1.000000
Training Epoch: 4 [14848/50000]	Loss: 4.3851	LR: 1.000000
Training Epoch: 4 [15104/50000]	Loss: 4.3115	LR: 1.000000
Training Epoch: 4 [15360/50000]	Loss: 4.3756	LR: 1.000000
Training Epoch: 4 [15616/50000]	Loss: 4.2484	LR: 1.000000
Training Epoch: 4 [15872/50000]	Loss: 4.3158	LR: 1.000000
Training Epoch: 4 [16128/50000]	Loss: 4.2210	LR: 1.000000
Training Epoch: 4 [16384/50000]	Loss: 4.3469	LR: 1.000000
Training Epoch: 4 [16640/50000]	Loss: 4.3644	LR: 1.000000
Training Epoch: 4 [16896/50000]	Loss: 4.3120	LR: 1.000000
Training Epoch: 4 [17152/50000]	Loss: 4.2743	LR: 1.000000
Training Epoch: 4 [17408/50000]	Loss: 4.3468	LR: 1.000000
Training Epoch: 4 [17664/50000]	Loss: 4.3373	LR: 1.000000
Training Epoch: 4 [17920/50000]	Loss: 4.3818	LR: 1.000000
Training Epoch: 4 [18176/50000]	Loss: 4.4187	LR: 1.000000
Training Epoch: 4 [18432/50000]	Loss: 4.2865	LR: 1.000000
Training Epoch: 4 [18688/50000]	Loss: 4.2882	LR: 1.000000
Training Epoch: 4 [18944/50000]	Loss: 4.3197	LR: 1.000000
Training Epoch: 4 [19200/50000]	Loss: 4.2422	LR: 1.000000
Training Epoch: 4 [19456/50000]	Loss: 4.3183	LR: 1.000000
Training Epoch: 4 [19712/50000]	Loss: 4.2655	LR: 1.000000
Training Epoch: 4 [19968/50000]	Loss: 4.3331	LR: 1.000000
Training Epoch: 4 [20224/50000]	Loss: 4.2033	LR: 1.000000
Training Epoch: 4 [20480/50000]	Loss: 4.3912	LR: 1.000000
Training Epoch: 4 [20736/50000]	Loss: 4.3204	LR: 1.000000
Training Epoch: 4 [20992/50000]	Loss: 4.2630	LR: 1.000000
Training Epoch: 4 [21248/50000]	Loss: 4.2480	LR: 1.000000
Training Epoch: 4 [21504/50000]	Loss: 4.2662	LR: 1.000000
Training Epoch: 4 [21760/50000]	Loss: 4.2287	LR: 1.000000
Training Epoch: 4 [22016/50000]	Loss: 4.2153	LR: 1.000000
Training Epoch: 4 [22272/50000]	Loss: 4.2921	LR: 1.000000
Training Epoch: 4 [22528/50000]	Loss: 4.3283	LR: 1.000000
Training Epoch: 4 [22784/50000]	Loss: 4.3043	LR: 1.000000
Training Epoch: 4 [23040/50000]	Loss: 4.4808	LR: 1.000000
Training Epoch: 4 [23296/50000]	Loss: 4.2518	LR: 1.000000
Training Epoch: 4 [23552/50000]	Loss: 4.4308	LR: 1.000000
Training Epoch: 4 [23808/50000]	Loss: 4.3586	LR: 1.000000
Training Epoch: 4 [24064/50000]	Loss: 4.3406	LR: 1.000000
Training Epoch: 4 [24320/50000]	Loss: 4.3061	LR: 1.000000
Training Epoch: 4 [24576/50000]	Loss: 4.3616	LR: 1.000000
Training Epoch: 4 [24832/50000]	Loss: 4.3229	LR: 1.000000
Training Epoch: 4 [25088/50000]	Loss: 4.2319	LR: 1.000000
Training Epoch: 4 [25344/50000]	Loss: 4.3400	LR: 1.000000
Training Epoch: 4 [25600/50000]	Loss: 4.3224	LR: 1.000000
Training Epoch: 4 [25856/50000]	Loss: 4.3372	LR: 1.000000
Training Epoch: 4 [26112/50000]	Loss: 4.2201	LR: 1.000000
Training Epoch: 4 [26368/50000]	Loss: 4.2555	LR: 1.000000
Training Epoch: 4 [26624/50000]	Loss: 4.3634	LR: 1.000000
Training Epoch: 4 [26880/50000]	Loss: 4.3442	LR: 1.000000
Training Epoch: 4 [27136/50000]	Loss: 4.4469	LR: 1.000000
Training Epoch: 4 [27392/50000]	Loss: 4.4142	LR: 1.000000
Training Epoch: 4 [27648/50000]	Loss: 4.3575	LR: 1.000000
Training Epoch: 4 [27904/50000]	Loss: 4.3102	LR: 1.000000
Training Epoch: 4 [28160/50000]	Loss: 4.3910	LR: 1.000000
Training Epoch: 4 [28416/50000]	Loss: 4.3944	LR: 1.000000
Training Epoch: 4 [28672/50000]	Loss: 4.2837	LR: 1.000000
Training Epoch: 4 [28928/50000]	Loss: 4.3356	LR: 1.000000
Training Epoch: 4 [29184/50000]	Loss: 4.3696	LR: 1.000000
Training Epoch: 4 [29440/50000]	Loss: 4.3470	LR: 1.000000
Training Epoch: 4 [29696/50000]	Loss: 4.3684	LR: 1.000000
Training Epoch: 4 [29952/50000]	Loss: 4.2680	LR: 1.000000
Training Epoch: 4 [30208/50000]	Loss: 4.4012	LR: 1.000000
Training Epoch: 4 [30464/50000]	Loss: 4.4100	LR: 1.000000
Training Epoch: 4 [30720/50000]	Loss: 4.3822	LR: 1.000000
Training Epoch: 4 [30976/50000]	Loss: 4.2957	LR: 1.000000
Training Epoch: 4 [31232/50000]	Loss: 4.4284	LR: 1.000000
Training Epoch: 4 [31488/50000]	Loss: 4.3686	LR: 1.000000
Training Epoch: 4 [31744/50000]	Loss: 4.4493	LR: 1.000000
Training Epoch: 4 [32000/50000]	Loss: 4.3796	LR: 1.000000
Training Epoch: 4 [32256/50000]	Loss: 4.3192	LR: 1.000000
Training Epoch: 4 [32512/50000]	Loss: 4.3674	LR: 1.000000
Training Epoch: 4 [32768/50000]	Loss: 4.4114	LR: 1.000000
Training Epoch: 4 [33024/50000]	Loss: 4.3266	LR: 1.000000
Training Epoch: 4 [33280/50000]	Loss: 4.2782	LR: 1.000000
Training Epoch: 4 [33536/50000]	Loss: 4.4309	LR: 1.000000
Training Epoch: 4 [33792/50000]	Loss: 4.3973	LR: 1.000000
Training Epoch: 4 [34048/50000]	Loss: 4.3925	LR: 1.000000
Training Epoch: 4 [34304/50000]	Loss: 4.2693	LR: 1.000000
Training Epoch: 4 [34560/50000]	Loss: 4.3352	LR: 1.000000
Training Epoch: 4 [34816/50000]	Loss: 4.3738	LR: 1.000000
Training Epoch: 4 [35072/50000]	Loss: 4.3459	LR: 1.000000
Training Epoch: 4 [35328/50000]	Loss: 4.3774	LR: 1.000000
Training Epoch: 4 [35584/50000]	Loss: 4.4156	LR: 1.000000
Training Epoch: 4 [35840/50000]	Loss: 4.2791	LR: 1.000000
Training Epoch: 4 [36096/50000]	Loss: 4.3352	LR: 1.000000
Training Epoch: 4 [36352/50000]	Loss: 4.3642	LR: 1.000000
Training Epoch: 4 [36608/50000]	Loss: 4.3143	LR: 1.000000
Training Epoch: 4 [36864/50000]	Loss: 4.3108	LR: 1.000000
Training Epoch: 4 [37120/50000]	Loss: 4.3372	LR: 1.000000
Training Epoch: 4 [37376/50000]	Loss: 4.3327	LR: 1.000000
Training Epoch: 4 [37632/50000]	Loss: 4.3021	LR: 1.000000
Training Epoch: 4 [37888/50000]	Loss: 4.3676	LR: 1.000000
Training Epoch: 4 [38144/50000]	Loss: 4.3043	LR: 1.000000
Training Epoch: 4 [38400/50000]	Loss: 4.3591	LR: 1.000000
Training Epoch: 4 [38656/50000]	Loss: 4.3008	LR: 1.000000
Training Epoch: 4 [38912/50000]	Loss: 4.2869	LR: 1.000000
Training Epoch: 4 [39168/50000]	Loss: 4.2700	LR: 1.000000
Training Epoch: 4 [39424/50000]	Loss: 4.2676	LR: 1.000000
Training Epoch: 4 [39680/50000]	Loss: 4.3967	LR: 1.000000
Training Epoch: 4 [39936/50000]	Loss: 4.2753	LR: 1.000000
Training Epoch: 4 [40192/50000]	Loss: 4.3724	LR: 1.000000
Training Epoch: 4 [40448/50000]	Loss: 4.3239	LR: 1.000000
Training Epoch: 4 [40704/50000]	Loss: 4.3846	LR: 1.000000
Training Epoch: 4 [40960/50000]	Loss: 4.3125	LR: 1.000000
Training Epoch: 4 [41216/50000]	Loss: 4.3547	LR: 1.000000
Training Epoch: 4 [41472/50000]	Loss: 4.2825	LR: 1.000000
Training Epoch: 4 [41728/50000]	Loss: 4.2732	LR: 1.000000
Training Epoch: 4 [41984/50000]	Loss: 4.3563	LR: 1.000000
Training Epoch: 4 [42240/50000]	Loss: 4.2749	LR: 1.000000
Training Epoch: 4 [42496/50000]	Loss: 4.3473	LR: 1.000000
Training Epoch: 4 [42752/50000]	Loss: 4.3457	LR: 1.000000
Training Epoch: 4 [43008/50000]	Loss: 4.2688	LR: 1.000000
Training Epoch: 4 [43264/50000]	Loss: 4.3423	LR: 1.000000
Training Epoch: 4 [43520/50000]	Loss: 4.3289	LR: 1.000000
Training Epoch: 4 [43776/50000]	Loss: 4.3386	LR: 1.000000
Training Epoch: 4 [44032/50000]	Loss: 4.2575	LR: 1.000000
Training Epoch: 4 [44288/50000]	Loss: 4.3454	LR: 1.000000
Training Epoch: 4 [44544/50000]	Loss: 4.2890	LR: 1.000000
Training Epoch: 4 [44800/50000]	Loss: 4.3473	LR: 1.000000
Training Epoch: 4 [45056/50000]	Loss: 4.3576	LR: 1.000000
Training Epoch: 4 [45312/50000]	Loss: 4.2746	LR: 1.000000
Training Epoch: 4 [45568/50000]	Loss: 4.2664	LR: 1.000000
Training Epoch: 4 [45824/50000]	Loss: 4.1738	LR: 1.000000
Training Epoch: 4 [46080/50000]	Loss: 4.2418	LR: 1.000000
Training Epoch: 4 [46336/50000]	Loss: 4.3663	LR: 1.000000
Training Epoch: 4 [46592/50000]	Loss: 4.3652	LR: 1.000000
Training Epoch: 4 [46848/50000]	Loss: 4.3605	LR: 1.000000
Training Epoch: 4 [47104/50000]	Loss: 4.3726	LR: 1.000000
Training Epoch: 4 [47360/50000]	Loss: 4.2763	LR: 1.000000
Training Epoch: 4 [47616/50000]	Loss: 4.3741	LR: 1.000000
Training Epoch: 4 [47872/50000]	Loss: 4.3257	LR: 1.000000
Training Epoch: 4 [48128/50000]	Loss: 4.2903	LR: 1.000000
Training Epoch: 4 [48384/50000]	Loss: 4.3634	LR: 1.000000
Training Epoch: 4 [48640/50000]	Loss: 4.2853	LR: 1.000000
Training Epoch: 4 [48896/50000]	Loss: 4.2842	LR: 1.000000
Training Epoch: 4 [49152/50000]	Loss: 4.3001	LR: 1.000000
Training Epoch: 4 [49408/50000]	Loss: 4.3284	LR: 1.000000
Training Epoch: 4 [49664/50000]	Loss: 4.3717	LR: 1.000000
Training Epoch: 4 [49920/50000]	Loss: 4.3917	LR: 1.000000
Training Epoch: 4 [50000/50000]	Loss: 4.4246	LR: 1.000000
epoch 4 training time consumed: 22.22s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |    6001 GB |    6001 GB |
|       from large pool |  400448 KB |    1770 MB |    5996 GB |    5995 GB |
|       from small pool |    3549 KB |       9 MB |       5 GB |       5 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |    6001 GB |    6001 GB |
|       from large pool |  400448 KB |    1770 MB |    5996 GB |    5995 GB |
|       from small pool |    3549 KB |       9 MB |       5 GB |       5 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |    3694 GB |    3693 GB |
|       from large pool |  244672 KB |  473024 KB |    3688 GB |    3687 GB |
|       from small pool |    4642 KB |    4843 KB |       6 GB |       6 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  256551    |  256329    |
|       from large pool |      36    |      77    |  123910    |  123874    |
|       from small pool |     186    |     224    |  132641    |  132455    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  256551    |  256329    |
|       from large pool |      36    |      77    |  123910    |  123874    |
|       from small pool |     186    |     224    |  132641    |  132455    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      31    |  130728    |  130705    |
|       from large pool |      10    |      11    |   51324    |   51314    |
|       from small pool |      13    |      21    |   79404    |   79391    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 4, Average loss: 0.0172, Accuracy: 0.0315, Time consumed:1.45s

Training Epoch: 5 [256/50000]	Loss: 4.3441	LR: 1.000000
Training Epoch: 5 [512/50000]	Loss: 4.3097	LR: 1.000000
Training Epoch: 5 [768/50000]	Loss: 4.2668	LR: 1.000000
Training Epoch: 5 [1024/50000]	Loss: 4.4345	LR: 1.000000
Training Epoch: 5 [1280/50000]	Loss: 4.4241	LR: 1.000000
Training Epoch: 5 [1536/50000]	Loss: 4.3696	LR: 1.000000
Training Epoch: 5 [1792/50000]	Loss: 4.4614	LR: 1.000000
Training Epoch: 5 [2048/50000]	Loss: 4.3789	LR: 1.000000
Training Epoch: 5 [2304/50000]	Loss: 4.3554	LR: 1.000000
Training Epoch: 5 [2560/50000]	Loss: 4.3528	LR: 1.000000
Training Epoch: 5 [2816/50000]	Loss: 4.4096	LR: 1.000000
Training Epoch: 5 [3072/50000]	Loss: 4.3475	LR: 1.000000
Training Epoch: 5 [3328/50000]	Loss: 4.4000	LR: 1.000000
Training Epoch: 5 [3584/50000]	Loss: 4.3014	LR: 1.000000
Training Epoch: 5 [3840/50000]	Loss: 4.2840	LR: 1.000000
Training Epoch: 5 [4096/50000]	Loss: 4.3270	LR: 1.000000
Training Epoch: 5 [4352/50000]	Loss: 4.3952	LR: 1.000000
Training Epoch: 5 [4608/50000]	Loss: 4.2593	LR: 1.000000
Training Epoch: 5 [4864/50000]	Loss: 4.4370	LR: 1.000000
Training Epoch: 5 [5120/50000]	Loss: 4.3893	LR: 1.000000
Training Epoch: 5 [5376/50000]	Loss: 4.3982	LR: 1.000000
Training Epoch: 5 [5632/50000]	Loss: 4.2854	LR: 1.000000
Training Epoch: 5 [5888/50000]	Loss: 4.3619	LR: 1.000000
Training Epoch: 5 [6144/50000]	Loss: 4.3520	LR: 1.000000
Training Epoch: 5 [6400/50000]	Loss: 4.3750	LR: 1.000000
Training Epoch: 5 [6656/50000]	Loss: 4.3844	LR: 1.000000
Training Epoch: 5 [6912/50000]	Loss: 4.3675	LR: 1.000000
Training Epoch: 5 [7168/50000]	Loss: 4.4076	LR: 1.000000
Training Epoch: 5 [7424/50000]	Loss: 4.3953	LR: 1.000000
Training Epoch: 5 [7680/50000]	Loss: 4.3683	LR: 1.000000
Training Epoch: 5 [7936/50000]	Loss: 4.3505	LR: 1.000000
Training Epoch: 5 [8192/50000]	Loss: 4.3689	LR: 1.000000
Training Epoch: 5 [8448/50000]	Loss: 4.3778	LR: 1.000000
Training Epoch: 5 [8704/50000]	Loss: 4.4133	LR: 1.000000
Training Epoch: 5 [8960/50000]	Loss: 4.2570	LR: 1.000000
Training Epoch: 5 [9216/50000]	Loss: 4.3967	LR: 1.000000
Training Epoch: 5 [9472/50000]	Loss: 4.3488	LR: 1.000000
Training Epoch: 5 [9728/50000]	Loss: 4.2669	LR: 1.000000
Training Epoch: 5 [9984/50000]	Loss: 4.3707	LR: 1.000000
Training Epoch: 5 [10240/50000]	Loss: 4.2628	LR: 1.000000
Training Epoch: 5 [10496/50000]	Loss: 4.3426	LR: 1.000000
Training Epoch: 5 [10752/50000]	Loss: 4.3159	LR: 1.000000
Training Epoch: 5 [11008/50000]	Loss: 4.3370	LR: 1.000000
Training Epoch: 5 [11264/50000]	Loss: 4.3527	LR: 1.000000
Training Epoch: 5 [11520/50000]	Loss: 4.3041	LR: 1.000000
Training Epoch: 5 [11776/50000]	Loss: 4.2317	LR: 1.000000
Training Epoch: 5 [12032/50000]	Loss: 4.4076	LR: 1.000000
Training Epoch: 5 [12288/50000]	Loss: 4.3363	LR: 1.000000
Training Epoch: 5 [12544/50000]	Loss: 4.3772	LR: 1.000000
Training Epoch: 5 [12800/50000]	Loss: 4.3421	LR: 1.000000
Training Epoch: 5 [13056/50000]	Loss: 4.4481	LR: 1.000000
Training Epoch: 5 [13312/50000]	Loss: 4.3456	LR: 1.000000
Training Epoch: 5 [13568/50000]	Loss: 4.3305	LR: 1.000000
Training Epoch: 5 [13824/50000]	Loss: 4.3944	LR: 1.000000
Training Epoch: 5 [14080/50000]	Loss: 4.2861	LR: 1.000000
Training Epoch: 5 [14336/50000]	Loss: 4.3183	LR: 1.000000
Training Epoch: 5 [14592/50000]	Loss: 4.3312	LR: 1.000000
Training Epoch: 5 [14848/50000]	Loss: 4.3554	LR: 1.000000
Training Epoch: 5 [15104/50000]	Loss: 4.3131	LR: 1.000000
Training Epoch: 5 [15360/50000]	Loss: 4.3022	LR: 1.000000
Training Epoch: 5 [15616/50000]	Loss: 4.2433	LR: 1.000000
Training Epoch: 5 [15872/50000]	Loss: 4.3139	LR: 1.000000
Training Epoch: 5 [16128/50000]	Loss: 4.3235	LR: 1.000000
Training Epoch: 5 [16384/50000]	Loss: 4.3978	LR: 1.000000
Training Epoch: 5 [16640/50000]	Loss: 4.2720	LR: 1.000000
Training Epoch: 5 [16896/50000]	Loss: 4.4041	LR: 1.000000
Training Epoch: 5 [17152/50000]	Loss: 4.2841	LR: 1.000000
Training Epoch: 5 [17408/50000]	Loss: 4.2772	LR: 1.000000
Training Epoch: 5 [17664/50000]	Loss: 4.3111	LR: 1.000000
Training Epoch: 5 [17920/50000]	Loss: 4.2618	LR: 1.000000
Training Epoch: 5 [18176/50000]	Loss: 4.3301	LR: 1.000000
Training Epoch: 5 [18432/50000]	Loss: 4.2878	LR: 1.000000
Training Epoch: 5 [18688/50000]	Loss: 4.3013	LR: 1.000000
Training Epoch: 5 [18944/50000]	Loss: 4.2523	LR: 1.000000
Training Epoch: 5 [19200/50000]	Loss: 4.2253	LR: 1.000000
Training Epoch: 5 [19456/50000]	Loss: 4.3464	LR: 1.000000
Training Epoch: 5 [19712/50000]	Loss: 4.2907	LR: 1.000000
Training Epoch: 5 [19968/50000]	Loss: 4.3457	LR: 1.000000
Training Epoch: 5 [20224/50000]	Loss: 4.3951	LR: 1.000000
Training Epoch: 5 [20480/50000]	Loss: 4.3281	LR: 1.000000
Training Epoch: 5 [20736/50000]	Loss: 4.3901	LR: 1.000000
Training Epoch: 5 [20992/50000]	Loss: 4.2954	LR: 1.000000
Training Epoch: 5 [21248/50000]	Loss: 4.3822	LR: 1.000000
Training Epoch: 5 [21504/50000]	Loss: 4.2777	LR: 1.000000
Training Epoch: 5 [21760/50000]	Loss: 4.3941	LR: 1.000000
Training Epoch: 5 [22016/50000]	Loss: 4.2913	LR: 1.000000
Training Epoch: 5 [22272/50000]	Loss: 4.3109	LR: 1.000000
Training Epoch: 5 [22528/50000]	Loss: 4.2317	LR: 1.000000
Training Epoch: 5 [22784/50000]	Loss: 4.3272	LR: 1.000000
Training Epoch: 5 [23040/50000]	Loss: 4.3162	LR: 1.000000
Training Epoch: 5 [23296/50000]	Loss: 4.2889	LR: 1.000000
Training Epoch: 5 [23552/50000]	Loss: 4.3392	LR: 1.000000
Training Epoch: 5 [23808/50000]	Loss: 4.3603	LR: 1.000000
Training Epoch: 5 [24064/50000]	Loss: 4.3979	LR: 1.000000
Training Epoch: 5 [24320/50000]	Loss: 4.3616	LR: 1.000000
Training Epoch: 5 [24576/50000]	Loss: 4.2929	LR: 1.000000
Training Epoch: 5 [24832/50000]	Loss: 4.4149	LR: 1.000000
Training Epoch: 5 [25088/50000]	Loss: 4.3692	LR: 1.000000
Training Epoch: 5 [25344/50000]	Loss: 4.3014	LR: 1.000000
Training Epoch: 5 [25600/50000]	Loss: 4.3451	LR: 1.000000
Training Epoch: 5 [25856/50000]	Loss: 4.3781	LR: 1.000000
Training Epoch: 5 [26112/50000]	Loss: 4.3158	LR: 1.000000
Training Epoch: 5 [26368/50000]	Loss: 4.3110	LR: 1.000000
Training Epoch: 5 [26624/50000]	Loss: 4.3883	LR: 1.000000
Training Epoch: 5 [26880/50000]	Loss: 4.3482	LR: 1.000000
Training Epoch: 5 [27136/50000]	Loss: 4.3054	LR: 1.000000
Training Epoch: 5 [27392/50000]	Loss: 4.3094	LR: 1.000000
Training Epoch: 5 [27648/50000]	Loss: 4.2793	LR: 1.000000
Training Epoch: 5 [27904/50000]	Loss: 4.3176	LR: 1.000000
Training Epoch: 5 [28160/50000]	Loss: 4.3184	LR: 1.000000
Training Epoch: 5 [28416/50000]	Loss: 4.3572	LR: 1.000000
Training Epoch: 5 [28672/50000]	Loss: 4.2743	LR: 1.000000
Training Epoch: 5 [28928/50000]	Loss: 4.2445	LR: 1.000000
Training Epoch: 5 [29184/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 5 [29440/50000]	Loss: 4.3500	LR: 1.000000
Training Epoch: 5 [29696/50000]	Loss: 4.3363	LR: 1.000000
Training Epoch: 5 [29952/50000]	Loss: 4.4135	LR: 1.000000
Training Epoch: 5 [30208/50000]	Loss: 4.3809	LR: 1.000000
Training Epoch: 5 [30464/50000]	Loss: 4.2932	LR: 1.000000
Training Epoch: 5 [30720/50000]	Loss: 4.3539	LR: 1.000000
Training Epoch: 5 [30976/50000]	Loss: 4.3388	LR: 1.000000
Training Epoch: 5 [31232/50000]	Loss: 4.3290	LR: 1.000000
Training Epoch: 5 [31488/50000]	Loss: 4.2671	LR: 1.000000
Training Epoch: 5 [31744/50000]	Loss: 4.3721	LR: 1.000000
Training Epoch: 5 [32000/50000]	Loss: 4.2794	LR: 1.000000
Training Epoch: 5 [32256/50000]	Loss: 4.2716	LR: 1.000000
Training Epoch: 5 [32512/50000]	Loss: 4.3602	LR: 1.000000
Training Epoch: 5 [32768/50000]	Loss: 4.3037	LR: 1.000000
Training Epoch: 5 [33024/50000]	Loss: 4.3718	LR: 1.000000
Training Epoch: 5 [33280/50000]	Loss: 4.3182	LR: 1.000000
Training Epoch: 5 [33536/50000]	Loss: 4.3249	LR: 1.000000
Training Epoch: 5 [33792/50000]	Loss: 4.3637	LR: 1.000000
Training Epoch: 5 [34048/50000]	Loss: 4.3651	LR: 1.000000
Training Epoch: 5 [34304/50000]	Loss: 4.2447	LR: 1.000000
Training Epoch: 5 [34560/50000]	Loss: 4.3469	LR: 1.000000
Training Epoch: 5 [34816/50000]	Loss: 4.2902	LR: 1.000000
Training Epoch: 5 [35072/50000]	Loss: 4.2984	LR: 1.000000
Training Epoch: 5 [35328/50000]	Loss: 4.2475	LR: 1.000000
Training Epoch: 5 [35584/50000]	Loss: 4.4329	LR: 1.000000
Training Epoch: 5 [35840/50000]	Loss: 4.3427	LR: 1.000000
Training Epoch: 5 [36096/50000]	Loss: 4.3456	LR: 1.000000
Training Epoch: 5 [36352/50000]	Loss: 4.3841	LR: 1.000000
Training Epoch: 5 [36608/50000]	Loss: 4.3052	LR: 1.000000
Training Epoch: 5 [36864/50000]	Loss: 4.3236	LR: 1.000000
Training Epoch: 5 [37120/50000]	Loss: 4.2756	LR: 1.000000
Training Epoch: 5 [37376/50000]	Loss: 4.3551	LR: 1.000000
Training Epoch: 5 [37632/50000]	Loss: 4.4163	LR: 1.000000
Training Epoch: 5 [37888/50000]	Loss: 4.3955	LR: 1.000000
Training Epoch: 5 [38144/50000]	Loss: 4.4358	LR: 1.000000
Training Epoch: 5 [38400/50000]	Loss: 4.3189	LR: 1.000000
Training Epoch: 5 [38656/50000]	Loss: 4.3775	LR: 1.000000
Training Epoch: 5 [38912/50000]	Loss: 4.3068	LR: 1.000000
Training Epoch: 5 [39168/50000]	Loss: 4.3769	LR: 1.000000
Training Epoch: 5 [39424/50000]	Loss: 4.4220	LR: 1.000000
Training Epoch: 5 [39680/50000]	Loss: 4.3770	LR: 1.000000
Training Epoch: 5 [39936/50000]	Loss: 4.3718	LR: 1.000000
Training Epoch: 5 [40192/50000]	Loss: 4.3743	LR: 1.000000
Training Epoch: 5 [40448/50000]	Loss: 4.4526	LR: 1.000000
Training Epoch: 5 [40704/50000]	Loss: 4.3899	LR: 1.000000
Training Epoch: 5 [40960/50000]	Loss: 4.3332	LR: 1.000000
Training Epoch: 5 [41216/50000]	Loss: 4.3738	LR: 1.000000
Training Epoch: 5 [41472/50000]	Loss: 4.3003	LR: 1.000000
Training Epoch: 5 [41728/50000]	Loss: 4.2507	LR: 1.000000
Training Epoch: 5 [41984/50000]	Loss: 4.3431	LR: 1.000000
Training Epoch: 5 [42240/50000]	Loss: 4.2723	LR: 1.000000
Training Epoch: 5 [42496/50000]	Loss: 4.3319	LR: 1.000000
Training Epoch: 5 [42752/50000]	Loss: 4.3585	LR: 1.000000
Training Epoch: 5 [43008/50000]	Loss: 4.3383	LR: 1.000000
Training Epoch: 5 [43264/50000]	Loss: 4.3418	LR: 1.000000
Training Epoch: 5 [43520/50000]	Loss: 4.2486	LR: 1.000000
Training Epoch: 5 [43776/50000]	Loss: 4.4111	LR: 1.000000
Training Epoch: 5 [44032/50000]	Loss: 4.2656	LR: 1.000000
Training Epoch: 5 [44288/50000]	Loss: 4.3085	LR: 1.000000
Training Epoch: 5 [44544/50000]	Loss: 4.3591	LR: 1.000000
Training Epoch: 5 [44800/50000]	Loss: 4.2703	LR: 1.000000
Training Epoch: 5 [45056/50000]	Loss: 4.2730	LR: 1.000000
Training Epoch: 5 [45312/50000]	Loss: 4.3689	LR: 1.000000
Training Epoch: 5 [45568/50000]	Loss: 4.3906	LR: 1.000000
Training Epoch: 5 [45824/50000]	Loss: 4.3601	LR: 1.000000
Training Epoch: 5 [46080/50000]	Loss: 4.4777	LR: 1.000000
Training Epoch: 5 [46336/50000]	Loss: 4.3024	LR: 1.000000
Training Epoch: 5 [46592/50000]	Loss: 4.4236	LR: 1.000000
Training Epoch: 5 [46848/50000]	Loss: 4.3243	LR: 1.000000
Training Epoch: 5 [47104/50000]	Loss: 4.3473	LR: 1.000000
Training Epoch: 5 [47360/50000]	Loss: 4.3360	LR: 1.000000
Training Epoch: 5 [47616/50000]	Loss: 4.4209	LR: 1.000000
Training Epoch: 5 [47872/50000]	Loss: 4.3661	LR: 1.000000
Training Epoch: 5 [48128/50000]	Loss: 4.3903	LR: 1.000000
Training Epoch: 5 [48384/50000]	Loss: 4.4045	LR: 1.000000
Training Epoch: 5 [48640/50000]	Loss: 4.4312	LR: 1.000000
Training Epoch: 5 [48896/50000]	Loss: 4.4408	LR: 1.000000
Training Epoch: 5 [49152/50000]	Loss: 4.4400	LR: 1.000000
Training Epoch: 5 [49408/50000]	Loss: 4.3937	LR: 1.000000
Training Epoch: 5 [49664/50000]	Loss: 4.4409	LR: 1.000000
Training Epoch: 5 [49920/50000]	Loss: 4.3672	LR: 1.000000
Training Epoch: 5 [50000/50000]	Loss: 4.3279	LR: 1.000000
epoch 5 training time consumed: 22.32s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |    7501 GB |    7501 GB |
|       from large pool |  400448 KB |    1770 MB |    7495 GB |    7494 GB |
|       from small pool |    3549 KB |       9 MB |       6 GB |       6 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |    7501 GB |    7501 GB |
|       from large pool |  400448 KB |    1770 MB |    7495 GB |    7494 GB |
|       from small pool |    3549 KB |       9 MB |       6 GB |       6 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |    4617 GB |    4617 GB |
|       from large pool |  244672 KB |  473024 KB |    4609 GB |    4609 GB |
|       from small pool |    2594 KB |    4843 KB |       7 GB |       7 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  320575    |  320353    |
|       from large pool |      36    |      77    |  154869    |  154833    |
|       from small pool |     186    |     224    |  165706    |  165520    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  320575    |  320353    |
|       from large pool |      36    |      77    |  154869    |  154833    |
|       from small pool |     186    |     224    |  165706    |  165520    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      31    |  162789    |  162768    |
|       from large pool |      10    |      11    |   64147    |   64137    |
|       from small pool |      11    |      21    |   98642    |   98631    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 5, Average loss: 0.0177, Accuracy: 0.0220, Time consumed:1.46s

Training Epoch: 6 [256/50000]	Loss: 4.3959	LR: 1.000000
Training Epoch: 6 [512/50000]	Loss: 4.4479	LR: 1.000000
Training Epoch: 6 [768/50000]	Loss: 4.3394	LR: 1.000000
Training Epoch: 6 [1024/50000]	Loss: 4.3671	LR: 1.000000
Training Epoch: 6 [1280/50000]	Loss: 4.3840	LR: 1.000000
Training Epoch: 6 [1536/50000]	Loss: 4.3231	LR: 1.000000
Training Epoch: 6 [1792/50000]	Loss: 4.4231	LR: 1.000000
Training Epoch: 6 [2048/50000]	Loss: 4.3779	LR: 1.000000
Training Epoch: 6 [2304/50000]	Loss: 4.3475	LR: 1.000000
Training Epoch: 6 [2560/50000]	Loss: 4.3690	LR: 1.000000
Training Epoch: 6 [2816/50000]	Loss: 4.3716	LR: 1.000000
Training Epoch: 6 [3072/50000]	Loss: 4.3538	LR: 1.000000
Training Epoch: 6 [3328/50000]	Loss: 4.3432	LR: 1.000000
Training Epoch: 6 [3584/50000]	Loss: 4.3278	LR: 1.000000
Training Epoch: 6 [3840/50000]	Loss: 4.3247	LR: 1.000000
Training Epoch: 6 [4096/50000]	Loss: 4.3279	LR: 1.000000
Training Epoch: 6 [4352/50000]	Loss: 4.3316	LR: 1.000000
Training Epoch: 6 [4608/50000]	Loss: 4.4125	LR: 1.000000
Training Epoch: 6 [4864/50000]	Loss: 4.3113	LR: 1.000000
Training Epoch: 6 [5120/50000]	Loss: 4.4055	LR: 1.000000
Training Epoch: 6 [5376/50000]	Loss: 4.2651	LR: 1.000000
Training Epoch: 6 [5632/50000]	Loss: 4.4483	LR: 1.000000
Training Epoch: 6 [5888/50000]	Loss: 4.2684	LR: 1.000000
Training Epoch: 6 [6144/50000]	Loss: 4.3368	LR: 1.000000
Training Epoch: 6 [6400/50000]	Loss: 4.3636	LR: 1.000000
Training Epoch: 6 [6656/50000]	Loss: 4.4304	LR: 1.000000
Training Epoch: 6 [6912/50000]	Loss: 4.3797	LR: 1.000000
Training Epoch: 6 [7168/50000]	Loss: 4.3566	LR: 1.000000
Training Epoch: 6 [7424/50000]	Loss: 4.3397	LR: 1.000000
Training Epoch: 6 [7680/50000]	Loss: 4.3343	LR: 1.000000
Training Epoch: 6 [7936/50000]	Loss: 4.4298	LR: 1.000000
Training Epoch: 6 [8192/50000]	Loss: 4.3770	LR: 1.000000
Training Epoch: 6 [8448/50000]	Loss: 4.4003	LR: 1.000000
Training Epoch: 6 [8704/50000]	Loss: 4.3680	LR: 1.000000
Training Epoch: 6 [8960/50000]	Loss: 4.4054	LR: 1.000000
Training Epoch: 6 [9216/50000]	Loss: 4.4072	LR: 1.000000
Training Epoch: 6 [9472/50000]	Loss: 4.3591	LR: 1.000000
Training Epoch: 6 [9728/50000]	Loss: 4.4782	LR: 1.000000
Training Epoch: 6 [9984/50000]	Loss: 4.3611	LR: 1.000000
Training Epoch: 6 [10240/50000]	Loss: 4.3366	LR: 1.000000
Training Epoch: 6 [10496/50000]	Loss: 4.4097	LR: 1.000000
Training Epoch: 6 [10752/50000]	Loss: 4.3021	LR: 1.000000
Training Epoch: 6 [11008/50000]	Loss: 4.3574	LR: 1.000000
Training Epoch: 6 [11264/50000]	Loss: 4.3052	LR: 1.000000
Training Epoch: 6 [11520/50000]	Loss: 4.3379	LR: 1.000000
Training Epoch: 6 [11776/50000]	Loss: 4.4045	LR: 1.000000
Training Epoch: 6 [12032/50000]	Loss: 4.3404	LR: 1.000000
Training Epoch: 6 [12288/50000]	Loss: 4.3845	LR: 1.000000
Training Epoch: 6 [12544/50000]	Loss: 4.3310	LR: 1.000000
Training Epoch: 6 [12800/50000]	Loss: 4.3198	LR: 1.000000
Training Epoch: 6 [13056/50000]	Loss: 4.3723	LR: 1.000000
Training Epoch: 6 [13312/50000]	Loss: 4.4188	LR: 1.000000
Training Epoch: 6 [13568/50000]	Loss: 4.3305	LR: 1.000000
Training Epoch: 6 [13824/50000]	Loss: 4.3131	LR: 1.000000
Training Epoch: 6 [14080/50000]	Loss: 4.5044	LR: 1.000000
Training Epoch: 6 [14336/50000]	Loss: 4.3380	LR: 1.000000
Training Epoch: 6 [14592/50000]	Loss: 4.3563	LR: 1.000000
Training Epoch: 6 [14848/50000]	Loss: 4.3931	LR: 1.000000
Training Epoch: 6 [15104/50000]	Loss: 4.3388	LR: 1.000000
Training Epoch: 6 [15360/50000]	Loss: 4.4407	LR: 1.000000
Training Epoch: 6 [15616/50000]	Loss: 4.3320	LR: 1.000000
Training Epoch: 6 [15872/50000]	Loss: 4.4328	LR: 1.000000
Training Epoch: 6 [16128/50000]	Loss: 4.3264	LR: 1.000000
Training Epoch: 6 [16384/50000]	Loss: 4.3417	LR: 1.000000
Training Epoch: 6 [16640/50000]	Loss: 4.3979	LR: 1.000000
Training Epoch: 6 [16896/50000]	Loss: 4.3537	LR: 1.000000
Training Epoch: 6 [17152/50000]	Loss: 4.3110	LR: 1.000000
Training Epoch: 6 [17408/50000]	Loss: 4.3836	LR: 1.000000
Training Epoch: 6 [17664/50000]	Loss: 4.3691	LR: 1.000000
Training Epoch: 6 [17920/50000]	Loss: 4.3042	LR: 1.000000
Training Epoch: 6 [18176/50000]	Loss: 4.3365	LR: 1.000000
Training Epoch: 6 [18432/50000]	Loss: 4.4152	LR: 1.000000
Training Epoch: 6 [18688/50000]	Loss: 4.3368	LR: 1.000000
Training Epoch: 6 [18944/50000]	Loss: 4.3030	LR: 1.000000
Training Epoch: 6 [19200/50000]	Loss: 4.3265	LR: 1.000000
Training Epoch: 6 [19456/50000]	Loss: 4.3287	LR: 1.000000
Training Epoch: 6 [19712/50000]	Loss: 4.4285	LR: 1.000000
Training Epoch: 6 [19968/50000]	Loss: 4.3558	LR: 1.000000
Training Epoch: 6 [20224/50000]	Loss: 4.3883	LR: 1.000000
Training Epoch: 6 [20480/50000]	Loss: 4.3356	LR: 1.000000
Training Epoch: 6 [20736/50000]	Loss: 4.2826	LR: 1.000000
Training Epoch: 6 [20992/50000]	Loss: 4.3402	LR: 1.000000
Training Epoch: 6 [21248/50000]	Loss: 4.4184	LR: 1.000000
Training Epoch: 6 [21504/50000]	Loss: 4.2455	LR: 1.000000
Training Epoch: 6 [21760/50000]	Loss: 4.4003	LR: 1.000000
Training Epoch: 6 [22016/50000]	Loss: 4.4668	LR: 1.000000
Training Epoch: 6 [22272/50000]	Loss: 4.3744	LR: 1.000000
Training Epoch: 6 [22528/50000]	Loss: 4.3602	LR: 1.000000
Training Epoch: 6 [22784/50000]	Loss: 4.3898	LR: 1.000000
Training Epoch: 6 [23040/50000]	Loss: 4.3347	LR: 1.000000
Training Epoch: 6 [23296/50000]	Loss: 4.3417	LR: 1.000000
Training Epoch: 6 [23552/50000]	Loss: 4.2782	LR: 1.000000
Training Epoch: 6 [23808/50000]	Loss: 4.3694	LR: 1.000000
Training Epoch: 6 [24064/50000]	Loss: 4.4935	LR: 1.000000
Training Epoch: 6 [24320/50000]	Loss: 4.3048	LR: 1.000000
Training Epoch: 6 [24576/50000]	Loss: 4.4459	LR: 1.000000
Training Epoch: 6 [24832/50000]	Loss: 4.3886	LR: 1.000000
Training Epoch: 6 [25088/50000]	Loss: 4.3531	LR: 1.000000
Training Epoch: 6 [25344/50000]	Loss: 4.4015	LR: 1.000000
Training Epoch: 6 [25600/50000]	Loss: 4.4026	LR: 1.000000
Training Epoch: 6 [25856/50000]	Loss: 4.2721	LR: 1.000000
Training Epoch: 6 [26112/50000]	Loss: 4.3613	LR: 1.000000
Training Epoch: 6 [26368/50000]	Loss: 4.3329	LR: 1.000000
Training Epoch: 6 [26624/50000]	Loss: 4.4010	LR: 1.000000
Training Epoch: 6 [26880/50000]	Loss: 4.3486	LR: 1.000000
Training Epoch: 6 [27136/50000]	Loss: 4.3658	LR: 1.000000
Training Epoch: 6 [27392/50000]	Loss: 4.2916	LR: 1.000000
Training Epoch: 6 [27648/50000]	Loss: 4.4073	LR: 1.000000
Training Epoch: 6 [27904/50000]	Loss: 4.3820	LR: 1.000000
Training Epoch: 6 [28160/50000]	Loss: 4.3199	LR: 1.000000
Training Epoch: 6 [28416/50000]	Loss: 4.3299	LR: 1.000000
Training Epoch: 6 [28672/50000]	Loss: 4.3482	LR: 1.000000
Training Epoch: 6 [28928/50000]	Loss: 4.3635	LR: 1.000000
Training Epoch: 6 [29184/50000]	Loss: 4.3701	LR: 1.000000
Training Epoch: 6 [29440/50000]	Loss: 4.2894	LR: 1.000000
Training Epoch: 6 [29696/50000]	Loss: 4.4260	LR: 1.000000
Training Epoch: 6 [29952/50000]	Loss: 4.3549	LR: 1.000000
Training Epoch: 6 [30208/50000]	Loss: 4.2940	LR: 1.000000
Training Epoch: 6 [30464/50000]	Loss: 4.4352	LR: 1.000000
Training Epoch: 6 [30720/50000]	Loss: 4.3155	LR: 1.000000
Training Epoch: 6 [30976/50000]	Loss: 4.2911	LR: 1.000000
Training Epoch: 6 [31232/50000]	Loss: 4.2924	LR: 1.000000
Training Epoch: 6 [31488/50000]	Loss: 4.3315	LR: 1.000000
Training Epoch: 6 [31744/50000]	Loss: 4.2056	LR: 1.000000
Training Epoch: 6 [32000/50000]	Loss: 4.3166	LR: 1.000000
Training Epoch: 6 [32256/50000]	Loss: 4.3883	LR: 1.000000
Training Epoch: 6 [32512/50000]	Loss: 4.3339	LR: 1.000000
Training Epoch: 6 [32768/50000]	Loss: 4.3796	LR: 1.000000
Training Epoch: 6 [33024/50000]	Loss: 4.3010	LR: 1.000000
Training Epoch: 6 [33280/50000]	Loss: 4.3228	LR: 1.000000
Training Epoch: 6 [33536/50000]	Loss: 4.3747	LR: 1.000000
Training Epoch: 6 [33792/50000]	Loss: 4.4379	LR: 1.000000
Training Epoch: 6 [34048/50000]	Loss: 4.3429	LR: 1.000000
Training Epoch: 6 [34304/50000]	Loss: 4.2469	LR: 1.000000
Training Epoch: 6 [34560/50000]	Loss: 4.3291	LR: 1.000000
Training Epoch: 6 [34816/50000]	Loss: 4.3561	LR: 1.000000
Training Epoch: 6 [35072/50000]	Loss: 4.3334	LR: 1.000000
Training Epoch: 6 [35328/50000]	Loss: 4.3819	LR: 1.000000
Training Epoch: 6 [35584/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 6 [35840/50000]	Loss: 4.4117	LR: 1.000000
Training Epoch: 6 [36096/50000]	Loss: 4.3432	LR: 1.000000
Training Epoch: 6 [36352/50000]	Loss: 4.4427	LR: 1.000000
Training Epoch: 6 [36608/50000]	Loss: 4.3131	LR: 1.000000
Training Epoch: 6 [36864/50000]	Loss: 4.4132	LR: 1.000000
Training Epoch: 6 [37120/50000]	Loss: 4.3699	LR: 1.000000
Training Epoch: 6 [37376/50000]	Loss: 4.3830	LR: 1.000000
Training Epoch: 6 [37632/50000]	Loss: 4.2862	LR: 1.000000
Training Epoch: 6 [37888/50000]	Loss: 4.3091	LR: 1.000000
Training Epoch: 6 [38144/50000]	Loss: 4.5101	LR: 1.000000
Training Epoch: 6 [38400/50000]	Loss: 4.2834	LR: 1.000000
Training Epoch: 6 [38656/50000]	Loss: 4.3598	LR: 1.000000
Training Epoch: 6 [38912/50000]	Loss: 4.3829	LR: 1.000000
Training Epoch: 6 [39168/50000]	Loss: 4.3235	LR: 1.000000
Training Epoch: 6 [39424/50000]	Loss: 4.1934	LR: 1.000000
Training Epoch: 6 [39680/50000]	Loss: 4.4450	LR: 1.000000
Training Epoch: 6 [39936/50000]	Loss: 4.3244	LR: 1.000000
Training Epoch: 6 [40192/50000]	Loss: 4.3419	LR: 1.000000
Training Epoch: 6 [40448/50000]	Loss: 4.3796	LR: 1.000000
Training Epoch: 6 [40704/50000]	Loss: 4.3592	LR: 1.000000
Training Epoch: 6 [40960/50000]	Loss: 4.4100	LR: 1.000000
Training Epoch: 6 [41216/50000]	Loss: 4.3801	LR: 1.000000
Training Epoch: 6 [41472/50000]	Loss: 4.3523	LR: 1.000000
Training Epoch: 6 [41728/50000]	Loss: 4.3351	LR: 1.000000
Training Epoch: 6 [41984/50000]	Loss: 4.4035	LR: 1.000000
Training Epoch: 6 [42240/50000]	Loss: 4.3030	LR: 1.000000
Training Epoch: 6 [42496/50000]	Loss: 4.3235	LR: 1.000000
Training Epoch: 6 [42752/50000]	Loss: 4.5147	LR: 1.000000
Training Epoch: 6 [43008/50000]	Loss: 4.3574	LR: 1.000000
Training Epoch: 6 [43264/50000]	Loss: 4.3786	LR: 1.000000
Training Epoch: 6 [43520/50000]	Loss: 4.4295	LR: 1.000000
Training Epoch: 6 [43776/50000]	Loss: 4.3742	LR: 1.000000
Training Epoch: 6 [44032/50000]	Loss: 4.3986	LR: 1.000000
Training Epoch: 6 [44288/50000]	Loss: 4.3953	LR: 1.000000
Training Epoch: 6 [44544/50000]	Loss: 4.4072	LR: 1.000000
Training Epoch: 6 [44800/50000]	Loss: 4.3776	LR: 1.000000
Training Epoch: 6 [45056/50000]	Loss: 4.3370	LR: 1.000000
Training Epoch: 6 [45312/50000]	Loss: 4.3782	LR: 1.000000
Training Epoch: 6 [45568/50000]	Loss: 4.3701	LR: 1.000000
Training Epoch: 6 [45824/50000]	Loss: 4.4142	LR: 1.000000
Training Epoch: 6 [46080/50000]	Loss: 4.3742	LR: 1.000000
Training Epoch: 6 [46336/50000]	Loss: 4.4576	LR: 1.000000
Training Epoch: 6 [46592/50000]	Loss: 4.3915	LR: 1.000000
Training Epoch: 6 [46848/50000]	Loss: 4.4544	LR: 1.000000
Training Epoch: 6 [47104/50000]	Loss: 4.3728	LR: 1.000000
Training Epoch: 6 [47360/50000]	Loss: 4.3654	LR: 1.000000
Training Epoch: 6 [47616/50000]	Loss: 4.3999	LR: 1.000000
Training Epoch: 6 [47872/50000]	Loss: 4.3910	LR: 1.000000
Training Epoch: 6 [48128/50000]	Loss: 4.3854	LR: 1.000000
Training Epoch: 6 [48384/50000]	Loss: 4.3395	LR: 1.000000
Training Epoch: 6 [48640/50000]	Loss: 4.3779	LR: 1.000000
Training Epoch: 6 [48896/50000]	Loss: 4.4466	LR: 1.000000
Training Epoch: 6 [49152/50000]	Loss: 4.3145	LR: 1.000000
Training Epoch: 6 [49408/50000]	Loss: 4.4809	LR: 1.000000
Training Epoch: 6 [49664/50000]	Loss: 4.3532	LR: 1.000000
Training Epoch: 6 [49920/50000]	Loss: 4.3687	LR: 1.000000
Training Epoch: 6 [50000/50000]	Loss: 4.3943	LR: 1.000000
epoch 6 training time consumed: 22.22s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |    9002 GB |    9001 GB |
|       from large pool |  400448 KB |    1770 MB |    8994 GB |    8993 GB |
|       from small pool |    3549 KB |       9 MB |       8 GB |       8 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |    9002 GB |    9001 GB |
|       from large pool |  400448 KB |    1770 MB |    8994 GB |    8993 GB |
|       from small pool |    3549 KB |       9 MB |       8 GB |       8 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |    5540 GB |    5540 GB |
|       from large pool |  244672 KB |  473024 KB |    5530 GB |    5530 GB |
|       from small pool |    2594 KB |    4843 KB |       9 GB |       9 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  384599    |  384377    |
|       from large pool |      36    |      77    |  185828    |  185792    |
|       from small pool |     186    |     224    |  198771    |  198585    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  384599    |  384377    |
|       from large pool |      36    |      77    |  185828    |  185792    |
|       from small pool |     186    |     224    |  198771    |  198585    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      31    |  195170    |  195150    |
|       from large pool |      10    |      11    |   76970    |   76960    |
|       from small pool |      10    |      21    |  118200    |  118190    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 6, Average loss: 0.0176, Accuracy: 0.0262, Time consumed:1.45s

Training Epoch: 7 [256/50000]	Loss: 4.3160	LR: 1.000000
Training Epoch: 7 [512/50000]	Loss: 4.4520	LR: 1.000000
Training Epoch: 7 [768/50000]	Loss: 4.4099	LR: 1.000000
Training Epoch: 7 [1024/50000]	Loss: 4.3877	LR: 1.000000
Training Epoch: 7 [1280/50000]	Loss: 4.3928	LR: 1.000000
Training Epoch: 7 [1536/50000]	Loss: 4.3651	LR: 1.000000
Training Epoch: 7 [1792/50000]	Loss: 4.3461	LR: 1.000000
Training Epoch: 7 [2048/50000]	Loss: 4.3027	LR: 1.000000
Training Epoch: 7 [2304/50000]	Loss: 4.3932	LR: 1.000000
Training Epoch: 7 [2560/50000]	Loss: 4.3817	LR: 1.000000
Training Epoch: 7 [2816/50000]	Loss: 4.4095	LR: 1.000000
Training Epoch: 7 [3072/50000]	Loss: 4.3664	LR: 1.000000
Training Epoch: 7 [3328/50000]	Loss: 4.4095	LR: 1.000000
Training Epoch: 7 [3584/50000]	Loss: 4.3936	LR: 1.000000
Training Epoch: 7 [3840/50000]	Loss: 4.3856	LR: 1.000000
Training Epoch: 7 [4096/50000]	Loss: 4.2970	LR: 1.000000
Training Epoch: 7 [4352/50000]	Loss: 4.3496	LR: 1.000000
Training Epoch: 7 [4608/50000]	Loss: 4.3784	LR: 1.000000
Training Epoch: 7 [4864/50000]	Loss: 4.4094	LR: 1.000000
Training Epoch: 7 [5120/50000]	Loss: 4.3920	LR: 1.000000
Training Epoch: 7 [5376/50000]	Loss: 4.3530	LR: 1.000000
Training Epoch: 7 [5632/50000]	Loss: 4.4399	LR: 1.000000
Training Epoch: 7 [5888/50000]	Loss: 4.3813	LR: 1.000000
Training Epoch: 7 [6144/50000]	Loss: 4.3794	LR: 1.000000
Training Epoch: 7 [6400/50000]	Loss: 4.4732	LR: 1.000000
Training Epoch: 7 [6656/50000]	Loss: 4.4128	LR: 1.000000
Training Epoch: 7 [6912/50000]	Loss: 4.3125	LR: 1.000000
Training Epoch: 7 [7168/50000]	Loss: 4.3594	LR: 1.000000
Training Epoch: 7 [7424/50000]	Loss: 4.3550	LR: 1.000000
Training Epoch: 7 [7680/50000]	Loss: 4.3567	LR: 1.000000
Training Epoch: 7 [7936/50000]	Loss: 4.3347	LR: 1.000000
Training Epoch: 7 [8192/50000]	Loss: 4.3274	LR: 1.000000
Training Epoch: 7 [8448/50000]	Loss: 4.3351	LR: 1.000000
Training Epoch: 7 [8704/50000]	Loss: 4.3656	LR: 1.000000
Training Epoch: 7 [8960/50000]	Loss: 4.4518	LR: 1.000000
Training Epoch: 7 [9216/50000]	Loss: 4.4231	LR: 1.000000
Training Epoch: 7 [9472/50000]	Loss: 4.3871	LR: 1.000000
Training Epoch: 7 [9728/50000]	Loss: 4.4219	LR: 1.000000
Training Epoch: 7 [9984/50000]	Loss: 4.3736	LR: 1.000000
Training Epoch: 7 [10240/50000]	Loss: 4.3413	LR: 1.000000
Training Epoch: 7 [10496/50000]	Loss: 4.3043	LR: 1.000000
Training Epoch: 7 [10752/50000]	Loss: 4.3952	LR: 1.000000
Training Epoch: 7 [11008/50000]	Loss: 4.3873	LR: 1.000000
Training Epoch: 7 [11264/50000]	Loss: 4.3403	LR: 1.000000
Training Epoch: 7 [11520/50000]	Loss: 4.3028	LR: 1.000000
Training Epoch: 7 [11776/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 7 [12032/50000]	Loss: 4.3590	LR: 1.000000
Training Epoch: 7 [12288/50000]	Loss: 4.4091	LR: 1.000000
Training Epoch: 7 [12544/50000]	Loss: 4.3950	LR: 1.000000
Training Epoch: 7 [12800/50000]	Loss: 4.3971	LR: 1.000000
Training Epoch: 7 [13056/50000]	Loss: 4.4113	LR: 1.000000
Training Epoch: 7 [13312/50000]	Loss: 4.3699	LR: 1.000000
Training Epoch: 7 [13568/50000]	Loss: 4.3797	LR: 1.000000
Training Epoch: 7 [13824/50000]	Loss: 4.3332	LR: 1.000000
Training Epoch: 7 [14080/50000]	Loss: 4.3149	LR: 1.000000
Training Epoch: 7 [14336/50000]	Loss: 4.3305	LR: 1.000000
Training Epoch: 7 [14592/50000]	Loss: 4.3161	LR: 1.000000
Training Epoch: 7 [14848/50000]	Loss: 4.3365	LR: 1.000000
Training Epoch: 7 [15104/50000]	Loss: 4.3060	LR: 1.000000
Training Epoch: 7 [15360/50000]	Loss: 4.3206	LR: 1.000000
Training Epoch: 7 [15616/50000]	Loss: 4.3933	LR: 1.000000
Training Epoch: 7 [15872/50000]	Loss: 4.3148	LR: 1.000000
Training Epoch: 7 [16128/50000]	Loss: 4.4389	LR: 1.000000
Training Epoch: 7 [16384/50000]	Loss: 4.3885	LR: 1.000000
Training Epoch: 7 [16640/50000]	Loss: 4.3481	LR: 1.000000
Training Epoch: 7 [16896/50000]	Loss: 4.4004	LR: 1.000000
Training Epoch: 7 [17152/50000]	Loss: 4.3546	LR: 1.000000
Training Epoch: 7 [17408/50000]	Loss: 4.2440	LR: 1.000000
Training Epoch: 7 [17664/50000]	Loss: 4.3508	LR: 1.000000
Training Epoch: 7 [17920/50000]	Loss: 4.4190	LR: 1.000000
Training Epoch: 7 [18176/50000]	Loss: 4.4577	LR: 1.000000
Training Epoch: 7 [18432/50000]	Loss: 4.3756	LR: 1.000000
Training Epoch: 7 [18688/50000]	Loss: 4.4208	LR: 1.000000
Training Epoch: 7 [18944/50000]	Loss: 4.3843	LR: 1.000000
Training Epoch: 7 [19200/50000]	Loss: 4.4320	LR: 1.000000
Training Epoch: 7 [19456/50000]	Loss: 4.4731	LR: 1.000000
Training Epoch: 7 [19712/50000]	Loss: 4.4045	LR: 1.000000
Training Epoch: 7 [19968/50000]	Loss: 4.4366	LR: 1.000000
Training Epoch: 7 [20224/50000]	Loss: 4.3510	LR: 1.000000
Training Epoch: 7 [20480/50000]	Loss: 4.4147	LR: 1.000000
Training Epoch: 7 [20736/50000]	Loss: 4.4503	LR: 1.000000
Training Epoch: 7 [20992/50000]	Loss: 4.3744	LR: 1.000000
Training Epoch: 7 [21248/50000]	Loss: 4.3304	LR: 1.000000
Training Epoch: 7 [21504/50000]	Loss: 4.4075	LR: 1.000000
Training Epoch: 7 [21760/50000]	Loss: 4.4064	LR: 1.000000
Training Epoch: 7 [22016/50000]	Loss: 4.3601	LR: 1.000000
Training Epoch: 7 [22272/50000]	Loss: 4.3546	LR: 1.000000
Training Epoch: 7 [22528/50000]	Loss: 4.3737	LR: 1.000000
Training Epoch: 7 [22784/50000]	Loss: 4.3158	LR: 1.000000
Training Epoch: 7 [23040/50000]	Loss: 4.3711	LR: 1.000000
Training Epoch: 7 [23296/50000]	Loss: 4.3734	LR: 1.000000
Training Epoch: 7 [23552/50000]	Loss: 4.3009	LR: 1.000000
Training Epoch: 7 [23808/50000]	Loss: 4.2826	LR: 1.000000
Training Epoch: 7 [24064/50000]	Loss: 4.2828	LR: 1.000000
Training Epoch: 7 [24320/50000]	Loss: 4.3036	LR: 1.000000
Training Epoch: 7 [24576/50000]	Loss: 4.3895	LR: 1.000000
Training Epoch: 7 [24832/50000]	Loss: 4.2700	LR: 1.000000
Training Epoch: 7 [25088/50000]	Loss: 4.3638	LR: 1.000000
Training Epoch: 7 [25344/50000]	Loss: 4.3579	LR: 1.000000
Training Epoch: 7 [25600/50000]	Loss: 4.3746	LR: 1.000000
Training Epoch: 7 [25856/50000]	Loss: 4.4483	LR: 1.000000
Training Epoch: 7 [26112/50000]	Loss: 4.3510	LR: 1.000000
Training Epoch: 7 [26368/50000]	Loss: 4.3845	LR: 1.000000
Training Epoch: 7 [26624/50000]	Loss: 4.3960	LR: 1.000000
Training Epoch: 7 [26880/50000]	Loss: 4.3634	LR: 1.000000
Training Epoch: 7 [27136/50000]	Loss: 4.3445	LR: 1.000000
Training Epoch: 7 [27392/50000]	Loss: 4.3324	LR: 1.000000
Training Epoch: 7 [27648/50000]	Loss: 4.2616	LR: 1.000000
Training Epoch: 7 [27904/50000]	Loss: 4.3000	LR: 1.000000
Training Epoch: 7 [28160/50000]	Loss: 4.2364	LR: 1.000000
Training Epoch: 7 [28416/50000]	Loss: 4.3182	LR: 1.000000
Training Epoch: 7 [28672/50000]	Loss: 4.3761	LR: 1.000000
Training Epoch: 7 [28928/50000]	Loss: 4.3653	LR: 1.000000
Training Epoch: 7 [29184/50000]	Loss: 4.3731	LR: 1.000000
Training Epoch: 7 [29440/50000]	Loss: 4.3534	LR: 1.000000
Training Epoch: 7 [29696/50000]	Loss: 4.3227	LR: 1.000000
Training Epoch: 7 [29952/50000]	Loss: 4.3785	LR: 1.000000
Training Epoch: 7 [30208/50000]	Loss: 4.3580	LR: 1.000000
Training Epoch: 7 [30464/50000]	Loss: 4.2938	LR: 1.000000
Training Epoch: 7 [30720/50000]	Loss: 4.3151	LR: 1.000000
Training Epoch: 7 [30976/50000]	Loss: 4.2839	LR: 1.000000
Training Epoch: 7 [31232/50000]	Loss: 4.3556	LR: 1.000000
Training Epoch: 7 [31488/50000]	Loss: 4.4375	LR: 1.000000
Training Epoch: 7 [31744/50000]	Loss: 4.4220	LR: 1.000000
Training Epoch: 7 [32000/50000]	Loss: 4.4178	LR: 1.000000
Training Epoch: 7 [32256/50000]	Loss: 4.3623	LR: 1.000000
Training Epoch: 7 [32512/50000]	Loss: 4.3696	LR: 1.000000
Training Epoch: 7 [32768/50000]	Loss: 4.3930	LR: 1.000000
Training Epoch: 7 [33024/50000]	Loss: 4.4088	LR: 1.000000
Training Epoch: 7 [33280/50000]	Loss: 4.3855	LR: 1.000000
Training Epoch: 7 [33536/50000]	Loss: 4.4184	LR: 1.000000
Training Epoch: 7 [33792/50000]	Loss: 4.4409	LR: 1.000000
Training Epoch: 7 [34048/50000]	Loss: 4.3312	LR: 1.000000
Training Epoch: 7 [34304/50000]	Loss: 4.3696	LR: 1.000000
Training Epoch: 7 [34560/50000]	Loss: 4.3421	LR: 1.000000
Training Epoch: 7 [34816/50000]	Loss: 4.4301	LR: 1.000000
Training Epoch: 7 [35072/50000]	Loss: 4.2939	LR: 1.000000
Training Epoch: 7 [35328/50000]	Loss: 4.3245	LR: 1.000000
Training Epoch: 7 [35584/50000]	Loss: 4.4466	LR: 1.000000
Training Epoch: 7 [35840/50000]	Loss: 4.3712	LR: 1.000000
Training Epoch: 7 [36096/50000]	Loss: 4.3377	LR: 1.000000
Training Epoch: 7 [36352/50000]	Loss: 4.4521	LR: 1.000000
Training Epoch: 7 [36608/50000]	Loss: 4.3433	LR: 1.000000
Training Epoch: 7 [36864/50000]	Loss: 4.3056	LR: 1.000000
Training Epoch: 7 [37120/50000]	Loss: 4.4164	LR: 1.000000
Training Epoch: 7 [37376/50000]	Loss: 4.3566	LR: 1.000000
Training Epoch: 7 [37632/50000]	Loss: 4.3517	LR: 1.000000
Training Epoch: 7 [37888/50000]	Loss: 4.3055	LR: 1.000000
Training Epoch: 7 [38144/50000]	Loss: 4.3484	LR: 1.000000
Training Epoch: 7 [38400/50000]	Loss: 4.3975	LR: 1.000000
Training Epoch: 7 [38656/50000]	Loss: 4.3969	LR: 1.000000
Training Epoch: 7 [38912/50000]	Loss: 4.3738	LR: 1.000000
Training Epoch: 7 [39168/50000]	Loss: 4.3875	LR: 1.000000
Training Epoch: 7 [39424/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 7 [39680/50000]	Loss: 4.3585	LR: 1.000000
Training Epoch: 7 [39936/50000]	Loss: 4.3564	LR: 1.000000
Training Epoch: 7 [40192/50000]	Loss: 4.4145	LR: 1.000000
Training Epoch: 7 [40448/50000]	Loss: 4.3979	LR: 1.000000
Training Epoch: 7 [40704/50000]	Loss: 4.3671	LR: 1.000000
Training Epoch: 7 [40960/50000]	Loss: 4.4187	LR: 1.000000
Training Epoch: 7 [41216/50000]	Loss: 4.2836	LR: 1.000000
Training Epoch: 7 [41472/50000]	Loss: 4.3495	LR: 1.000000
Training Epoch: 7 [41728/50000]	Loss: 4.3751	LR: 1.000000
Training Epoch: 7 [41984/50000]	Loss: 4.2825	LR: 1.000000
Training Epoch: 7 [42240/50000]	Loss: 4.3797	LR: 1.000000
Training Epoch: 7 [42496/50000]	Loss: 4.3662	LR: 1.000000
Training Epoch: 7 [42752/50000]	Loss: 4.2914	LR: 1.000000
Training Epoch: 7 [43008/50000]	Loss: 4.3929	LR: 1.000000
Training Epoch: 7 [43264/50000]	Loss: 4.2775	LR: 1.000000
Training Epoch: 7 [43520/50000]	Loss: 4.2875	LR: 1.000000
Training Epoch: 7 [43776/50000]	Loss: 4.5234	LR: 1.000000
Training Epoch: 7 [44032/50000]	Loss: 4.3064	LR: 1.000000
Training Epoch: 7 [44288/50000]	Loss: 4.3730	LR: 1.000000
Training Epoch: 7 [44544/50000]	Loss: 4.4586	LR: 1.000000
Training Epoch: 7 [44800/50000]	Loss: 4.4609	LR: 1.000000
Training Epoch: 7 [45056/50000]	Loss: 4.4021	LR: 1.000000
Training Epoch: 7 [45312/50000]	Loss: 4.4076	LR: 1.000000
Training Epoch: 7 [45568/50000]	Loss: 4.4018	LR: 1.000000
Training Epoch: 7 [45824/50000]	Loss: 4.3964	LR: 1.000000
Training Epoch: 7 [46080/50000]	Loss: 4.4734	LR: 1.000000
Training Epoch: 7 [46336/50000]	Loss: 4.3327	LR: 1.000000
Training Epoch: 7 [46592/50000]	Loss: 4.4114	LR: 1.000000
Training Epoch: 7 [46848/50000]	Loss: 4.4041	LR: 1.000000
Training Epoch: 7 [47104/50000]	Loss: 4.3432	LR: 1.000000
Training Epoch: 7 [47360/50000]	Loss: 4.3682	LR: 1.000000
Training Epoch: 7 [47616/50000]	Loss: 4.3884	LR: 1.000000
Training Epoch: 7 [47872/50000]	Loss: 4.3931	LR: 1.000000
Training Epoch: 7 [48128/50000]	Loss: 4.4153	LR: 1.000000
Training Epoch: 7 [48384/50000]	Loss: 4.3733	LR: 1.000000
Training Epoch: 7 [48640/50000]	Loss: 4.2925	LR: 1.000000
Training Epoch: 7 [48896/50000]	Loss: 4.3964	LR: 1.000000
Training Epoch: 7 [49152/50000]	Loss: 4.3860	LR: 1.000000
Training Epoch: 7 [49408/50000]	Loss: 4.3309	LR: 1.000000
Training Epoch: 7 [49664/50000]	Loss: 4.4693	LR: 1.000000
Training Epoch: 7 [49920/50000]	Loss: 4.4161	LR: 1.000000
Training Epoch: 7 [50000/50000]	Loss: 4.4285	LR: 1.000000
epoch 7 training time consumed: 22.08s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   10502 GB |   10501 GB |
|       from large pool |  400448 KB |    1770 MB |   10492 GB |   10492 GB |
|       from small pool |    3549 KB |       9 MB |       9 GB |       9 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   10502 GB |   10501 GB |
|       from large pool |  400448 KB |    1770 MB |   10492 GB |   10492 GB |
|       from small pool |    3549 KB |       9 MB |       9 GB |       9 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |    6463 GB |    6463 GB |
|       from large pool |  244672 KB |  473024 KB |    6452 GB |    6452 GB |
|       from small pool |    4642 KB |    4843 KB |      10 GB |      10 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  448623    |  448401    |
|       from large pool |      36    |      77    |  216787    |  216751    |
|       from small pool |     186    |     224    |  231836    |  231650    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  448623    |  448401    |
|       from large pool |      36    |      77    |  216787    |  216751    |
|       from small pool |     186    |     224    |  231836    |  231650    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      31    |  227641    |  227618    |
|       from large pool |      10    |      11    |   89793    |   89783    |
|       from small pool |      13    |      21    |  137848    |  137835    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 7, Average loss: 0.0175, Accuracy: 0.0220, Time consumed:1.47s

Training Epoch: 8 [256/50000]	Loss: 4.4389	LR: 1.000000
Training Epoch: 8 [512/50000]	Loss: 4.4018	LR: 1.000000
Training Epoch: 8 [768/50000]	Loss: 4.3711	LR: 1.000000
Training Epoch: 8 [1024/50000]	Loss: 4.4474	LR: 1.000000
Training Epoch: 8 [1280/50000]	Loss: 4.4085	LR: 1.000000
Training Epoch: 8 [1536/50000]	Loss: 4.3660	LR: 1.000000
Training Epoch: 8 [1792/50000]	Loss: 4.4110	LR: 1.000000
Training Epoch: 8 [2048/50000]	Loss: 4.3984	LR: 1.000000
Training Epoch: 8 [2304/50000]	Loss: 4.3809	LR: 1.000000
Training Epoch: 8 [2560/50000]	Loss: 4.4510	LR: 1.000000
Training Epoch: 8 [2816/50000]	Loss: 4.3930	LR: 1.000000
Training Epoch: 8 [3072/50000]	Loss: 4.3446	LR: 1.000000
Training Epoch: 8 [3328/50000]	Loss: 4.3205	LR: 1.000000
Training Epoch: 8 [3584/50000]	Loss: 4.3486	LR: 1.000000
Training Epoch: 8 [3840/50000]	Loss: 4.5256	LR: 1.000000
Training Epoch: 8 [4096/50000]	Loss: 4.4566	LR: 1.000000
Training Epoch: 8 [4352/50000]	Loss: 4.4956	LR: 1.000000
Training Epoch: 8 [4608/50000]	Loss: 4.4814	LR: 1.000000
Training Epoch: 8 [4864/50000]	Loss: 4.4433	LR: 1.000000
Training Epoch: 8 [5120/50000]	Loss: 4.4797	LR: 1.000000
Training Epoch: 8 [5376/50000]	Loss: 4.4481	LR: 1.000000
Training Epoch: 8 [5632/50000]	Loss: 4.4418	LR: 1.000000
Training Epoch: 8 [5888/50000]	Loss: 4.3876	LR: 1.000000
Training Epoch: 8 [6144/50000]	Loss: 4.5250	LR: 1.000000
Training Epoch: 8 [6400/50000]	Loss: 4.3089	LR: 1.000000
Training Epoch: 8 [6656/50000]	Loss: 4.3872	LR: 1.000000
Training Epoch: 8 [6912/50000]	Loss: 4.3548	LR: 1.000000
Training Epoch: 8 [7168/50000]	Loss: 4.4314	LR: 1.000000
Training Epoch: 8 [7424/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 8 [7680/50000]	Loss: 4.3183	LR: 1.000000
Training Epoch: 8 [7936/50000]	Loss: 4.3384	LR: 1.000000
Training Epoch: 8 [8192/50000]	Loss: 4.3293	LR: 1.000000
Training Epoch: 8 [8448/50000]	Loss: 4.3940	LR: 1.000000
Training Epoch: 8 [8704/50000]	Loss: 4.3235	LR: 1.000000
Training Epoch: 8 [8960/50000]	Loss: 4.4208	LR: 1.000000
Training Epoch: 8 [9216/50000]	Loss: 4.3781	LR: 1.000000
Training Epoch: 8 [9472/50000]	Loss: 4.4526	LR: 1.000000
Training Epoch: 8 [9728/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 8 [9984/50000]	Loss: 4.3489	LR: 1.000000
Training Epoch: 8 [10240/50000]	Loss: 4.3802	LR: 1.000000
Training Epoch: 8 [10496/50000]	Loss: 4.3353	LR: 1.000000
Training Epoch: 8 [10752/50000]	Loss: 4.3282	LR: 1.000000
Training Epoch: 8 [11008/50000]	Loss: 4.3124	LR: 1.000000
Training Epoch: 8 [11264/50000]	Loss: 4.3684	LR: 1.000000
Training Epoch: 8 [11520/50000]	Loss: 4.3593	LR: 1.000000
Training Epoch: 8 [11776/50000]	Loss: 4.3942	LR: 1.000000
Training Epoch: 8 [12032/50000]	Loss: 4.3966	LR: 1.000000
Training Epoch: 8 [12288/50000]	Loss: 4.3624	LR: 1.000000
Training Epoch: 8 [12544/50000]	Loss: 4.3039	LR: 1.000000
Training Epoch: 8 [12800/50000]	Loss: 4.3471	LR: 1.000000
Training Epoch: 8 [13056/50000]	Loss: 4.2738	LR: 1.000000
Training Epoch: 8 [13312/50000]	Loss: 4.3597	LR: 1.000000
Training Epoch: 8 [13568/50000]	Loss: 4.4138	LR: 1.000000
Training Epoch: 8 [13824/50000]	Loss: 4.4091	LR: 1.000000
Training Epoch: 8 [14080/50000]	Loss: 4.4184	LR: 1.000000
Training Epoch: 8 [14336/50000]	Loss: 4.3428	LR: 1.000000
Training Epoch: 8 [14592/50000]	Loss: 4.4373	LR: 1.000000
Training Epoch: 8 [14848/50000]	Loss: 4.3647	LR: 1.000000
Training Epoch: 8 [15104/50000]	Loss: 4.3767	LR: 1.000000
Training Epoch: 8 [15360/50000]	Loss: 4.2670	LR: 1.000000
Training Epoch: 8 [15616/50000]	Loss: 4.4040	LR: 1.000000
Training Epoch: 8 [15872/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 8 [16128/50000]	Loss: 4.3756	LR: 1.000000
Training Epoch: 8 [16384/50000]	Loss: 4.4327	LR: 1.000000
Training Epoch: 8 [16640/50000]	Loss: 4.4414	LR: 1.000000
Training Epoch: 8 [16896/50000]	Loss: 4.4063	LR: 1.000000
Training Epoch: 8 [17152/50000]	Loss: 4.4138	LR: 1.000000
Training Epoch: 8 [17408/50000]	Loss: 4.3486	LR: 1.000000
Training Epoch: 8 [17664/50000]	Loss: 4.4121	LR: 1.000000
Training Epoch: 8 [17920/50000]	Loss: 4.3081	LR: 1.000000
Training Epoch: 8 [18176/50000]	Loss: 4.3174	LR: 1.000000
Training Epoch: 8 [18432/50000]	Loss: 4.3626	LR: 1.000000
Training Epoch: 8 [18688/50000]	Loss: 4.3213	LR: 1.000000
Training Epoch: 8 [18944/50000]	Loss: 4.3011	LR: 1.000000
Training Epoch: 8 [19200/50000]	Loss: 4.2961	LR: 1.000000
Training Epoch: 8 [19456/50000]	Loss: 4.3516	LR: 1.000000
Training Epoch: 8 [19712/50000]	Loss: 4.3724	LR: 1.000000
Training Epoch: 8 [19968/50000]	Loss: 4.3286	LR: 1.000000
Training Epoch: 8 [20224/50000]	Loss: 4.3431	LR: 1.000000
Training Epoch: 8 [20480/50000]	Loss: 4.3577	LR: 1.000000
Training Epoch: 8 [20736/50000]	Loss: 4.4302	LR: 1.000000
Training Epoch: 8 [20992/50000]	Loss: 4.4374	LR: 1.000000
Training Epoch: 8 [21248/50000]	Loss: 4.4540	LR: 1.000000
Training Epoch: 8 [21504/50000]	Loss: 4.3707	LR: 1.000000
Training Epoch: 8 [21760/50000]	Loss: 4.3695	LR: 1.000000
Training Epoch: 8 [22016/50000]	Loss: 4.3969	LR: 1.000000
Training Epoch: 8 [22272/50000]	Loss: 4.4065	LR: 1.000000
Training Epoch: 8 [22528/50000]	Loss: 4.3872	LR: 1.000000
Training Epoch: 8 [22784/50000]	Loss: 4.3234	LR: 1.000000
Training Epoch: 8 [23040/50000]	Loss: 4.3846	LR: 1.000000
Training Epoch: 8 [23296/50000]	Loss: 4.3867	LR: 1.000000
Training Epoch: 8 [23552/50000]	Loss: 4.3535	LR: 1.000000
Training Epoch: 8 [23808/50000]	Loss: 4.2717	LR: 1.000000
Training Epoch: 8 [24064/50000]	Loss: 4.3354	LR: 1.000000
Training Epoch: 8 [24320/50000]	Loss: 4.3816	LR: 1.000000
Training Epoch: 8 [24576/50000]	Loss: 4.4089	LR: 1.000000
Training Epoch: 8 [24832/50000]	Loss: 4.3908	LR: 1.000000
Training Epoch: 8 [25088/50000]	Loss: 4.4193	LR: 1.000000
Training Epoch: 8 [25344/50000]	Loss: 4.3892	LR: 1.000000
Training Epoch: 8 [25600/50000]	Loss: 4.3724	LR: 1.000000
Training Epoch: 8 [25856/50000]	Loss: 4.4050	LR: 1.000000
Training Epoch: 8 [26112/50000]	Loss: 4.3386	LR: 1.000000
Training Epoch: 8 [26368/50000]	Loss: 4.3204	LR: 1.000000
Training Epoch: 8 [26624/50000]	Loss: 4.3531	LR: 1.000000
Training Epoch: 8 [26880/50000]	Loss: 4.3933	LR: 1.000000
Training Epoch: 8 [27136/50000]	Loss: 4.3776	LR: 1.000000
Training Epoch: 8 [27392/50000]	Loss: 4.4037	LR: 1.000000
Training Epoch: 8 [27648/50000]	Loss: 4.2907	LR: 1.000000
Training Epoch: 8 [27904/50000]	Loss: 4.3663	LR: 1.000000
Training Epoch: 8 [28160/50000]	Loss: 4.3175	LR: 1.000000
Training Epoch: 8 [28416/50000]	Loss: 4.4193	LR: 1.000000
Training Epoch: 8 [28672/50000]	Loss: 4.3606	LR: 1.000000
Training Epoch: 8 [28928/50000]	Loss: 4.4171	LR: 1.000000
Training Epoch: 8 [29184/50000]	Loss: 4.3368	LR: 1.000000
Training Epoch: 8 [29440/50000]	Loss: 4.4054	LR: 1.000000
Training Epoch: 8 [29696/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 8 [29952/50000]	Loss: 4.3866	LR: 1.000000
Training Epoch: 8 [30208/50000]	Loss: 4.2893	LR: 1.000000
Training Epoch: 8 [30464/50000]	Loss: 4.3237	LR: 1.000000
Training Epoch: 8 [30720/50000]	Loss: 4.4112	LR: 1.000000
Training Epoch: 8 [30976/50000]	Loss: 4.2957	LR: 1.000000
Training Epoch: 8 [31232/50000]	Loss: 4.2797	LR: 1.000000
Training Epoch: 8 [31488/50000]	Loss: 4.3526	LR: 1.000000
Training Epoch: 8 [31744/50000]	Loss: 4.3203	LR: 1.000000
Training Epoch: 8 [32000/50000]	Loss: 4.3504	LR: 1.000000
Training Epoch: 8 [32256/50000]	Loss: 4.3876	LR: 1.000000
Training Epoch: 8 [32512/50000]	Loss: 4.3960	LR: 1.000000
Training Epoch: 8 [32768/50000]	Loss: 4.4210	LR: 1.000000
Training Epoch: 8 [33024/50000]	Loss: 4.3347	LR: 1.000000
Training Epoch: 8 [33280/50000]	Loss: 4.3768	LR: 1.000000
Training Epoch: 8 [33536/50000]	Loss: 4.3674	LR: 1.000000
Training Epoch: 8 [33792/50000]	Loss: 4.3847	LR: 1.000000
Training Epoch: 8 [34048/50000]	Loss: 4.4591	LR: 1.000000
Training Epoch: 8 [34304/50000]	Loss: 4.3288	LR: 1.000000
Training Epoch: 8 [34560/50000]	Loss: 4.3799	LR: 1.000000
Training Epoch: 8 [34816/50000]	Loss: 4.3553	LR: 1.000000
Training Epoch: 8 [35072/50000]	Loss: 4.4043	LR: 1.000000
Training Epoch: 8 [35328/50000]	Loss: 4.4623	LR: 1.000000
Training Epoch: 8 [35584/50000]	Loss: 4.3611	LR: 1.000000
Training Epoch: 8 [35840/50000]	Loss: 4.3800	LR: 1.000000
Training Epoch: 8 [36096/50000]	Loss: 4.3536	LR: 1.000000
Training Epoch: 8 [36352/50000]	Loss: 4.3595	LR: 1.000000
Training Epoch: 8 [36608/50000]	Loss: 4.5086	LR: 1.000000
Training Epoch: 8 [36864/50000]	Loss: 4.3743	LR: 1.000000
Training Epoch: 8 [37120/50000]	Loss: 4.4514	LR: 1.000000
Training Epoch: 8 [37376/50000]	Loss: 4.4437	LR: 1.000000
Training Epoch: 8 [37632/50000]	Loss: 4.3542	LR: 1.000000
Training Epoch: 8 [37888/50000]	Loss: 4.3352	LR: 1.000000
Training Epoch: 8 [38144/50000]	Loss: 4.4574	LR: 1.000000
Training Epoch: 8 [38400/50000]	Loss: 4.3796	LR: 1.000000
Training Epoch: 8 [38656/50000]	Loss: 4.4178	LR: 1.000000
Training Epoch: 8 [38912/50000]	Loss: 4.3643	LR: 1.000000
Training Epoch: 8 [39168/50000]	Loss: 4.4100	LR: 1.000000
Training Epoch: 8 [39424/50000]	Loss: 4.4853	LR: 1.000000
Training Epoch: 8 [39680/50000]	Loss: 4.4022	LR: 1.000000
Training Epoch: 8 [39936/50000]	Loss: 4.3831	LR: 1.000000
Training Epoch: 8 [40192/50000]	Loss: 4.3863	LR: 1.000000
Training Epoch: 8 [40448/50000]	Loss: 4.3026	LR: 1.000000
Training Epoch: 8 [40704/50000]	Loss: 4.4812	LR: 1.000000
Training Epoch: 8 [40960/50000]	Loss: 4.4013	LR: 1.000000
Training Epoch: 8 [41216/50000]	Loss: 4.4292	LR: 1.000000
Training Epoch: 8 [41472/50000]	Loss: 4.3648	LR: 1.000000
Training Epoch: 8 [41728/50000]	Loss: 4.4299	LR: 1.000000
Training Epoch: 8 [41984/50000]	Loss: 4.4473	LR: 1.000000
Training Epoch: 8 [42240/50000]	Loss: 4.3243	LR: 1.000000
Training Epoch: 8 [42496/50000]	Loss: 4.3738	LR: 1.000000
Training Epoch: 8 [42752/50000]	Loss: 4.3577	LR: 1.000000
Training Epoch: 8 [43008/50000]	Loss: 4.4074	LR: 1.000000
Training Epoch: 8 [43264/50000]	Loss: 4.3289	LR: 1.000000
Training Epoch: 8 [43520/50000]	Loss: 4.3969	LR: 1.000000
Training Epoch: 8 [43776/50000]	Loss: 4.3100	LR: 1.000000
Training Epoch: 8 [44032/50000]	Loss: 4.4045	LR: 1.000000
Training Epoch: 8 [44288/50000]	Loss: 4.3444	LR: 1.000000
Training Epoch: 8 [44544/50000]	Loss: 4.3616	LR: 1.000000
Training Epoch: 8 [44800/50000]	Loss: 4.3651	LR: 1.000000
Training Epoch: 8 [45056/50000]	Loss: 4.2733	LR: 1.000000
Training Epoch: 8 [45312/50000]	Loss: 4.3846	LR: 1.000000
Training Epoch: 8 [45568/50000]	Loss: 4.3929	LR: 1.000000
Training Epoch: 8 [45824/50000]	Loss: 4.3106	LR: 1.000000
Training Epoch: 8 [46080/50000]	Loss: 4.3873	LR: 1.000000
Training Epoch: 8 [46336/50000]	Loss: 4.3733	LR: 1.000000
Training Epoch: 8 [46592/50000]	Loss: 4.3065	LR: 1.000000
Training Epoch: 8 [46848/50000]	Loss: 4.3965	LR: 1.000000
Training Epoch: 8 [47104/50000]	Loss: 4.3373	LR: 1.000000
Training Epoch: 8 [47360/50000]	Loss: 4.3790	LR: 1.000000
Training Epoch: 8 [47616/50000]	Loss: 4.3074	LR: 1.000000
Training Epoch: 8 [47872/50000]	Loss: 4.3715	LR: 1.000000
Training Epoch: 8 [48128/50000]	Loss: 4.4070	LR: 1.000000
Training Epoch: 8 [48384/50000]	Loss: 4.4300	LR: 1.000000
Training Epoch: 8 [48640/50000]	Loss: 4.3881	LR: 1.000000
Training Epoch: 8 [48896/50000]	Loss: 4.4273	LR: 1.000000
Training Epoch: 8 [49152/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 8 [49408/50000]	Loss: 4.4066	LR: 1.000000
Training Epoch: 8 [49664/50000]	Loss: 4.4235	LR: 1.000000
Training Epoch: 8 [49920/50000]	Loss: 4.4295	LR: 1.000000
Training Epoch: 8 [50000/50000]	Loss: 4.3440	LR: 1.000000
epoch 8 training time consumed: 22.08s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   12002 GB |   12002 GB |
|       from large pool |  400448 KB |    1770 MB |   11991 GB |   11991 GB |
|       from small pool |    3549 KB |       9 MB |      10 GB |      10 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   12002 GB |   12002 GB |
|       from large pool |  400448 KB |    1770 MB |   11991 GB |   11991 GB |
|       from small pool |    3549 KB |       9 MB |      10 GB |      10 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |    7386 GB |    7386 GB |
|       from large pool |  244672 KB |  473024 KB |    7373 GB |    7373 GB |
|       from small pool |    2594 KB |    4843 KB |      12 GB |      12 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  512647    |  512425    |
|       from large pool |      36    |      77    |  247746    |  247710    |
|       from small pool |     186    |     224    |  264901    |  264715    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  512647    |  512425    |
|       from large pool |      36    |      77    |  247746    |  247710    |
|       from small pool |     186    |     224    |  264901    |  264715    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      31    |  260286    |  260266    |
|       from large pool |      10    |      11    |  102616    |  102606    |
|       from small pool |      10    |      21    |  157670    |  157660    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 8, Average loss: 0.0176, Accuracy: 0.0269, Time consumed:1.44s

Training Epoch: 9 [256/50000]	Loss: 4.4348	LR: 1.000000
Training Epoch: 9 [512/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 9 [768/50000]	Loss: 4.4229	LR: 1.000000
Training Epoch: 9 [1024/50000]	Loss: 4.4205	LR: 1.000000
Training Epoch: 9 [1280/50000]	Loss: 4.2637	LR: 1.000000
Training Epoch: 9 [1536/50000]	Loss: 4.4028	LR: 1.000000
Training Epoch: 9 [1792/50000]	Loss: 4.3691	LR: 1.000000
Training Epoch: 9 [2048/50000]	Loss: 4.4004	LR: 1.000000
Training Epoch: 9 [2304/50000]	Loss: 4.4432	LR: 1.000000
Training Epoch: 9 [2560/50000]	Loss: 4.4211	LR: 1.000000
Training Epoch: 9 [2816/50000]	Loss: 4.3894	LR: 1.000000
Training Epoch: 9 [3072/50000]	Loss: 4.4684	LR: 1.000000
Training Epoch: 9 [3328/50000]	Loss: 4.3101	LR: 1.000000
Training Epoch: 9 [3584/50000]	Loss: 4.3615	LR: 1.000000
Training Epoch: 9 [3840/50000]	Loss: 4.3705	LR: 1.000000
Training Epoch: 9 [4096/50000]	Loss: 4.3537	LR: 1.000000
Training Epoch: 9 [4352/50000]	Loss: 4.4100	LR: 1.000000
Training Epoch: 9 [4608/50000]	Loss: 4.3301	LR: 1.000000
Training Epoch: 9 [4864/50000]	Loss: 4.3998	LR: 1.000000
Training Epoch: 9 [5120/50000]	Loss: 4.5019	LR: 1.000000
Training Epoch: 9 [5376/50000]	Loss: 4.4158	LR: 1.000000
Training Epoch: 9 [5632/50000]	Loss: 4.4547	LR: 1.000000
Training Epoch: 9 [5888/50000]	Loss: 4.4360	LR: 1.000000
Training Epoch: 9 [6144/50000]	Loss: 4.3258	LR: 1.000000
Training Epoch: 9 [6400/50000]	Loss: 4.4463	LR: 1.000000
Training Epoch: 9 [6656/50000]	Loss: 4.4057	LR: 1.000000
Training Epoch: 9 [6912/50000]	Loss: 4.4260	LR: 1.000000
Training Epoch: 9 [7168/50000]	Loss: 4.3747	LR: 1.000000
Training Epoch: 9 [7424/50000]	Loss: 4.4077	LR: 1.000000
Training Epoch: 9 [7680/50000]	Loss: 4.4254	LR: 1.000000
Training Epoch: 9 [7936/50000]	Loss: 4.3740	LR: 1.000000
Training Epoch: 9 [8192/50000]	Loss: 4.3870	LR: 1.000000
Training Epoch: 9 [8448/50000]	Loss: 4.3822	LR: 1.000000
Training Epoch: 9 [8704/50000]	Loss: 4.2804	LR: 1.000000
Training Epoch: 9 [8960/50000]	Loss: 4.3868	LR: 1.000000
Training Epoch: 9 [9216/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 9 [9472/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 9 [9728/50000]	Loss: 4.3250	LR: 1.000000
Training Epoch: 9 [9984/50000]	Loss: 4.3409	LR: 1.000000
Training Epoch: 9 [10240/50000]	Loss: 4.4204	LR: 1.000000
Training Epoch: 9 [10496/50000]	Loss: 4.2647	LR: 1.000000
Training Epoch: 9 [10752/50000]	Loss: 4.4277	LR: 1.000000
Training Epoch: 9 [11008/50000]	Loss: 4.2392	LR: 1.000000
Training Epoch: 9 [11264/50000]	Loss: 4.3573	LR: 1.000000
Training Epoch: 9 [11520/50000]	Loss: 4.4424	LR: 1.000000
Training Epoch: 9 [11776/50000]	Loss: 4.4289	LR: 1.000000
Training Epoch: 9 [12032/50000]	Loss: 4.3623	LR: 1.000000
Training Epoch: 9 [12288/50000]	Loss: 4.3923	LR: 1.000000
Training Epoch: 9 [12544/50000]	Loss: 4.3532	LR: 1.000000
Training Epoch: 9 [12800/50000]	Loss: 4.4492	LR: 1.000000
Training Epoch: 9 [13056/50000]	Loss: 4.4277	LR: 1.000000
Training Epoch: 9 [13312/50000]	Loss: 4.3697	LR: 1.000000
Training Epoch: 9 [13568/50000]	Loss: 4.4517	LR: 1.000000
Training Epoch: 9 [13824/50000]	Loss: 4.4004	LR: 1.000000
Training Epoch: 9 [14080/50000]	Loss: 4.3744	LR: 1.000000
Training Epoch: 9 [14336/50000]	Loss: 4.3423	LR: 1.000000
Training Epoch: 9 [14592/50000]	Loss: 4.3006	LR: 1.000000
Training Epoch: 9 [14848/50000]	Loss: 4.2923	LR: 1.000000
Training Epoch: 9 [15104/50000]	Loss: 4.4268	LR: 1.000000
Training Epoch: 9 [15360/50000]	Loss: 4.3490	LR: 1.000000
Training Epoch: 9 [15616/50000]	Loss: 4.3751	LR: 1.000000
Training Epoch: 9 [15872/50000]	Loss: 4.4778	LR: 1.000000
Training Epoch: 9 [16128/50000]	Loss: 4.4394	LR: 1.000000
Training Epoch: 9 [16384/50000]	Loss: 4.4156	LR: 1.000000
Training Epoch: 9 [16640/50000]	Loss: 4.4823	LR: 1.000000
Training Epoch: 9 [16896/50000]	Loss: 4.4023	LR: 1.000000
Training Epoch: 9 [17152/50000]	Loss: 4.3916	LR: 1.000000
Training Epoch: 9 [17408/50000]	Loss: 4.5021	LR: 1.000000
Training Epoch: 9 [17664/50000]	Loss: 4.3699	LR: 1.000000
Training Epoch: 9 [17920/50000]	Loss: 4.3896	LR: 1.000000
Training Epoch: 9 [18176/50000]	Loss: 4.3390	LR: 1.000000
Training Epoch: 9 [18432/50000]	Loss: 4.4516	LR: 1.000000
Training Epoch: 9 [18688/50000]	Loss: 4.3473	LR: 1.000000
Training Epoch: 9 [18944/50000]	Loss: 4.3746	LR: 1.000000
Training Epoch: 9 [19200/50000]	Loss: 4.4373	LR: 1.000000
Training Epoch: 9 [19456/50000]	Loss: 4.4782	LR: 1.000000
Training Epoch: 9 [19712/50000]	Loss: 4.4174	LR: 1.000000
Training Epoch: 9 [19968/50000]	Loss: 4.4176	LR: 1.000000
Training Epoch: 9 [20224/50000]	Loss: 4.4349	LR: 1.000000
Training Epoch: 9 [20480/50000]	Loss: 4.3798	LR: 1.000000
Training Epoch: 9 [20736/50000]	Loss: 4.3670	LR: 1.000000
Training Epoch: 9 [20992/50000]	Loss: 4.3449	LR: 1.000000
Training Epoch: 9 [21248/50000]	Loss: 4.3606	LR: 1.000000
Training Epoch: 9 [21504/50000]	Loss: 4.3009	LR: 1.000000
Training Epoch: 9 [21760/50000]	Loss: 4.4543	LR: 1.000000
Training Epoch: 9 [22016/50000]	Loss: 4.3762	LR: 1.000000
Training Epoch: 9 [22272/50000]	Loss: 4.3382	LR: 1.000000
Training Epoch: 9 [22528/50000]	Loss: 4.4488	LR: 1.000000
Training Epoch: 9 [22784/50000]	Loss: 4.4646	LR: 1.000000
Training Epoch: 9 [23040/50000]	Loss: 4.3637	LR: 1.000000
Training Epoch: 9 [23296/50000]	Loss: 4.3302	LR: 1.000000
Training Epoch: 9 [23552/50000]	Loss: 4.3543	LR: 1.000000
Training Epoch: 9 [23808/50000]	Loss: 4.4869	LR: 1.000000
Training Epoch: 9 [24064/50000]	Loss: 4.3878	LR: 1.000000
Training Epoch: 9 [24320/50000]	Loss: 4.3653	LR: 1.000000
Training Epoch: 9 [24576/50000]	Loss: 4.3978	LR: 1.000000
Training Epoch: 9 [24832/50000]	Loss: 4.4746	LR: 1.000000
Training Epoch: 9 [25088/50000]	Loss: 4.3933	LR: 1.000000
Training Epoch: 9 [25344/50000]	Loss: 4.4757	LR: 1.000000
Training Epoch: 9 [25600/50000]	Loss: 4.3353	LR: 1.000000
Training Epoch: 9 [25856/50000]	Loss: 4.4285	LR: 1.000000
Training Epoch: 9 [26112/50000]	Loss: 4.4296	LR: 1.000000
Training Epoch: 9 [26368/50000]	Loss: 4.3974	LR: 1.000000
Training Epoch: 9 [26624/50000]	Loss: 4.3904	LR: 1.000000
Training Epoch: 9 [26880/50000]	Loss: 4.3760	LR: 1.000000
Training Epoch: 9 [27136/50000]	Loss: 4.3508	LR: 1.000000
Training Epoch: 9 [27392/50000]	Loss: 4.3312	LR: 1.000000
Training Epoch: 9 [27648/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 9 [27904/50000]	Loss: 4.3498	LR: 1.000000
Training Epoch: 9 [28160/50000]	Loss: 4.3137	LR: 1.000000
Training Epoch: 9 [28416/50000]	Loss: 4.3920	LR: 1.000000
Training Epoch: 9 [28672/50000]	Loss: 4.4382	LR: 1.000000
Training Epoch: 9 [28928/50000]	Loss: 4.3791	LR: 1.000000
Training Epoch: 9 [29184/50000]	Loss: 4.3217	LR: 1.000000
Training Epoch: 9 [29440/50000]	Loss: 4.3331	LR: 1.000000
Training Epoch: 9 [29696/50000]	Loss: 4.3785	LR: 1.000000
Training Epoch: 9 [29952/50000]	Loss: 4.4268	LR: 1.000000
Training Epoch: 9 [30208/50000]	Loss: 4.4817	LR: 1.000000
Training Epoch: 9 [30464/50000]	Loss: 4.3783	LR: 1.000000
Training Epoch: 9 [30720/50000]	Loss: 4.4013	LR: 1.000000
Training Epoch: 9 [30976/50000]	Loss: 4.4046	LR: 1.000000
Training Epoch: 9 [31232/50000]	Loss: 4.4173	LR: 1.000000
Training Epoch: 9 [31488/50000]	Loss: 4.4675	LR: 1.000000
Training Epoch: 9 [31744/50000]	Loss: 4.3786	LR: 1.000000
Training Epoch: 9 [32000/50000]	Loss: 4.4357	LR: 1.000000
Training Epoch: 9 [32256/50000]	Loss: 4.3869	LR: 1.000000
Training Epoch: 9 [32512/50000]	Loss: 4.3507	LR: 1.000000
Training Epoch: 9 [32768/50000]	Loss: 4.3596	LR: 1.000000
Training Epoch: 9 [33024/50000]	Loss: 4.3751	LR: 1.000000
Training Epoch: 9 [33280/50000]	Loss: 4.3416	LR: 1.000000
Training Epoch: 9 [33536/50000]	Loss: 4.4412	LR: 1.000000
Training Epoch: 9 [33792/50000]	Loss: 4.3978	LR: 1.000000
Training Epoch: 9 [34048/50000]	Loss: 4.3105	LR: 1.000000
Training Epoch: 9 [34304/50000]	Loss: 4.3310	LR: 1.000000
Training Epoch: 9 [34560/50000]	Loss: 4.3582	LR: 1.000000
Training Epoch: 9 [34816/50000]	Loss: 4.3405	LR: 1.000000
Training Epoch: 9 [35072/50000]	Loss: 4.2929	LR: 1.000000
Training Epoch: 9 [35328/50000]	Loss: 4.2638	LR: 1.000000
Training Epoch: 9 [35584/50000]	Loss: 4.2378	LR: 1.000000
Training Epoch: 9 [35840/50000]	Loss: 4.3862	LR: 1.000000
Training Epoch: 9 [36096/50000]	Loss: 4.4019	LR: 1.000000
Training Epoch: 9 [36352/50000]	Loss: 4.3598	LR: 1.000000
Training Epoch: 9 [36608/50000]	Loss: 4.3912	LR: 1.000000
Training Epoch: 9 [36864/50000]	Loss: 4.3748	LR: 1.000000
Training Epoch: 9 [37120/50000]	Loss: 4.4426	LR: 1.000000
Training Epoch: 9 [37376/50000]	Loss: 4.3609	LR: 1.000000
Training Epoch: 9 [37632/50000]	Loss: 4.3926	LR: 1.000000
Training Epoch: 9 [37888/50000]	Loss: 4.3611	LR: 1.000000
Training Epoch: 9 [38144/50000]	Loss: 4.4365	LR: 1.000000
Training Epoch: 9 [38400/50000]	Loss: 4.2878	LR: 1.000000
Training Epoch: 9 [38656/50000]	Loss: 4.3338	LR: 1.000000
Training Epoch: 9 [38912/50000]	Loss: 4.4823	LR: 1.000000
Training Epoch: 9 [39168/50000]	Loss: 4.3413	LR: 1.000000
Training Epoch: 9 [39424/50000]	Loss: 4.3992	LR: 1.000000
Training Epoch: 9 [39680/50000]	Loss: 4.4374	LR: 1.000000
Training Epoch: 9 [39936/50000]	Loss: 4.3972	LR: 1.000000
Training Epoch: 9 [40192/50000]	Loss: 4.3888	LR: 1.000000
Training Epoch: 9 [40448/50000]	Loss: 4.3800	LR: 1.000000
Training Epoch: 9 [40704/50000]	Loss: 4.4648	LR: 1.000000
Training Epoch: 9 [40960/50000]	Loss: 4.3955	LR: 1.000000
Training Epoch: 9 [41216/50000]	Loss: 4.4348	LR: 1.000000
Training Epoch: 9 [41472/50000]	Loss: 4.3141	LR: 1.000000
Training Epoch: 9 [41728/50000]	Loss: 4.4169	LR: 1.000000
Training Epoch: 9 [41984/50000]	Loss: 4.4253	LR: 1.000000
Training Epoch: 9 [42240/50000]	Loss: 4.3786	LR: 1.000000
Training Epoch: 9 [42496/50000]	Loss: 4.4426	LR: 1.000000
Training Epoch: 9 [42752/50000]	Loss: 4.3677	LR: 1.000000
Training Epoch: 9 [43008/50000]	Loss: 4.4014	LR: 1.000000
Training Epoch: 9 [43264/50000]	Loss: 4.3965	LR: 1.000000
Training Epoch: 9 [43520/50000]	Loss: 4.3386	LR: 1.000000
Training Epoch: 9 [43776/50000]	Loss: 4.3150	LR: 1.000000
Training Epoch: 9 [44032/50000]	Loss: 4.4838	LR: 1.000000
Training Epoch: 9 [44288/50000]	Loss: 4.3158	LR: 1.000000
Training Epoch: 9 [44544/50000]	Loss: 4.3593	LR: 1.000000
Training Epoch: 9 [44800/50000]	Loss: 4.3681	LR: 1.000000
Training Epoch: 9 [45056/50000]	Loss: 4.3390	LR: 1.000000
Training Epoch: 9 [45312/50000]	Loss: 4.3020	LR: 1.000000
Training Epoch: 9 [45568/50000]	Loss: 4.3443	LR: 1.000000
Training Epoch: 9 [45824/50000]	Loss: 4.4223	LR: 1.000000
Training Epoch: 9 [46080/50000]	Loss: 4.3639	LR: 1.000000
Training Epoch: 9 [46336/50000]	Loss: 4.2977	LR: 1.000000
Training Epoch: 9 [46592/50000]	Loss: 4.3211	LR: 1.000000
Training Epoch: 9 [46848/50000]	Loss: 4.4453	LR: 1.000000
Training Epoch: 9 [47104/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 9 [47360/50000]	Loss: 4.3788	LR: 1.000000
Training Epoch: 9 [47616/50000]	Loss: 4.4780	LR: 1.000000
Training Epoch: 9 [47872/50000]	Loss: 4.3446	LR: 1.000000
Training Epoch: 9 [48128/50000]	Loss: 4.3995	LR: 1.000000
Training Epoch: 9 [48384/50000]	Loss: 4.3638	LR: 1.000000
Training Epoch: 9 [48640/50000]	Loss: 4.3068	LR: 1.000000
Training Epoch: 9 [48896/50000]	Loss: 4.3115	LR: 1.000000
Training Epoch: 9 [49152/50000]	Loss: 4.4684	LR: 1.000000
Training Epoch: 9 [49408/50000]	Loss: 4.3349	LR: 1.000000
Training Epoch: 9 [49664/50000]	Loss: 4.2973	LR: 1.000000
Training Epoch: 9 [49920/50000]	Loss: 4.3029	LR: 1.000000
Training Epoch: 9 [50000/50000]	Loss: 4.2061	LR: 1.000000
epoch 9 training time consumed: 22.08s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   13502 GB |   13502 GB |
|       from large pool |  400448 KB |    1770 MB |   13490 GB |   13490 GB |
|       from small pool |    3549 KB |       9 MB |      12 GB |      12 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   13502 GB |   13502 GB |
|       from large pool |  400448 KB |    1770 MB |   13490 GB |   13490 GB |
|       from small pool |    3549 KB |       9 MB |      12 GB |      12 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |    8309 GB |    8309 GB |
|       from large pool |  244672 KB |  473024 KB |    8295 GB |    8295 GB |
|       from small pool |    2594 KB |    4843 KB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  576671    |  576449    |
|       from large pool |      36    |      77    |  278705    |  278669    |
|       from small pool |     186    |     224    |  297966    |  297780    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  576671    |  576449    |
|       from large pool |      36    |      77    |  278705    |  278669    |
|       from small pool |     186    |     224    |  297966    |  297780    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      31    |  293136    |  293112    |
|       from large pool |      10    |      11    |  115439    |  115429    |
|       from small pool |      14    |      21    |  177697    |  177683    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 9, Average loss: 0.0177, Accuracy: 0.0231, Time consumed:1.48s

Training Epoch: 10 [256/50000]	Loss: 4.4667	LR: 1.000000
Training Epoch: 10 [512/50000]	Loss: 4.4285	LR: 1.000000
Training Epoch: 10 [768/50000]	Loss: 4.3896	LR: 1.000000
Training Epoch: 10 [1024/50000]	Loss: 4.3704	LR: 1.000000
Training Epoch: 10 [1280/50000]	Loss: 4.4252	LR: 1.000000
Training Epoch: 10 [1536/50000]	Loss: 4.3912	LR: 1.000000
Training Epoch: 10 [1792/50000]	Loss: 4.4970	LR: 1.000000
Training Epoch: 10 [2048/50000]	Loss: 4.4077	LR: 1.000000
Training Epoch: 10 [2304/50000]	Loss: 4.4098	LR: 1.000000
Training Epoch: 10 [2560/50000]	Loss: 4.3727	LR: 1.000000
Training Epoch: 10 [2816/50000]	Loss: 4.3967	LR: 1.000000
Training Epoch: 10 [3072/50000]	Loss: 4.3750	LR: 1.000000
Training Epoch: 10 [3328/50000]	Loss: 4.4213	LR: 1.000000
Training Epoch: 10 [3584/50000]	Loss: 4.4050	LR: 1.000000
Training Epoch: 10 [3840/50000]	Loss: 4.4136	LR: 1.000000
Training Epoch: 10 [4096/50000]	Loss: 4.4840	LR: 1.000000
Training Epoch: 10 [4352/50000]	Loss: 4.3978	LR: 1.000000
Training Epoch: 10 [4608/50000]	Loss: 4.4392	LR: 1.000000
Training Epoch: 10 [4864/50000]	Loss: 4.3332	LR: 1.000000
Training Epoch: 10 [5120/50000]	Loss: 4.4434	LR: 1.000000
Training Epoch: 10 [5376/50000]	Loss: 4.4029	LR: 1.000000
Training Epoch: 10 [5632/50000]	Loss: 4.4212	LR: 1.000000
Training Epoch: 10 [5888/50000]	Loss: 4.3761	LR: 1.000000
Training Epoch: 10 [6144/50000]	Loss: 4.3548	LR: 1.000000
Training Epoch: 10 [6400/50000]	Loss: 4.3652	LR: 1.000000
Training Epoch: 10 [6656/50000]	Loss: 4.3373	LR: 1.000000
Training Epoch: 10 [6912/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 10 [7168/50000]	Loss: 4.4077	LR: 1.000000
Training Epoch: 10 [7424/50000]	Loss: 4.3806	LR: 1.000000
Training Epoch: 10 [7680/50000]	Loss: 4.4070	LR: 1.000000
Training Epoch: 10 [7936/50000]	Loss: 4.4047	LR: 1.000000
Training Epoch: 10 [8192/50000]	Loss: 4.3571	LR: 1.000000
Training Epoch: 10 [8448/50000]	Loss: 4.4043	LR: 1.000000
Training Epoch: 10 [8704/50000]	Loss: 4.2644	LR: 1.000000
Training Epoch: 10 [8960/50000]	Loss: 4.3744	LR: 1.000000
Training Epoch: 10 [9216/50000]	Loss: 4.2972	LR: 1.000000
Training Epoch: 10 [9472/50000]	Loss: 4.3668	LR: 1.000000
Training Epoch: 10 [9728/50000]	Loss: 4.3401	LR: 1.000000
Training Epoch: 10 [9984/50000]	Loss: 4.3842	LR: 1.000000
Training Epoch: 10 [10240/50000]	Loss: 4.3810	LR: 1.000000
Training Epoch: 10 [10496/50000]	Loss: 4.3389	LR: 1.000000
Training Epoch: 10 [10752/50000]	Loss: 4.3403	LR: 1.000000
Training Epoch: 10 [11008/50000]	Loss: 4.4204	LR: 1.000000
Training Epoch: 10 [11264/50000]	Loss: 4.3635	LR: 1.000000
Training Epoch: 10 [11520/50000]	Loss: 4.3566	LR: 1.000000
Training Epoch: 10 [11776/50000]	Loss: 4.3634	LR: 1.000000
Training Epoch: 10 [12032/50000]	Loss: 4.4149	LR: 1.000000
Training Epoch: 10 [12288/50000]	Loss: 4.4572	LR: 1.000000
Training Epoch: 10 [12544/50000]	Loss: 4.3185	LR: 1.000000
Training Epoch: 10 [12800/50000]	Loss: 4.3476	LR: 1.000000
Training Epoch: 10 [13056/50000]	Loss: 4.3671	LR: 1.000000
Training Epoch: 10 [13312/50000]	Loss: 4.3626	LR: 1.000000
Training Epoch: 10 [13568/50000]	Loss: 4.3784	LR: 1.000000
Training Epoch: 10 [13824/50000]	Loss: 4.3548	LR: 1.000000
Training Epoch: 10 [14080/50000]	Loss: 4.3228	LR: 1.000000
Training Epoch: 10 [14336/50000]	Loss: 4.3705	LR: 1.000000
Training Epoch: 10 [14592/50000]	Loss: 4.3950	LR: 1.000000
Training Epoch: 10 [14848/50000]	Loss: 4.3257	LR: 1.000000
Training Epoch: 10 [15104/50000]	Loss: 4.3307	LR: 1.000000
Training Epoch: 10 [15360/50000]	Loss: 4.3909	LR: 1.000000
Training Epoch: 10 [15616/50000]	Loss: 4.2881	LR: 1.000000
Training Epoch: 10 [15872/50000]	Loss: 4.3953	LR: 1.000000
Training Epoch: 10 [16128/50000]	Loss: 4.3806	LR: 1.000000
Training Epoch: 10 [16384/50000]	Loss: 4.3814	LR: 1.000000
Training Epoch: 10 [16640/50000]	Loss: 4.3393	LR: 1.000000
Training Epoch: 10 [16896/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 10 [17152/50000]	Loss: 4.4383	LR: 1.000000
Training Epoch: 10 [17408/50000]	Loss: 4.4235	LR: 1.000000
Training Epoch: 10 [17664/50000]	Loss: 4.4351	LR: 1.000000
Training Epoch: 10 [17920/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 10 [18176/50000]	Loss: 4.4117	LR: 1.000000
Training Epoch: 10 [18432/50000]	Loss: 4.4075	LR: 1.000000
Training Epoch: 10 [18688/50000]	Loss: 4.4210	LR: 1.000000
Training Epoch: 10 [18944/50000]	Loss: 4.3561	LR: 1.000000
Training Epoch: 10 [19200/50000]	Loss: 4.3173	LR: 1.000000
Training Epoch: 10 [19456/50000]	Loss: 4.3901	LR: 1.000000
Training Epoch: 10 [19712/50000]	Loss: 4.4101	LR: 1.000000
Training Epoch: 10 [19968/50000]	Loss: 4.3729	LR: 1.000000
Training Epoch: 10 [20224/50000]	Loss: 4.3777	LR: 1.000000
Training Epoch: 10 [20480/50000]	Loss: 4.3991	LR: 1.000000
Training Epoch: 10 [20736/50000]	Loss: 4.4249	LR: 1.000000
Training Epoch: 10 [20992/50000]	Loss: 4.3980	LR: 1.000000
Training Epoch: 10 [21248/50000]	Loss: 4.4177	LR: 1.000000
Training Epoch: 10 [21504/50000]	Loss: 4.4194	LR: 1.000000
Training Epoch: 10 [21760/50000]	Loss: 4.4184	LR: 1.000000
Training Epoch: 10 [22016/50000]	Loss: 4.3897	LR: 1.000000
Training Epoch: 10 [22272/50000]	Loss: 4.4289	LR: 1.000000
Training Epoch: 10 [22528/50000]	Loss: 4.4407	LR: 1.000000
Training Epoch: 10 [22784/50000]	Loss: 4.3311	LR: 1.000000
Training Epoch: 10 [23040/50000]	Loss: 4.4174	LR: 1.000000
Training Epoch: 10 [23296/50000]	Loss: 4.3219	LR: 1.000000
Training Epoch: 10 [23552/50000]	Loss: 4.3759	LR: 1.000000
Training Epoch: 10 [23808/50000]	Loss: 4.4273	LR: 1.000000
Training Epoch: 10 [24064/50000]	Loss: 4.4091	LR: 1.000000
Training Epoch: 10 [24320/50000]	Loss: 4.3677	LR: 1.000000
Training Epoch: 10 [24576/50000]	Loss: 4.3407	LR: 1.000000
Training Epoch: 10 [24832/50000]	Loss: 4.3547	LR: 1.000000
Training Epoch: 10 [25088/50000]	Loss: 4.2745	LR: 1.000000
Training Epoch: 10 [25344/50000]	Loss: 4.4980	LR: 1.000000
Training Epoch: 10 [25600/50000]	Loss: 4.4031	LR: 1.000000
Training Epoch: 10 [25856/50000]	Loss: 4.3498	LR: 1.000000
Training Epoch: 10 [26112/50000]	Loss: 4.4130	LR: 1.000000
Training Epoch: 10 [26368/50000]	Loss: 4.3689	LR: 1.000000
Training Epoch: 10 [26624/50000]	Loss: 4.3750	LR: 1.000000
Training Epoch: 10 [26880/50000]	Loss: 4.4002	LR: 1.000000
Training Epoch: 10 [27136/50000]	Loss: 4.3635	LR: 1.000000
Training Epoch: 10 [27392/50000]	Loss: 4.3440	LR: 1.000000
Training Epoch: 10 [27648/50000]	Loss: 4.4483	LR: 1.000000
Training Epoch: 10 [27904/50000]	Loss: 4.3159	LR: 1.000000
Training Epoch: 10 [28160/50000]	Loss: 4.4204	LR: 1.000000
Training Epoch: 10 [28416/50000]	Loss: 4.4840	LR: 1.000000
Training Epoch: 10 [28672/50000]	Loss: 4.4161	LR: 1.000000
Training Epoch: 10 [28928/50000]	Loss: 4.4323	LR: 1.000000
Training Epoch: 10 [29184/50000]	Loss: 4.3836	LR: 1.000000
Training Epoch: 10 [29440/50000]	Loss: 4.4186	LR: 1.000000
Training Epoch: 10 [29696/50000]	Loss: 4.4176	LR: 1.000000
Training Epoch: 10 [29952/50000]	Loss: 4.3983	LR: 1.000000
Training Epoch: 10 [30208/50000]	Loss: 4.3852	LR: 1.000000
Training Epoch: 10 [30464/50000]	Loss: 4.3046	LR: 1.000000
Training Epoch: 10 [30720/50000]	Loss: 4.3356	LR: 1.000000
Training Epoch: 10 [30976/50000]	Loss: 4.3801	LR: 1.000000
Training Epoch: 10 [31232/50000]	Loss: 4.3620	LR: 1.000000
Training Epoch: 10 [31488/50000]	Loss: 4.4026	LR: 1.000000
Training Epoch: 10 [31744/50000]	Loss: 4.4592	LR: 1.000000
Training Epoch: 10 [32000/50000]	Loss: 4.4152	LR: 1.000000
Training Epoch: 10 [32256/50000]	Loss: 4.3825	LR: 1.000000
Training Epoch: 10 [32512/50000]	Loss: 4.4154	LR: 1.000000
Training Epoch: 10 [32768/50000]	Loss: 4.3965	LR: 1.000000
Training Epoch: 10 [33024/50000]	Loss: 4.4051	LR: 1.000000
Training Epoch: 10 [33280/50000]	Loss: 4.4222	LR: 1.000000
Training Epoch: 10 [33536/50000]	Loss: 4.3931	LR: 1.000000
Training Epoch: 10 [33792/50000]	Loss: 4.3035	LR: 1.000000
Training Epoch: 10 [34048/50000]	Loss: 4.3468	LR: 1.000000
Training Epoch: 10 [34304/50000]	Loss: 4.3732	LR: 1.000000
Training Epoch: 10 [34560/50000]	Loss: 4.3929	LR: 1.000000
Training Epoch: 10 [34816/50000]	Loss: 4.3840	LR: 1.000000
Training Epoch: 10 [35072/50000]	Loss: 4.3386	LR: 1.000000
Training Epoch: 10 [35328/50000]	Loss: 4.4586	LR: 1.000000
Training Epoch: 10 [35584/50000]	Loss: 4.4676	LR: 1.000000
Training Epoch: 10 [35840/50000]	Loss: 4.3498	LR: 1.000000
Training Epoch: 10 [36096/50000]	Loss: 4.4056	LR: 1.000000
Training Epoch: 10 [36352/50000]	Loss: 4.4301	LR: 1.000000
Training Epoch: 10 [36608/50000]	Loss: 4.3574	LR: 1.000000
Training Epoch: 10 [36864/50000]	Loss: 4.3778	LR: 1.000000
Training Epoch: 10 [37120/50000]	Loss: 4.5108	LR: 1.000000
Training Epoch: 10 [37376/50000]	Loss: 4.3614	LR: 1.000000
Training Epoch: 10 [37632/50000]	Loss: 4.4738	LR: 1.000000
Training Epoch: 10 [37888/50000]	Loss: 4.3974	LR: 1.000000
Training Epoch: 10 [38144/50000]	Loss: 4.4455	LR: 1.000000
Training Epoch: 10 [38400/50000]	Loss: 4.4171	LR: 1.000000
Training Epoch: 10 [38656/50000]	Loss: 4.4788	LR: 1.000000
Training Epoch: 10 [38912/50000]	Loss: 4.3432	LR: 1.000000
Training Epoch: 10 [39168/50000]	Loss: 4.3933	LR: 1.000000
Training Epoch: 10 [39424/50000]	Loss: 4.4304	LR: 1.000000
Training Epoch: 10 [39680/50000]	Loss: 4.3522	LR: 1.000000
Training Epoch: 10 [39936/50000]	Loss: 4.3552	LR: 1.000000
Training Epoch: 10 [40192/50000]	Loss: 4.4106	LR: 1.000000
Training Epoch: 10 [40448/50000]	Loss: 4.4246	LR: 1.000000
Training Epoch: 10 [40704/50000]	Loss: 4.4134	LR: 1.000000
Training Epoch: 10 [40960/50000]	Loss: 4.3733	LR: 1.000000
Training Epoch: 10 [41216/50000]	Loss: 4.3534	LR: 1.000000
Training Epoch: 10 [41472/50000]	Loss: 4.4064	LR: 1.000000
Training Epoch: 10 [41728/50000]	Loss: 4.3971	LR: 1.000000
Training Epoch: 10 [41984/50000]	Loss: 4.4685	LR: 1.000000
Training Epoch: 10 [42240/50000]	Loss: 4.3648	LR: 1.000000
Training Epoch: 10 [42496/50000]	Loss: 4.3685	LR: 1.000000
Training Epoch: 10 [42752/50000]	Loss: 4.3672	LR: 1.000000
Training Epoch: 10 [43008/50000]	Loss: 4.4025	LR: 1.000000
Training Epoch: 10 [43264/50000]	Loss: 4.3680	LR: 1.000000
Training Epoch: 10 [43520/50000]	Loss: 4.3433	LR: 1.000000
Training Epoch: 10 [43776/50000]	Loss: 4.3418	LR: 1.000000
Training Epoch: 10 [44032/50000]	Loss: 4.4348	LR: 1.000000
Training Epoch: 10 [44288/50000]	Loss: 4.3717	LR: 1.000000
Training Epoch: 10 [44544/50000]	Loss: 4.3654	LR: 1.000000
Training Epoch: 10 [44800/50000]	Loss: 4.2921	LR: 1.000000
Training Epoch: 10 [45056/50000]	Loss: 4.4378	LR: 1.000000
Training Epoch: 10 [45312/50000]	Loss: 4.3461	LR: 1.000000
Training Epoch: 10 [45568/50000]	Loss: 4.3207	LR: 1.000000
Training Epoch: 10 [45824/50000]	Loss: 4.3308	LR: 1.000000
Training Epoch: 10 [46080/50000]	Loss: 4.3337	LR: 1.000000
Training Epoch: 10 [46336/50000]	Loss: 4.3403	LR: 1.000000
Training Epoch: 10 [46592/50000]	Loss: 4.4061	LR: 1.000000
Training Epoch: 10 [46848/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 10 [47104/50000]	Loss: 4.3834	LR: 1.000000
Training Epoch: 10 [47360/50000]	Loss: 4.4540	LR: 1.000000
Training Epoch: 10 [47616/50000]	Loss: 4.3550	LR: 1.000000
Training Epoch: 10 [47872/50000]	Loss: 4.3992	LR: 1.000000
Training Epoch: 10 [48128/50000]	Loss: 4.4148	LR: 1.000000
Training Epoch: 10 [48384/50000]	Loss: 4.3498	LR: 1.000000
Training Epoch: 10 [48640/50000]	Loss: 4.3973	LR: 1.000000
Training Epoch: 10 [48896/50000]	Loss: 4.3816	LR: 1.000000
Training Epoch: 10 [49152/50000]	Loss: 4.3566	LR: 1.000000
Training Epoch: 10 [49408/50000]	Loss: 4.4018	LR: 1.000000
Training Epoch: 10 [49664/50000]	Loss: 4.3733	LR: 1.000000
Training Epoch: 10 [49920/50000]	Loss: 4.4438	LR: 1.000000
Training Epoch: 10 [50000/50000]	Loss: 4.3882	LR: 1.000000
epoch 10 training time consumed: 22.09s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   15002 GB |   15002 GB |
|       from large pool |  400448 KB |    1770 MB |   14989 GB |   14988 GB |
|       from small pool |    3549 KB |       9 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   15002 GB |   15002 GB |
|       from large pool |  400448 KB |    1770 MB |   14989 GB |   14988 GB |
|       from small pool |    3549 KB |       9 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |    9232 GB |    9232 GB |
|       from large pool |  244672 KB |  473024 KB |    9216 GB |    9216 GB |
|       from small pool |    2594 KB |    4843 KB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  640695    |  640473    |
|       from large pool |      36    |      77    |  309664    |  309628    |
|       from small pool |     186    |     224    |  331031    |  330845    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  640695    |  640473    |
|       from large pool |      36    |      77    |  309664    |  309628    |
|       from small pool |     186    |     224    |  331031    |  330845    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      31    |  325899    |  325877    |
|       from large pool |      10    |      11    |  128262    |  128252    |
|       from small pool |      12    |      21    |  197637    |  197625    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 10, Average loss: 0.0173, Accuracy: 0.0300, Time consumed:1.45s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-10-regular.pth
Training Epoch: 11 [256/50000]	Loss: 4.3433	LR: 1.000000
Training Epoch: 11 [512/50000]	Loss: 4.3785	LR: 1.000000
Training Epoch: 11 [768/50000]	Loss: 4.3481	LR: 1.000000
Training Epoch: 11 [1024/50000]	Loss: 4.4042	LR: 1.000000
Training Epoch: 11 [1280/50000]	Loss: 4.3482	LR: 1.000000
Training Epoch: 11 [1536/50000]	Loss: 4.3850	LR: 1.000000
Training Epoch: 11 [1792/50000]	Loss: 4.3304	LR: 1.000000
Training Epoch: 11 [2048/50000]	Loss: 4.3793	LR: 1.000000
Training Epoch: 11 [2304/50000]	Loss: 4.3459	LR: 1.000000
Training Epoch: 11 [2560/50000]	Loss: 4.3732	LR: 1.000000
Training Epoch: 11 [2816/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 11 [3072/50000]	Loss: 4.3892	LR: 1.000000
Training Epoch: 11 [3328/50000]	Loss: 4.3735	LR: 1.000000
Training Epoch: 11 [3584/50000]	Loss: 4.3903	LR: 1.000000
Training Epoch: 11 [3840/50000]	Loss: 4.4127	LR: 1.000000
Training Epoch: 11 [4096/50000]	Loss: 4.3736	LR: 1.000000
Training Epoch: 11 [4352/50000]	Loss: 4.4443	LR: 1.000000
Training Epoch: 11 [4608/50000]	Loss: 4.3544	LR: 1.000000
Training Epoch: 11 [4864/50000]	Loss: 4.3631	LR: 1.000000
Training Epoch: 11 [5120/50000]	Loss: 4.3867	LR: 1.000000
Training Epoch: 11 [5376/50000]	Loss: 4.3500	LR: 1.000000
Training Epoch: 11 [5632/50000]	Loss: 4.3848	LR: 1.000000
Training Epoch: 11 [5888/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 11 [6144/50000]	Loss: 4.3881	LR: 1.000000
Training Epoch: 11 [6400/50000]	Loss: 4.4085	LR: 1.000000
Training Epoch: 11 [6656/50000]	Loss: 4.3958	LR: 1.000000
Training Epoch: 11 [6912/50000]	Loss: 4.3928	LR: 1.000000
Training Epoch: 11 [7168/50000]	Loss: 4.3673	LR: 1.000000
Training Epoch: 11 [7424/50000]	Loss: 4.3661	LR: 1.000000
Training Epoch: 11 [7680/50000]	Loss: 4.4841	LR: 1.000000
Training Epoch: 11 [7936/50000]	Loss: 4.3522	LR: 1.000000
Training Epoch: 11 [8192/50000]	Loss: 4.4117	LR: 1.000000
Training Epoch: 11 [8448/50000]	Loss: 4.3590	LR: 1.000000
Training Epoch: 11 [8704/50000]	Loss: 4.4059	LR: 1.000000
Training Epoch: 11 [8960/50000]	Loss: 4.3361	LR: 1.000000
Training Epoch: 11 [9216/50000]	Loss: 4.3826	LR: 1.000000
Training Epoch: 11 [9472/50000]	Loss: 4.3295	LR: 1.000000
Training Epoch: 11 [9728/50000]	Loss: 4.3217	LR: 1.000000
Training Epoch: 11 [9984/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 11 [10240/50000]	Loss: 4.3697	LR: 1.000000
Training Epoch: 11 [10496/50000]	Loss: 4.4236	LR: 1.000000
Training Epoch: 11 [10752/50000]	Loss: 4.4160	LR: 1.000000
Training Epoch: 11 [11008/50000]	Loss: 4.3903	LR: 1.000000
Training Epoch: 11 [11264/50000]	Loss: 4.3510	LR: 1.000000
Training Epoch: 11 [11520/50000]	Loss: 4.4194	LR: 1.000000
Training Epoch: 11 [11776/50000]	Loss: 4.3294	LR: 1.000000
Training Epoch: 11 [12032/50000]	Loss: 4.4424	LR: 1.000000
Training Epoch: 11 [12288/50000]	Loss: 4.3396	LR: 1.000000
Training Epoch: 11 [12544/50000]	Loss: 4.3791	LR: 1.000000
Training Epoch: 11 [12800/50000]	Loss: 4.3393	LR: 1.000000
Training Epoch: 11 [13056/50000]	Loss: 4.2730	LR: 1.000000
Training Epoch: 11 [13312/50000]	Loss: 4.3843	LR: 1.000000
Training Epoch: 11 [13568/50000]	Loss: 4.4170	LR: 1.000000
Training Epoch: 11 [13824/50000]	Loss: 4.3157	LR: 1.000000
Training Epoch: 11 [14080/50000]	Loss: 4.3268	LR: 1.000000
Training Epoch: 11 [14336/50000]	Loss: 4.3912	LR: 1.000000
Training Epoch: 11 [14592/50000]	Loss: 4.4063	LR: 1.000000
Training Epoch: 11 [14848/50000]	Loss: 4.3654	LR: 1.000000
Training Epoch: 11 [15104/50000]	Loss: 4.3690	LR: 1.000000
Training Epoch: 11 [15360/50000]	Loss: 4.3471	LR: 1.000000
Training Epoch: 11 [15616/50000]	Loss: 4.4149	LR: 1.000000
Training Epoch: 11 [15872/50000]	Loss: 4.3607	LR: 1.000000
Training Epoch: 11 [16128/50000]	Loss: 4.3466	LR: 1.000000
Training Epoch: 11 [16384/50000]	Loss: 4.4123	LR: 1.000000
Training Epoch: 11 [16640/50000]	Loss: 4.3632	LR: 1.000000
Training Epoch: 11 [16896/50000]	Loss: 4.3686	LR: 1.000000
Training Epoch: 11 [17152/50000]	Loss: 4.3872	LR: 1.000000
Training Epoch: 11 [17408/50000]	Loss: 4.3606	LR: 1.000000
Training Epoch: 11 [17664/50000]	Loss: 4.3878	LR: 1.000000
Training Epoch: 11 [17920/50000]	Loss: 4.4662	LR: 1.000000
Training Epoch: 11 [18176/50000]	Loss: 4.2567	LR: 1.000000
Training Epoch: 11 [18432/50000]	Loss: 4.4245	LR: 1.000000
Training Epoch: 11 [18688/50000]	Loss: 4.4145	LR: 1.000000
Training Epoch: 11 [18944/50000]	Loss: 4.3840	LR: 1.000000
Training Epoch: 11 [19200/50000]	Loss: 4.3347	LR: 1.000000
Training Epoch: 11 [19456/50000]	Loss: 4.3829	LR: 1.000000
Training Epoch: 11 [19712/50000]	Loss: 4.4272	LR: 1.000000
Training Epoch: 11 [19968/50000]	Loss: 4.3382	LR: 1.000000
Training Epoch: 11 [20224/50000]	Loss: 4.4071	LR: 1.000000
Training Epoch: 11 [20480/50000]	Loss: 4.3521	LR: 1.000000
Training Epoch: 11 [20736/50000]	Loss: 4.3470	LR: 1.000000
Training Epoch: 11 [20992/50000]	Loss: 4.2527	LR: 1.000000
Training Epoch: 11 [21248/50000]	Loss: 4.4259	LR: 1.000000
Training Epoch: 11 [21504/50000]	Loss: 4.3564	LR: 1.000000
Training Epoch: 11 [21760/50000]	Loss: 4.3574	LR: 1.000000
Training Epoch: 11 [22016/50000]	Loss: 4.3655	LR: 1.000000
Training Epoch: 11 [22272/50000]	Loss: 4.3842	LR: 1.000000
Training Epoch: 11 [22528/50000]	Loss: 4.3782	LR: 1.000000
Training Epoch: 11 [22784/50000]	Loss: 4.2994	LR: 1.000000
Training Epoch: 11 [23040/50000]	Loss: 4.3421	LR: 1.000000
Training Epoch: 11 [23296/50000]	Loss: 4.3320	LR: 1.000000
Training Epoch: 11 [23552/50000]	Loss: 4.3652	LR: 1.000000
Training Epoch: 11 [23808/50000]	Loss: 4.3485	LR: 1.000000
Training Epoch: 11 [24064/50000]	Loss: 4.2829	LR: 1.000000
Training Epoch: 11 [24320/50000]	Loss: 4.3468	LR: 1.000000
Training Epoch: 11 [24576/50000]	Loss: 4.4110	LR: 1.000000
Training Epoch: 11 [24832/50000]	Loss: 4.3648	LR: 1.000000
Training Epoch: 11 [25088/50000]	Loss: 4.3682	LR: 1.000000
Training Epoch: 11 [25344/50000]	Loss: 4.3207	LR: 1.000000
Training Epoch: 11 [25600/50000]	Loss: 4.3852	LR: 1.000000
Training Epoch: 11 [25856/50000]	Loss: 4.3895	LR: 1.000000
Training Epoch: 11 [26112/50000]	Loss: 4.3298	LR: 1.000000
Training Epoch: 11 [26368/50000]	Loss: 4.4319	LR: 1.000000
Training Epoch: 11 [26624/50000]	Loss: 4.3034	LR: 1.000000
Training Epoch: 11 [26880/50000]	Loss: 4.2901	LR: 1.000000
Training Epoch: 11 [27136/50000]	Loss: 4.4072	LR: 1.000000
Training Epoch: 11 [27392/50000]	Loss: 4.2934	LR: 1.000000
Training Epoch: 11 [27648/50000]	Loss: 4.3808	LR: 1.000000
Training Epoch: 11 [27904/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 11 [28160/50000]	Loss: 4.4110	LR: 1.000000
Training Epoch: 11 [28416/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 11 [28672/50000]	Loss: 4.3077	LR: 1.000000
Training Epoch: 11 [28928/50000]	Loss: 4.4799	LR: 1.000000
Training Epoch: 11 [29184/50000]	Loss: 4.3214	LR: 1.000000
Training Epoch: 11 [29440/50000]	Loss: 4.3722	LR: 1.000000
Training Epoch: 11 [29696/50000]	Loss: 4.3568	LR: 1.000000
Training Epoch: 11 [29952/50000]	Loss: 4.3250	LR: 1.000000
Training Epoch: 11 [30208/50000]	Loss: 4.4488	LR: 1.000000
Training Epoch: 11 [30464/50000]	Loss: 4.2929	LR: 1.000000
Training Epoch: 11 [30720/50000]	Loss: 4.3831	LR: 1.000000
Training Epoch: 11 [30976/50000]	Loss: 4.4120	LR: 1.000000
Training Epoch: 11 [31232/50000]	Loss: 4.3792	LR: 1.000000
Training Epoch: 11 [31488/50000]	Loss: 4.3873	LR: 1.000000
Training Epoch: 11 [31744/50000]	Loss: 4.3782	LR: 1.000000
Training Epoch: 11 [32000/50000]	Loss: 4.4499	LR: 1.000000
Training Epoch: 11 [32256/50000]	Loss: 4.3754	LR: 1.000000
Training Epoch: 11 [32512/50000]	Loss: 4.4253	LR: 1.000000
Training Epoch: 11 [32768/50000]	Loss: 4.3100	LR: 1.000000
Training Epoch: 11 [33024/50000]	Loss: 4.4303	LR: 1.000000
Training Epoch: 11 [33280/50000]	Loss: 4.3330	LR: 1.000000
Training Epoch: 11 [33536/50000]	Loss: 4.3699	LR: 1.000000
Training Epoch: 11 [33792/50000]	Loss: 4.3871	LR: 1.000000
Training Epoch: 11 [34048/50000]	Loss: 4.3881	LR: 1.000000
Training Epoch: 11 [34304/50000]	Loss: 4.4053	LR: 1.000000
Training Epoch: 11 [34560/50000]	Loss: 4.3996	LR: 1.000000
Training Epoch: 11 [34816/50000]	Loss: 4.3813	LR: 1.000000
Training Epoch: 11 [35072/50000]	Loss: 4.3397	LR: 1.000000
Training Epoch: 11 [35328/50000]	Loss: 4.4323	LR: 1.000000
Training Epoch: 11 [35584/50000]	Loss: 4.3401	LR: 1.000000
Training Epoch: 11 [35840/50000]	Loss: 4.3513	LR: 1.000000
Training Epoch: 11 [36096/50000]	Loss: 4.4271	LR: 1.000000
Training Epoch: 11 [36352/50000]	Loss: 4.3512	LR: 1.000000
Training Epoch: 11 [36608/50000]	Loss: 4.3513	LR: 1.000000
Training Epoch: 11 [36864/50000]	Loss: 4.3514	LR: 1.000000
Training Epoch: 11 [37120/50000]	Loss: 4.3662	LR: 1.000000
Training Epoch: 11 [37376/50000]	Loss: 4.2934	LR: 1.000000
Training Epoch: 11 [37632/50000]	Loss: 4.5084	LR: 1.000000
Training Epoch: 11 [37888/50000]	Loss: 4.3580	LR: 1.000000
Training Epoch: 11 [38144/50000]	Loss: 4.3649	LR: 1.000000
Training Epoch: 11 [38400/50000]	Loss: 4.3536	LR: 1.000000
Training Epoch: 11 [38656/50000]	Loss: 4.3651	LR: 1.000000
Training Epoch: 11 [38912/50000]	Loss: 4.3079	LR: 1.000000
Training Epoch: 11 [39168/50000]	Loss: 4.3889	LR: 1.000000
Training Epoch: 11 [39424/50000]	Loss: 4.3802	LR: 1.000000
Training Epoch: 11 [39680/50000]	Loss: 4.5168	LR: 1.000000
Training Epoch: 11 [39936/50000]	Loss: 4.4036	LR: 1.000000
Training Epoch: 11 [40192/50000]	Loss: 4.4167	LR: 1.000000
Training Epoch: 11 [40448/50000]	Loss: 4.3790	LR: 1.000000
Training Epoch: 11 [40704/50000]	Loss: 4.3117	LR: 1.000000
Training Epoch: 11 [40960/50000]	Loss: 4.3488	LR: 1.000000
Training Epoch: 11 [41216/50000]	Loss: 4.3916	LR: 1.000000
Training Epoch: 11 [41472/50000]	Loss: 4.3802	LR: 1.000000
Training Epoch: 11 [41728/50000]	Loss: 4.3903	LR: 1.000000
Training Epoch: 11 [41984/50000]	Loss: 4.4438	LR: 1.000000
Training Epoch: 11 [42240/50000]	Loss: 4.4161	LR: 1.000000
Training Epoch: 11 [42496/50000]	Loss: 4.4621	LR: 1.000000
Training Epoch: 11 [42752/50000]	Loss: 4.4174	LR: 1.000000
Training Epoch: 11 [43008/50000]	Loss: 4.3429	LR: 1.000000
Training Epoch: 11 [43264/50000]	Loss: 4.4206	LR: 1.000000
Training Epoch: 11 [43520/50000]	Loss: 4.3739	LR: 1.000000
Training Epoch: 11 [43776/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 11 [44032/50000]	Loss: 4.3698	LR: 1.000000
Training Epoch: 11 [44288/50000]	Loss: 4.4687	LR: 1.000000
Training Epoch: 11 [44544/50000]	Loss: 4.3404	LR: 1.000000
Training Epoch: 11 [44800/50000]	Loss: 4.3741	LR: 1.000000
Training Epoch: 11 [45056/50000]	Loss: 4.3496	LR: 1.000000
Training Epoch: 11 [45312/50000]	Loss: 4.4142	LR: 1.000000
Training Epoch: 11 [45568/50000]	Loss: 4.3885	LR: 1.000000
Training Epoch: 11 [45824/50000]	Loss: 4.3397	LR: 1.000000
Training Epoch: 11 [46080/50000]	Loss: 4.3296	LR: 1.000000
Training Epoch: 11 [46336/50000]	Loss: 4.4417	LR: 1.000000
Training Epoch: 11 [46592/50000]	Loss: 4.3211	LR: 1.000000
Training Epoch: 11 [46848/50000]	Loss: 4.5137	LR: 1.000000
Training Epoch: 11 [47104/50000]	Loss: 4.4876	LR: 1.000000
Training Epoch: 11 [47360/50000]	Loss: 4.4014	LR: 1.000000
Training Epoch: 11 [47616/50000]	Loss: 4.4306	LR: 1.000000
Training Epoch: 11 [47872/50000]	Loss: 4.4416	LR: 1.000000
Training Epoch: 11 [48128/50000]	Loss: 4.3650	LR: 1.000000
Training Epoch: 11 [48384/50000]	Loss: 4.3957	LR: 1.000000
Training Epoch: 11 [48640/50000]	Loss: 4.4816	LR: 1.000000
Training Epoch: 11 [48896/50000]	Loss: 4.3285	LR: 1.000000
Training Epoch: 11 [49152/50000]	Loss: 4.3678	LR: 1.000000
Training Epoch: 11 [49408/50000]	Loss: 4.4263	LR: 1.000000
Training Epoch: 11 [49664/50000]	Loss: 4.3613	LR: 1.000000
Training Epoch: 11 [49920/50000]	Loss: 4.4514	LR: 1.000000
Training Epoch: 11 [50000/50000]	Loss: 4.3713	LR: 1.000000
epoch 11 training time consumed: 22.08s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   16503 GB |   16502 GB |
|       from large pool |  400448 KB |    1770 MB |   16488 GB |   16487 GB |
|       from small pool |    3549 KB |       9 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   16503 GB |   16502 GB |
|       from large pool |  400448 KB |    1770 MB |   16488 GB |   16487 GB |
|       from small pool |    3549 KB |       9 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   10155 GB |   10155 GB |
|       from large pool |  244672 KB |  473024 KB |   10138 GB |   10138 GB |
|       from small pool |    2594 KB |    4843 KB |      17 GB |      16 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |  704719    |  704497    |
|       from large pool |      36    |      77    |  340623    |  340587    |
|       from small pool |     186    |     224    |  364096    |  363910    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |  704719    |  704497    |
|       from large pool |      36    |      77    |  340623    |  340587    |
|       from small pool |     186    |     224    |  364096    |  363910    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      31    |  358191    |  358168    |
|       from large pool |      10    |      11    |  141085    |  141075    |
|       from small pool |      13    |      21    |  217106    |  217093    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 11, Average loss: 0.0176, Accuracy: 0.0269, Time consumed:1.46s

Training Epoch: 12 [256/50000]	Loss: 4.4065	LR: 1.000000
Training Epoch: 12 [512/50000]	Loss: 4.3938	LR: 1.000000
Training Epoch: 12 [768/50000]	Loss: 4.4976	LR: 1.000000
Training Epoch: 12 [1024/50000]	Loss: 4.3669	LR: 1.000000
Training Epoch: 12 [1280/50000]	Loss: 4.4398	LR: 1.000000
Training Epoch: 12 [1536/50000]	Loss: 4.4238	LR: 1.000000
Training Epoch: 12 [1792/50000]	Loss: 4.4131	LR: 1.000000
Training Epoch: 12 [2048/50000]	Loss: 4.3851	LR: 1.000000
Training Epoch: 12 [2304/50000]	Loss: 4.4202	LR: 1.000000
Training Epoch: 12 [2560/50000]	Loss: 4.3868	LR: 1.000000
Training Epoch: 12 [2816/50000]	Loss: 4.3866	LR: 1.000000
Training Epoch: 12 [3072/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 12 [3328/50000]	Loss: 4.3460	LR: 1.000000
Training Epoch: 12 [3584/50000]	Loss: 4.3601	LR: 1.000000
Training Epoch: 12 [3840/50000]	Loss: 4.3567	LR: 1.000000
Training Epoch: 12 [4096/50000]	Loss: 4.3313	LR: 1.000000
Training Epoch: 12 [4352/50000]	Loss: 4.3263	LR: 1.000000
Training Epoch: 12 [4608/50000]	Loss: 4.3706	LR: 1.000000
Training Epoch: 12 [4864/50000]	Loss: 4.3567	LR: 1.000000
Training Epoch: 12 [5120/50000]	Loss: 4.3478	LR: 1.000000
Training Epoch: 12 [5376/50000]	Loss: 4.3556	LR: 1.000000
Training Epoch: 12 [5632/50000]	Loss: 4.4232	LR: 1.000000
Training Epoch: 12 [5888/50000]	Loss: 4.4040	LR: 1.000000
Training Epoch: 12 [6144/50000]	Loss: 4.3845	LR: 1.000000
Training Epoch: 12 [6400/50000]	Loss: 4.3805	LR: 1.000000
Training Epoch: 12 [6656/50000]	Loss: 4.4232	LR: 1.000000
Training Epoch: 12 [6912/50000]	Loss: 4.3452	LR: 1.000000
Training Epoch: 12 [7168/50000]	Loss: 4.3439	LR: 1.000000
Training Epoch: 12 [7424/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 12 [7680/50000]	Loss: 4.4026	LR: 1.000000
Training Epoch: 12 [7936/50000]	Loss: 4.4125	LR: 1.000000
Training Epoch: 12 [8192/50000]	Loss: 4.3258	LR: 1.000000
Training Epoch: 12 [8448/50000]	Loss: 4.3781	LR: 1.000000
Training Epoch: 12 [8704/50000]	Loss: 4.3507	LR: 1.000000
Training Epoch: 12 [8960/50000]	Loss: 4.3252	LR: 1.000000
Training Epoch: 12 [9216/50000]	Loss: 4.3842	LR: 1.000000
Training Epoch: 12 [9472/50000]	Loss: 4.3808	LR: 1.000000
Training Epoch: 12 [9728/50000]	Loss: 4.3518	LR: 1.000000
Training Epoch: 12 [9984/50000]	Loss: 4.3433	LR: 1.000000
Training Epoch: 12 [10240/50000]	Loss: 4.4171	LR: 1.000000
Training Epoch: 12 [10496/50000]	Loss: 4.3928	LR: 1.000000
Training Epoch: 12 [10752/50000]	Loss: 4.3510	LR: 1.000000
Training Epoch: 12 [11008/50000]	Loss: 4.3493	LR: 1.000000
Training Epoch: 12 [11264/50000]	Loss: 4.3412	LR: 1.000000
Training Epoch: 12 [11520/50000]	Loss: 4.3375	LR: 1.000000
Training Epoch: 12 [11776/50000]	Loss: 4.3825	LR: 1.000000
Training Epoch: 12 [12032/50000]	Loss: 4.3348	LR: 1.000000
Training Epoch: 12 [12288/50000]	Loss: 4.3620	LR: 1.000000
Training Epoch: 12 [12544/50000]	Loss: 4.3964	LR: 1.000000
Training Epoch: 12 [12800/50000]	Loss: 4.2929	LR: 1.000000
Training Epoch: 12 [13056/50000]	Loss: 4.3615	LR: 1.000000
Training Epoch: 12 [13312/50000]	Loss: 4.3954	LR: 1.000000
Training Epoch: 12 [13568/50000]	Loss: 4.3798	LR: 1.000000
Training Epoch: 12 [13824/50000]	Loss: 4.3611	LR: 1.000000
Training Epoch: 12 [14080/50000]	Loss: 4.4259	LR: 1.000000
Training Epoch: 12 [14336/50000]	Loss: 4.3409	LR: 1.000000
Training Epoch: 12 [14592/50000]	Loss: 4.3868	LR: 1.000000
Training Epoch: 12 [14848/50000]	Loss: 4.3493	LR: 1.000000
Training Epoch: 12 [15104/50000]	Loss: 4.3226	LR: 1.000000
Training Epoch: 12 [15360/50000]	Loss: 4.2956	LR: 1.000000
Training Epoch: 12 [15616/50000]	Loss: 4.3874	LR: 1.000000
Training Epoch: 12 [15872/50000]	Loss: 4.3483	LR: 1.000000
Training Epoch: 12 [16128/50000]	Loss: 4.3076	LR: 1.000000
Training Epoch: 12 [16384/50000]	Loss: 4.3961	LR: 1.000000
Training Epoch: 12 [16640/50000]	Loss: 4.4628	LR: 1.000000
Training Epoch: 12 [16896/50000]	Loss: 4.3538	LR: 1.000000
Training Epoch: 12 [17152/50000]	Loss: 4.4083	LR: 1.000000
Training Epoch: 12 [17408/50000]	Loss: 4.3651	LR: 1.000000
Training Epoch: 12 [17664/50000]	Loss: 4.3821	LR: 1.000000
Training Epoch: 12 [17920/50000]	Loss: 4.3579	LR: 1.000000
Training Epoch: 12 [18176/50000]	Loss: 4.3392	LR: 1.000000
Training Epoch: 12 [18432/50000]	Loss: 4.3701	LR: 1.000000
Training Epoch: 12 [18688/50000]	Loss: 4.3478	LR: 1.000000
Training Epoch: 12 [18944/50000]	Loss: 4.3101	LR: 1.000000
Training Epoch: 12 [19200/50000]	Loss: 4.4125	LR: 1.000000
Training Epoch: 12 [19456/50000]	Loss: 4.3087	LR: 1.000000
Training Epoch: 12 [19712/50000]	Loss: 4.3087	LR: 1.000000
Training Epoch: 12 [19968/50000]	Loss: 4.4678	LR: 1.000000
Training Epoch: 12 [20224/50000]	Loss: 4.4212	LR: 1.000000
Training Epoch: 12 [20480/50000]	Loss: 4.3365	LR: 1.000000
Training Epoch: 12 [20736/50000]	Loss: 4.3661	LR: 1.000000
Training Epoch: 12 [20992/50000]	Loss: 4.3353	LR: 1.000000
Training Epoch: 12 [21248/50000]	Loss: 4.3849	LR: 1.000000
Training Epoch: 12 [21504/50000]	Loss: 4.4571	LR: 1.000000
Training Epoch: 12 [21760/50000]	Loss: 4.4512	LR: 1.000000
Training Epoch: 12 [22016/50000]	Loss: 4.4312	LR: 1.000000
Training Epoch: 12 [22272/50000]	Loss: 4.3958	LR: 1.000000
Training Epoch: 12 [22528/50000]	Loss: 4.4426	LR: 1.000000
Training Epoch: 12 [22784/50000]	Loss: 4.3187	LR: 1.000000
Training Epoch: 12 [23040/50000]	Loss: 4.4390	LR: 1.000000
Training Epoch: 12 [23296/50000]	Loss: 4.3850	LR: 1.000000
Training Epoch: 12 [23552/50000]	Loss: 4.3629	LR: 1.000000
Training Epoch: 12 [23808/50000]	Loss: 4.4542	LR: 1.000000
Training Epoch: 12 [24064/50000]	Loss: 4.4287	LR: 1.000000
Training Epoch: 12 [24320/50000]	Loss: 4.4944	LR: 1.000000
Training Epoch: 12 [24576/50000]	Loss: 4.3913	LR: 1.000000
Training Epoch: 12 [24832/50000]	Loss: 4.3958	LR: 1.000000
Training Epoch: 12 [25088/50000]	Loss: 4.3692	LR: 1.000000
Training Epoch: 12 [25344/50000]	Loss: 4.3954	LR: 1.000000
Training Epoch: 12 [25600/50000]	Loss: 4.4121	LR: 1.000000
Training Epoch: 12 [25856/50000]	Loss: 4.3901	LR: 1.000000
Training Epoch: 12 [26112/50000]	Loss: 4.4574	LR: 1.000000
Training Epoch: 12 [26368/50000]	Loss: 4.4019	LR: 1.000000
Training Epoch: 12 [26624/50000]	Loss: 4.4041	LR: 1.000000
Training Epoch: 12 [26880/50000]	Loss: 4.3450	LR: 1.000000
Training Epoch: 12 [27136/50000]	Loss: 4.4970	LR: 1.000000
Training Epoch: 12 [27392/50000]	Loss: 4.3504	LR: 1.000000
Training Epoch: 12 [27648/50000]	Loss: 4.4558	LR: 1.000000
Training Epoch: 12 [27904/50000]	Loss: 4.4063	LR: 1.000000
Training Epoch: 12 [28160/50000]	Loss: 4.4502	LR: 1.000000
Training Epoch: 12 [28416/50000]	Loss: 4.4312	LR: 1.000000
Training Epoch: 12 [28672/50000]	Loss: 4.3816	LR: 1.000000
Training Epoch: 12 [28928/50000]	Loss: 4.3957	LR: 1.000000
Training Epoch: 12 [29184/50000]	Loss: 4.3134	LR: 1.000000
Training Epoch: 12 [29440/50000]	Loss: 4.4386	LR: 1.000000
Training Epoch: 12 [29696/50000]	Loss: 4.3732	LR: 1.000000
Training Epoch: 12 [29952/50000]	Loss: 4.4356	LR: 1.000000
Training Epoch: 12 [30208/50000]	Loss: 4.3291	LR: 1.000000
Training Epoch: 12 [30464/50000]	Loss: 4.3494	LR: 1.000000
Training Epoch: 12 [30720/50000]	Loss: 4.3857	LR: 1.000000
Training Epoch: 12 [30976/50000]	Loss: 4.3699	LR: 1.000000
Training Epoch: 12 [31232/50000]	Loss: 4.3376	LR: 1.000000
Training Epoch: 12 [31488/50000]	Loss: 4.4950	LR: 1.000000
Training Epoch: 12 [31744/50000]	Loss: 4.3193	LR: 1.000000
Training Epoch: 12 [32000/50000]	Loss: 4.3362	LR: 1.000000
Training Epoch: 12 [32256/50000]	Loss: 4.3571	LR: 1.000000
Training Epoch: 12 [32512/50000]	Loss: 4.3035	LR: 1.000000
Training Epoch: 12 [32768/50000]	Loss: 4.3406	LR: 1.000000
Training Epoch: 12 [33024/50000]	Loss: 4.3735	LR: 1.000000
Training Epoch: 12 [33280/50000]	Loss: 4.3865	LR: 1.000000
Training Epoch: 12 [33536/50000]	Loss: 4.3996	LR: 1.000000
Training Epoch: 12 [33792/50000]	Loss: 4.3824	LR: 1.000000
Training Epoch: 12 [34048/50000]	Loss: 4.4288	LR: 1.000000
Training Epoch: 12 [34304/50000]	Loss: 4.4021	LR: 1.000000
Training Epoch: 12 [34560/50000]	Loss: 4.3951	LR: 1.000000
Training Epoch: 12 [34816/50000]	Loss: 4.4235	LR: 1.000000
Training Epoch: 12 [35072/50000]	Loss: 4.4094	LR: 1.000000
Training Epoch: 12 [35328/50000]	Loss: 4.4763	LR: 1.000000
Training Epoch: 12 [35584/50000]	Loss: 4.4665	LR: 1.000000
Training Epoch: 12 [35840/50000]	Loss: 4.4316	LR: 1.000000
Training Epoch: 12 [36096/50000]	Loss: 4.3922	LR: 1.000000
Training Epoch: 12 [36352/50000]	Loss: 4.3387	LR: 1.000000
Training Epoch: 12 [36608/50000]	Loss: 4.4276	LR: 1.000000
Training Epoch: 12 [36864/50000]	Loss: 4.4541	LR: 1.000000
Training Epoch: 12 [37120/50000]	Loss: 4.4260	LR: 1.000000
Training Epoch: 12 [37376/50000]	Loss: 4.3550	LR: 1.000000
Training Epoch: 12 [37632/50000]	Loss: 4.4013	LR: 1.000000
Training Epoch: 12 [37888/50000]	Loss: 4.3537	LR: 1.000000
Training Epoch: 12 [38144/50000]	Loss: 4.4412	LR: 1.000000
Training Epoch: 12 [38400/50000]	Loss: 4.4257	LR: 1.000000
Training Epoch: 12 [38656/50000]	Loss: 4.3744	LR: 1.000000
Training Epoch: 12 [38912/50000]	Loss: 4.2911	LR: 1.000000
Training Epoch: 12 [39168/50000]	Loss: 4.3852	LR: 1.000000
Training Epoch: 12 [39424/50000]	Loss: 4.4515	LR: 1.000000
Training Epoch: 12 [39680/50000]	Loss: 4.4399	LR: 1.000000
Training Epoch: 12 [39936/50000]	Loss: 4.3939	LR: 1.000000
Training Epoch: 12 [40192/50000]	Loss: 4.3551	LR: 1.000000
Training Epoch: 12 [40448/50000]	Loss: 4.3039	LR: 1.000000
Training Epoch: 12 [40704/50000]	Loss: 4.3596	LR: 1.000000
Training Epoch: 12 [40960/50000]	Loss: 4.4440	LR: 1.000000
Training Epoch: 12 [41216/50000]	Loss: 4.3410	LR: 1.000000
Training Epoch: 12 [41472/50000]	Loss: 4.4179	LR: 1.000000
Training Epoch: 12 [41728/50000]	Loss: 4.4919	LR: 1.000000
Training Epoch: 12 [41984/50000]	Loss: 4.4962	LR: 1.000000
Training Epoch: 12 [42240/50000]	Loss: 4.3681	LR: 1.000000
Training Epoch: 12 [42496/50000]	Loss: 4.4967	LR: 1.000000
Training Epoch: 12 [42752/50000]	Loss: 4.5322	LR: 1.000000
Training Epoch: 12 [43008/50000]	Loss: 4.4461	LR: 1.000000
Training Epoch: 12 [43264/50000]	Loss: 4.4161	LR: 1.000000
Training Epoch: 12 [43520/50000]	Loss: 4.4084	LR: 1.000000
Training Epoch: 12 [43776/50000]	Loss: 4.4459	LR: 1.000000
Training Epoch: 12 [44032/50000]	Loss: 4.4300	LR: 1.000000
Training Epoch: 12 [44288/50000]	Loss: 4.4836	LR: 1.000000
Training Epoch: 12 [44544/50000]	Loss: 4.3766	LR: 1.000000
Training Epoch: 12 [44800/50000]	Loss: 4.4345	LR: 1.000000
Training Epoch: 12 [45056/50000]	Loss: 4.3693	LR: 1.000000
Training Epoch: 12 [45312/50000]	Loss: 4.4212	LR: 1.000000
Training Epoch: 12 [45568/50000]	Loss: 4.4317	LR: 1.000000
Training Epoch: 12 [45824/50000]	Loss: 4.4504	LR: 1.000000
Training Epoch: 12 [46080/50000]	Loss: 4.3415	LR: 1.000000
Training Epoch: 12 [46336/50000]	Loss: 4.4059	LR: 1.000000
Training Epoch: 12 [46592/50000]	Loss: 4.3790	LR: 1.000000
Training Epoch: 12 [46848/50000]	Loss: 4.3671	LR: 1.000000
Training Epoch: 12 [47104/50000]	Loss: 4.4712	LR: 1.000000
Training Epoch: 12 [47360/50000]	Loss: 4.4473	LR: 1.000000
Training Epoch: 12 [47616/50000]	Loss: 4.4609	LR: 1.000000
Training Epoch: 12 [47872/50000]	Loss: 4.3112	LR: 1.000000
Training Epoch: 12 [48128/50000]	Loss: 4.4546	LR: 1.000000
Training Epoch: 12 [48384/50000]	Loss: 4.4188	LR: 1.000000
Training Epoch: 12 [48640/50000]	Loss: 4.3863	LR: 1.000000
Training Epoch: 12 [48896/50000]	Loss: 4.4079	LR: 1.000000
Training Epoch: 12 [49152/50000]	Loss: 4.4876	LR: 1.000000
Training Epoch: 12 [49408/50000]	Loss: 4.4423	LR: 1.000000
Training Epoch: 12 [49664/50000]	Loss: 4.4402	LR: 1.000000
Training Epoch: 12 [49920/50000]	Loss: 4.4442	LR: 1.000000
Training Epoch: 12 [50000/50000]	Loss: 4.4437	LR: 1.000000
epoch 12 training time consumed: 22.09s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   18003 GB |   18002 GB |
|       from large pool |  400448 KB |    1770 MB |   17987 GB |   17986 GB |
|       from small pool |    3549 KB |       9 MB |      16 GB |      16 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   18003 GB |   18002 GB |
|       from large pool |  400448 KB |    1770 MB |   17987 GB |   17986 GB |
|       from small pool |    3549 KB |       9 MB |      16 GB |      16 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   11078 GB |   11078 GB |
|       from large pool |  244672 KB |  473024 KB |   11059 GB |   11059 GB |
|       from small pool |    2594 KB |    4843 KB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |     768 K  |     768 K  |
|       from large pool |      36    |      77    |     371 K  |     371 K  |
|       from small pool |     186    |     224    |     397 K  |     396 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |     768 K  |     768 K  |
|       from large pool |      36    |      77    |     371 K  |     371 K  |
|       from small pool |     186    |     224    |     397 K  |     396 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      31    |  390382    |  390360    |
|       from large pool |      10    |      11    |  153908    |  153898    |
|       from small pool |      12    |      21    |  236474    |  236462    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 12, Average loss: 0.0181, Accuracy: 0.0174, Time consumed:1.44s

Training Epoch: 13 [256/50000]	Loss: 4.4691	LR: 1.000000
Training Epoch: 13 [512/50000]	Loss: 4.4259	LR: 1.000000
Training Epoch: 13 [768/50000]	Loss: 4.4144	LR: 1.000000
Training Epoch: 13 [1024/50000]	Loss: 4.4732	LR: 1.000000
Training Epoch: 13 [1280/50000]	Loss: 4.3969	LR: 1.000000
Training Epoch: 13 [1536/50000]	Loss: 4.4313	LR: 1.000000
Training Epoch: 13 [1792/50000]	Loss: 4.4142	LR: 1.000000
Training Epoch: 13 [2048/50000]	Loss: 4.3574	LR: 1.000000
Training Epoch: 13 [2304/50000]	Loss: 4.3565	LR: 1.000000
Training Epoch: 13 [2560/50000]	Loss: 4.3928	LR: 1.000000
Training Epoch: 13 [2816/50000]	Loss: 4.4561	LR: 1.000000
Training Epoch: 13 [3072/50000]	Loss: 4.3374	LR: 1.000000
Training Epoch: 13 [3328/50000]	Loss: 4.3892	LR: 1.000000
Training Epoch: 13 [3584/50000]	Loss: 4.4466	LR: 1.000000
Training Epoch: 13 [3840/50000]	Loss: 4.3530	LR: 1.000000
Training Epoch: 13 [4096/50000]	Loss: 4.4861	LR: 1.000000
Training Epoch: 13 [4352/50000]	Loss: 4.3296	LR: 1.000000
Training Epoch: 13 [4608/50000]	Loss: 4.3464	LR: 1.000000
Training Epoch: 13 [4864/50000]	Loss: 4.4505	LR: 1.000000
Training Epoch: 13 [5120/50000]	Loss: 4.4181	LR: 1.000000
Training Epoch: 13 [5376/50000]	Loss: 4.3777	LR: 1.000000
Training Epoch: 13 [5632/50000]	Loss: 4.3496	LR: 1.000000
Training Epoch: 13 [5888/50000]	Loss: 4.3770	LR: 1.000000
Training Epoch: 13 [6144/50000]	Loss: 4.3292	LR: 1.000000
Training Epoch: 13 [6400/50000]	Loss: 4.4290	LR: 1.000000
Training Epoch: 13 [6656/50000]	Loss: 4.3759	LR: 1.000000
Training Epoch: 13 [6912/50000]	Loss: 4.3341	LR: 1.000000
Training Epoch: 13 [7168/50000]	Loss: 4.3951	LR: 1.000000
Training Epoch: 13 [7424/50000]	Loss: 4.3910	LR: 1.000000
Training Epoch: 13 [7680/50000]	Loss: 4.4025	LR: 1.000000
Training Epoch: 13 [7936/50000]	Loss: 4.3989	LR: 1.000000
Training Epoch: 13 [8192/50000]	Loss: 4.3955	LR: 1.000000
Training Epoch: 13 [8448/50000]	Loss: 4.4217	LR: 1.000000
Training Epoch: 13 [8704/50000]	Loss: 4.3953	LR: 1.000000
Training Epoch: 13 [8960/50000]	Loss: 4.4618	LR: 1.000000
Training Epoch: 13 [9216/50000]	Loss: 4.4248	LR: 1.000000
Training Epoch: 13 [9472/50000]	Loss: 4.4003	LR: 1.000000
Training Epoch: 13 [9728/50000]	Loss: 4.3935	LR: 1.000000
Training Epoch: 13 [9984/50000]	Loss: 4.4267	LR: 1.000000
Training Epoch: 13 [10240/50000]	Loss: 4.4189	LR: 1.000000
Training Epoch: 13 [10496/50000]	Loss: 4.4035	LR: 1.000000
Training Epoch: 13 [10752/50000]	Loss: 4.3554	LR: 1.000000
Training Epoch: 13 [11008/50000]	Loss: 4.4039	LR: 1.000000
Training Epoch: 13 [11264/50000]	Loss: 4.4343	LR: 1.000000
Training Epoch: 13 [11520/50000]	Loss: 4.4000	LR: 1.000000
Training Epoch: 13 [11776/50000]	Loss: 4.4522	LR: 1.000000
Training Epoch: 13 [12032/50000]	Loss: 4.3982	LR: 1.000000
Training Epoch: 13 [12288/50000]	Loss: 4.3886	LR: 1.000000
Training Epoch: 13 [12544/50000]	Loss: 4.4437	LR: 1.000000
Training Epoch: 13 [12800/50000]	Loss: 4.3912	LR: 1.000000
Training Epoch: 13 [13056/50000]	Loss: 4.4177	LR: 1.000000
Training Epoch: 13 [13312/50000]	Loss: 4.4542	LR: 1.000000
Training Epoch: 13 [13568/50000]	Loss: 4.3918	LR: 1.000000
Training Epoch: 13 [13824/50000]	Loss: 4.4101	LR: 1.000000
Training Epoch: 13 [14080/50000]	Loss: 4.4085	LR: 1.000000
Training Epoch: 13 [14336/50000]	Loss: 4.4557	LR: 1.000000
Training Epoch: 13 [14592/50000]	Loss: 4.4520	LR: 1.000000
Training Epoch: 13 [14848/50000]	Loss: 4.3686	LR: 1.000000
Training Epoch: 13 [15104/50000]	Loss: 4.3846	LR: 1.000000
Training Epoch: 13 [15360/50000]	Loss: 4.3867	LR: 1.000000
Training Epoch: 13 [15616/50000]	Loss: 4.3688	LR: 1.000000
Training Epoch: 13 [15872/50000]	Loss: 4.4030	LR: 1.000000
Training Epoch: 13 [16128/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 13 [16384/50000]	Loss: 4.4537	LR: 1.000000
Training Epoch: 13 [16640/50000]	Loss: 4.3990	LR: 1.000000
Training Epoch: 13 [16896/50000]	Loss: 4.4197	LR: 1.000000
Training Epoch: 13 [17152/50000]	Loss: 4.4773	LR: 1.000000
Training Epoch: 13 [17408/50000]	Loss: 4.3974	LR: 1.000000
Training Epoch: 13 [17664/50000]	Loss: 4.4085	LR: 1.000000
Training Epoch: 13 [17920/50000]	Loss: 4.4897	LR: 1.000000
Training Epoch: 13 [18176/50000]	Loss: 4.3929	LR: 1.000000
Training Epoch: 13 [18432/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 13 [18688/50000]	Loss: 4.3851	LR: 1.000000
Training Epoch: 13 [18944/50000]	Loss: 4.4680	LR: 1.000000
Training Epoch: 13 [19200/50000]	Loss: 4.3475	LR: 1.000000
Training Epoch: 13 [19456/50000]	Loss: 4.4612	LR: 1.000000
Training Epoch: 13 [19712/50000]	Loss: 4.3913	LR: 1.000000
Training Epoch: 13 [19968/50000]	Loss: 4.4282	LR: 1.000000
Training Epoch: 13 [20224/50000]	Loss: 4.4672	LR: 1.000000
Training Epoch: 13 [20480/50000]	Loss: 4.3841	LR: 1.000000
Training Epoch: 13 [20736/50000]	Loss: 4.3471	LR: 1.000000
Training Epoch: 13 [20992/50000]	Loss: 4.3907	LR: 1.000000
Training Epoch: 13 [21248/50000]	Loss: 4.4416	LR: 1.000000
Training Epoch: 13 [21504/50000]	Loss: 4.4249	LR: 1.000000
Training Epoch: 13 [21760/50000]	Loss: 4.4073	LR: 1.000000
Training Epoch: 13 [22016/50000]	Loss: 4.4734	LR: 1.000000
Training Epoch: 13 [22272/50000]	Loss: 4.4675	LR: 1.000000
Training Epoch: 13 [22528/50000]	Loss: 4.5306	LR: 1.000000
Training Epoch: 13 [22784/50000]	Loss: 4.5672	LR: 1.000000
Training Epoch: 13 [23040/50000]	Loss: 4.4985	LR: 1.000000
Training Epoch: 13 [23296/50000]	Loss: 4.4246	LR: 1.000000
Training Epoch: 13 [23552/50000]	Loss: 4.4920	LR: 1.000000
Training Epoch: 13 [23808/50000]	Loss: 4.4262	LR: 1.000000
Training Epoch: 13 [24064/50000]	Loss: 4.4964	LR: 1.000000
Training Epoch: 13 [24320/50000]	Loss: 4.4321	LR: 1.000000
Training Epoch: 13 [24576/50000]	Loss: 4.4629	LR: 1.000000
Training Epoch: 13 [24832/50000]	Loss: 4.4558	LR: 1.000000
Training Epoch: 13 [25088/50000]	Loss: 4.4674	LR: 1.000000
Training Epoch: 13 [25344/50000]	Loss: 4.4668	LR: 1.000000
Training Epoch: 13 [25600/50000]	Loss: 4.4169	LR: 1.000000
Training Epoch: 13 [25856/50000]	Loss: 4.4934	LR: 1.000000
Training Epoch: 13 [26112/50000]	Loss: 4.4215	LR: 1.000000
Training Epoch: 13 [26368/50000]	Loss: 4.4409	LR: 1.000000
Training Epoch: 13 [26624/50000]	Loss: 4.4649	LR: 1.000000
Training Epoch: 13 [26880/50000]	Loss: 4.4315	LR: 1.000000
Training Epoch: 13 [27136/50000]	Loss: 4.4293	LR: 1.000000
Training Epoch: 13 [27392/50000]	Loss: 4.4548	LR: 1.000000
Training Epoch: 13 [27648/50000]	Loss: 4.4199	LR: 1.000000
Training Epoch: 13 [27904/50000]	Loss: 4.4548	LR: 1.000000
Training Epoch: 13 [28160/50000]	Loss: 4.4409	LR: 1.000000
Training Epoch: 13 [28416/50000]	Loss: 4.5508	LR: 1.000000
Training Epoch: 13 [28672/50000]	Loss: 4.3821	LR: 1.000000
Training Epoch: 13 [28928/50000]	Loss: 4.4229	LR: 1.000000
Training Epoch: 13 [29184/50000]	Loss: 4.3918	LR: 1.000000
Training Epoch: 13 [29440/50000]	Loss: 4.4007	LR: 1.000000
Training Epoch: 13 [29696/50000]	Loss: 4.5371	LR: 1.000000
Training Epoch: 13 [29952/50000]	Loss: 4.4468	LR: 1.000000
Training Epoch: 13 [30208/50000]	Loss: 4.4541	LR: 1.000000
Training Epoch: 13 [30464/50000]	Loss: 4.4104	LR: 1.000000
Training Epoch: 13 [30720/50000]	Loss: 4.3660	LR: 1.000000
Training Epoch: 13 [30976/50000]	Loss: 4.4170	LR: 1.000000
Training Epoch: 13 [31232/50000]	Loss: 4.4975	LR: 1.000000
Training Epoch: 13 [31488/50000]	Loss: 4.4381	LR: 1.000000
Training Epoch: 13 [31744/50000]	Loss: 4.4096	LR: 1.000000
Training Epoch: 13 [32000/50000]	Loss: 4.4669	LR: 1.000000
Training Epoch: 13 [32256/50000]	Loss: 4.3395	LR: 1.000000
Training Epoch: 13 [32512/50000]	Loss: 4.5921	LR: 1.000000
Training Epoch: 13 [32768/50000]	Loss: 4.3809	LR: 1.000000
Training Epoch: 13 [33024/50000]	Loss: 4.4299	LR: 1.000000
Training Epoch: 13 [33280/50000]	Loss: 4.4106	LR: 1.000000
Training Epoch: 13 [33536/50000]	Loss: 4.3687	LR: 1.000000
Training Epoch: 13 [33792/50000]	Loss: 4.3871	LR: 1.000000
Training Epoch: 13 [34048/50000]	Loss: 4.5079	LR: 1.000000
Training Epoch: 13 [34304/50000]	Loss: 4.4086	LR: 1.000000
Training Epoch: 13 [34560/50000]	Loss: 4.4424	LR: 1.000000
Training Epoch: 13 [34816/50000]	Loss: 4.5000	LR: 1.000000
Training Epoch: 13 [35072/50000]	Loss: 4.4681	LR: 1.000000
Training Epoch: 13 [35328/50000]	Loss: 4.5031	LR: 1.000000
Training Epoch: 13 [35584/50000]	Loss: 4.4918	LR: 1.000000
Training Epoch: 13 [35840/50000]	Loss: 4.4511	LR: 1.000000
Training Epoch: 13 [36096/50000]	Loss: 4.5857	LR: 1.000000
Training Epoch: 13 [36352/50000]	Loss: 4.4835	LR: 1.000000
Training Epoch: 13 [36608/50000]	Loss: 4.5113	LR: 1.000000
Training Epoch: 13 [36864/50000]	Loss: 4.4570	LR: 1.000000
Training Epoch: 13 [37120/50000]	Loss: 4.4846	LR: 1.000000
Training Epoch: 13 [37376/50000]	Loss: 4.4527	LR: 1.000000
Training Epoch: 13 [37632/50000]	Loss: 4.4172	LR: 1.000000
Training Epoch: 13 [37888/50000]	Loss: 4.4285	LR: 1.000000
Training Epoch: 13 [38144/50000]	Loss: 4.4107	LR: 1.000000
Training Epoch: 13 [38400/50000]	Loss: 4.4818	LR: 1.000000
Training Epoch: 13 [38656/50000]	Loss: 4.4407	LR: 1.000000
Training Epoch: 13 [38912/50000]	Loss: 4.3780	LR: 1.000000
Training Epoch: 13 [39168/50000]	Loss: 4.4332	LR: 1.000000
Training Epoch: 13 [39424/50000]	Loss: 4.4347	LR: 1.000000
Training Epoch: 13 [39680/50000]	Loss: 4.4806	LR: 1.000000
Training Epoch: 13 [39936/50000]	Loss: 4.4807	LR: 1.000000
Training Epoch: 13 [40192/50000]	Loss: 4.5060	LR: 1.000000
Training Epoch: 13 [40448/50000]	Loss: 4.5149	LR: 1.000000
Training Epoch: 13 [40704/50000]	Loss: 4.4783	LR: 1.000000
Training Epoch: 13 [40960/50000]	Loss: 4.4759	LR: 1.000000
Training Epoch: 13 [41216/50000]	Loss: 4.4549	LR: 1.000000
Training Epoch: 13 [41472/50000]	Loss: 4.4277	LR: 1.000000
Training Epoch: 13 [41728/50000]	Loss: 4.4934	LR: 1.000000
Training Epoch: 13 [41984/50000]	Loss: 4.4591	LR: 1.000000
Training Epoch: 13 [42240/50000]	Loss: 4.3893	LR: 1.000000
Training Epoch: 13 [42496/50000]	Loss: 4.4398	LR: 1.000000
Training Epoch: 13 [42752/50000]	Loss: 4.4049	LR: 1.000000
Training Epoch: 13 [43008/50000]	Loss: 4.4645	LR: 1.000000
Training Epoch: 13 [43264/50000]	Loss: 4.3734	LR: 1.000000
Training Epoch: 13 [43520/50000]	Loss: 4.4124	LR: 1.000000
Training Epoch: 13 [43776/50000]	Loss: 4.3298	LR: 1.000000
Training Epoch: 13 [44032/50000]	Loss: 4.4221	LR: 1.000000
Training Epoch: 13 [44288/50000]	Loss: 4.4569	LR: 1.000000
Training Epoch: 13 [44544/50000]	Loss: 4.4677	LR: 1.000000
Training Epoch: 13 [44800/50000]	Loss: 4.4934	LR: 1.000000
Training Epoch: 13 [45056/50000]	Loss: 4.4568	LR: 1.000000
Training Epoch: 13 [45312/50000]	Loss: 4.4289	LR: 1.000000
Training Epoch: 13 [45568/50000]	Loss: 4.5020	LR: 1.000000
Training Epoch: 13 [45824/50000]	Loss: 4.4191	LR: 1.000000
Training Epoch: 13 [46080/50000]	Loss: 4.3327	LR: 1.000000
Training Epoch: 13 [46336/50000]	Loss: 4.4178	LR: 1.000000
Training Epoch: 13 [46592/50000]	Loss: 4.3979	LR: 1.000000
Training Epoch: 13 [46848/50000]	Loss: 4.4258	LR: 1.000000
Training Epoch: 13 [47104/50000]	Loss: 4.4759	LR: 1.000000
Training Epoch: 13 [47360/50000]	Loss: 4.4551	LR: 1.000000
Training Epoch: 13 [47616/50000]	Loss: 4.4759	LR: 1.000000
Training Epoch: 13 [47872/50000]	Loss: 4.4657	LR: 1.000000
Training Epoch: 13 [48128/50000]	Loss: 4.4421	LR: 1.000000
Training Epoch: 13 [48384/50000]	Loss: 4.3928	LR: 1.000000
Training Epoch: 13 [48640/50000]	Loss: 4.4591	LR: 1.000000
Training Epoch: 13 [48896/50000]	Loss: 4.4622	LR: 1.000000
Training Epoch: 13 [49152/50000]	Loss: 4.4403	LR: 1.000000
Training Epoch: 13 [49408/50000]	Loss: 4.3724	LR: 1.000000
Training Epoch: 13 [49664/50000]	Loss: 4.4055	LR: 1.000000
Training Epoch: 13 [49920/50000]	Loss: 4.4652	LR: 1.000000
Training Epoch: 13 [50000/50000]	Loss: 4.3990	LR: 1.000000
epoch 13 training time consumed: 22.03s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   19503 GB |   19503 GB |
|       from large pool |  400448 KB |    1770 MB |   19485 GB |   19485 GB |
|       from small pool |    3549 KB |       9 MB |      17 GB |      17 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   19503 GB |   19503 GB |
|       from large pool |  400448 KB |    1770 MB |   19485 GB |   19485 GB |
|       from small pool |    3549 KB |       9 MB |      17 GB |      17 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   12001 GB |   12001 GB |
|       from large pool |  244672 KB |  473024 KB |   11981 GB |   11981 GB |
|       from small pool |    2594 KB |    4843 KB |      20 GB |      20 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |     832 K  |     832 K  |
|       from large pool |      36    |      77    |     402 K  |     402 K  |
|       from small pool |     186    |     224    |     430 K  |     430 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |     832 K  |     832 K  |
|       from large pool |      36    |      77    |     402 K  |     402 K  |
|       from small pool |     186    |     224    |     430 K  |     430 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      31    |  423073    |  423053    |
|       from large pool |      10    |      11    |  166731    |  166721    |
|       from small pool |      10    |      21    |  256342    |  256332    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 13, Average loss: 0.0185, Accuracy: 0.0127, Time consumed:1.47s

Training Epoch: 14 [256/50000]	Loss: 4.4226	LR: 1.000000
Training Epoch: 14 [512/50000]	Loss: 4.3560	LR: 1.000000
Training Epoch: 14 [768/50000]	Loss: 4.4030	LR: 1.000000
Training Epoch: 14 [1024/50000]	Loss: 4.4603	LR: 1.000000
Training Epoch: 14 [1280/50000]	Loss: 4.4296	LR: 1.000000
Training Epoch: 14 [1536/50000]	Loss: 4.4507	LR: 1.000000
Training Epoch: 14 [1792/50000]	Loss: 4.4624	LR: 1.000000
Training Epoch: 14 [2048/50000]	Loss: 4.4890	LR: 1.000000
Training Epoch: 14 [2304/50000]	Loss: 4.4053	LR: 1.000000
Training Epoch: 14 [2560/50000]	Loss: 4.4338	LR: 1.000000
Training Epoch: 14 [2816/50000]	Loss: 4.4569	LR: 1.000000
Training Epoch: 14 [3072/50000]	Loss: 4.3837	LR: 1.000000
Training Epoch: 14 [3328/50000]	Loss: 4.4184	LR: 1.000000
Training Epoch: 14 [3584/50000]	Loss: 4.4744	LR: 1.000000
Training Epoch: 14 [3840/50000]	Loss: 4.4138	LR: 1.000000
Training Epoch: 14 [4096/50000]	Loss: 4.3463	LR: 1.000000
Training Epoch: 14 [4352/50000]	Loss: 4.3613	LR: 1.000000
Training Epoch: 14 [4608/50000]	Loss: 4.4681	LR: 1.000000
Training Epoch: 14 [4864/50000]	Loss: 4.4552	LR: 1.000000
Training Epoch: 14 [5120/50000]	Loss: 4.4164	LR: 1.000000
Training Epoch: 14 [5376/50000]	Loss: 4.4094	LR: 1.000000
Training Epoch: 14 [5632/50000]	Loss: 4.4392	LR: 1.000000
Training Epoch: 14 [5888/50000]	Loss: 4.3802	LR: 1.000000
Training Epoch: 14 [6144/50000]	Loss: 4.4205	LR: 1.000000
Training Epoch: 14 [6400/50000]	Loss: 4.4043	LR: 1.000000
Training Epoch: 14 [6656/50000]	Loss: 4.3799	LR: 1.000000
Training Epoch: 14 [6912/50000]	Loss: 4.4761	LR: 1.000000
Training Epoch: 14 [7168/50000]	Loss: 4.4214	LR: 1.000000
Training Epoch: 14 [7424/50000]	Loss: 4.4433	LR: 1.000000
Training Epoch: 14 [7680/50000]	Loss: 4.4200	LR: 1.000000
Training Epoch: 14 [7936/50000]	Loss: 4.3470	LR: 1.000000
Training Epoch: 14 [8192/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 14 [8448/50000]	Loss: 4.4176	LR: 1.000000
Training Epoch: 14 [8704/50000]	Loss: 4.4118	LR: 1.000000
Training Epoch: 14 [8960/50000]	Loss: 4.4170	LR: 1.000000
Training Epoch: 14 [9216/50000]	Loss: 4.4453	LR: 1.000000
Training Epoch: 14 [9472/50000]	Loss: 4.4460	LR: 1.000000
Training Epoch: 14 [9728/50000]	Loss: 4.4106	LR: 1.000000
Training Epoch: 14 [9984/50000]	Loss: 4.4951	LR: 1.000000
Training Epoch: 14 [10240/50000]	Loss: 4.3580	LR: 1.000000
Training Epoch: 14 [10496/50000]	Loss: 4.4330	LR: 1.000000
Training Epoch: 14 [10752/50000]	Loss: 4.4183	LR: 1.000000
Training Epoch: 14 [11008/50000]	Loss: 4.4584	LR: 1.000000
Training Epoch: 14 [11264/50000]	Loss: 4.3261	LR: 1.000000
Training Epoch: 14 [11520/50000]	Loss: 4.4543	LR: 1.000000
Training Epoch: 14 [11776/50000]	Loss: 4.4537	LR: 1.000000
Training Epoch: 14 [12032/50000]	Loss: 4.4525	LR: 1.000000
Training Epoch: 14 [12288/50000]	Loss: 4.4143	LR: 1.000000
Training Epoch: 14 [12544/50000]	Loss: 4.4522	LR: 1.000000
Training Epoch: 14 [12800/50000]	Loss: 4.4211	LR: 1.000000
Training Epoch: 14 [13056/50000]	Loss: 4.4092	LR: 1.000000
Training Epoch: 14 [13312/50000]	Loss: 4.3670	LR: 1.000000
Training Epoch: 14 [13568/50000]	Loss: 4.4334	LR: 1.000000
Training Epoch: 14 [13824/50000]	Loss: 4.4440	LR: 1.000000
Training Epoch: 14 [14080/50000]	Loss: 4.4479	LR: 1.000000
Training Epoch: 14 [14336/50000]	Loss: 4.4403	LR: 1.000000
Training Epoch: 14 [14592/50000]	Loss: 4.4298	LR: 1.000000
Training Epoch: 14 [14848/50000]	Loss: 4.4096	LR: 1.000000
Training Epoch: 14 [15104/50000]	Loss: 4.5021	LR: 1.000000
Training Epoch: 14 [15360/50000]	Loss: 4.4714	LR: 1.000000
Training Epoch: 14 [15616/50000]	Loss: 4.4811	LR: 1.000000
Training Epoch: 14 [15872/50000]	Loss: 4.5024	LR: 1.000000
Training Epoch: 14 [16128/50000]	Loss: 4.4082	LR: 1.000000
Training Epoch: 14 [16384/50000]	Loss: 4.4790	LR: 1.000000
Training Epoch: 14 [16640/50000]	Loss: 4.3930	LR: 1.000000
Training Epoch: 14 [16896/50000]	Loss: 4.4211	LR: 1.000000
Training Epoch: 14 [17152/50000]	Loss: 4.4255	LR: 1.000000
Training Epoch: 14 [17408/50000]	Loss: 4.4289	LR: 1.000000
Training Epoch: 14 [17664/50000]	Loss: 4.4235	LR: 1.000000
Training Epoch: 14 [17920/50000]	Loss: 4.4189	LR: 1.000000
Training Epoch: 14 [18176/50000]	Loss: 4.4966	LR: 1.000000
Training Epoch: 14 [18432/50000]	Loss: 4.4772	LR: 1.000000
Training Epoch: 14 [18688/50000]	Loss: 4.4134	LR: 1.000000
Training Epoch: 14 [18944/50000]	Loss: 4.5221	LR: 1.000000
Training Epoch: 14 [19200/50000]	Loss: 4.4305	LR: 1.000000
Training Epoch: 14 [19456/50000]	Loss: 4.4540	LR: 1.000000
Training Epoch: 14 [19712/50000]	Loss: 4.3816	LR: 1.000000
Training Epoch: 14 [19968/50000]	Loss: 4.4578	LR: 1.000000
Training Epoch: 14 [20224/50000]	Loss: 4.3790	LR: 1.000000
Training Epoch: 14 [20480/50000]	Loss: 4.4233	LR: 1.000000
Training Epoch: 14 [20736/50000]	Loss: 4.4121	LR: 1.000000
Training Epoch: 14 [20992/50000]	Loss: 4.4849	LR: 1.000000
Training Epoch: 14 [21248/50000]	Loss: 4.4475	LR: 1.000000
Training Epoch: 14 [21504/50000]	Loss: 4.4227	LR: 1.000000
Training Epoch: 14 [21760/50000]	Loss: 4.3868	LR: 1.000000
Training Epoch: 14 [22016/50000]	Loss: 4.4663	LR: 1.000000
Training Epoch: 14 [22272/50000]	Loss: 4.4333	LR: 1.000000
Training Epoch: 14 [22528/50000]	Loss: 4.3805	LR: 1.000000
Training Epoch: 14 [22784/50000]	Loss: 4.4394	LR: 1.000000
Training Epoch: 14 [23040/50000]	Loss: 4.3511	LR: 1.000000
Training Epoch: 14 [23296/50000]	Loss: 4.3875	LR: 1.000000
Training Epoch: 14 [23552/50000]	Loss: 4.3564	LR: 1.000000
Training Epoch: 14 [23808/50000]	Loss: 4.3278	LR: 1.000000
Training Epoch: 14 [24064/50000]	Loss: 4.4561	LR: 1.000000
Training Epoch: 14 [24320/50000]	Loss: 4.5045	LR: 1.000000
Training Epoch: 14 [24576/50000]	Loss: 4.5175	LR: 1.000000
Training Epoch: 14 [24832/50000]	Loss: 4.5162	LR: 1.000000
Training Epoch: 14 [25088/50000]	Loss: 4.5599	LR: 1.000000
Training Epoch: 14 [25344/50000]	Loss: 4.3813	LR: 1.000000
Training Epoch: 14 [25600/50000]	Loss: 4.4818	LR: 1.000000
Training Epoch: 14 [25856/50000]	Loss: 4.4451	LR: 1.000000
Training Epoch: 14 [26112/50000]	Loss: 4.5135	LR: 1.000000
Training Epoch: 14 [26368/50000]	Loss: 4.5422	LR: 1.000000
Training Epoch: 14 [26624/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 14 [26880/50000]	Loss: 4.4491	LR: 1.000000
Training Epoch: 14 [27136/50000]	Loss: 4.4582	LR: 1.000000
Training Epoch: 14 [27392/50000]	Loss: 4.4130	LR: 1.000000
Training Epoch: 14 [27648/50000]	Loss: 4.4756	LR: 1.000000
Training Epoch: 14 [27904/50000]	Loss: 4.4260	LR: 1.000000
Training Epoch: 14 [28160/50000]	Loss: 4.4513	LR: 1.000000
Training Epoch: 14 [28416/50000]	Loss: 4.3917	LR: 1.000000
Training Epoch: 14 [28672/50000]	Loss: 4.3889	LR: 1.000000
Training Epoch: 14 [28928/50000]	Loss: 4.5068	LR: 1.000000
Training Epoch: 14 [29184/50000]	Loss: 4.3519	LR: 1.000000
Training Epoch: 14 [29440/50000]	Loss: 4.4226	LR: 1.000000
Training Epoch: 14 [29696/50000]	Loss: 4.3828	LR: 1.000000
Training Epoch: 14 [29952/50000]	Loss: 4.4608	LR: 1.000000
Training Epoch: 14 [30208/50000]	Loss: 4.4183	LR: 1.000000
Training Epoch: 14 [30464/50000]	Loss: 4.4217	LR: 1.000000
Training Epoch: 14 [30720/50000]	Loss: 4.4201	LR: 1.000000
Training Epoch: 14 [30976/50000]	Loss: 4.4058	LR: 1.000000
Training Epoch: 14 [31232/50000]	Loss: 4.4429	LR: 1.000000
Training Epoch: 14 [31488/50000]	Loss: 4.4933	LR: 1.000000
Training Epoch: 14 [31744/50000]	Loss: 4.4604	LR: 1.000000
Training Epoch: 14 [32000/50000]	Loss: 4.4671	LR: 1.000000
Training Epoch: 14 [32256/50000]	Loss: 4.4398	LR: 1.000000
Training Epoch: 14 [32512/50000]	Loss: 4.4144	LR: 1.000000
Training Epoch: 14 [32768/50000]	Loss: 4.4789	LR: 1.000000
Training Epoch: 14 [33024/50000]	Loss: 4.4977	LR: 1.000000
Training Epoch: 14 [33280/50000]	Loss: 4.4522	LR: 1.000000
Training Epoch: 14 [33536/50000]	Loss: 4.4054	LR: 1.000000
Training Epoch: 14 [33792/50000]	Loss: 4.4522	LR: 1.000000
Training Epoch: 14 [34048/50000]	Loss: 4.4287	LR: 1.000000
Training Epoch: 14 [34304/50000]	Loss: 4.4152	LR: 1.000000
Training Epoch: 14 [34560/50000]	Loss: 4.4233	LR: 1.000000
Training Epoch: 14 [34816/50000]	Loss: 4.3970	LR: 1.000000
Training Epoch: 14 [35072/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 14 [35328/50000]	Loss: 4.4807	LR: 1.000000
Training Epoch: 14 [35584/50000]	Loss: 4.4271	LR: 1.000000
Training Epoch: 14 [35840/50000]	Loss: 4.4861	LR: 1.000000
Training Epoch: 14 [36096/50000]	Loss: 4.3813	LR: 1.000000
Training Epoch: 14 [36352/50000]	Loss: 4.4585	LR: 1.000000
Training Epoch: 14 [36608/50000]	Loss: 4.4122	LR: 1.000000
Training Epoch: 14 [36864/50000]	Loss: 4.3769	LR: 1.000000
Training Epoch: 14 [37120/50000]	Loss: 4.5008	LR: 1.000000
Training Epoch: 14 [37376/50000]	Loss: 4.3891	LR: 1.000000
Training Epoch: 14 [37632/50000]	Loss: 4.4672	LR: 1.000000
Training Epoch: 14 [37888/50000]	Loss: 4.4389	LR: 1.000000
Training Epoch: 14 [38144/50000]	Loss: 4.4467	LR: 1.000000
Training Epoch: 14 [38400/50000]	Loss: 4.4544	LR: 1.000000
Training Epoch: 14 [38656/50000]	Loss: 4.4254	LR: 1.000000
Training Epoch: 14 [38912/50000]	Loss: 4.4563	LR: 1.000000
Training Epoch: 14 [39168/50000]	Loss: 4.4353	LR: 1.000000
Training Epoch: 14 [39424/50000]	Loss: 4.4999	LR: 1.000000
Training Epoch: 14 [39680/50000]	Loss: 4.4681	LR: 1.000000
Training Epoch: 14 [39936/50000]	Loss: 4.4165	LR: 1.000000
Training Epoch: 14 [40192/50000]	Loss: 4.4173	LR: 1.000000
Training Epoch: 14 [40448/50000]	Loss: 4.4387	LR: 1.000000
Training Epoch: 14 [40704/50000]	Loss: 4.4472	LR: 1.000000
Training Epoch: 14 [40960/50000]	Loss: 4.4143	LR: 1.000000
Training Epoch: 14 [41216/50000]	Loss: 4.5126	LR: 1.000000
Training Epoch: 14 [41472/50000]	Loss: 4.3570	LR: 1.000000
Training Epoch: 14 [41728/50000]	Loss: 4.5170	LR: 1.000000
Training Epoch: 14 [41984/50000]	Loss: 4.3901	LR: 1.000000
Training Epoch: 14 [42240/50000]	Loss: 4.3691	LR: 1.000000
Training Epoch: 14 [42496/50000]	Loss: 4.4624	LR: 1.000000
Training Epoch: 14 [42752/50000]	Loss: 4.4973	LR: 1.000000
Training Epoch: 14 [43008/50000]	Loss: 4.3836	LR: 1.000000
Training Epoch: 14 [43264/50000]	Loss: 4.4255	LR: 1.000000
Training Epoch: 14 [43520/50000]	Loss: 4.4508	LR: 1.000000
Training Epoch: 14 [43776/50000]	Loss: 4.4729	LR: 1.000000
Training Epoch: 14 [44032/50000]	Loss: 4.4134	LR: 1.000000
Training Epoch: 14 [44288/50000]	Loss: 4.4500	LR: 1.000000
Training Epoch: 14 [44544/50000]	Loss: 4.5119	LR: 1.000000
Training Epoch: 14 [44800/50000]	Loss: 4.4453	LR: 1.000000
Training Epoch: 14 [45056/50000]	Loss: 4.4883	LR: 1.000000
Training Epoch: 14 [45312/50000]	Loss: 4.5270	LR: 1.000000
Training Epoch: 14 [45568/50000]	Loss: 4.4847	LR: 1.000000
Training Epoch: 14 [45824/50000]	Loss: 4.4548	LR: 1.000000
Training Epoch: 14 [46080/50000]	Loss: 4.4972	LR: 1.000000
Training Epoch: 14 [46336/50000]	Loss: 4.4640	LR: 1.000000
Training Epoch: 14 [46592/50000]	Loss: 4.4876	LR: 1.000000
Training Epoch: 14 [46848/50000]	Loss: 4.4349	LR: 1.000000
Training Epoch: 14 [47104/50000]	Loss: 4.5075	LR: 1.000000
Training Epoch: 14 [47360/50000]	Loss: 4.4929	LR: 1.000000
Training Epoch: 14 [47616/50000]	Loss: 4.4705	LR: 1.000000
Training Epoch: 14 [47872/50000]	Loss: 4.3975	LR: 1.000000
Training Epoch: 14 [48128/50000]	Loss: 4.4632	LR: 1.000000
Training Epoch: 14 [48384/50000]	Loss: 4.4304	LR: 1.000000
Training Epoch: 14 [48640/50000]	Loss: 4.4318	LR: 1.000000
Training Epoch: 14 [48896/50000]	Loss: 4.4519	LR: 1.000000
Training Epoch: 14 [49152/50000]	Loss: 4.3866	LR: 1.000000
Training Epoch: 14 [49408/50000]	Loss: 4.4734	LR: 1.000000
Training Epoch: 14 [49664/50000]	Loss: 4.4652	LR: 1.000000
Training Epoch: 14 [49920/50000]	Loss: 4.4215	LR: 1.000000
Training Epoch: 14 [50000/50000]	Loss: 4.4303	LR: 1.000000
epoch 14 training time consumed: 22.02s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   21003 GB |   21003 GB |
|       from large pool |  400448 KB |    1770 MB |   20984 GB |   20984 GB |
|       from small pool |    3549 KB |       9 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   21003 GB |   21003 GB |
|       from large pool |  400448 KB |    1770 MB |   20984 GB |   20984 GB |
|       from small pool |    3549 KB |       9 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   12924 GB |   12924 GB |
|       from large pool |  244672 KB |  473024 KB |   12902 GB |   12902 GB |
|       from small pool |    2594 KB |    4843 KB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |     896 K  |     896 K  |
|       from large pool |      36    |      77    |     433 K  |     433 K  |
|       from small pool |     186    |     224    |     463 K  |     463 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |     896 K  |     896 K  |
|       from large pool |      36    |      77    |     433 K  |     433 K  |
|       from small pool |     186    |     224    |     463 K  |     463 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      31    |  455710    |  455688    |
|       from large pool |      10    |      11    |  179554    |  179544    |
|       from small pool |      12    |      21    |  276156    |  276144    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 14, Average loss: 0.0179, Accuracy: 0.0204, Time consumed:1.45s

Training Epoch: 15 [256/50000]	Loss: 4.4481	LR: 1.000000
Training Epoch: 15 [512/50000]	Loss: 4.4409	LR: 1.000000
Training Epoch: 15 [768/50000]	Loss: 4.5361	LR: 1.000000
Training Epoch: 15 [1024/50000]	Loss: 4.4153	LR: 1.000000
Training Epoch: 15 [1280/50000]	Loss: 4.3984	LR: 1.000000
Training Epoch: 15 [1536/50000]	Loss: 4.4206	LR: 1.000000
Training Epoch: 15 [1792/50000]	Loss: 4.4993	LR: 1.000000
Training Epoch: 15 [2048/50000]	Loss: 4.4170	LR: 1.000000
Training Epoch: 15 [2304/50000]	Loss: 4.4895	LR: 1.000000
Training Epoch: 15 [2560/50000]	Loss: 4.4238	LR: 1.000000
Training Epoch: 15 [2816/50000]	Loss: 4.4618	LR: 1.000000
Training Epoch: 15 [3072/50000]	Loss: 4.4118	LR: 1.000000
Training Epoch: 15 [3328/50000]	Loss: 4.4417	LR: 1.000000
Training Epoch: 15 [3584/50000]	Loss: 4.4403	LR: 1.000000
Training Epoch: 15 [3840/50000]	Loss: 4.4534	LR: 1.000000
Training Epoch: 15 [4096/50000]	Loss: 4.4105	LR: 1.000000
Training Epoch: 15 [4352/50000]	Loss: 4.4822	LR: 1.000000
Training Epoch: 15 [4608/50000]	Loss: 4.4309	LR: 1.000000
Training Epoch: 15 [4864/50000]	Loss: 4.4063	LR: 1.000000
Training Epoch: 15 [5120/50000]	Loss: 4.3451	LR: 1.000000
Training Epoch: 15 [5376/50000]	Loss: 4.4304	LR: 1.000000
Training Epoch: 15 [5632/50000]	Loss: 4.4375	LR: 1.000000
Training Epoch: 15 [5888/50000]	Loss: 4.4228	LR: 1.000000
Training Epoch: 15 [6144/50000]	Loss: 4.3791	LR: 1.000000
Training Epoch: 15 [6400/50000]	Loss: 4.4307	LR: 1.000000
Training Epoch: 15 [6656/50000]	Loss: 4.4912	LR: 1.000000
Training Epoch: 15 [6912/50000]	Loss: 4.4599	LR: 1.000000
Training Epoch: 15 [7168/50000]	Loss: 4.5088	LR: 1.000000
Training Epoch: 15 [7424/50000]	Loss: 4.5166	LR: 1.000000
Training Epoch: 15 [7680/50000]	Loss: 4.5071	LR: 1.000000
Training Epoch: 15 [7936/50000]	Loss: 4.4276	LR: 1.000000
Training Epoch: 15 [8192/50000]	Loss: 4.4770	LR: 1.000000
Training Epoch: 15 [8448/50000]	Loss: 4.4587	LR: 1.000000
Training Epoch: 15 [8704/50000]	Loss: 4.4998	LR: 1.000000
Training Epoch: 15 [8960/50000]	Loss: 4.4475	LR: 1.000000
Training Epoch: 15 [9216/50000]	Loss: 4.4558	LR: 1.000000
Training Epoch: 15 [9472/50000]	Loss: 4.4708	LR: 1.000000
Training Epoch: 15 [9728/50000]	Loss: 4.4244	LR: 1.000000
Training Epoch: 15 [9984/50000]	Loss: 4.4860	LR: 1.000000
Training Epoch: 15 [10240/50000]	Loss: 4.5108	LR: 1.000000
Training Epoch: 15 [10496/50000]	Loss: 4.5462	LR: 1.000000
Training Epoch: 15 [10752/50000]	Loss: 4.4604	LR: 1.000000
Training Epoch: 15 [11008/50000]	Loss: 4.5204	LR: 1.000000
Training Epoch: 15 [11264/50000]	Loss: 4.5652	LR: 1.000000
Training Epoch: 15 [11520/50000]	Loss: 4.4072	LR: 1.000000
Training Epoch: 15 [11776/50000]	Loss: 4.4491	LR: 1.000000
Training Epoch: 15 [12032/50000]	Loss: 4.5151	LR: 1.000000
Training Epoch: 15 [12288/50000]	Loss: 4.4539	LR: 1.000000
Training Epoch: 15 [12544/50000]	Loss: 4.4571	LR: 1.000000
Training Epoch: 15 [12800/50000]	Loss: 4.3950	LR: 1.000000
Training Epoch: 15 [13056/50000]	Loss: 4.4681	LR: 1.000000
Training Epoch: 15 [13312/50000]	Loss: 4.4412	LR: 1.000000
Training Epoch: 15 [13568/50000]	Loss: 4.4210	LR: 1.000000
Training Epoch: 15 [13824/50000]	Loss: 4.5234	LR: 1.000000
Training Epoch: 15 [14080/50000]	Loss: 4.4477	LR: 1.000000
Training Epoch: 15 [14336/50000]	Loss: 4.4384	LR: 1.000000
Training Epoch: 15 [14592/50000]	Loss: 4.4357	LR: 1.000000
Training Epoch: 15 [14848/50000]	Loss: 4.4124	LR: 1.000000
Training Epoch: 15 [15104/50000]	Loss: 4.4509	LR: 1.000000
Training Epoch: 15 [15360/50000]	Loss: 4.4441	LR: 1.000000
Training Epoch: 15 [15616/50000]	Loss: 4.5164	LR: 1.000000
Training Epoch: 15 [15872/50000]	Loss: 4.4111	LR: 1.000000
Training Epoch: 15 [16128/50000]	Loss: 4.4507	LR: 1.000000
Training Epoch: 15 [16384/50000]	Loss: 4.5450	LR: 1.000000
Training Epoch: 15 [16640/50000]	Loss: 4.4855	LR: 1.000000
Training Epoch: 15 [16896/50000]	Loss: 4.4001	LR: 1.000000
Training Epoch: 15 [17152/50000]	Loss: 4.4522	LR: 1.000000
Training Epoch: 15 [17408/50000]	Loss: 4.4745	LR: 1.000000
Training Epoch: 15 [17664/50000]	Loss: 4.4264	LR: 1.000000
Training Epoch: 15 [17920/50000]	Loss: 4.3783	LR: 1.000000
Training Epoch: 15 [18176/50000]	Loss: 4.4750	LR: 1.000000
Training Epoch: 15 [18432/50000]	Loss: 4.4699	LR: 1.000000
Training Epoch: 15 [18688/50000]	Loss: 4.4354	LR: 1.000000
Training Epoch: 15 [18944/50000]	Loss: 4.4537	LR: 1.000000
Training Epoch: 15 [19200/50000]	Loss: 4.4584	LR: 1.000000
Training Epoch: 15 [19456/50000]	Loss: 4.3593	LR: 1.000000
Training Epoch: 15 [19712/50000]	Loss: 4.4693	LR: 1.000000
Training Epoch: 15 [19968/50000]	Loss: 4.4025	LR: 1.000000
Training Epoch: 15 [20224/50000]	Loss: 4.4635	LR: 1.000000
Training Epoch: 15 [20480/50000]	Loss: 4.4049	LR: 1.000000
Training Epoch: 15 [20736/50000]	Loss: 4.4192	LR: 1.000000
Training Epoch: 15 [20992/50000]	Loss: 4.4678	LR: 1.000000
Training Epoch: 15 [21248/50000]	Loss: 4.4868	LR: 1.000000
Training Epoch: 15 [21504/50000]	Loss: 4.4653	LR: 1.000000
Training Epoch: 15 [21760/50000]	Loss: 4.4309	LR: 1.000000
Training Epoch: 15 [22016/50000]	Loss: 4.4291	LR: 1.000000
Training Epoch: 15 [22272/50000]	Loss: 4.3987	LR: 1.000000
Training Epoch: 15 [22528/50000]	Loss: 4.4055	LR: 1.000000
Training Epoch: 15 [22784/50000]	Loss: 4.3950	LR: 1.000000
Training Epoch: 15 [23040/50000]	Loss: 4.4568	LR: 1.000000
Training Epoch: 15 [23296/50000]	Loss: 4.4343	LR: 1.000000
Training Epoch: 15 [23552/50000]	Loss: 4.4560	LR: 1.000000
Training Epoch: 15 [23808/50000]	Loss: 4.4557	LR: 1.000000
Training Epoch: 15 [24064/50000]	Loss: 4.4327	LR: 1.000000
Training Epoch: 15 [24320/50000]	Loss: 4.3904	LR: 1.000000
Training Epoch: 15 [24576/50000]	Loss: 4.4691	LR: 1.000000
Training Epoch: 15 [24832/50000]	Loss: 4.3930	LR: 1.000000
Training Epoch: 15 [25088/50000]	Loss: 4.4892	LR: 1.000000
Training Epoch: 15 [25344/50000]	Loss: 4.4021	LR: 1.000000
Training Epoch: 15 [25600/50000]	Loss: 4.3813	LR: 1.000000
Training Epoch: 15 [25856/50000]	Loss: 4.3959	LR: 1.000000
Training Epoch: 15 [26112/50000]	Loss: 4.4373	LR: 1.000000
Training Epoch: 15 [26368/50000]	Loss: 4.4125	LR: 1.000000
Training Epoch: 15 [26624/50000]	Loss: 4.3878	LR: 1.000000
Training Epoch: 15 [26880/50000]	Loss: 4.4699	LR: 1.000000
Training Epoch: 15 [27136/50000]	Loss: 4.3957	LR: 1.000000
Training Epoch: 15 [27392/50000]	Loss: 4.4198	LR: 1.000000
Training Epoch: 15 [27648/50000]	Loss: 4.4713	LR: 1.000000
Training Epoch: 15 [27904/50000]	Loss: 4.4685	LR: 1.000000
Training Epoch: 15 [28160/50000]	Loss: 4.4222	LR: 1.000000
Training Epoch: 15 [28416/50000]	Loss: 4.4740	LR: 1.000000
Training Epoch: 15 [28672/50000]	Loss: 4.3676	LR: 1.000000
Training Epoch: 15 [28928/50000]	Loss: 4.3736	LR: 1.000000
Training Epoch: 15 [29184/50000]	Loss: 4.4433	LR: 1.000000
Training Epoch: 15 [29440/50000]	Loss: 4.4419	LR: 1.000000
Training Epoch: 15 [29696/50000]	Loss: 4.4348	LR: 1.000000
Training Epoch: 15 [29952/50000]	Loss: 4.4185	LR: 1.000000
Training Epoch: 15 [30208/50000]	Loss: 4.4590	LR: 1.000000
Training Epoch: 15 [30464/50000]	Loss: 4.4499	LR: 1.000000
Training Epoch: 15 [30720/50000]	Loss: 4.4048	LR: 1.000000
Training Epoch: 15 [30976/50000]	Loss: 4.4220	LR: 1.000000
Training Epoch: 15 [31232/50000]	Loss: 4.4878	LR: 1.000000
Training Epoch: 15 [31488/50000]	Loss: 4.4249	LR: 1.000000
Training Epoch: 15 [31744/50000]	Loss: 4.4145	LR: 1.000000
Training Epoch: 15 [32000/50000]	Loss: 4.4380	LR: 1.000000
Training Epoch: 15 [32256/50000]	Loss: 4.5039	LR: 1.000000
Training Epoch: 15 [32512/50000]	Loss: 4.4345	LR: 1.000000
Training Epoch: 15 [32768/50000]	Loss: 4.5688	LR: 1.000000
Training Epoch: 15 [33024/50000]	Loss: 4.4852	LR: 1.000000
Training Epoch: 15 [33280/50000]	Loss: 4.4339	LR: 1.000000
Training Epoch: 15 [33536/50000]	Loss: 4.4427	LR: 1.000000
Training Epoch: 15 [33792/50000]	Loss: 4.4399	LR: 1.000000
Training Epoch: 15 [34048/50000]	Loss: 4.4618	LR: 1.000000
Training Epoch: 15 [34304/50000]	Loss: 4.4478	LR: 1.000000
Training Epoch: 15 [34560/50000]	Loss: 4.4707	LR: 1.000000
Training Epoch: 15 [34816/50000]	Loss: 4.4479	LR: 1.000000
Training Epoch: 15 [35072/50000]	Loss: 4.4110	LR: 1.000000
Training Epoch: 15 [35328/50000]	Loss: 4.4590	LR: 1.000000
Training Epoch: 15 [35584/50000]	Loss: 4.4925	LR: 1.000000
Training Epoch: 15 [35840/50000]	Loss: 4.3908	LR: 1.000000
Training Epoch: 15 [36096/50000]	Loss: 4.3815	LR: 1.000000
Training Epoch: 15 [36352/50000]	Loss: 4.3941	LR: 1.000000
Training Epoch: 15 [36608/50000]	Loss: 4.3931	LR: 1.000000
Training Epoch: 15 [36864/50000]	Loss: 4.3684	LR: 1.000000
Training Epoch: 15 [37120/50000]	Loss: 4.4209	LR: 1.000000
Training Epoch: 15 [37376/50000]	Loss: 4.3724	LR: 1.000000
Training Epoch: 15 [37632/50000]	Loss: 4.3951	LR: 1.000000
Training Epoch: 15 [37888/50000]	Loss: 4.3771	LR: 1.000000
Training Epoch: 15 [38144/50000]	Loss: 4.3943	LR: 1.000000
Training Epoch: 15 [38400/50000]	Loss: 4.4168	LR: 1.000000
Training Epoch: 15 [38656/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 15 [38912/50000]	Loss: 4.3726	LR: 1.000000
Training Epoch: 15 [39168/50000]	Loss: 4.4236	LR: 1.000000
Training Epoch: 15 [39424/50000]	Loss: 4.4638	LR: 1.000000
Training Epoch: 15 [39680/50000]	Loss: 4.5296	LR: 1.000000
Training Epoch: 15 [39936/50000]	Loss: 4.5252	LR: 1.000000
Training Epoch: 15 [40192/50000]	Loss: 4.4464	LR: 1.000000
Training Epoch: 15 [40448/50000]	Loss: 4.4919	LR: 1.000000
Training Epoch: 15 [40704/50000]	Loss: 4.4636	LR: 1.000000
Training Epoch: 15 [40960/50000]	Loss: 4.4868	LR: 1.000000
Training Epoch: 15 [41216/50000]	Loss: 4.5184	LR: 1.000000
Training Epoch: 15 [41472/50000]	Loss: 4.4750	LR: 1.000000
Training Epoch: 15 [41728/50000]	Loss: 4.4542	LR: 1.000000
Training Epoch: 15 [41984/50000]	Loss: 4.4627	LR: 1.000000
Training Epoch: 15 [42240/50000]	Loss: 4.4965	LR: 1.000000
Training Epoch: 15 [42496/50000]	Loss: 4.4634	LR: 1.000000
Training Epoch: 15 [42752/50000]	Loss: 4.5191	LR: 1.000000
Training Epoch: 15 [43008/50000]	Loss: 4.5891	LR: 1.000000
Training Epoch: 15 [43264/50000]	Loss: 4.4876	LR: 1.000000
Training Epoch: 15 [43520/50000]	Loss: 4.4407	LR: 1.000000
Training Epoch: 15 [43776/50000]	Loss: 4.4920	LR: 1.000000
Training Epoch: 15 [44032/50000]	Loss: 4.5205	LR: 1.000000
Training Epoch: 15 [44288/50000]	Loss: 4.4583	LR: 1.000000
Training Epoch: 15 [44544/50000]	Loss: 4.4390	LR: 1.000000
Training Epoch: 15 [44800/50000]	Loss: 4.4337	LR: 1.000000
Training Epoch: 15 [45056/50000]	Loss: 4.4298	LR: 1.000000
Training Epoch: 15 [45312/50000]	Loss: 4.4099	LR: 1.000000
Training Epoch: 15 [45568/50000]	Loss: 4.4034	LR: 1.000000
Training Epoch: 15 [45824/50000]	Loss: 4.4059	LR: 1.000000
Training Epoch: 15 [46080/50000]	Loss: 4.4783	LR: 1.000000
Training Epoch: 15 [46336/50000]	Loss: 4.4179	LR: 1.000000
Training Epoch: 15 [46592/50000]	Loss: 4.3632	LR: 1.000000
Training Epoch: 15 [46848/50000]	Loss: 4.4094	LR: 1.000000
Training Epoch: 15 [47104/50000]	Loss: 4.4350	LR: 1.000000
Training Epoch: 15 [47360/50000]	Loss: 4.3541	LR: 1.000000
Training Epoch: 15 [47616/50000]	Loss: 4.4963	LR: 1.000000
Training Epoch: 15 [47872/50000]	Loss: 4.4532	LR: 1.000000
Training Epoch: 15 [48128/50000]	Loss: 4.3823	LR: 1.000000
Training Epoch: 15 [48384/50000]	Loss: 4.4260	LR: 1.000000
Training Epoch: 15 [48640/50000]	Loss: 4.3926	LR: 1.000000
Training Epoch: 15 [48896/50000]	Loss: 4.3947	LR: 1.000000
Training Epoch: 15 [49152/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 15 [49408/50000]	Loss: 4.4727	LR: 1.000000
Training Epoch: 15 [49664/50000]	Loss: 4.4707	LR: 1.000000
Training Epoch: 15 [49920/50000]	Loss: 4.3706	LR: 1.000000
Training Epoch: 15 [50000/50000]	Loss: 4.4749	LR: 1.000000
epoch 15 training time consumed: 22.03s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   22503 GB |   22503 GB |
|       from large pool |  400448 KB |    1770 MB |   22483 GB |   22483 GB |
|       from small pool |    3549 KB |       9 MB |      20 GB |      20 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   22503 GB |   22503 GB |
|       from large pool |  400448 KB |    1770 MB |   22483 GB |   22483 GB |
|       from small pool |    3549 KB |       9 MB |      20 GB |      20 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   13847 GB |   13847 GB |
|       from large pool |  244672 KB |  473024 KB |   13824 GB |   13824 GB |
|       from small pool |    2594 KB |    4843 KB |      23 GB |      23 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |     960 K  |     960 K  |
|       from large pool |      36    |      77    |     464 K  |     464 K  |
|       from small pool |     186    |     224    |     496 K  |     496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |     960 K  |     960 K  |
|       from large pool |      36    |      77    |     464 K  |     464 K  |
|       from small pool |     186    |     224    |     496 K  |     496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      31    |  487795    |  487774    |
|       from large pool |      10    |      11    |  192377    |  192367    |
|       from small pool |      11    |      21    |  295418    |  295407    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 15, Average loss: 0.0177, Accuracy: 0.0278, Time consumed:1.42s

Training Epoch: 16 [256/50000]	Loss: 4.5456	LR: 1.000000
Training Epoch: 16 [512/50000]	Loss: 4.4432	LR: 1.000000
Training Epoch: 16 [768/50000]	Loss: 4.4606	LR: 1.000000
Training Epoch: 16 [1024/50000]	Loss: 4.5240	LR: 1.000000
Training Epoch: 16 [1280/50000]	Loss: 4.4451	LR: 1.000000
Training Epoch: 16 [1536/50000]	Loss: 4.4980	LR: 1.000000
Training Epoch: 16 [1792/50000]	Loss: 4.5264	LR: 1.000000
Training Epoch: 16 [2048/50000]	Loss: 4.4200	LR: 1.000000
Training Epoch: 16 [2304/50000]	Loss: 4.3860	LR: 1.000000
Training Epoch: 16 [2560/50000]	Loss: 4.4612	LR: 1.000000
Training Epoch: 16 [2816/50000]	Loss: 4.4068	LR: 1.000000
Training Epoch: 16 [3072/50000]	Loss: 4.4922	LR: 1.000000
Training Epoch: 16 [3328/50000]	Loss: 4.5312	LR: 1.000000
Training Epoch: 16 [3584/50000]	Loss: 4.5563	LR: 1.000000
Training Epoch: 16 [3840/50000]	Loss: 4.4193	LR: 1.000000
Training Epoch: 16 [4096/50000]	Loss: 4.4766	LR: 1.000000
Training Epoch: 16 [4352/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 16 [4608/50000]	Loss: 4.5150	LR: 1.000000
Training Epoch: 16 [4864/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 16 [5120/50000]	Loss: 4.4710	LR: 1.000000
Training Epoch: 16 [5376/50000]	Loss: 4.3913	LR: 1.000000
Training Epoch: 16 [5632/50000]	Loss: 4.3809	LR: 1.000000
Training Epoch: 16 [5888/50000]	Loss: 4.3880	LR: 1.000000
Training Epoch: 16 [6144/50000]	Loss: 4.5156	LR: 1.000000
Training Epoch: 16 [6400/50000]	Loss: 4.4470	LR: 1.000000
Training Epoch: 16 [6656/50000]	Loss: 4.4910	LR: 1.000000
Training Epoch: 16 [6912/50000]	Loss: 4.4315	LR: 1.000000
Training Epoch: 16 [7168/50000]	Loss: 4.4627	LR: 1.000000
Training Epoch: 16 [7424/50000]	Loss: 4.5281	LR: 1.000000
Training Epoch: 16 [7680/50000]	Loss: 4.4860	LR: 1.000000
Training Epoch: 16 [7936/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 16 [8192/50000]	Loss: 4.4384	LR: 1.000000
Training Epoch: 16 [8448/50000]	Loss: 4.4505	LR: 1.000000
Training Epoch: 16 [8704/50000]	Loss: 4.4375	LR: 1.000000
Training Epoch: 16 [8960/50000]	Loss: 4.4247	LR: 1.000000
Training Epoch: 16 [9216/50000]	Loss: 4.4466	LR: 1.000000
Training Epoch: 16 [9472/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 16 [9728/50000]	Loss: 4.4406	LR: 1.000000
Training Epoch: 16 [9984/50000]	Loss: 4.4433	LR: 1.000000
Training Epoch: 16 [10240/50000]	Loss: 4.4692	LR: 1.000000
Training Epoch: 16 [10496/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 16 [10752/50000]	Loss: 4.4416	LR: 1.000000
Training Epoch: 16 [11008/50000]	Loss: 4.4696	LR: 1.000000
Training Epoch: 16 [11264/50000]	Loss: 4.4183	LR: 1.000000
Training Epoch: 16 [11520/50000]	Loss: 4.4371	LR: 1.000000
Training Epoch: 16 [11776/50000]	Loss: 4.3951	LR: 1.000000
Training Epoch: 16 [12032/50000]	Loss: 4.4003	LR: 1.000000
Training Epoch: 16 [12288/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 16 [12544/50000]	Loss: 4.5082	LR: 1.000000
Training Epoch: 16 [12800/50000]	Loss: 4.4579	LR: 1.000000
Training Epoch: 16 [13056/50000]	Loss: 4.4269	LR: 1.000000
Training Epoch: 16 [13312/50000]	Loss: 4.4019	LR: 1.000000
Training Epoch: 16 [13568/50000]	Loss: 4.3968	LR: 1.000000
Training Epoch: 16 [13824/50000]	Loss: 4.4244	LR: 1.000000
Training Epoch: 16 [14080/50000]	Loss: 4.4374	LR: 1.000000
Training Epoch: 16 [14336/50000]	Loss: 4.4343	LR: 1.000000
Training Epoch: 16 [14592/50000]	Loss: 4.4510	LR: 1.000000
Training Epoch: 16 [14848/50000]	Loss: 4.4261	LR: 1.000000
Training Epoch: 16 [15104/50000]	Loss: 4.3648	LR: 1.000000
Training Epoch: 16 [15360/50000]	Loss: 4.4450	LR: 1.000000
Training Epoch: 16 [15616/50000]	Loss: 4.3823	LR: 1.000000
Training Epoch: 16 [15872/50000]	Loss: 4.3762	LR: 1.000000
Training Epoch: 16 [16128/50000]	Loss: 4.4322	LR: 1.000000
Training Epoch: 16 [16384/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 16 [16640/50000]	Loss: 4.5082	LR: 1.000000
Training Epoch: 16 [16896/50000]	Loss: 4.4177	LR: 1.000000
Training Epoch: 16 [17152/50000]	Loss: 4.4520	LR: 1.000000
Training Epoch: 16 [17408/50000]	Loss: 4.3931	LR: 1.000000
Training Epoch: 16 [17664/50000]	Loss: 4.4232	LR: 1.000000
Training Epoch: 16 [17920/50000]	Loss: 4.3971	LR: 1.000000
Training Epoch: 16 [18176/50000]	Loss: 4.4068	LR: 1.000000
Training Epoch: 16 [18432/50000]	Loss: 4.4536	LR: 1.000000
Training Epoch: 16 [18688/50000]	Loss: 4.4729	LR: 1.000000
Training Epoch: 16 [18944/50000]	Loss: 4.4564	LR: 1.000000
Training Epoch: 16 [19200/50000]	Loss: 4.4916	LR: 1.000000
Training Epoch: 16 [19456/50000]	Loss: 4.4064	LR: 1.000000
Training Epoch: 16 [19712/50000]	Loss: 4.4448	LR: 1.000000
Training Epoch: 16 [19968/50000]	Loss: 4.4549	LR: 1.000000
Training Epoch: 16 [20224/50000]	Loss: 4.4522	LR: 1.000000
Training Epoch: 16 [20480/50000]	Loss: 4.4110	LR: 1.000000
Training Epoch: 16 [20736/50000]	Loss: 4.4604	LR: 1.000000
Training Epoch: 16 [20992/50000]	Loss: 4.4093	LR: 1.000000
Training Epoch: 16 [21248/50000]	Loss: 4.3994	LR: 1.000000
Training Epoch: 16 [21504/50000]	Loss: 4.4265	LR: 1.000000
Training Epoch: 16 [21760/50000]	Loss: 4.3933	LR: 1.000000
Training Epoch: 16 [22016/50000]	Loss: 4.4294	LR: 1.000000
Training Epoch: 16 [22272/50000]	Loss: 4.4604	LR: 1.000000
Training Epoch: 16 [22528/50000]	Loss: 4.4910	LR: 1.000000
Training Epoch: 16 [22784/50000]	Loss: 4.4518	LR: 1.000000
Training Epoch: 16 [23040/50000]	Loss: 4.3906	LR: 1.000000
Training Epoch: 16 [23296/50000]	Loss: 4.4575	LR: 1.000000
Training Epoch: 16 [23552/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 16 [23808/50000]	Loss: 4.4595	LR: 1.000000
Training Epoch: 16 [24064/50000]	Loss: 4.3578	LR: 1.000000
Training Epoch: 16 [24320/50000]	Loss: 4.4252	LR: 1.000000
Training Epoch: 16 [24576/50000]	Loss: 4.4226	LR: 1.000000
Training Epoch: 16 [24832/50000]	Loss: 4.4472	LR: 1.000000
Training Epoch: 16 [25088/50000]	Loss: 4.4578	LR: 1.000000
Training Epoch: 16 [25344/50000]	Loss: 4.4685	LR: 1.000000
Training Epoch: 16 [25600/50000]	Loss: 4.3718	LR: 1.000000
Training Epoch: 16 [25856/50000]	Loss: 4.4794	LR: 1.000000
Training Epoch: 16 [26112/50000]	Loss: 4.4708	LR: 1.000000
Training Epoch: 16 [26368/50000]	Loss: 4.4591	LR: 1.000000
Training Epoch: 16 [26624/50000]	Loss: 4.4410	LR: 1.000000
Training Epoch: 16 [26880/50000]	Loss: 4.4119	LR: 1.000000
Training Epoch: 16 [27136/50000]	Loss: 4.4266	LR: 1.000000
Training Epoch: 16 [27392/50000]	Loss: 4.4292	LR: 1.000000
Training Epoch: 16 [27648/50000]	Loss: 4.4241	LR: 1.000000
Training Epoch: 16 [27904/50000]	Loss: 4.4399	LR: 1.000000
Training Epoch: 16 [28160/50000]	Loss: 4.4243	LR: 1.000000
Training Epoch: 16 [28416/50000]	Loss: 4.4602	LR: 1.000000
Training Epoch: 16 [28672/50000]	Loss: 4.3775	LR: 1.000000
Training Epoch: 16 [28928/50000]	Loss: 4.4121	LR: 1.000000
Training Epoch: 16 [29184/50000]	Loss: 4.4111	LR: 1.000000
Training Epoch: 16 [29440/50000]	Loss: 4.3843	LR: 1.000000
Training Epoch: 16 [29696/50000]	Loss: 4.4513	LR: 1.000000
Training Epoch: 16 [29952/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 16 [30208/50000]	Loss: 4.3531	LR: 1.000000
Training Epoch: 16 [30464/50000]	Loss: 4.3731	LR: 1.000000
Training Epoch: 16 [30720/50000]	Loss: 4.4898	LR: 1.000000
Training Epoch: 16 [30976/50000]	Loss: 4.4070	LR: 1.000000
Training Epoch: 16 [31232/50000]	Loss: 4.5231	LR: 1.000000
Training Epoch: 16 [31488/50000]	Loss: 4.3431	LR: 1.000000
Training Epoch: 16 [31744/50000]	Loss: 4.4250	LR: 1.000000
Training Epoch: 16 [32000/50000]	Loss: 4.3910	LR: 1.000000
Training Epoch: 16 [32256/50000]	Loss: 4.5740	LR: 1.000000
Training Epoch: 16 [32512/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 16 [32768/50000]	Loss: 4.3785	LR: 1.000000
Training Epoch: 16 [33024/50000]	Loss: 4.3915	LR: 1.000000
Training Epoch: 16 [33280/50000]	Loss: 4.3966	LR: 1.000000
Training Epoch: 16 [33536/50000]	Loss: 4.4422	LR: 1.000000
Training Epoch: 16 [33792/50000]	Loss: 4.4668	LR: 1.000000
Training Epoch: 16 [34048/50000]	Loss: 4.4216	LR: 1.000000
Training Epoch: 16 [34304/50000]	Loss: 4.4847	LR: 1.000000
Training Epoch: 16 [34560/50000]	Loss: 4.4326	LR: 1.000000
Training Epoch: 16 [34816/50000]	Loss: 4.4031	LR: 1.000000
Training Epoch: 16 [35072/50000]	Loss: 4.4080	LR: 1.000000
Training Epoch: 16 [35328/50000]	Loss: 4.4635	LR: 1.000000
Training Epoch: 16 [35584/50000]	Loss: 4.4439	LR: 1.000000
Training Epoch: 16 [35840/50000]	Loss: 4.4398	LR: 1.000000
Training Epoch: 16 [36096/50000]	Loss: 4.4906	LR: 1.000000
Training Epoch: 16 [36352/50000]	Loss: 4.3845	LR: 1.000000
Training Epoch: 16 [36608/50000]	Loss: 4.4278	LR: 1.000000
Training Epoch: 16 [36864/50000]	Loss: 4.4578	LR: 1.000000
Training Epoch: 16 [37120/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 16 [37376/50000]	Loss: 4.4869	LR: 1.000000
Training Epoch: 16 [37632/50000]	Loss: 4.6366	LR: 1.000000
Training Epoch: 16 [37888/50000]	Loss: 4.4130	LR: 1.000000
Training Epoch: 16 [38144/50000]	Loss: 4.3885	LR: 1.000000
Training Epoch: 16 [38400/50000]	Loss: 4.3969	LR: 1.000000
Training Epoch: 16 [38656/50000]	Loss: 4.4301	LR: 1.000000
Training Epoch: 16 [38912/50000]	Loss: 4.4270	LR: 1.000000
Training Epoch: 16 [39168/50000]	Loss: 4.4185	LR: 1.000000
Training Epoch: 16 [39424/50000]	Loss: 4.4620	LR: 1.000000
Training Epoch: 16 [39680/50000]	Loss: 4.7943	LR: 1.000000
Training Epoch: 16 [39936/50000]	Loss: 4.4851	LR: 1.000000
Training Epoch: 16 [40192/50000]	Loss: 4.4857	LR: 1.000000
Training Epoch: 16 [40448/50000]	Loss: 4.4794	LR: 1.000000
Training Epoch: 16 [40704/50000]	Loss: 4.5026	LR: 1.000000
Training Epoch: 16 [40960/50000]	Loss: 4.4923	LR: 1.000000
Training Epoch: 16 [41216/50000]	Loss: 4.4636	LR: 1.000000
Training Epoch: 16 [41472/50000]	Loss: 4.4162	LR: 1.000000
Training Epoch: 16 [41728/50000]	Loss: 4.4164	LR: 1.000000
Training Epoch: 16 [41984/50000]	Loss: 4.4663	LR: 1.000000
Training Epoch: 16 [42240/50000]	Loss: 4.4875	LR: 1.000000
Training Epoch: 16 [42496/50000]	Loss: 4.4393	LR: 1.000000
Training Epoch: 16 [42752/50000]	Loss: 4.4932	LR: 1.000000
Training Epoch: 16 [43008/50000]	Loss: 4.4578	LR: 1.000000
Training Epoch: 16 [43264/50000]	Loss: 4.3971	LR: 1.000000
Training Epoch: 16 [43520/50000]	Loss: 4.4860	LR: 1.000000
Training Epoch: 16 [43776/50000]	Loss: 4.4237	LR: 1.000000
Training Epoch: 16 [44032/50000]	Loss: 4.4708	LR: 1.000000
Training Epoch: 16 [44288/50000]	Loss: 4.3758	LR: 1.000000
Training Epoch: 16 [44544/50000]	Loss: 4.4642	LR: 1.000000
Training Epoch: 16 [44800/50000]	Loss: 4.4612	LR: 1.000000
Training Epoch: 16 [45056/50000]	Loss: 4.4533	LR: 1.000000
Training Epoch: 16 [45312/50000]	Loss: 4.4014	LR: 1.000000
Training Epoch: 16 [45568/50000]	Loss: 4.4619	LR: 1.000000
Training Epoch: 16 [45824/50000]	Loss: 4.3931	LR: 1.000000
Training Epoch: 16 [46080/50000]	Loss: 4.4291	LR: 1.000000
Training Epoch: 16 [46336/50000]	Loss: 4.4384	LR: 1.000000
Training Epoch: 16 [46592/50000]	Loss: 4.4304	LR: 1.000000
Training Epoch: 16 [46848/50000]	Loss: 4.5177	LR: 1.000000
Training Epoch: 16 [47104/50000]	Loss: 4.4880	LR: 1.000000
Training Epoch: 16 [47360/50000]	Loss: 4.5224	LR: 1.000000
Training Epoch: 16 [47616/50000]	Loss: 4.4863	LR: 1.000000
Training Epoch: 16 [47872/50000]	Loss: 4.4196	LR: 1.000000
Training Epoch: 16 [48128/50000]	Loss: 4.3850	LR: 1.000000
Training Epoch: 16 [48384/50000]	Loss: 4.4593	LR: 1.000000
Training Epoch: 16 [48640/50000]	Loss: 4.4625	LR: 1.000000
Training Epoch: 16 [48896/50000]	Loss: 4.4458	LR: 1.000000
Training Epoch: 16 [49152/50000]	Loss: 4.4682	LR: 1.000000
Training Epoch: 16 [49408/50000]	Loss: 4.4354	LR: 1.000000
Training Epoch: 16 [49664/50000]	Loss: 4.3809	LR: 1.000000
Training Epoch: 16 [49920/50000]	Loss: 4.4121	LR: 1.000000
Training Epoch: 16 [50000/50000]	Loss: 4.5133	LR: 1.000000
epoch 16 training time consumed: 21.99s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   24003 GB |   24003 GB |
|       from large pool |  400448 KB |    1770 MB |   23982 GB |   23981 GB |
|       from small pool |    3549 KB |       9 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   24003 GB |   24003 GB |
|       from large pool |  400448 KB |    1770 MB |   23982 GB |   23981 GB |
|       from small pool |    3549 KB |       9 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   14770 GB |   14770 GB |
|       from large pool |  244672 KB |  473024 KB |   14745 GB |   14745 GB |
|       from small pool |    2594 KB |    4843 KB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1024 K  |    1024 K  |
|       from large pool |      36    |      77    |     495 K  |     495 K  |
|       from small pool |     186    |     224    |     529 K  |     529 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1024 K  |    1024 K  |
|       from large pool |      36    |      77    |     495 K  |     495 K  |
|       from small pool |     186    |     224    |     529 K  |     529 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      31    |  520523    |  520502    |
|       from large pool |      10    |      11    |  205200    |  205190    |
|       from small pool |      11    |      21    |  315323    |  315312    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 16, Average loss: 0.0177, Accuracy: 0.0258, Time consumed:1.44s

Training Epoch: 17 [256/50000]	Loss: 4.4439	LR: 1.000000
Training Epoch: 17 [512/50000]	Loss: 4.3922	LR: 1.000000
Training Epoch: 17 [768/50000]	Loss: 4.4076	LR: 1.000000
Training Epoch: 17 [1024/50000]	Loss: 4.4169	LR: 1.000000
Training Epoch: 17 [1280/50000]	Loss: 4.3831	LR: 1.000000
Training Epoch: 17 [1536/50000]	Loss: 4.4375	LR: 1.000000
Training Epoch: 17 [1792/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 17 [2048/50000]	Loss: 4.4037	LR: 1.000000
Training Epoch: 17 [2304/50000]	Loss: 4.3914	LR: 1.000000
Training Epoch: 17 [2560/50000]	Loss: 4.4315	LR: 1.000000
Training Epoch: 17 [2816/50000]	Loss: 4.4335	LR: 1.000000
Training Epoch: 17 [3072/50000]	Loss: 4.4965	LR: 1.000000
Training Epoch: 17 [3328/50000]	Loss: 4.4692	LR: 1.000000
Training Epoch: 17 [3584/50000]	Loss: 4.4886	LR: 1.000000
Training Epoch: 17 [3840/50000]	Loss: 4.4684	LR: 1.000000
Training Epoch: 17 [4096/50000]	Loss: 4.4618	LR: 1.000000
Training Epoch: 17 [4352/50000]	Loss: 4.4717	LR: 1.000000
Training Epoch: 17 [4608/50000]	Loss: 4.4912	LR: 1.000000
Training Epoch: 17 [4864/50000]	Loss: 4.4786	LR: 1.000000
Training Epoch: 17 [5120/50000]	Loss: 4.4120	LR: 1.000000
Training Epoch: 17 [5376/50000]	Loss: 4.4485	LR: 1.000000
Training Epoch: 17 [5632/50000]	Loss: 4.4005	LR: 1.000000
Training Epoch: 17 [5888/50000]	Loss: 4.4109	LR: 1.000000
Training Epoch: 17 [6144/50000]	Loss: 4.3981	LR: 1.000000
Training Epoch: 17 [6400/50000]	Loss: 4.4083	LR: 1.000000
Training Epoch: 17 [6656/50000]	Loss: 4.4537	LR: 1.000000
Training Epoch: 17 [6912/50000]	Loss: 4.4336	LR: 1.000000
Training Epoch: 17 [7168/50000]	Loss: 4.4525	LR: 1.000000
Training Epoch: 17 [7424/50000]	Loss: 4.4860	LR: 1.000000
Training Epoch: 17 [7680/50000]	Loss: 4.4392	LR: 1.000000
Training Epoch: 17 [7936/50000]	Loss: 4.4759	LR: 1.000000
Training Epoch: 17 [8192/50000]	Loss: 4.4756	LR: 1.000000
Training Epoch: 17 [8448/50000]	Loss: 4.4064	LR: 1.000000
Training Epoch: 17 [8704/50000]	Loss: 4.4068	LR: 1.000000
Training Epoch: 17 [8960/50000]	Loss: 4.4517	LR: 1.000000
Training Epoch: 17 [9216/50000]	Loss: 4.4368	LR: 1.000000
Training Epoch: 17 [9472/50000]	Loss: 4.4867	LR: 1.000000
Training Epoch: 17 [9728/50000]	Loss: 4.4416	LR: 1.000000
Training Epoch: 17 [9984/50000]	Loss: 4.4471	LR: 1.000000
Training Epoch: 17 [10240/50000]	Loss: 4.4408	LR: 1.000000
Training Epoch: 17 [10496/50000]	Loss: 4.4607	LR: 1.000000
Training Epoch: 17 [10752/50000]	Loss: 4.4921	LR: 1.000000
Training Epoch: 17 [11008/50000]	Loss: 4.4221	LR: 1.000000
Training Epoch: 17 [11264/50000]	Loss: 4.4699	LR: 1.000000
Training Epoch: 17 [11520/50000]	Loss: 4.4107	LR: 1.000000
Training Epoch: 17 [11776/50000]	Loss: 4.4067	LR: 1.000000
Training Epoch: 17 [12032/50000]	Loss: 4.4357	LR: 1.000000
Training Epoch: 17 [12288/50000]	Loss: 4.3587	LR: 1.000000
Training Epoch: 17 [12544/50000]	Loss: 4.4521	LR: 1.000000
Training Epoch: 17 [12800/50000]	Loss: 4.4244	LR: 1.000000
Training Epoch: 17 [13056/50000]	Loss: 4.5393	LR: 1.000000
Training Epoch: 17 [13312/50000]	Loss: 4.3786	LR: 1.000000
Training Epoch: 17 [13568/50000]	Loss: 4.5039	LR: 1.000000
Training Epoch: 17 [13824/50000]	Loss: 4.5652	LR: 1.000000
Training Epoch: 17 [14080/50000]	Loss: 4.4559	LR: 1.000000
Training Epoch: 17 [14336/50000]	Loss: 4.4068	LR: 1.000000
Training Epoch: 17 [14592/50000]	Loss: 4.4333	LR: 1.000000
Training Epoch: 17 [14848/50000]	Loss: 4.4678	LR: 1.000000
Training Epoch: 17 [15104/50000]	Loss: 4.4700	LR: 1.000000
Training Epoch: 17 [15360/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 17 [15616/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 17 [15872/50000]	Loss: 4.4414	LR: 1.000000
Training Epoch: 17 [16128/50000]	Loss: 4.4439	LR: 1.000000
Training Epoch: 17 [16384/50000]	Loss: 4.4355	LR: 1.000000
Training Epoch: 17 [16640/50000]	Loss: 4.4614	LR: 1.000000
Training Epoch: 17 [16896/50000]	Loss: 4.4661	LR: 1.000000
Training Epoch: 17 [17152/50000]	Loss: 4.4399	LR: 1.000000
Training Epoch: 17 [17408/50000]	Loss: 4.4576	LR: 1.000000
Training Epoch: 17 [17664/50000]	Loss: 4.4302	LR: 1.000000
Training Epoch: 17 [17920/50000]	Loss: 4.4463	LR: 1.000000
Training Epoch: 17 [18176/50000]	Loss: 4.4264	LR: 1.000000
Training Epoch: 17 [18432/50000]	Loss: 4.4400	LR: 1.000000
Training Epoch: 17 [18688/50000]	Loss: 4.4254	LR: 1.000000
Training Epoch: 17 [18944/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 17 [19200/50000]	Loss: 4.5279	LR: 1.000000
Training Epoch: 17 [19456/50000]	Loss: 4.5213	LR: 1.000000
Training Epoch: 17 [19712/50000]	Loss: 4.5735	LR: 1.000000
Training Epoch: 17 [19968/50000]	Loss: 4.5107	LR: 1.000000
Training Epoch: 17 [20224/50000]	Loss: 4.5346	LR: 1.000000
Training Epoch: 17 [20480/50000]	Loss: 4.5498	LR: 1.000000
Training Epoch: 17 [20736/50000]	Loss: 4.5870	LR: 1.000000
Training Epoch: 17 [20992/50000]	Loss: 4.5754	LR: 1.000000
Training Epoch: 17 [21248/50000]	Loss: 4.5873	LR: 1.000000
Training Epoch: 17 [21504/50000]	Loss: 4.5755	LR: 1.000000
Training Epoch: 17 [21760/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 17 [22016/50000]	Loss: 4.5266	LR: 1.000000
Training Epoch: 17 [22272/50000]	Loss: 4.5815	LR: 1.000000
Training Epoch: 17 [22528/50000]	Loss: 4.5030	LR: 1.000000
Training Epoch: 17 [22784/50000]	Loss: 4.5053	LR: 1.000000
Training Epoch: 17 [23040/50000]	Loss: 4.5091	LR: 1.000000
Training Epoch: 17 [23296/50000]	Loss: 4.5597	LR: 1.000000
Training Epoch: 17 [23552/50000]	Loss: 4.5583	LR: 1.000000
Training Epoch: 17 [23808/50000]	Loss: 4.5174	LR: 1.000000
Training Epoch: 17 [24064/50000]	Loss: 4.4130	LR: 1.000000
Training Epoch: 17 [24320/50000]	Loss: 4.4802	LR: 1.000000
Training Epoch: 17 [24576/50000]	Loss: 4.4780	LR: 1.000000
Training Epoch: 17 [24832/50000]	Loss: 4.4821	LR: 1.000000
Training Epoch: 17 [25088/50000]	Loss: 4.4631	LR: 1.000000
Training Epoch: 17 [25344/50000]	Loss: 4.4679	LR: 1.000000
Training Epoch: 17 [25600/50000]	Loss: 4.4088	LR: 1.000000
Training Epoch: 17 [25856/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 17 [26112/50000]	Loss: 4.4398	LR: 1.000000
Training Epoch: 17 [26368/50000]	Loss: 4.4929	LR: 1.000000
Training Epoch: 17 [26624/50000]	Loss: 4.4855	LR: 1.000000
Training Epoch: 17 [26880/50000]	Loss: 4.4556	LR: 1.000000
Training Epoch: 17 [27136/50000]	Loss: 4.4357	LR: 1.000000
Training Epoch: 17 [27392/50000]	Loss: 4.4442	LR: 1.000000
Training Epoch: 17 [27648/50000]	Loss: 4.4400	LR: 1.000000
Training Epoch: 17 [27904/50000]	Loss: 4.4265	LR: 1.000000
Training Epoch: 17 [28160/50000]	Loss: 4.3597	LR: 1.000000
Training Epoch: 17 [28416/50000]	Loss: 4.4169	LR: 1.000000
Training Epoch: 17 [28672/50000]	Loss: 4.3802	LR: 1.000000
Training Epoch: 17 [28928/50000]	Loss: 4.4869	LR: 1.000000
Training Epoch: 17 [29184/50000]	Loss: 4.4475	LR: 1.000000
Training Epoch: 17 [29440/50000]	Loss: 4.4462	LR: 1.000000
Training Epoch: 17 [29696/50000]	Loss: 4.4106	LR: 1.000000
Training Epoch: 17 [29952/50000]	Loss: 4.3916	LR: 1.000000
Training Epoch: 17 [30208/50000]	Loss: 4.3929	LR: 1.000000
Training Epoch: 17 [30464/50000]	Loss: 4.4477	LR: 1.000000
Training Epoch: 17 [30720/50000]	Loss: 4.5028	LR: 1.000000
Training Epoch: 17 [30976/50000]	Loss: 4.3376	LR: 1.000000
Training Epoch: 17 [31232/50000]	Loss: 4.4360	LR: 1.000000
Training Epoch: 17 [31488/50000]	Loss: 4.4197	LR: 1.000000
Training Epoch: 17 [31744/50000]	Loss: 4.4281	LR: 1.000000
Training Epoch: 17 [32000/50000]	Loss: 4.4227	LR: 1.000000
Training Epoch: 17 [32256/50000]	Loss: 4.4321	LR: 1.000000
Training Epoch: 17 [32512/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 17 [32768/50000]	Loss: 4.4671	LR: 1.000000
Training Epoch: 17 [33024/50000]	Loss: 4.4975	LR: 1.000000
Training Epoch: 17 [33280/50000]	Loss: 4.4310	LR: 1.000000
Training Epoch: 17 [33536/50000]	Loss: 4.4106	LR: 1.000000
Training Epoch: 17 [33792/50000]	Loss: 4.4718	LR: 1.000000
Training Epoch: 17 [34048/50000]	Loss: 4.4429	LR: 1.000000
Training Epoch: 17 [34304/50000]	Loss: 4.5049	LR: 1.000000
Training Epoch: 17 [34560/50000]	Loss: 4.4332	LR: 1.000000
Training Epoch: 17 [34816/50000]	Loss: 4.4417	LR: 1.000000
Training Epoch: 17 [35072/50000]	Loss: 4.4107	LR: 1.000000
Training Epoch: 17 [35328/50000]	Loss: 4.4795	LR: 1.000000
Training Epoch: 17 [35584/50000]	Loss: 4.4829	LR: 1.000000
Training Epoch: 17 [35840/50000]	Loss: 4.4548	LR: 1.000000
Training Epoch: 17 [36096/50000]	Loss: 4.3868	LR: 1.000000
Training Epoch: 17 [36352/50000]	Loss: 4.3632	LR: 1.000000
Training Epoch: 17 [36608/50000]	Loss: 4.5371	LR: 1.000000
Training Epoch: 17 [36864/50000]	Loss: 4.4330	LR: 1.000000
Training Epoch: 17 [37120/50000]	Loss: 4.4232	LR: 1.000000
Training Epoch: 17 [37376/50000]	Loss: 4.4037	LR: 1.000000
Training Epoch: 17 [37632/50000]	Loss: 4.4895	LR: 1.000000
Training Epoch: 17 [37888/50000]	Loss: 4.4265	LR: 1.000000
Training Epoch: 17 [38144/50000]	Loss: 4.4763	LR: 1.000000
Training Epoch: 17 [38400/50000]	Loss: 4.3750	LR: 1.000000
Training Epoch: 17 [38656/50000]	Loss: 4.3655	LR: 1.000000
Training Epoch: 17 [38912/50000]	Loss: 4.4558	LR: 1.000000
Training Epoch: 17 [39168/50000]	Loss: 4.4936	LR: 1.000000
Training Epoch: 17 [39424/50000]	Loss: 4.4037	LR: 1.000000
Training Epoch: 17 [39680/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 17 [39936/50000]	Loss: 4.3755	LR: 1.000000
Training Epoch: 17 [40192/50000]	Loss: 4.4556	LR: 1.000000
Training Epoch: 17 [40448/50000]	Loss: 4.4426	LR: 1.000000
Training Epoch: 17 [40704/50000]	Loss: 4.4326	LR: 1.000000
Training Epoch: 17 [40960/50000]	Loss: 4.4574	LR: 1.000000
Training Epoch: 17 [41216/50000]	Loss: 4.4531	LR: 1.000000
Training Epoch: 17 [41472/50000]	Loss: 4.4443	LR: 1.000000
Training Epoch: 17 [41728/50000]	Loss: 4.4459	LR: 1.000000
Training Epoch: 17 [41984/50000]	Loss: 4.3961	LR: 1.000000
Training Epoch: 17 [42240/50000]	Loss: 4.3890	LR: 1.000000
Training Epoch: 17 [42496/50000]	Loss: 4.4397	LR: 1.000000
Training Epoch: 17 [42752/50000]	Loss: 4.4776	LR: 1.000000
Training Epoch: 17 [43008/50000]	Loss: 4.4102	LR: 1.000000
Training Epoch: 17 [43264/50000]	Loss: 4.4218	LR: 1.000000
Training Epoch: 17 [43520/50000]	Loss: 4.5161	LR: 1.000000
Training Epoch: 17 [43776/50000]	Loss: 4.4959	LR: 1.000000
Training Epoch: 17 [44032/50000]	Loss: 4.4618	LR: 1.000000
Training Epoch: 17 [44288/50000]	Loss: 4.4239	LR: 1.000000
Training Epoch: 17 [44544/50000]	Loss: 4.4559	LR: 1.000000
Training Epoch: 17 [44800/50000]	Loss: 4.3979	LR: 1.000000
Training Epoch: 17 [45056/50000]	Loss: 4.4329	LR: 1.000000
Training Epoch: 17 [45312/50000]	Loss: 4.4582	LR: 1.000000
Training Epoch: 17 [45568/50000]	Loss: 4.4315	LR: 1.000000
Training Epoch: 17 [45824/50000]	Loss: 4.4111	LR: 1.000000
Training Epoch: 17 [46080/50000]	Loss: 4.3561	LR: 1.000000
Training Epoch: 17 [46336/50000]	Loss: 4.4044	LR: 1.000000
Training Epoch: 17 [46592/50000]	Loss: 4.3879	LR: 1.000000
Training Epoch: 17 [46848/50000]	Loss: 4.4274	LR: 1.000000
Training Epoch: 17 [47104/50000]	Loss: 4.4232	LR: 1.000000
Training Epoch: 17 [47360/50000]	Loss: 4.4248	LR: 1.000000
Training Epoch: 17 [47616/50000]	Loss: 4.4000	LR: 1.000000
Training Epoch: 17 [47872/50000]	Loss: 4.5011	LR: 1.000000
Training Epoch: 17 [48128/50000]	Loss: 4.5262	LR: 1.000000
Training Epoch: 17 [48384/50000]	Loss: 4.5157	LR: 1.000000
Training Epoch: 17 [48640/50000]	Loss: 4.4311	LR: 1.000000
Training Epoch: 17 [48896/50000]	Loss: 4.4617	LR: 1.000000
Training Epoch: 17 [49152/50000]	Loss: 4.4952	LR: 1.000000
Training Epoch: 17 [49408/50000]	Loss: 4.4175	LR: 1.000000
Training Epoch: 17 [49664/50000]	Loss: 4.4893	LR: 1.000000
Training Epoch: 17 [49920/50000]	Loss: 4.4853	LR: 1.000000
Training Epoch: 17 [50000/50000]	Loss: 4.6148	LR: 1.000000
epoch 17 training time consumed: 22.05s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   25504 GB |   25503 GB |
|       from large pool |  400448 KB |    1770 MB |   25481 GB |   25480 GB |
|       from small pool |    3549 KB |       9 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   25504 GB |   25503 GB |
|       from large pool |  400448 KB |    1770 MB |   25481 GB |   25480 GB |
|       from small pool |    3549 KB |       9 MB |      22 GB |      22 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   15693 GB |   15693 GB |
|       from large pool |  244672 KB |  473024 KB |   15667 GB |   15667 GB |
|       from small pool |    2594 KB |    4843 KB |      26 GB |      26 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1088 K  |    1088 K  |
|       from large pool |      36    |      77    |     526 K  |     526 K  |
|       from small pool |     186    |     224    |     562 K  |     562 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1088 K  |    1088 K  |
|       from large pool |      36    |      77    |     526 K  |     526 K  |
|       from small pool |     186    |     224    |     562 K  |     562 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      31    |  552568    |  552546    |
|       from large pool |      10    |      11    |  218023    |  218013    |
|       from small pool |      12    |      21    |  334545    |  334533    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 17, Average loss: 0.0181, Accuracy: 0.0207, Time consumed:1.46s

Training Epoch: 18 [256/50000]	Loss: 4.4817	LR: 1.000000
Training Epoch: 18 [512/50000]	Loss: 4.4559	LR: 1.000000
Training Epoch: 18 [768/50000]	Loss: 4.4136	LR: 1.000000
Training Epoch: 18 [1024/50000]	Loss: 4.4727	LR: 1.000000
Training Epoch: 18 [1280/50000]	Loss: 4.4186	LR: 1.000000
Training Epoch: 18 [1536/50000]	Loss: 4.4356	LR: 1.000000
Training Epoch: 18 [1792/50000]	Loss: 4.4237	LR: 1.000000
Training Epoch: 18 [2048/50000]	Loss: 4.4993	LR: 1.000000
Training Epoch: 18 [2304/50000]	Loss: 4.4697	LR: 1.000000
Training Epoch: 18 [2560/50000]	Loss: 4.7629	LR: 1.000000
Training Epoch: 18 [2816/50000]	Loss: 4.4510	LR: 1.000000
Training Epoch: 18 [3072/50000]	Loss: 4.4957	LR: 1.000000
Training Epoch: 18 [3328/50000]	Loss: 4.4566	LR: 1.000000
Training Epoch: 18 [3584/50000]	Loss: 4.4209	LR: 1.000000
Training Epoch: 18 [3840/50000]	Loss: 4.4262	LR: 1.000000
Training Epoch: 18 [4096/50000]	Loss: 4.5046	LR: 1.000000
Training Epoch: 18 [4352/50000]	Loss: 4.4189	LR: 1.000000
Training Epoch: 18 [4608/50000]	Loss: 4.4463	LR: 1.000000
Training Epoch: 18 [4864/50000]	Loss: 4.3973	LR: 1.000000
Training Epoch: 18 [5120/50000]	Loss: 4.4034	LR: 1.000000
Training Epoch: 18 [5376/50000]	Loss: 4.4942	LR: 1.000000
Training Epoch: 18 [5632/50000]	Loss: 4.4958	LR: 1.000000
Training Epoch: 18 [5888/50000]	Loss: 4.3717	LR: 1.000000
Training Epoch: 18 [6144/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 18 [6400/50000]	Loss: 4.4181	LR: 1.000000
Training Epoch: 18 [6656/50000]	Loss: 4.4267	LR: 1.000000
Training Epoch: 18 [6912/50000]	Loss: 4.5019	LR: 1.000000
Training Epoch: 18 [7168/50000]	Loss: 4.4298	LR: 1.000000
Training Epoch: 18 [7424/50000]	Loss: 4.4018	LR: 1.000000
Training Epoch: 18 [7680/50000]	Loss: 4.4423	LR: 1.000000
Training Epoch: 18 [7936/50000]	Loss: 4.3470	LR: 1.000000
Training Epoch: 18 [8192/50000]	Loss: 4.4386	LR: 1.000000
Training Epoch: 18 [8448/50000]	Loss: 4.3856	LR: 1.000000
Training Epoch: 18 [8704/50000]	Loss: 4.4206	LR: 1.000000
Training Epoch: 18 [8960/50000]	Loss: 4.4549	LR: 1.000000
Training Epoch: 18 [9216/50000]	Loss: 4.4695	LR: 1.000000
Training Epoch: 18 [9472/50000]	Loss: 4.4587	LR: 1.000000
Training Epoch: 18 [9728/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 18 [9984/50000]	Loss: 4.4326	LR: 1.000000
Training Epoch: 18 [10240/50000]	Loss: 4.5034	LR: 1.000000
Training Epoch: 18 [10496/50000]	Loss: 4.4376	LR: 1.000000
Training Epoch: 18 [10752/50000]	Loss: 4.5132	LR: 1.000000
Training Epoch: 18 [11008/50000]	Loss: 4.4493	LR: 1.000000
Training Epoch: 18 [11264/50000]	Loss: 4.4366	LR: 1.000000
Training Epoch: 18 [11520/50000]	Loss: 4.4655	LR: 1.000000
Training Epoch: 18 [11776/50000]	Loss: 4.4023	LR: 1.000000
Training Epoch: 18 [12032/50000]	Loss: 4.4403	LR: 1.000000
Training Epoch: 18 [12288/50000]	Loss: 4.4511	LR: 1.000000
Training Epoch: 18 [12544/50000]	Loss: 4.5195	LR: 1.000000
Training Epoch: 18 [12800/50000]	Loss: 4.4131	LR: 1.000000
Training Epoch: 18 [13056/50000]	Loss: 4.4095	LR: 1.000000
Training Epoch: 18 [13312/50000]	Loss: 4.3853	LR: 1.000000
Training Epoch: 18 [13568/50000]	Loss: 4.5265	LR: 1.000000
Training Epoch: 18 [13824/50000]	Loss: 4.4525	LR: 1.000000
Training Epoch: 18 [14080/50000]	Loss: 4.5245	LR: 1.000000
Training Epoch: 18 [14336/50000]	Loss: 4.4393	LR: 1.000000
Training Epoch: 18 [14592/50000]	Loss: 4.4406	LR: 1.000000
Training Epoch: 18 [14848/50000]	Loss: 4.4725	LR: 1.000000
Training Epoch: 18 [15104/50000]	Loss: 4.3951	LR: 1.000000
Training Epoch: 18 [15360/50000]	Loss: 4.5231	LR: 1.000000
Training Epoch: 18 [15616/50000]	Loss: 4.5099	LR: 1.000000
Training Epoch: 18 [15872/50000]	Loss: 4.3966	LR: 1.000000
Training Epoch: 18 [16128/50000]	Loss: 4.4344	LR: 1.000000
Training Epoch: 18 [16384/50000]	Loss: 4.5064	LR: 1.000000
Training Epoch: 18 [16640/50000]	Loss: 4.5320	LR: 1.000000
Training Epoch: 18 [16896/50000]	Loss: 4.5225	LR: 1.000000
Training Epoch: 18 [17152/50000]	Loss: 4.4105	LR: 1.000000
Training Epoch: 18 [17408/50000]	Loss: 4.4264	LR: 1.000000
Training Epoch: 18 [17664/50000]	Loss: 4.4112	LR: 1.000000
Training Epoch: 18 [17920/50000]	Loss: 4.4008	LR: 1.000000
Training Epoch: 18 [18176/50000]	Loss: 4.4456	LR: 1.000000
Training Epoch: 18 [18432/50000]	Loss: 4.4932	LR: 1.000000
Training Epoch: 18 [18688/50000]	Loss: 4.5129	LR: 1.000000
Training Epoch: 18 [18944/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 18 [19200/50000]	Loss: 4.4717	LR: 1.000000
Training Epoch: 18 [19456/50000]	Loss: 4.5110	LR: 1.000000
Training Epoch: 18 [19712/50000]	Loss: 4.4858	LR: 1.000000
Training Epoch: 18 [19968/50000]	Loss: 4.4759	LR: 1.000000
Training Epoch: 18 [20224/50000]	Loss: 4.4011	LR: 1.000000
Training Epoch: 18 [20480/50000]	Loss: 4.4977	LR: 1.000000
Training Epoch: 18 [20736/50000]	Loss: 4.4962	LR: 1.000000
Training Epoch: 18 [20992/50000]	Loss: 4.3923	LR: 1.000000
Training Epoch: 18 [21248/50000]	Loss: 4.4679	LR: 1.000000
Training Epoch: 18 [21504/50000]	Loss: 4.4740	LR: 1.000000
Training Epoch: 18 [21760/50000]	Loss: 4.4628	LR: 1.000000
Training Epoch: 18 [22016/50000]	Loss: 4.4756	LR: 1.000000
Training Epoch: 18 [22272/50000]	Loss: 4.4106	LR: 1.000000
Training Epoch: 18 [22528/50000]	Loss: 4.4204	LR: 1.000000
Training Epoch: 18 [22784/50000]	Loss: 4.4255	LR: 1.000000
Training Epoch: 18 [23040/50000]	Loss: 4.4865	LR: 1.000000
Training Epoch: 18 [23296/50000]	Loss: 4.4605	LR: 1.000000
Training Epoch: 18 [23552/50000]	Loss: 4.4730	LR: 1.000000
Training Epoch: 18 [23808/50000]	Loss: 4.4576	LR: 1.000000
Training Epoch: 18 [24064/50000]	Loss: 4.4455	LR: 1.000000
Training Epoch: 18 [24320/50000]	Loss: 4.4580	LR: 1.000000
Training Epoch: 18 [24576/50000]	Loss: 4.4474	LR: 1.000000
Training Epoch: 18 [24832/50000]	Loss: 4.4927	LR: 1.000000
Training Epoch: 18 [25088/50000]	Loss: 4.5057	LR: 1.000000
Training Epoch: 18 [25344/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 18 [25600/50000]	Loss: 4.4724	LR: 1.000000
Training Epoch: 18 [25856/50000]	Loss: 4.4379	LR: 1.000000
Training Epoch: 18 [26112/50000]	Loss: 4.4104	LR: 1.000000
Training Epoch: 18 [26368/50000]	Loss: 4.4085	LR: 1.000000
Training Epoch: 18 [26624/50000]	Loss: 4.4836	LR: 1.000000
Training Epoch: 18 [26880/50000]	Loss: 4.4724	LR: 1.000000
Training Epoch: 18 [27136/50000]	Loss: 4.4473	LR: 1.000000
Training Epoch: 18 [27392/50000]	Loss: 4.4608	LR: 1.000000
Training Epoch: 18 [27648/50000]	Loss: 4.4459	LR: 1.000000
Training Epoch: 18 [27904/50000]	Loss: 4.4362	LR: 1.000000
Training Epoch: 18 [28160/50000]	Loss: 4.4606	LR: 1.000000
Training Epoch: 18 [28416/50000]	Loss: 4.4714	LR: 1.000000
Training Epoch: 18 [28672/50000]	Loss: 4.5337	LR: 1.000000
Training Epoch: 18 [28928/50000]	Loss: 4.4311	LR: 1.000000
Training Epoch: 18 [29184/50000]	Loss: 4.4598	LR: 1.000000
Training Epoch: 18 [29440/50000]	Loss: 4.4140	LR: 1.000000
Training Epoch: 18 [29696/50000]	Loss: 4.4621	LR: 1.000000
Training Epoch: 18 [29952/50000]	Loss: 4.4434	LR: 1.000000
Training Epoch: 18 [30208/50000]	Loss: 4.4124	LR: 1.000000
Training Epoch: 18 [30464/50000]	Loss: 4.4690	LR: 1.000000
Training Epoch: 18 [30720/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 18 [30976/50000]	Loss: 4.4329	LR: 1.000000
Training Epoch: 18 [31232/50000]	Loss: 4.4331	LR: 1.000000
Training Epoch: 18 [31488/50000]	Loss: 4.4480	LR: 1.000000
Training Epoch: 18 [31744/50000]	Loss: 4.4497	LR: 1.000000
Training Epoch: 18 [32000/50000]	Loss: 4.5139	LR: 1.000000
Training Epoch: 18 [32256/50000]	Loss: 4.4308	LR: 1.000000
Training Epoch: 18 [32512/50000]	Loss: 4.4787	LR: 1.000000
Training Epoch: 18 [32768/50000]	Loss: 4.4013	LR: 1.000000
Training Epoch: 18 [33024/50000]	Loss: 4.4386	LR: 1.000000
Training Epoch: 18 [33280/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 18 [33536/50000]	Loss: 4.4980	LR: 1.000000
Training Epoch: 18 [33792/50000]	Loss: 4.4157	LR: 1.000000
Training Epoch: 18 [34048/50000]	Loss: 4.4122	LR: 1.000000
Training Epoch: 18 [34304/50000]	Loss: 4.3920	LR: 1.000000
Training Epoch: 18 [34560/50000]	Loss: 4.4787	LR: 1.000000
Training Epoch: 18 [34816/50000]	Loss: 4.3975	LR: 1.000000
Training Epoch: 18 [35072/50000]	Loss: 4.4903	LR: 1.000000
Training Epoch: 18 [35328/50000]	Loss: 4.4537	LR: 1.000000
Training Epoch: 18 [35584/50000]	Loss: 4.4109	LR: 1.000000
Training Epoch: 18 [35840/50000]	Loss: 4.3877	LR: 1.000000
Training Epoch: 18 [36096/50000]	Loss: 4.4341	LR: 1.000000
Training Epoch: 18 [36352/50000]	Loss: 4.4844	LR: 1.000000
Training Epoch: 18 [36608/50000]	Loss: 4.4514	LR: 1.000000
Training Epoch: 18 [36864/50000]	Loss: 4.4618	LR: 1.000000
Training Epoch: 18 [37120/50000]	Loss: 4.4371	LR: 1.000000
Training Epoch: 18 [37376/50000]	Loss: 4.4188	LR: 1.000000
Training Epoch: 18 [37632/50000]	Loss: 4.4150	LR: 1.000000
Training Epoch: 18 [37888/50000]	Loss: 4.5172	LR: 1.000000
Training Epoch: 18 [38144/50000]	Loss: 4.5381	LR: 1.000000
Training Epoch: 18 [38400/50000]	Loss: 4.4413	LR: 1.000000
Training Epoch: 18 [38656/50000]	Loss: 4.4576	LR: 1.000000
Training Epoch: 18 [38912/50000]	Loss: 4.4752	LR: 1.000000
Training Epoch: 18 [39168/50000]	Loss: 4.4331	LR: 1.000000
Training Epoch: 18 [39424/50000]	Loss: 4.4548	LR: 1.000000
Training Epoch: 18 [39680/50000]	Loss: 4.4823	LR: 1.000000
Training Epoch: 18 [39936/50000]	Loss: 4.4682	LR: 1.000000
Training Epoch: 18 [40192/50000]	Loss: 4.4527	LR: 1.000000
Training Epoch: 18 [40448/50000]	Loss: 4.4593	LR: 1.000000
Training Epoch: 18 [40704/50000]	Loss: 4.4290	LR: 1.000000
Training Epoch: 18 [40960/50000]	Loss: 4.4575	LR: 1.000000
Training Epoch: 18 [41216/50000]	Loss: 4.4476	LR: 1.000000
Training Epoch: 18 [41472/50000]	Loss: 4.4066	LR: 1.000000
Training Epoch: 18 [41728/50000]	Loss: 4.4559	LR: 1.000000
Training Epoch: 18 [41984/50000]	Loss: 4.4233	LR: 1.000000
Training Epoch: 18 [42240/50000]	Loss: 4.3993	LR: 1.000000
Training Epoch: 18 [42496/50000]	Loss: 4.4350	LR: 1.000000
Training Epoch: 18 [42752/50000]	Loss: 4.3927	LR: 1.000000
Training Epoch: 18 [43008/50000]	Loss: 4.3407	LR: 1.000000
Training Epoch: 18 [43264/50000]	Loss: 4.5022	LR: 1.000000
Training Epoch: 18 [43520/50000]	Loss: 4.4569	LR: 1.000000
Training Epoch: 18 [43776/50000]	Loss: 4.4469	LR: 1.000000
Training Epoch: 18 [44032/50000]	Loss: 4.5198	LR: 1.000000
Training Epoch: 18 [44288/50000]	Loss: 4.4199	LR: 1.000000
Training Epoch: 18 [44544/50000]	Loss: 4.4379	LR: 1.000000
Training Epoch: 18 [44800/50000]	Loss: 4.4425	LR: 1.000000
Training Epoch: 18 [45056/50000]	Loss: 4.4742	LR: 1.000000
Training Epoch: 18 [45312/50000]	Loss: 4.4428	LR: 1.000000
Training Epoch: 18 [45568/50000]	Loss: 4.4973	LR: 1.000000
Training Epoch: 18 [45824/50000]	Loss: 4.4204	LR: 1.000000
Training Epoch: 18 [46080/50000]	Loss: 4.4731	LR: 1.000000
Training Epoch: 18 [46336/50000]	Loss: 4.4302	LR: 1.000000
Training Epoch: 18 [46592/50000]	Loss: 4.5241	LR: 1.000000
Training Epoch: 18 [46848/50000]	Loss: 4.3898	LR: 1.000000
Training Epoch: 18 [47104/50000]	Loss: 4.4995	LR: 1.000000
Training Epoch: 18 [47360/50000]	Loss: 4.4434	LR: 1.000000
Training Epoch: 18 [47616/50000]	Loss: 4.4392	LR: 1.000000
Training Epoch: 18 [47872/50000]	Loss: 4.4424	LR: 1.000000
Training Epoch: 18 [48128/50000]	Loss: 4.4301	LR: 1.000000
Training Epoch: 18 [48384/50000]	Loss: 4.4821	LR: 1.000000
Training Epoch: 18 [48640/50000]	Loss: 4.4188	LR: 1.000000
Training Epoch: 18 [48896/50000]	Loss: 4.4236	LR: 1.000000
Training Epoch: 18 [49152/50000]	Loss: 4.4772	LR: 1.000000
Training Epoch: 18 [49408/50000]	Loss: 4.4462	LR: 1.000000
Training Epoch: 18 [49664/50000]	Loss: 4.5000	LR: 1.000000
Training Epoch: 18 [49920/50000]	Loss: 4.3710	LR: 1.000000
Training Epoch: 18 [50000/50000]	Loss: 4.5878	LR: 1.000000
epoch 18 training time consumed: 21.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   27004 GB |   27003 GB |
|       from large pool |  400448 KB |    1770 MB |   26979 GB |   26979 GB |
|       from small pool |    3549 KB |       9 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   27004 GB |   27003 GB |
|       from large pool |  400448 KB |    1770 MB |   26979 GB |   26979 GB |
|       from small pool |    3549 KB |       9 MB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   16616 GB |   16616 GB |
|       from large pool |  244672 KB |  473024 KB |   16588 GB |   16588 GB |
|       from small pool |    2594 KB |    4843 KB |      27 GB |      27 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1152 K  |    1152 K  |
|       from large pool |      36    |      77    |     557 K  |     557 K  |
|       from small pool |     186    |     224    |     595 K  |     595 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1152 K  |    1152 K  |
|       from large pool |      36    |      77    |     557 K  |     557 K  |
|       from small pool |     186    |     224    |     595 K  |     595 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      31    |  585540    |  585519    |
|       from large pool |      10    |      11    |  230846    |  230836    |
|       from small pool |      11    |      21    |  354694    |  354683    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 18, Average loss: 0.0178, Accuracy: 0.0185, Time consumed:1.46s

Training Epoch: 19 [256/50000]	Loss: 4.4101	LR: 1.000000
Training Epoch: 19 [512/50000]	Loss: 4.4319	LR: 1.000000
Training Epoch: 19 [768/50000]	Loss: 4.4465	LR: 1.000000
Training Epoch: 19 [1024/50000]	Loss: 4.4752	LR: 1.000000
Training Epoch: 19 [1280/50000]	Loss: 4.5360	LR: 1.000000
Training Epoch: 19 [1536/50000]	Loss: 4.4939	LR: 1.000000
Training Epoch: 19 [1792/50000]	Loss: 4.5573	LR: 1.000000
Training Epoch: 19 [2048/50000]	Loss: 4.5024	LR: 1.000000
Training Epoch: 19 [2304/50000]	Loss: 4.5475	LR: 1.000000
Training Epoch: 19 [2560/50000]	Loss: 4.4649	LR: 1.000000
Training Epoch: 19 [2816/50000]	Loss: 4.5175	LR: 1.000000
Training Epoch: 19 [3072/50000]	Loss: 4.4717	LR: 1.000000
Training Epoch: 19 [3328/50000]	Loss: 4.5335	LR: 1.000000
Training Epoch: 19 [3584/50000]	Loss: 4.5000	LR: 1.000000
Training Epoch: 19 [3840/50000]	Loss: 4.4851	LR: 1.000000
Training Epoch: 19 [4096/50000]	Loss: 4.4628	LR: 1.000000
Training Epoch: 19 [4352/50000]	Loss: 4.4644	LR: 1.000000
Training Epoch: 19 [4608/50000]	Loss: 4.5144	LR: 1.000000
Training Epoch: 19 [4864/50000]	Loss: 4.4583	LR: 1.000000
Training Epoch: 19 [5120/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 19 [5376/50000]	Loss: 4.4815	LR: 1.000000
Training Epoch: 19 [5632/50000]	Loss: 4.4770	LR: 1.000000
Training Epoch: 19 [5888/50000]	Loss: 4.4354	LR: 1.000000
Training Epoch: 19 [6144/50000]	Loss: 4.4571	LR: 1.000000
Training Epoch: 19 [6400/50000]	Loss: 4.4619	LR: 1.000000
Training Epoch: 19 [6656/50000]	Loss: 4.4454	LR: 1.000000
Training Epoch: 19 [6912/50000]	Loss: 4.4348	LR: 1.000000
Training Epoch: 19 [7168/50000]	Loss: 4.4410	LR: 1.000000
Training Epoch: 19 [7424/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 19 [7680/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 19 [7936/50000]	Loss: 4.4319	LR: 1.000000
Training Epoch: 19 [8192/50000]	Loss: 4.4493	LR: 1.000000
Training Epoch: 19 [8448/50000]	Loss: 4.5739	LR: 1.000000
Training Epoch: 19 [8704/50000]	Loss: 4.4336	LR: 1.000000
Training Epoch: 19 [8960/50000]	Loss: 4.4385	LR: 1.000000
Training Epoch: 19 [9216/50000]	Loss: 4.4359	LR: 1.000000
Training Epoch: 19 [9472/50000]	Loss: 4.4414	LR: 1.000000
Training Epoch: 19 [9728/50000]	Loss: 4.4429	LR: 1.000000
Training Epoch: 19 [9984/50000]	Loss: 4.3117	LR: 1.000000
Training Epoch: 19 [10240/50000]	Loss: 4.4621	LR: 1.000000
Training Epoch: 19 [10496/50000]	Loss: 4.4744	LR: 1.000000
Training Epoch: 19 [10752/50000]	Loss: 4.4236	LR: 1.000000
Training Epoch: 19 [11008/50000]	Loss: 4.4456	LR: 1.000000
Training Epoch: 19 [11264/50000]	Loss: 4.4388	LR: 1.000000
Training Epoch: 19 [11520/50000]	Loss: 4.3711	LR: 1.000000
Training Epoch: 19 [11776/50000]	Loss: 4.4528	LR: 1.000000
Training Epoch: 19 [12032/50000]	Loss: 4.4689	LR: 1.000000
Training Epoch: 19 [12288/50000]	Loss: 4.4375	LR: 1.000000
Training Epoch: 19 [12544/50000]	Loss: 4.5068	LR: 1.000000
Training Epoch: 19 [12800/50000]	Loss: 4.4911	LR: 1.000000
Training Epoch: 19 [13056/50000]	Loss: 4.4384	LR: 1.000000
Training Epoch: 19 [13312/50000]	Loss: 4.4488	LR: 1.000000
Training Epoch: 19 [13568/50000]	Loss: 4.4395	LR: 1.000000
Training Epoch: 19 [13824/50000]	Loss: 4.4257	LR: 1.000000
Training Epoch: 19 [14080/50000]	Loss: 4.4458	LR: 1.000000
Training Epoch: 19 [14336/50000]	Loss: 4.4289	LR: 1.000000
Training Epoch: 19 [14592/50000]	Loss: 4.4102	LR: 1.000000
Training Epoch: 19 [14848/50000]	Loss: 4.4397	LR: 1.000000
Training Epoch: 19 [15104/50000]	Loss: 4.4430	LR: 1.000000
Training Epoch: 19 [15360/50000]	Loss: 4.4555	LR: 1.000000
Training Epoch: 19 [15616/50000]	Loss: 4.5517	LR: 1.000000
Training Epoch: 19 [15872/50000]	Loss: 4.4476	LR: 1.000000
Training Epoch: 19 [16128/50000]	Loss: 4.5468	LR: 1.000000
Training Epoch: 19 [16384/50000]	Loss: 4.5713	LR: 1.000000
Training Epoch: 19 [16640/50000]	Loss: 4.5805	LR: 1.000000
Training Epoch: 19 [16896/50000]	Loss: 4.5859	LR: 1.000000
Training Epoch: 19 [17152/50000]	Loss: 4.5529	LR: 1.000000
Training Epoch: 19 [17408/50000]	Loss: 4.5178	LR: 1.000000
Training Epoch: 19 [17664/50000]	Loss: 4.5047	LR: 1.000000
Training Epoch: 19 [17920/50000]	Loss: 4.5485	LR: 1.000000
Training Epoch: 19 [18176/50000]	Loss: 4.5196	LR: 1.000000
Training Epoch: 19 [18432/50000]	Loss: 4.5470	LR: 1.000000
Training Epoch: 19 [18688/50000]	Loss: 4.5397	LR: 1.000000
Training Epoch: 19 [18944/50000]	Loss: 4.5133	LR: 1.000000
Training Epoch: 19 [19200/50000]	Loss: 4.5299	LR: 1.000000
Training Epoch: 19 [19456/50000]	Loss: 4.5531	LR: 1.000000
Training Epoch: 19 [19712/50000]	Loss: 4.5415	LR: 1.000000
Training Epoch: 19 [19968/50000]	Loss: 4.4960	LR: 1.000000
Training Epoch: 19 [20224/50000]	Loss: 4.5008	LR: 1.000000
Training Epoch: 19 [20480/50000]	Loss: 4.5154	LR: 1.000000
Training Epoch: 19 [20736/50000]	Loss: 4.5123	LR: 1.000000
Training Epoch: 19 [20992/50000]	Loss: 4.5352	LR: 1.000000
Training Epoch: 19 [21248/50000]	Loss: 4.5379	LR: 1.000000
Training Epoch: 19 [21504/50000]	Loss: 4.5713	LR: 1.000000
Training Epoch: 19 [21760/50000]	Loss: 4.5390	LR: 1.000000
Training Epoch: 19 [22016/50000]	Loss: 4.5681	LR: 1.000000
Training Epoch: 19 [22272/50000]	Loss: 4.5343	LR: 1.000000
Training Epoch: 19 [22528/50000]	Loss: 4.5188	LR: 1.000000
Training Epoch: 19 [22784/50000]	Loss: 4.5311	LR: 1.000000
Training Epoch: 19 [23040/50000]	Loss: 4.5328	LR: 1.000000
Training Epoch: 19 [23296/50000]	Loss: 4.5094	LR: 1.000000
Training Epoch: 19 [23552/50000]	Loss: 4.5752	LR: 1.000000
Training Epoch: 19 [23808/50000]	Loss: 4.5407	LR: 1.000000
Training Epoch: 19 [24064/50000]	Loss: 4.5667	LR: 1.000000
Training Epoch: 19 [24320/50000]	Loss: 4.5284	LR: 1.000000
Training Epoch: 19 [24576/50000]	Loss: 4.5333	LR: 1.000000
Training Epoch: 19 [24832/50000]	Loss: 4.5067	LR: 1.000000
Training Epoch: 19 [25088/50000]	Loss: 4.5236	LR: 1.000000
Training Epoch: 19 [25344/50000]	Loss: 4.5424	LR: 1.000000
Training Epoch: 19 [25600/50000]	Loss: 4.4868	LR: 1.000000
Training Epoch: 19 [25856/50000]	Loss: 4.5132	LR: 1.000000
Training Epoch: 19 [26112/50000]	Loss: 4.5047	LR: 1.000000
Training Epoch: 19 [26368/50000]	Loss: 4.5085	LR: 1.000000
Training Epoch: 19 [26624/50000]	Loss: 4.5277	LR: 1.000000
Training Epoch: 19 [26880/50000]	Loss: 4.5740	LR: 1.000000
Training Epoch: 19 [27136/50000]	Loss: 4.4894	LR: 1.000000
Training Epoch: 19 [27392/50000]	Loss: 4.5082	LR: 1.000000
Training Epoch: 19 [27648/50000]	Loss: 4.5473	LR: 1.000000
Training Epoch: 19 [27904/50000]	Loss: 4.5333	LR: 1.000000
Training Epoch: 19 [28160/50000]	Loss: 4.4975	LR: 1.000000
Training Epoch: 19 [28416/50000]	Loss: 4.5390	LR: 1.000000
Training Epoch: 19 [28672/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 19 [28928/50000]	Loss: 4.5185	LR: 1.000000
Training Epoch: 19 [29184/50000]	Loss: 4.5254	LR: 1.000000
Training Epoch: 19 [29440/50000]	Loss: 4.5105	LR: 1.000000
Training Epoch: 19 [29696/50000]	Loss: 4.5222	LR: 1.000000
Training Epoch: 19 [29952/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 19 [30208/50000]	Loss: 4.6348	LR: 1.000000
Training Epoch: 19 [30464/50000]	Loss: 4.5222	LR: 1.000000
Training Epoch: 19 [30720/50000]	Loss: 4.5411	LR: 1.000000
Training Epoch: 19 [30976/50000]	Loss: 4.5378	LR: 1.000000
Training Epoch: 19 [31232/50000]	Loss: 4.5305	LR: 1.000000
Training Epoch: 19 [31488/50000]	Loss: 4.5050	LR: 1.000000
Training Epoch: 19 [31744/50000]	Loss: 4.5639	LR: 1.000000
Training Epoch: 19 [32000/50000]	Loss: 4.5404	LR: 1.000000
Training Epoch: 19 [32256/50000]	Loss: 4.5416	LR: 1.000000
Training Epoch: 19 [32512/50000]	Loss: 4.4770	LR: 1.000000
Training Epoch: 19 [32768/50000]	Loss: 4.5269	LR: 1.000000
Training Epoch: 19 [33024/50000]	Loss: 4.5573	LR: 1.000000
Training Epoch: 19 [33280/50000]	Loss: 4.5401	LR: 1.000000
Training Epoch: 19 [33536/50000]	Loss: 4.4929	LR: 1.000000
Training Epoch: 19 [33792/50000]	Loss: 4.5254	LR: 1.000000
Training Epoch: 19 [34048/50000]	Loss: 4.5382	LR: 1.000000
Training Epoch: 19 [34304/50000]	Loss: 4.5569	LR: 1.000000
Training Epoch: 19 [34560/50000]	Loss: 4.5288	LR: 1.000000
Training Epoch: 19 [34816/50000]	Loss: 4.4996	LR: 1.000000
Training Epoch: 19 [35072/50000]	Loss: 4.5195	LR: 1.000000
Training Epoch: 19 [35328/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 19 [35584/50000]	Loss: 4.4852	LR: 1.000000
Training Epoch: 19 [35840/50000]	Loss: 4.5133	LR: 1.000000
Training Epoch: 19 [36096/50000]	Loss: 4.5574	LR: 1.000000
Training Epoch: 19 [36352/50000]	Loss: 4.5258	LR: 1.000000
Training Epoch: 19 [36608/50000]	Loss: 4.5639	LR: 1.000000
Training Epoch: 19 [36864/50000]	Loss: 4.5377	LR: 1.000000
Training Epoch: 19 [37120/50000]	Loss: 4.5293	LR: 1.000000
Training Epoch: 19 [37376/50000]	Loss: 4.5759	LR: 1.000000
Training Epoch: 19 [37632/50000]	Loss: 4.5447	LR: 1.000000
Training Epoch: 19 [37888/50000]	Loss: 4.5513	LR: 1.000000
Training Epoch: 19 [38144/50000]	Loss: 4.5335	LR: 1.000000
Training Epoch: 19 [38400/50000]	Loss: 4.5298	LR: 1.000000
Training Epoch: 19 [38656/50000]	Loss: 4.5730	LR: 1.000000
Training Epoch: 19 [38912/50000]	Loss: 4.5857	LR: 1.000000
Training Epoch: 19 [39168/50000]	Loss: 4.5163	LR: 1.000000
Training Epoch: 19 [39424/50000]	Loss: 4.5436	LR: 1.000000
Training Epoch: 19 [39680/50000]	Loss: 4.5105	LR: 1.000000
Training Epoch: 19 [39936/50000]	Loss: 4.5572	LR: 1.000000
Training Epoch: 19 [40192/50000]	Loss: 4.4856	LR: 1.000000
Training Epoch: 19 [40448/50000]	Loss: 4.5354	LR: 1.000000
Training Epoch: 19 [40704/50000]	Loss: 4.5395	LR: 1.000000
Training Epoch: 19 [40960/50000]	Loss: 4.4870	LR: 1.000000
Training Epoch: 19 [41216/50000]	Loss: 4.5721	LR: 1.000000
Training Epoch: 19 [41472/50000]	Loss: 4.5286	LR: 1.000000
Training Epoch: 19 [41728/50000]	Loss: 4.4789	LR: 1.000000
Training Epoch: 19 [41984/50000]	Loss: 4.5195	LR: 1.000000
Training Epoch: 19 [42240/50000]	Loss: 4.5074	LR: 1.000000
Training Epoch: 19 [42496/50000]	Loss: 4.5095	LR: 1.000000
Training Epoch: 19 [42752/50000]	Loss: 4.4768	LR: 1.000000
Training Epoch: 19 [43008/50000]	Loss: 4.4746	LR: 1.000000
Training Epoch: 19 [43264/50000]	Loss: 4.5074	LR: 1.000000
Training Epoch: 19 [43520/50000]	Loss: 4.5368	LR: 1.000000
Training Epoch: 19 [43776/50000]	Loss: 4.4500	LR: 1.000000
Training Epoch: 19 [44032/50000]	Loss: 4.4685	LR: 1.000000
Training Epoch: 19 [44288/50000]	Loss: 4.4625	LR: 1.000000
Training Epoch: 19 [44544/50000]	Loss: 4.5027	LR: 1.000000
Training Epoch: 19 [44800/50000]	Loss: 4.4616	LR: 1.000000
Training Epoch: 19 [45056/50000]	Loss: 4.4503	LR: 1.000000
Training Epoch: 19 [45312/50000]	Loss: 4.4506	LR: 1.000000
Training Epoch: 19 [45568/50000]	Loss: 4.5173	LR: 1.000000
Training Epoch: 19 [45824/50000]	Loss: 4.4865	LR: 1.000000
Training Epoch: 19 [46080/50000]	Loss: 4.5441	LR: 1.000000
Training Epoch: 19 [46336/50000]	Loss: 4.4890	LR: 1.000000
Training Epoch: 19 [46592/50000]	Loss: 4.5658	LR: 1.000000
Training Epoch: 19 [46848/50000]	Loss: 4.4917	LR: 1.000000
Training Epoch: 19 [47104/50000]	Loss: 4.5505	LR: 1.000000
Training Epoch: 19 [47360/50000]	Loss: 4.5003	LR: 1.000000
Training Epoch: 19 [47616/50000]	Loss: 4.4882	LR: 1.000000
Training Epoch: 19 [47872/50000]	Loss: 4.4720	LR: 1.000000
Training Epoch: 19 [48128/50000]	Loss: 4.4792	LR: 1.000000
Training Epoch: 19 [48384/50000]	Loss: 4.4319	LR: 1.000000
Training Epoch: 19 [48640/50000]	Loss: 4.5595	LR: 1.000000
Training Epoch: 19 [48896/50000]	Loss: 4.4879	LR: 1.000000
Training Epoch: 19 [49152/50000]	Loss: 4.4831	LR: 1.000000
Training Epoch: 19 [49408/50000]	Loss: 4.5387	LR: 1.000000
Training Epoch: 19 [49664/50000]	Loss: 4.5028	LR: 1.000000
Training Epoch: 19 [49920/50000]	Loss: 4.5359	LR: 1.000000
Training Epoch: 19 [50000/50000]	Loss: 4.4510	LR: 1.000000
epoch 19 training time consumed: 21.96s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   28504 GB |   28504 GB |
|       from large pool |  400448 KB |    1770 MB |   28478 GB |   28478 GB |
|       from small pool |    3549 KB |       9 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   28504 GB |   28504 GB |
|       from large pool |  400448 KB |    1770 MB |   28478 GB |   28478 GB |
|       from small pool |    3549 KB |       9 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   17539 GB |   17539 GB |
|       from large pool |  244672 KB |  473024 KB |   17510 GB |   17510 GB |
|       from small pool |    2594 KB |    4843 KB |      29 GB |      29 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1216 K  |    1216 K  |
|       from large pool |      36    |      77    |     588 K  |     588 K  |
|       from small pool |     186    |     224    |     628 K  |     628 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1216 K  |    1216 K  |
|       from large pool |      36    |      77    |     588 K  |     588 K  |
|       from small pool |     186    |     224    |     628 K  |     628 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      31    |  618386    |  618365    |
|       from large pool |      10    |      11    |  243669    |  243659    |
|       from small pool |      11    |      21    |  374717    |  374706    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 19, Average loss: 0.0179, Accuracy: 0.0185, Time consumed:1.44s

Training Epoch: 20 [256/50000]	Loss: 4.5204	LR: 1.000000
Training Epoch: 20 [512/50000]	Loss: 4.5434	LR: 1.000000
Training Epoch: 20 [768/50000]	Loss: 4.4718	LR: 1.000000
Training Epoch: 20 [1024/50000]	Loss: 4.4999	LR: 1.000000
Training Epoch: 20 [1280/50000]	Loss: 4.4871	LR: 1.000000
Training Epoch: 20 [1536/50000]	Loss: 4.4500	LR: 1.000000
Training Epoch: 20 [1792/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 20 [2048/50000]	Loss: 4.4475	LR: 1.000000
Training Epoch: 20 [2304/50000]	Loss: 4.4219	LR: 1.000000
Training Epoch: 20 [2560/50000]	Loss: 4.4473	LR: 1.000000
Training Epoch: 20 [2816/50000]	Loss: 4.4782	LR: 1.000000
Training Epoch: 20 [3072/50000]	Loss: 4.5029	LR: 1.000000
Training Epoch: 20 [3328/50000]	Loss: 4.4424	LR: 1.000000
Training Epoch: 20 [3584/50000]	Loss: 4.4343	LR: 1.000000
Training Epoch: 20 [3840/50000]	Loss: 4.5024	LR: 1.000000
Training Epoch: 20 [4096/50000]	Loss: 4.4000	LR: 1.000000
Training Epoch: 20 [4352/50000]	Loss: 4.4462	LR: 1.000000
Training Epoch: 20 [4608/50000]	Loss: 4.5345	LR: 1.000000
Training Epoch: 20 [4864/50000]	Loss: 4.5861	LR: 1.000000
Training Epoch: 20 [5120/50000]	Loss: 4.5425	LR: 1.000000
Training Epoch: 20 [5376/50000]	Loss: 4.5076	LR: 1.000000
Training Epoch: 20 [5632/50000]	Loss: 4.5110	LR: 1.000000
Training Epoch: 20 [5888/50000]	Loss: 4.5484	LR: 1.000000
Training Epoch: 20 [6144/50000]	Loss: 4.5083	LR: 1.000000
Training Epoch: 20 [6400/50000]	Loss: 4.4930	LR: 1.000000
Training Epoch: 20 [6656/50000]	Loss: 4.4953	LR: 1.000000
Training Epoch: 20 [6912/50000]	Loss: 4.5035	LR: 1.000000
Training Epoch: 20 [7168/50000]	Loss: 4.3988	LR: 1.000000
Training Epoch: 20 [7424/50000]	Loss: 4.4712	LR: 1.000000
Training Epoch: 20 [7680/50000]	Loss: 4.4300	LR: 1.000000
Training Epoch: 20 [7936/50000]	Loss: 4.5610	LR: 1.000000
Training Epoch: 20 [8192/50000]	Loss: 4.4654	LR: 1.000000
Training Epoch: 20 [8448/50000]	Loss: 4.4644	LR: 1.000000
Training Epoch: 20 [8704/50000]	Loss: 4.4857	LR: 1.000000
Training Epoch: 20 [8960/50000]	Loss: 4.4428	LR: 1.000000
Training Epoch: 20 [9216/50000]	Loss: 4.3965	LR: 1.000000
Training Epoch: 20 [9472/50000]	Loss: 4.4686	LR: 1.000000
Training Epoch: 20 [9728/50000]	Loss: 4.4327	LR: 1.000000
Training Epoch: 20 [9984/50000]	Loss: 4.4515	LR: 1.000000
Training Epoch: 20 [10240/50000]	Loss: 4.4416	LR: 1.000000
Training Epoch: 20 [10496/50000]	Loss: 4.4854	LR: 1.000000
Training Epoch: 20 [10752/50000]	Loss: 4.5261	LR: 1.000000
Training Epoch: 20 [11008/50000]	Loss: 4.5460	LR: 1.000000
Training Epoch: 20 [11264/50000]	Loss: 4.5283	LR: 1.000000
Training Epoch: 20 [11520/50000]	Loss: 4.5171	LR: 1.000000
Training Epoch: 20 [11776/50000]	Loss: 4.5590	LR: 1.000000
Training Epoch: 20 [12032/50000]	Loss: 4.5097	LR: 1.000000
Training Epoch: 20 [12288/50000]	Loss: 4.5575	LR: 1.000000
Training Epoch: 20 [12544/50000]	Loss: 4.5100	LR: 1.000000
Training Epoch: 20 [12800/50000]	Loss: 4.4838	LR: 1.000000
Training Epoch: 20 [13056/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 20 [13312/50000]	Loss: 4.4675	LR: 1.000000
Training Epoch: 20 [13568/50000]	Loss: 4.5732	LR: 1.000000
Training Epoch: 20 [13824/50000]	Loss: 4.5611	LR: 1.000000
Training Epoch: 20 [14080/50000]	Loss: 4.5358	LR: 1.000000
Training Epoch: 20 [14336/50000]	Loss: 4.5363	LR: 1.000000
Training Epoch: 20 [14592/50000]	Loss: 4.5598	LR: 1.000000
Training Epoch: 20 [14848/50000]	Loss: 4.5434	LR: 1.000000
Training Epoch: 20 [15104/50000]	Loss: 4.5403	LR: 1.000000
Training Epoch: 20 [15360/50000]	Loss: 4.5316	LR: 1.000000
Training Epoch: 20 [15616/50000]	Loss: 4.5030	LR: 1.000000
Training Epoch: 20 [15872/50000]	Loss: 4.5240	LR: 1.000000
Training Epoch: 20 [16128/50000]	Loss: 4.5180	LR: 1.000000
Training Epoch: 20 [16384/50000]	Loss: 4.5052	LR: 1.000000
Training Epoch: 20 [16640/50000]	Loss: 4.5508	LR: 1.000000
Training Epoch: 20 [16896/50000]	Loss: 4.4888	LR: 1.000000
Training Epoch: 20 [17152/50000]	Loss: 4.5262	LR: 1.000000
Training Epoch: 20 [17408/50000]	Loss: 4.5381	LR: 1.000000
Training Epoch: 20 [17664/50000]	Loss: 4.5672	LR: 1.000000
Training Epoch: 20 [17920/50000]	Loss: 4.5003	LR: 1.000000
Training Epoch: 20 [18176/50000]	Loss: 4.5312	LR: 1.000000
Training Epoch: 20 [18432/50000]	Loss: 4.5833	LR: 1.000000
Training Epoch: 20 [18688/50000]	Loss: 4.5069	LR: 1.000000
Training Epoch: 20 [18944/50000]	Loss: 4.5342	LR: 1.000000
Training Epoch: 20 [19200/50000]	Loss: 4.5041	LR: 1.000000
Training Epoch: 20 [19456/50000]	Loss: 4.4442	LR: 1.000000
Training Epoch: 20 [19712/50000]	Loss: 4.4871	LR: 1.000000
Training Epoch: 20 [19968/50000]	Loss: 4.4285	LR: 1.000000
Training Epoch: 20 [20224/50000]	Loss: 4.4927	LR: 1.000000
Training Epoch: 20 [20480/50000]	Loss: 4.4454	LR: 1.000000
Training Epoch: 20 [20736/50000]	Loss: 4.4939	LR: 1.000000
Training Epoch: 20 [20992/50000]	Loss: 4.4709	LR: 1.000000
Training Epoch: 20 [21248/50000]	Loss: 4.5070	LR: 1.000000
Training Epoch: 20 [21504/50000]	Loss: 4.4518	LR: 1.000000
Training Epoch: 20 [21760/50000]	Loss: 4.4995	LR: 1.000000
Training Epoch: 20 [22016/50000]	Loss: 4.4115	LR: 1.000000
Training Epoch: 20 [22272/50000]	Loss: 4.4268	LR: 1.000000
Training Epoch: 20 [22528/50000]	Loss: 4.4616	LR: 1.000000
Training Epoch: 20 [22784/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 20 [23040/50000]	Loss: 4.5259	LR: 1.000000
Training Epoch: 20 [23296/50000]	Loss: 4.4177	LR: 1.000000
Training Epoch: 20 [23552/50000]	Loss: 4.4227	LR: 1.000000
Training Epoch: 20 [23808/50000]	Loss: 4.3915	LR: 1.000000
Training Epoch: 20 [24064/50000]	Loss: 4.4258	LR: 1.000000
Training Epoch: 20 [24320/50000]	Loss: 4.4927	LR: 1.000000
Training Epoch: 20 [24576/50000]	Loss: 4.4841	LR: 1.000000
Training Epoch: 20 [24832/50000]	Loss: 4.5192	LR: 1.000000
Training Epoch: 20 [25088/50000]	Loss: 4.5221	LR: 1.000000
Training Epoch: 20 [25344/50000]	Loss: 4.5271	LR: 1.000000
Training Epoch: 20 [25600/50000]	Loss: 4.4842	LR: 1.000000
Training Epoch: 20 [25856/50000]	Loss: 4.4153	LR: 1.000000
Training Epoch: 20 [26112/50000]	Loss: 4.4989	LR: 1.000000
Training Epoch: 20 [26368/50000]	Loss: 4.4449	LR: 1.000000
Training Epoch: 20 [26624/50000]	Loss: 4.5149	LR: 1.000000
Training Epoch: 20 [26880/50000]	Loss: 4.4921	LR: 1.000000
Training Epoch: 20 [27136/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 20 [27392/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 20 [27648/50000]	Loss: 4.4631	LR: 1.000000
Training Epoch: 20 [27904/50000]	Loss: 4.3856	LR: 1.000000
Training Epoch: 20 [28160/50000]	Loss: 4.4240	LR: 1.000000
Training Epoch: 20 [28416/50000]	Loss: 4.4042	LR: 1.000000
Training Epoch: 20 [28672/50000]	Loss: 4.4674	LR: 1.000000
Training Epoch: 20 [28928/50000]	Loss: 4.4473	LR: 1.000000
Training Epoch: 20 [29184/50000]	Loss: 4.4648	LR: 1.000000
Training Epoch: 20 [29440/50000]	Loss: 4.4620	LR: 1.000000
Training Epoch: 20 [29696/50000]	Loss: 4.4821	LR: 1.000000
Training Epoch: 20 [29952/50000]	Loss: 4.4947	LR: 1.000000
Training Epoch: 20 [30208/50000]	Loss: 4.4941	LR: 1.000000
Training Epoch: 20 [30464/50000]	Loss: 4.3979	LR: 1.000000
Training Epoch: 20 [30720/50000]	Loss: 4.4632	LR: 1.000000
Training Epoch: 20 [30976/50000]	Loss: 4.4515	LR: 1.000000
Training Epoch: 20 [31232/50000]	Loss: 4.4497	LR: 1.000000
Training Epoch: 20 [31488/50000]	Loss: 4.4064	LR: 1.000000
Training Epoch: 20 [31744/50000]	Loss: 4.4590	LR: 1.000000
Training Epoch: 20 [32000/50000]	Loss: 4.4438	LR: 1.000000
Training Epoch: 20 [32256/50000]	Loss: 4.4179	LR: 1.000000
Training Epoch: 20 [32512/50000]	Loss: 4.4296	LR: 1.000000
Training Epoch: 20 [32768/50000]	Loss: 4.4072	LR: 1.000000
Training Epoch: 20 [33024/50000]	Loss: 4.4025	LR: 1.000000
Training Epoch: 20 [33280/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 20 [33536/50000]	Loss: 4.3921	LR: 1.000000
Training Epoch: 20 [33792/50000]	Loss: 4.4352	LR: 1.000000
Training Epoch: 20 [34048/50000]	Loss: 4.4208	LR: 1.000000
Training Epoch: 20 [34304/50000]	Loss: 4.4198	LR: 1.000000
Training Epoch: 20 [34560/50000]	Loss: 4.4369	LR: 1.000000
Training Epoch: 20 [34816/50000]	Loss: 4.3790	LR: 1.000000
Training Epoch: 20 [35072/50000]	Loss: 4.4647	LR: 1.000000
Training Epoch: 20 [35328/50000]	Loss: 4.4552	LR: 1.000000
Training Epoch: 20 [35584/50000]	Loss: 4.4497	LR: 1.000000
Training Epoch: 20 [35840/50000]	Loss: 4.4504	LR: 1.000000
Training Epoch: 20 [36096/50000]	Loss: 4.4661	LR: 1.000000
Training Epoch: 20 [36352/50000]	Loss: 4.4109	LR: 1.000000
Training Epoch: 20 [36608/50000]	Loss: 4.4215	LR: 1.000000
Training Epoch: 20 [36864/50000]	Loss: 4.4481	LR: 1.000000
Training Epoch: 20 [37120/50000]	Loss: 4.4709	LR: 1.000000
Training Epoch: 20 [37376/50000]	Loss: 4.4354	LR: 1.000000
Training Epoch: 20 [37632/50000]	Loss: 4.3925	LR: 1.000000
Training Epoch: 20 [37888/50000]	Loss: 4.3043	LR: 1.000000
Training Epoch: 20 [38144/50000]	Loss: 4.4098	LR: 1.000000
Training Epoch: 20 [38400/50000]	Loss: 4.3769	LR: 1.000000
Training Epoch: 20 [38656/50000]	Loss: 4.4748	LR: 1.000000
Training Epoch: 20 [38912/50000]	Loss: 4.4601	LR: 1.000000
Training Epoch: 20 [39168/50000]	Loss: 4.4104	LR: 1.000000
Training Epoch: 20 [39424/50000]	Loss: 4.4882	LR: 1.000000
Training Epoch: 20 [39680/50000]	Loss: 4.4943	LR: 1.000000
Training Epoch: 20 [39936/50000]	Loss: 4.4401	LR: 1.000000
Training Epoch: 20 [40192/50000]	Loss: 4.4716	LR: 1.000000
Training Epoch: 20 [40448/50000]	Loss: 4.4695	LR: 1.000000
Training Epoch: 20 [40704/50000]	Loss: 4.3950	LR: 1.000000
Training Epoch: 20 [40960/50000]	Loss: 4.4532	LR: 1.000000
Training Epoch: 20 [41216/50000]	Loss: 4.4781	LR: 1.000000
Training Epoch: 20 [41472/50000]	Loss: 4.4585	LR: 1.000000
Training Epoch: 20 [41728/50000]	Loss: 4.4643	LR: 1.000000
Training Epoch: 20 [41984/50000]	Loss: 4.4738	LR: 1.000000
Training Epoch: 20 [42240/50000]	Loss: 4.5158	LR: 1.000000
Training Epoch: 20 [42496/50000]	Loss: 4.4160	LR: 1.000000
Training Epoch: 20 [42752/50000]	Loss: 4.4297	LR: 1.000000
Training Epoch: 20 [43008/50000]	Loss: 4.5072	LR: 1.000000
Training Epoch: 20 [43264/50000]	Loss: 4.4032	LR: 1.000000
Training Epoch: 20 [43520/50000]	Loss: 4.4472	LR: 1.000000
Training Epoch: 20 [43776/50000]	Loss: 4.4693	LR: 1.000000
Training Epoch: 20 [44032/50000]	Loss: 4.5059	LR: 1.000000
Training Epoch: 20 [44288/50000]	Loss: 4.4546	LR: 1.000000
Training Epoch: 20 [44544/50000]	Loss: 4.4665	LR: 1.000000
Training Epoch: 20 [44800/50000]	Loss: 4.4683	LR: 1.000000
Training Epoch: 20 [45056/50000]	Loss: 4.4768	LR: 1.000000
Training Epoch: 20 [45312/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 20 [45568/50000]	Loss: 4.4505	LR: 1.000000
Training Epoch: 20 [45824/50000]	Loss: 4.4928	LR: 1.000000
Training Epoch: 20 [46080/50000]	Loss: 4.4415	LR: 1.000000
Training Epoch: 20 [46336/50000]	Loss: 4.5168	LR: 1.000000
Training Epoch: 20 [46592/50000]	Loss: 4.4862	LR: 1.000000
Training Epoch: 20 [46848/50000]	Loss: 4.4254	LR: 1.000000
Training Epoch: 20 [47104/50000]	Loss: 4.4732	LR: 1.000000
Training Epoch: 20 [47360/50000]	Loss: 4.4488	LR: 1.000000
Training Epoch: 20 [47616/50000]	Loss: 4.4087	LR: 1.000000
Training Epoch: 20 [47872/50000]	Loss: 4.4256	LR: 1.000000
Training Epoch: 20 [48128/50000]	Loss: 4.4456	LR: 1.000000
Training Epoch: 20 [48384/50000]	Loss: 4.4140	LR: 1.000000
Training Epoch: 20 [48640/50000]	Loss: 4.5653	LR: 1.000000
Training Epoch: 20 [48896/50000]	Loss: 4.4846	LR: 1.000000
Training Epoch: 20 [49152/50000]	Loss: 4.5074	LR: 1.000000
Training Epoch: 20 [49408/50000]	Loss: 4.5865	LR: 1.000000
Training Epoch: 20 [49664/50000]	Loss: 4.5023	LR: 1.000000
Training Epoch: 20 [49920/50000]	Loss: 4.4783	LR: 1.000000
Training Epoch: 20 [50000/50000]	Loss: 4.5345	LR: 1.000000
epoch 20 training time consumed: 21.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   30004 GB |   30004 GB |
|       from large pool |  400448 KB |    1770 MB |   29977 GB |   29977 GB |
|       from small pool |    3549 KB |       9 MB |      27 GB |      27 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   30004 GB |   30004 GB |
|       from large pool |  400448 KB |    1770 MB |   29977 GB |   29977 GB |
|       from small pool |    3549 KB |       9 MB |      27 GB |      27 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   18462 GB |   18462 GB |
|       from large pool |  244672 KB |  473024 KB |   18431 GB |   18431 GB |
|       from small pool |    2594 KB |    4843 KB |      30 GB |      30 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1280 K  |    1280 K  |
|       from large pool |      36    |      77    |     619 K  |     619 K  |
|       from small pool |     186    |     224    |     661 K  |     661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1280 K  |    1280 K  |
|       from large pool |      36    |      77    |     619 K  |     619 K  |
|       from small pool |     186    |     224    |     661 K  |     661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      31    |  650726    |  650703    |
|       from large pool |      10    |      11    |  256492    |  256482    |
|       from small pool |      13    |      21    |  394234    |  394221    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 20, Average loss: 0.0212, Accuracy: 0.0200, Time consumed:1.46s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-20-regular.pth
Training Epoch: 21 [256/50000]	Loss: 4.5147	LR: 1.000000
Training Epoch: 21 [512/50000]	Loss: 4.4968	LR: 1.000000
Training Epoch: 21 [768/50000]	Loss: 4.5254	LR: 1.000000
Training Epoch: 21 [1024/50000]	Loss: 4.4850	LR: 1.000000
Training Epoch: 21 [1280/50000]	Loss: 4.5378	LR: 1.000000
Training Epoch: 21 [1536/50000]	Loss: 4.4975	LR: 1.000000
Training Epoch: 21 [1792/50000]	Loss: 4.5526	LR: 1.000000
Training Epoch: 21 [2048/50000]	Loss: 4.4360	LR: 1.000000
Training Epoch: 21 [2304/50000]	Loss: 4.6567	LR: 1.000000
Training Epoch: 21 [2560/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 21 [2816/50000]	Loss: 4.6614	LR: 1.000000
Training Epoch: 21 [3072/50000]	Loss: 4.6340	LR: 1.000000
Training Epoch: 21 [3328/50000]	Loss: 4.6466	LR: 1.000000
Training Epoch: 21 [3584/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 21 [3840/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 21 [4096/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 21 [4352/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 21 [4608/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 21 [4864/50000]	Loss: 4.5852	LR: 1.000000
Training Epoch: 21 [5120/50000]	Loss: 4.5859	LR: 1.000000
Training Epoch: 21 [5376/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 21 [5632/50000]	Loss: 4.5918	LR: 1.000000
Training Epoch: 21 [5888/50000]	Loss: 4.5919	LR: 1.000000
Training Epoch: 21 [6144/50000]	Loss: 4.5893	LR: 1.000000
Training Epoch: 21 [6400/50000]	Loss: 4.5898	LR: 1.000000
Training Epoch: 21 [6656/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 21 [6912/50000]	Loss: 4.5817	LR: 1.000000
Training Epoch: 21 [7168/50000]	Loss: 4.5879	LR: 1.000000
Training Epoch: 21 [7424/50000]	Loss: 4.5637	LR: 1.000000
Training Epoch: 21 [7680/50000]	Loss: 4.5958	LR: 1.000000
Training Epoch: 21 [7936/50000]	Loss: 4.5769	LR: 1.000000
Training Epoch: 21 [8192/50000]	Loss: 4.5177	LR: 1.000000
Training Epoch: 21 [8448/50000]	Loss: 4.5260	LR: 1.000000
Training Epoch: 21 [8704/50000]	Loss: 4.4969	LR: 1.000000
Training Epoch: 21 [8960/50000]	Loss: 4.5635	LR: 1.000000
Training Epoch: 21 [9216/50000]	Loss: 4.5519	LR: 1.000000
Training Epoch: 21 [9472/50000]	Loss: 4.4809	LR: 1.000000
Training Epoch: 21 [9728/50000]	Loss: 4.5152	LR: 1.000000
Training Epoch: 21 [9984/50000]	Loss: 4.5763	LR: 1.000000
Training Epoch: 21 [10240/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 21 [10496/50000]	Loss: 4.5372	LR: 1.000000
Training Epoch: 21 [10752/50000]	Loss: 4.5300	LR: 1.000000
Training Epoch: 21 [11008/50000]	Loss: 4.5058	LR: 1.000000
Training Epoch: 21 [11264/50000]	Loss: 4.5608	LR: 1.000000
Training Epoch: 21 [11520/50000]	Loss: 4.5055	LR: 1.000000
Training Epoch: 21 [11776/50000]	Loss: 4.5501	LR: 1.000000
Training Epoch: 21 [12032/50000]	Loss: 4.5400	LR: 1.000000
Training Epoch: 21 [12288/50000]	Loss: 4.4948	LR: 1.000000
Training Epoch: 21 [12544/50000]	Loss: 4.5398	LR: 1.000000
Training Epoch: 21 [12800/50000]	Loss: 4.5325	LR: 1.000000
Training Epoch: 21 [13056/50000]	Loss: 4.5194	LR: 1.000000
Training Epoch: 21 [13312/50000]	Loss: 4.4604	LR: 1.000000
Training Epoch: 21 [13568/50000]	Loss: 4.4870	LR: 1.000000
Training Epoch: 21 [13824/50000]	Loss: 4.5018	LR: 1.000000
Training Epoch: 21 [14080/50000]	Loss: 4.5044	LR: 1.000000
Training Epoch: 21 [14336/50000]	Loss: 4.4728	LR: 1.000000
Training Epoch: 21 [14592/50000]	Loss: 4.4745	LR: 1.000000
Training Epoch: 21 [14848/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 21 [15104/50000]	Loss: 4.4397	LR: 1.000000
Training Epoch: 21 [15360/50000]	Loss: 4.5168	LR: 1.000000
Training Epoch: 21 [15616/50000]	Loss: 4.4455	LR: 1.000000
Training Epoch: 21 [15872/50000]	Loss: 4.4666	LR: 1.000000
Training Epoch: 21 [16128/50000]	Loss: 4.4551	LR: 1.000000
Training Epoch: 21 [16384/50000]	Loss: 4.4300	LR: 1.000000
Training Epoch: 21 [16640/50000]	Loss: 4.4592	LR: 1.000000
Training Epoch: 21 [16896/50000]	Loss: 4.4993	LR: 1.000000
Training Epoch: 21 [17152/50000]	Loss: 4.4806	LR: 1.000000
Training Epoch: 21 [17408/50000]	Loss: 4.4968	LR: 1.000000
Training Epoch: 21 [17664/50000]	Loss: 4.4466	LR: 1.000000
Training Epoch: 21 [17920/50000]	Loss: 4.3973	LR: 1.000000
Training Epoch: 21 [18176/50000]	Loss: 4.5044	LR: 1.000000
Training Epoch: 21 [18432/50000]	Loss: 4.5070	LR: 1.000000
Training Epoch: 21 [18688/50000]	Loss: 4.4736	LR: 1.000000
Training Epoch: 21 [18944/50000]	Loss: 4.4622	LR: 1.000000
Training Epoch: 21 [19200/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 21 [19456/50000]	Loss: 4.4753	LR: 1.000000
Training Epoch: 21 [19712/50000]	Loss: 4.5063	LR: 1.000000
Training Epoch: 21 [19968/50000]	Loss: 4.4837	LR: 1.000000
Training Epoch: 21 [20224/50000]	Loss: 4.4471	LR: 1.000000
Training Epoch: 21 [20480/50000]	Loss: 4.4479	LR: 1.000000
Training Epoch: 21 [20736/50000]	Loss: 4.4718	LR: 1.000000
Training Epoch: 21 [20992/50000]	Loss: 4.4148	LR: 1.000000
Training Epoch: 21 [21248/50000]	Loss: 4.4423	LR: 1.000000
Training Epoch: 21 [21504/50000]	Loss: 4.5523	LR: 1.000000
Training Epoch: 21 [21760/50000]	Loss: 4.5062	LR: 1.000000
Training Epoch: 21 [22016/50000]	Loss: 4.4997	LR: 1.000000
Training Epoch: 21 [22272/50000]	Loss: 4.5252	LR: 1.000000
Training Epoch: 21 [22528/50000]	Loss: 4.4952	LR: 1.000000
Training Epoch: 21 [22784/50000]	Loss: 4.4435	LR: 1.000000
Training Epoch: 21 [23040/50000]	Loss: 4.4751	LR: 1.000000
Training Epoch: 21 [23296/50000]	Loss: 4.4273	LR: 1.000000
Training Epoch: 21 [23552/50000]	Loss: 4.4399	LR: 1.000000
Training Epoch: 21 [23808/50000]	Loss: 4.5090	LR: 1.000000
Training Epoch: 21 [24064/50000]	Loss: 4.4616	LR: 1.000000
Training Epoch: 21 [24320/50000]	Loss: 4.4847	LR: 1.000000
Training Epoch: 21 [24576/50000]	Loss: 4.4803	LR: 1.000000
Training Epoch: 21 [24832/50000]	Loss: 4.4095	LR: 1.000000
Training Epoch: 21 [25088/50000]	Loss: 4.4700	LR: 1.000000
Training Epoch: 21 [25344/50000]	Loss: 4.3969	LR: 1.000000
Training Epoch: 21 [25600/50000]	Loss: 4.3908	LR: 1.000000
Training Epoch: 21 [25856/50000]	Loss: 4.4255	LR: 1.000000
Training Epoch: 21 [26112/50000]	Loss: 4.5155	LR: 1.000000
Training Epoch: 21 [26368/50000]	Loss: 4.4594	LR: 1.000000
Training Epoch: 21 [26624/50000]	Loss: 4.4449	LR: 1.000000
Training Epoch: 21 [26880/50000]	Loss: 4.4867	LR: 1.000000
Training Epoch: 21 [27136/50000]	Loss: 4.4147	LR: 1.000000
Training Epoch: 21 [27392/50000]	Loss: 4.4500	LR: 1.000000
Training Epoch: 21 [27648/50000]	Loss: 4.5558	LR: 1.000000
Training Epoch: 21 [27904/50000]	Loss: 4.5144	LR: 1.000000
Training Epoch: 21 [28160/50000]	Loss: 4.5056	LR: 1.000000
Training Epoch: 21 [28416/50000]	Loss: 4.4920	LR: 1.000000
Training Epoch: 21 [28672/50000]	Loss: 4.4156	LR: 1.000000
Training Epoch: 21 [28928/50000]	Loss: 4.5108	LR: 1.000000
Training Epoch: 21 [29184/50000]	Loss: 4.4720	LR: 1.000000
Training Epoch: 21 [29440/50000]	Loss: 4.4238	LR: 1.000000
Training Epoch: 21 [29696/50000]	Loss: 4.4802	LR: 1.000000
Training Epoch: 21 [29952/50000]	Loss: 4.5630	LR: 1.000000
Training Epoch: 21 [30208/50000]	Loss: 4.5066	LR: 1.000000
Training Epoch: 21 [30464/50000]	Loss: 4.4636	LR: 1.000000
Training Epoch: 21 [30720/50000]	Loss: 4.4610	LR: 1.000000
Training Epoch: 21 [30976/50000]	Loss: 4.4530	LR: 1.000000
Training Epoch: 21 [31232/50000]	Loss: 4.4345	LR: 1.000000
Training Epoch: 21 [31488/50000]	Loss: 4.5550	LR: 1.000000
Training Epoch: 21 [31744/50000]	Loss: 4.4413	LR: 1.000000
Training Epoch: 21 [32000/50000]	Loss: 4.4757	LR: 1.000000
Training Epoch: 21 [32256/50000]	Loss: 4.4800	LR: 1.000000
Training Epoch: 21 [32512/50000]	Loss: 4.4379	LR: 1.000000
Training Epoch: 21 [32768/50000]	Loss: 4.5079	LR: 1.000000
Training Epoch: 21 [33024/50000]	Loss: 4.4260	LR: 1.000000
Training Epoch: 21 [33280/50000]	Loss: 4.4249	LR: 1.000000
Training Epoch: 21 [33536/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 21 [33792/50000]	Loss: 4.4246	LR: 1.000000
Training Epoch: 21 [34048/50000]	Loss: 4.5932	LR: 1.000000
Training Epoch: 21 [34304/50000]	Loss: 4.4754	LR: 1.000000
Training Epoch: 21 [34560/50000]	Loss: 4.5023	LR: 1.000000
Training Epoch: 21 [34816/50000]	Loss: 4.5349	LR: 1.000000
Training Epoch: 21 [35072/50000]	Loss: 4.4487	LR: 1.000000
Training Epoch: 21 [35328/50000]	Loss: 4.4674	LR: 1.000000
Training Epoch: 21 [35584/50000]	Loss: 4.4784	LR: 1.000000
Training Epoch: 21 [35840/50000]	Loss: 4.4931	LR: 1.000000
Training Epoch: 21 [36096/50000]	Loss: 4.4979	LR: 1.000000
Training Epoch: 21 [36352/50000]	Loss: 4.4359	LR: 1.000000
Training Epoch: 21 [36608/50000]	Loss: 4.4102	LR: 1.000000
Training Epoch: 21 [36864/50000]	Loss: 4.4693	LR: 1.000000
Training Epoch: 21 [37120/50000]	Loss: 4.4597	LR: 1.000000
Training Epoch: 21 [37376/50000]	Loss: 4.4628	LR: 1.000000
Training Epoch: 21 [37632/50000]	Loss: 4.4872	LR: 1.000000
Training Epoch: 21 [37888/50000]	Loss: 4.5177	LR: 1.000000
Training Epoch: 21 [38144/50000]	Loss: 4.5200	LR: 1.000000
Training Epoch: 21 [38400/50000]	Loss: 4.5810	LR: 1.000000
Training Epoch: 21 [38656/50000]	Loss: 4.4360	LR: 1.000000
Training Epoch: 21 [38912/50000]	Loss: 4.5566	LR: 1.000000
Training Epoch: 21 [39168/50000]	Loss: 4.4650	LR: 1.000000
Training Epoch: 21 [39424/50000]	Loss: 4.5027	LR: 1.000000
Training Epoch: 21 [39680/50000]	Loss: 4.4787	LR: 1.000000
Training Epoch: 21 [39936/50000]	Loss: 4.5223	LR: 1.000000
Training Epoch: 21 [40192/50000]	Loss: 4.4837	LR: 1.000000
Training Epoch: 21 [40448/50000]	Loss: 4.4717	LR: 1.000000
Training Epoch: 21 [40704/50000]	Loss: 4.4387	LR: 1.000000
Training Epoch: 21 [40960/50000]	Loss: 4.3891	LR: 1.000000
Training Epoch: 21 [41216/50000]	Loss: 4.4894	LR: 1.000000
Training Epoch: 21 [41472/50000]	Loss: 4.4789	LR: 1.000000
Training Epoch: 21 [41728/50000]	Loss: 4.5279	LR: 1.000000
Training Epoch: 21 [41984/50000]	Loss: 4.4810	LR: 1.000000
Training Epoch: 21 [42240/50000]	Loss: 4.4957	LR: 1.000000
Training Epoch: 21 [42496/50000]	Loss: 4.4801	LR: 1.000000
Training Epoch: 21 [42752/50000]	Loss: 4.5350	LR: 1.000000
Training Epoch: 21 [43008/50000]	Loss: 4.4881	LR: 1.000000
Training Epoch: 21 [43264/50000]	Loss: 4.4552	LR: 1.000000
Training Epoch: 21 [43520/50000]	Loss: 4.4607	LR: 1.000000
Training Epoch: 21 [43776/50000]	Loss: 4.5016	LR: 1.000000
Training Epoch: 21 [44032/50000]	Loss: 4.5033	LR: 1.000000
Training Epoch: 21 [44288/50000]	Loss: 4.4633	LR: 1.000000
Training Epoch: 21 [44544/50000]	Loss: 4.4718	LR: 1.000000
Training Epoch: 21 [44800/50000]	Loss: 4.4915	LR: 1.000000
Training Epoch: 21 [45056/50000]	Loss: 4.4983	LR: 1.000000
Training Epoch: 21 [45312/50000]	Loss: 4.5226	LR: 1.000000
Training Epoch: 21 [45568/50000]	Loss: 4.5364	LR: 1.000000
Training Epoch: 21 [45824/50000]	Loss: 4.4995	LR: 1.000000
Training Epoch: 21 [46080/50000]	Loss: 4.4818	LR: 1.000000
Training Epoch: 21 [46336/50000]	Loss: 4.5149	LR: 1.000000
Training Epoch: 21 [46592/50000]	Loss: 4.4937	LR: 1.000000
Training Epoch: 21 [46848/50000]	Loss: 4.5822	LR: 1.000000
Training Epoch: 21 [47104/50000]	Loss: 4.4669	LR: 1.000000
Training Epoch: 21 [47360/50000]	Loss: 4.8744	LR: 1.000000
Training Epoch: 21 [47616/50000]	Loss: 4.5506	LR: 1.000000
Training Epoch: 21 [47872/50000]	Loss: 4.8242	LR: 1.000000
Training Epoch: 21 [48128/50000]	Loss: 4.6921	LR: 1.000000
Training Epoch: 21 [48384/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 21 [48640/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 21 [48896/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 21 [49152/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 21 [49408/50000]	Loss: 4.6273	LR: 1.000000
Training Epoch: 21 [49664/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 21 [49920/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 21 [50000/50000]	Loss: 4.6099	LR: 1.000000
epoch 21 training time consumed: 21.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   31504 GB |   31504 GB |
|       from large pool |  400448 KB |    1770 MB |   31476 GB |   31476 GB |
|       from small pool |    3549 KB |       9 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   31504 GB |   31504 GB |
|       from large pool |  400448 KB |    1770 MB |   31476 GB |   31476 GB |
|       from small pool |    3549 KB |       9 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   19385 GB |   19385 GB |
|       from large pool |  244672 KB |  473024 KB |   19353 GB |   19353 GB |
|       from small pool |    2594 KB |    4843 KB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1344 K  |    1344 K  |
|       from large pool |      36    |      77    |     650 K  |     650 K  |
|       from small pool |     186    |     224    |     694 K  |     694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1344 K  |    1344 K  |
|       from large pool |      36    |      77    |     650 K  |     650 K  |
|       from small pool |     186    |     224    |     694 K  |     694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      32    |  683035    |  683014    |
|       from large pool |      10    |      11    |  269315    |  269305    |
|       from small pool |      11    |      22    |  413720    |  413709    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 21, Average loss: 0.0183, Accuracy: 0.0167, Time consumed:1.48s

Training Epoch: 22 [256/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 22 [512/50000]	Loss: 4.5889	LR: 1.000000
Training Epoch: 22 [768/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 22 [1024/50000]	Loss: 4.5877	LR: 1.000000
Training Epoch: 22 [1280/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 22 [1536/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 22 [1792/50000]	Loss: 4.5986	LR: 1.000000
Training Epoch: 22 [2048/50000]	Loss: 4.5885	LR: 1.000000
Training Epoch: 22 [2304/50000]	Loss: 4.5889	LR: 1.000000
Training Epoch: 22 [2560/50000]	Loss: 4.5687	LR: 1.000000
Training Epoch: 22 [2816/50000]	Loss: 4.5787	LR: 1.000000
Training Epoch: 22 [3072/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 22 [3328/50000]	Loss: 4.5938	LR: 1.000000
Training Epoch: 22 [3584/50000]	Loss: 4.5808	LR: 1.000000
Training Epoch: 22 [3840/50000]	Loss: 4.5742	LR: 1.000000
Training Epoch: 22 [4096/50000]	Loss: 4.5330	LR: 1.000000
Training Epoch: 22 [4352/50000]	Loss: 4.5840	LR: 1.000000
Training Epoch: 22 [4608/50000]	Loss: 4.5516	LR: 1.000000
Training Epoch: 22 [4864/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 22 [5120/50000]	Loss: 4.5771	LR: 1.000000
Training Epoch: 22 [5376/50000]	Loss: 4.5724	LR: 1.000000
Training Epoch: 22 [5632/50000]	Loss: 4.5542	LR: 1.000000
Training Epoch: 22 [5888/50000]	Loss: 4.5645	LR: 1.000000
Training Epoch: 22 [6144/50000]	Loss: 4.5484	LR: 1.000000
Training Epoch: 22 [6400/50000]	Loss: 4.5910	LR: 1.000000
Training Epoch: 22 [6656/50000]	Loss: 4.5349	LR: 1.000000
Training Epoch: 22 [6912/50000]	Loss: 4.5709	LR: 1.000000
Training Epoch: 22 [7168/50000]	Loss: 4.5522	LR: 1.000000
Training Epoch: 22 [7424/50000]	Loss: 4.5676	LR: 1.000000
Training Epoch: 22 [7680/50000]	Loss: 4.5642	LR: 1.000000
Training Epoch: 22 [7936/50000]	Loss: 4.5774	LR: 1.000000
Training Epoch: 22 [8192/50000]	Loss: 4.5244	LR: 1.000000
Training Epoch: 22 [8448/50000]	Loss: 4.5645	LR: 1.000000
Training Epoch: 22 [8704/50000]	Loss: 4.5820	LR: 1.000000
Training Epoch: 22 [8960/50000]	Loss: 4.5719	LR: 1.000000
Training Epoch: 22 [9216/50000]	Loss: 4.5172	LR: 1.000000
Training Epoch: 22 [9472/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 22 [9728/50000]	Loss: 4.5552	LR: 1.000000
Training Epoch: 22 [9984/50000]	Loss: 4.5510	LR: 1.000000
Training Epoch: 22 [10240/50000]	Loss: 4.5420	LR: 1.000000
Training Epoch: 22 [10496/50000]	Loss: 4.5570	LR: 1.000000
Training Epoch: 22 [10752/50000]	Loss: 4.5156	LR: 1.000000
Training Epoch: 22 [11008/50000]	Loss: 4.5342	LR: 1.000000
Training Epoch: 22 [11264/50000]	Loss: 4.5092	LR: 1.000000
Training Epoch: 22 [11520/50000]	Loss: 4.5209	LR: 1.000000
Training Epoch: 22 [11776/50000]	Loss: 4.5701	LR: 1.000000
Training Epoch: 22 [12032/50000]	Loss: 4.5512	LR: 1.000000
Training Epoch: 22 [12288/50000]	Loss: 4.5274	LR: 1.000000
Training Epoch: 22 [12544/50000]	Loss: 4.5656	LR: 1.000000
Training Epoch: 22 [12800/50000]	Loss: 4.5311	LR: 1.000000
Training Epoch: 22 [13056/50000]	Loss: 4.5536	LR: 1.000000
Training Epoch: 22 [13312/50000]	Loss: 4.5483	LR: 1.000000
Training Epoch: 22 [13568/50000]	Loss: 4.5010	LR: 1.000000
Training Epoch: 22 [13824/50000]	Loss: 4.5530	LR: 1.000000
Training Epoch: 22 [14080/50000]	Loss: 4.5578	LR: 1.000000
Training Epoch: 22 [14336/50000]	Loss: 4.5805	LR: 1.000000
Training Epoch: 22 [14592/50000]	Loss: 4.4841	LR: 1.000000
Training Epoch: 22 [14848/50000]	Loss: 4.5761	LR: 1.000000
Training Epoch: 22 [15104/50000]	Loss: 4.5000	LR: 1.000000
Training Epoch: 22 [15360/50000]	Loss: 4.5236	LR: 1.000000
Training Epoch: 22 [15616/50000]	Loss: 4.5310	LR: 1.000000
Training Epoch: 22 [15872/50000]	Loss: 4.5233	LR: 1.000000
Training Epoch: 22 [16128/50000]	Loss: 4.5232	LR: 1.000000
Training Epoch: 22 [16384/50000]	Loss: 4.5405	LR: 1.000000
Training Epoch: 22 [16640/50000]	Loss: 4.4963	LR: 1.000000
Training Epoch: 22 [16896/50000]	Loss: 4.4997	LR: 1.000000
Training Epoch: 22 [17152/50000]	Loss: 4.4719	LR: 1.000000
Training Epoch: 22 [17408/50000]	Loss: 4.5618	LR: 1.000000
Training Epoch: 22 [17664/50000]	Loss: 4.5451	LR: 1.000000
Training Epoch: 22 [17920/50000]	Loss: 4.4833	LR: 1.000000
Training Epoch: 22 [18176/50000]	Loss: 4.5364	LR: 1.000000
Training Epoch: 22 [18432/50000]	Loss: 4.5251	LR: 1.000000
Training Epoch: 22 [18688/50000]	Loss: 4.4810	LR: 1.000000
Training Epoch: 22 [18944/50000]	Loss: 4.4745	LR: 1.000000
Training Epoch: 22 [19200/50000]	Loss: 4.5258	LR: 1.000000
Training Epoch: 22 [19456/50000]	Loss: 4.5472	LR: 1.000000
Training Epoch: 22 [19712/50000]	Loss: 4.5038	LR: 1.000000
Training Epoch: 22 [19968/50000]	Loss: 4.5322	LR: 1.000000
Training Epoch: 22 [20224/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 22 [20480/50000]	Loss: 4.5128	LR: 1.000000
Training Epoch: 22 [20736/50000]	Loss: 4.5445	LR: 1.000000
Training Epoch: 22 [20992/50000]	Loss: 4.4918	LR: 1.000000
Training Epoch: 22 [21248/50000]	Loss: 4.5236	LR: 1.000000
Training Epoch: 22 [21504/50000]	Loss: 4.4917	LR: 1.000000
Training Epoch: 22 [21760/50000]	Loss: 4.4762	LR: 1.000000
Training Epoch: 22 [22016/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 22 [22272/50000]	Loss: 4.4902	LR: 1.000000
Training Epoch: 22 [22528/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 22 [22784/50000]	Loss: 4.5608	LR: 1.000000
Training Epoch: 22 [23040/50000]	Loss: 4.5352	LR: 1.000000
Training Epoch: 22 [23296/50000]	Loss: 4.4927	LR: 1.000000
Training Epoch: 22 [23552/50000]	Loss: 4.4839	LR: 1.000000
Training Epoch: 22 [23808/50000]	Loss: 4.5065	LR: 1.000000
Training Epoch: 22 [24064/50000]	Loss: 4.4769	LR: 1.000000
Training Epoch: 22 [24320/50000]	Loss: 4.5182	LR: 1.000000
Training Epoch: 22 [24576/50000]	Loss: 4.5699	LR: 1.000000
Training Epoch: 22 [24832/50000]	Loss: 4.5179	LR: 1.000000
Training Epoch: 22 [25088/50000]	Loss: 4.5160	LR: 1.000000
Training Epoch: 22 [25344/50000]	Loss: 4.4966	LR: 1.000000
Training Epoch: 22 [25600/50000]	Loss: 4.5063	LR: 1.000000
Training Epoch: 22 [25856/50000]	Loss: 4.4719	LR: 1.000000
Training Epoch: 22 [26112/50000]	Loss: 4.4664	LR: 1.000000
Training Epoch: 22 [26368/50000]	Loss: 4.5026	LR: 1.000000
Training Epoch: 22 [26624/50000]	Loss: 4.5002	LR: 1.000000
Training Epoch: 22 [26880/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 22 [27136/50000]	Loss: 4.4769	LR: 1.000000
Training Epoch: 22 [27392/50000]	Loss: 4.5157	LR: 1.000000
Training Epoch: 22 [27648/50000]	Loss: 4.4981	LR: 1.000000
Training Epoch: 22 [27904/50000]	Loss: 4.4885	LR: 1.000000
Training Epoch: 22 [28160/50000]	Loss: 4.5293	LR: 1.000000
Training Epoch: 22 [28416/50000]	Loss: 4.4983	LR: 1.000000
Training Epoch: 22 [28672/50000]	Loss: 4.5098	LR: 1.000000
Training Epoch: 22 [28928/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 22 [29184/50000]	Loss: 4.5416	LR: 1.000000
Training Epoch: 22 [29440/50000]	Loss: 4.5353	LR: 1.000000
Training Epoch: 22 [29696/50000]	Loss: 4.5079	LR: 1.000000
Training Epoch: 22 [29952/50000]	Loss: 4.5093	LR: 1.000000
Training Epoch: 22 [30208/50000]	Loss: 4.5075	LR: 1.000000
Training Epoch: 22 [30464/50000]	Loss: 4.4834	LR: 1.000000
Training Epoch: 22 [30720/50000]	Loss: 4.5208	LR: 1.000000
Training Epoch: 22 [30976/50000]	Loss: 4.5009	LR: 1.000000
Training Epoch: 22 [31232/50000]	Loss: 4.4687	LR: 1.000000
Training Epoch: 22 [31488/50000]	Loss: 4.5602	LR: 1.000000
Training Epoch: 22 [31744/50000]	Loss: 4.5488	LR: 1.000000
Training Epoch: 22 [32000/50000]	Loss: 4.4641	LR: 1.000000
Training Epoch: 22 [32256/50000]	Loss: 4.5159	LR: 1.000000
Training Epoch: 22 [32512/50000]	Loss: 4.5269	LR: 1.000000
Training Epoch: 22 [32768/50000]	Loss: 4.5049	LR: 1.000000
Training Epoch: 22 [33024/50000]	Loss: 4.5091	LR: 1.000000
Training Epoch: 22 [33280/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 22 [33536/50000]	Loss: 4.5871	LR: 1.000000
Training Epoch: 22 [33792/50000]	Loss: 4.5238	LR: 1.000000
Training Epoch: 22 [34048/50000]	Loss: 4.5954	LR: 1.000000
Training Epoch: 22 [34304/50000]	Loss: 4.5446	LR: 1.000000
Training Epoch: 22 [34560/50000]	Loss: 4.5730	LR: 1.000000
Training Epoch: 22 [34816/50000]	Loss: 4.5513	LR: 1.000000
Training Epoch: 22 [35072/50000]	Loss: 4.5383	LR: 1.000000
Training Epoch: 22 [35328/50000]	Loss: 4.5106	LR: 1.000000
Training Epoch: 22 [35584/50000]	Loss: 4.5534	LR: 1.000000
Training Epoch: 22 [35840/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 22 [36096/50000]	Loss: 4.5351	LR: 1.000000
Training Epoch: 22 [36352/50000]	Loss: 4.5500	LR: 1.000000
Training Epoch: 22 [36608/50000]	Loss: 4.5476	LR: 1.000000
Training Epoch: 22 [36864/50000]	Loss: 4.5384	LR: 1.000000
Training Epoch: 22 [37120/50000]	Loss: 4.5833	LR: 1.000000
Training Epoch: 22 [37376/50000]	Loss: 4.5604	LR: 1.000000
Training Epoch: 22 [37632/50000]	Loss: 4.5602	LR: 1.000000
Training Epoch: 22 [37888/50000]	Loss: 4.5765	LR: 1.000000
Training Epoch: 22 [38144/50000]	Loss: 4.5210	LR: 1.000000
Training Epoch: 22 [38400/50000]	Loss: 4.5581	LR: 1.000000
Training Epoch: 22 [38656/50000]	Loss: 4.5246	LR: 1.000000
Training Epoch: 22 [38912/50000]	Loss: 4.5434	LR: 1.000000
Training Epoch: 22 [39168/50000]	Loss: 4.5237	LR: 1.000000
Training Epoch: 22 [39424/50000]	Loss: 4.4796	LR: 1.000000
Training Epoch: 22 [39680/50000]	Loss: 4.5018	LR: 1.000000
Training Epoch: 22 [39936/50000]	Loss: 4.5564	LR: 1.000000
Training Epoch: 22 [40192/50000]	Loss: 4.5096	LR: 1.000000
Training Epoch: 22 [40448/50000]	Loss: 4.5602	LR: 1.000000
Training Epoch: 22 [40704/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 22 [40960/50000]	Loss: 4.5107	LR: 1.000000
Training Epoch: 22 [41216/50000]	Loss: 4.4745	LR: 1.000000
Training Epoch: 22 [41472/50000]	Loss: 4.5624	LR: 1.000000
Training Epoch: 22 [41728/50000]	Loss: 4.5398	LR: 1.000000
Training Epoch: 22 [41984/50000]	Loss: 4.4978	LR: 1.000000
Training Epoch: 22 [42240/50000]	Loss: 4.5153	LR: 1.000000
Training Epoch: 22 [42496/50000]	Loss: 4.4811	LR: 1.000000
Training Epoch: 22 [42752/50000]	Loss: 4.4809	LR: 1.000000
Training Epoch: 22 [43008/50000]	Loss: 4.5104	LR: 1.000000
Training Epoch: 22 [43264/50000]	Loss: 4.4841	LR: 1.000000
Training Epoch: 22 [43520/50000]	Loss: 4.5066	LR: 1.000000
Training Epoch: 22 [43776/50000]	Loss: 4.5224	LR: 1.000000
Training Epoch: 22 [44032/50000]	Loss: 4.4933	LR: 1.000000
Training Epoch: 22 [44288/50000]	Loss: 4.5583	LR: 1.000000
Training Epoch: 22 [44544/50000]	Loss: 4.5055	LR: 1.000000
Training Epoch: 22 [44800/50000]	Loss: 4.5001	LR: 1.000000
Training Epoch: 22 [45056/50000]	Loss: 4.5778	LR: 1.000000
Training Epoch: 22 [45312/50000]	Loss: 4.5132	LR: 1.000000
Training Epoch: 22 [45568/50000]	Loss: 4.5181	LR: 1.000000
Training Epoch: 22 [45824/50000]	Loss: 4.5910	LR: 1.000000
Training Epoch: 22 [46080/50000]	Loss: 4.4934	LR: 1.000000
Training Epoch: 22 [46336/50000]	Loss: 4.5021	LR: 1.000000
Training Epoch: 22 [46592/50000]	Loss: 4.5235	LR: 1.000000
Training Epoch: 22 [46848/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 22 [47104/50000]	Loss: 4.5483	LR: 1.000000
Training Epoch: 22 [47360/50000]	Loss: 4.5139	LR: 1.000000
Training Epoch: 22 [47616/50000]	Loss: 4.4885	LR: 1.000000
Training Epoch: 22 [47872/50000]	Loss: 4.4635	LR: 1.000000
Training Epoch: 22 [48128/50000]	Loss: 4.4889	LR: 1.000000
Training Epoch: 22 [48384/50000]	Loss: 4.5235	LR: 1.000000
Training Epoch: 22 [48640/50000]	Loss: 4.4903	LR: 1.000000
Training Epoch: 22 [48896/50000]	Loss: 4.5202	LR: 1.000000
Training Epoch: 22 [49152/50000]	Loss: 4.4593	LR: 1.000000
Training Epoch: 22 [49408/50000]	Loss: 4.5254	LR: 1.000000
Training Epoch: 22 [49664/50000]	Loss: 4.5472	LR: 1.000000
Training Epoch: 22 [49920/50000]	Loss: 4.4738	LR: 1.000000
Training Epoch: 22 [50000/50000]	Loss: 4.5286	LR: 1.000000
epoch 22 training time consumed: 21.92s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   33005 GB |   33004 GB |
|       from large pool |  400448 KB |    1770 MB |   32975 GB |   32974 GB |
|       from small pool |    3549 KB |       9 MB |      29 GB |      29 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   33005 GB |   33004 GB |
|       from large pool |  400448 KB |    1770 MB |   32975 GB |   32974 GB |
|       from small pool |    3549 KB |       9 MB |      29 GB |      29 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   20308 GB |   20308 GB |
|       from large pool |  244672 KB |  473024 KB |   20274 GB |   20274 GB |
|       from small pool |    2594 KB |    4843 KB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1408 K  |    1408 K  |
|       from large pool |      36    |      77    |     681 K  |     681 K  |
|       from small pool |     186    |     224    |     727 K  |     727 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1408 K  |    1408 K  |
|       from large pool |      36    |      77    |     681 K  |     681 K  |
|       from small pool |     186    |     224    |     727 K  |     727 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      32    |  716293    |  716270    |
|       from large pool |      10    |      11    |  282138    |  282128    |
|       from small pool |      13    |      22    |  434155    |  434142    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 22, Average loss: 0.0179, Accuracy: 0.0192, Time consumed:1.43s

Training Epoch: 23 [256/50000]	Loss: 4.5678	LR: 1.000000
Training Epoch: 23 [512/50000]	Loss: 4.4964	LR: 1.000000
Training Epoch: 23 [768/50000]	Loss: 4.4828	LR: 1.000000
Training Epoch: 23 [1024/50000]	Loss: 4.4861	LR: 1.000000
Training Epoch: 23 [1280/50000]	Loss: 4.5538	LR: 1.000000
Training Epoch: 23 [1536/50000]	Loss: 4.4658	LR: 1.000000
Training Epoch: 23 [1792/50000]	Loss: 4.5183	LR: 1.000000
Training Epoch: 23 [2048/50000]	Loss: 4.4720	LR: 1.000000
Training Epoch: 23 [2304/50000]	Loss: 4.4834	LR: 1.000000
Training Epoch: 23 [2560/50000]	Loss: 4.5865	LR: 1.000000
Training Epoch: 23 [2816/50000]	Loss: 4.5601	LR: 1.000000
Training Epoch: 23 [3072/50000]	Loss: 4.5154	LR: 1.000000
Training Epoch: 23 [3328/50000]	Loss: 4.5544	LR: 1.000000
Training Epoch: 23 [3584/50000]	Loss: 4.5185	LR: 1.000000
Training Epoch: 23 [3840/50000]	Loss: 4.5187	LR: 1.000000
Training Epoch: 23 [4096/50000]	Loss: 4.5033	LR: 1.000000
Training Epoch: 23 [4352/50000]	Loss: 4.5200	LR: 1.000000
Training Epoch: 23 [4608/50000]	Loss: 4.5189	LR: 1.000000
Training Epoch: 23 [4864/50000]	Loss: 4.4687	LR: 1.000000
Training Epoch: 23 [5120/50000]	Loss: 4.5447	LR: 1.000000
Training Epoch: 23 [5376/50000]	Loss: 4.5192	LR: 1.000000
Training Epoch: 23 [5632/50000]	Loss: 4.4691	LR: 1.000000
Training Epoch: 23 [5888/50000]	Loss: 4.4655	LR: 1.000000
Training Epoch: 23 [6144/50000]	Loss: 4.5702	LR: 1.000000
Training Epoch: 23 [6400/50000]	Loss: 4.5001	LR: 1.000000
Training Epoch: 23 [6656/50000]	Loss: 4.4897	LR: 1.000000
Training Epoch: 23 [6912/50000]	Loss: 4.5253	LR: 1.000000
Training Epoch: 23 [7168/50000]	Loss: 4.5374	LR: 1.000000
Training Epoch: 23 [7424/50000]	Loss: 4.5148	LR: 1.000000
Training Epoch: 23 [7680/50000]	Loss: 4.4869	LR: 1.000000
Training Epoch: 23 [7936/50000]	Loss: 4.4793	LR: 1.000000
Training Epoch: 23 [8192/50000]	Loss: 4.4986	LR: 1.000000
Training Epoch: 23 [8448/50000]	Loss: 4.5314	LR: 1.000000
Training Epoch: 23 [8704/50000]	Loss: 4.4789	LR: 1.000000
Training Epoch: 23 [8960/50000]	Loss: 4.5310	LR: 1.000000
Training Epoch: 23 [9216/50000]	Loss: 4.4776	LR: 1.000000
Training Epoch: 23 [9472/50000]	Loss: 4.5163	LR: 1.000000
Training Epoch: 23 [9728/50000]	Loss: 4.5036	LR: 1.000000
Training Epoch: 23 [9984/50000]	Loss: 4.4755	LR: 1.000000
Training Epoch: 23 [10240/50000]	Loss: 4.4940	LR: 1.000000
Training Epoch: 23 [10496/50000]	Loss: 4.4494	LR: 1.000000
Training Epoch: 23 [10752/50000]	Loss: 4.5302	LR: 1.000000
Training Epoch: 23 [11008/50000]	Loss: 4.4905	LR: 1.000000
Training Epoch: 23 [11264/50000]	Loss: 4.5391	LR: 1.000000
Training Epoch: 23 [11520/50000]	Loss: 4.4697	LR: 1.000000
Training Epoch: 23 [11776/50000]	Loss: 4.5274	LR: 1.000000
Training Epoch: 23 [12032/50000]	Loss: 4.4838	LR: 1.000000
Training Epoch: 23 [12288/50000]	Loss: 4.4518	LR: 1.000000
Training Epoch: 23 [12544/50000]	Loss: 4.4850	LR: 1.000000
Training Epoch: 23 [12800/50000]	Loss: 4.5138	LR: 1.000000
Training Epoch: 23 [13056/50000]	Loss: 4.4784	LR: 1.000000
Training Epoch: 23 [13312/50000]	Loss: 4.5419	LR: 1.000000
Training Epoch: 23 [13568/50000]	Loss: 4.4963	LR: 1.000000
Training Epoch: 23 [13824/50000]	Loss: 4.4879	LR: 1.000000
Training Epoch: 23 [14080/50000]	Loss: 4.4705	LR: 1.000000
Training Epoch: 23 [14336/50000]	Loss: 4.4636	LR: 1.000000
Training Epoch: 23 [14592/50000]	Loss: 4.5172	LR: 1.000000
Training Epoch: 23 [14848/50000]	Loss: 4.4931	LR: 1.000000
Training Epoch: 23 [15104/50000]	Loss: 4.4621	LR: 1.000000
Training Epoch: 23 [15360/50000]	Loss: 4.5098	LR: 1.000000
Training Epoch: 23 [15616/50000]	Loss: 4.4940	LR: 1.000000
Training Epoch: 23 [15872/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 23 [16128/50000]	Loss: 4.5100	LR: 1.000000
Training Epoch: 23 [16384/50000]	Loss: 4.4945	LR: 1.000000
Training Epoch: 23 [16640/50000]	Loss: 4.4307	LR: 1.000000
Training Epoch: 23 [16896/50000]	Loss: 4.4672	LR: 1.000000
Training Epoch: 23 [17152/50000]	Loss: 4.5295	LR: 1.000000
Training Epoch: 23 [17408/50000]	Loss: 4.4679	LR: 1.000000
Training Epoch: 23 [17664/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 23 [17920/50000]	Loss: 4.5146	LR: 1.000000
Training Epoch: 23 [18176/50000]	Loss: 4.4882	LR: 1.000000
Training Epoch: 23 [18432/50000]	Loss: 4.5075	LR: 1.000000
Training Epoch: 23 [18688/50000]	Loss: 4.5202	LR: 1.000000
Training Epoch: 23 [18944/50000]	Loss: 4.5458	LR: 1.000000
Training Epoch: 23 [19200/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 23 [19456/50000]	Loss: 4.5399	LR: 1.000000
Training Epoch: 23 [19712/50000]	Loss: 4.5438	LR: 1.000000
Training Epoch: 23 [19968/50000]	Loss: 4.4719	LR: 1.000000
Training Epoch: 23 [20224/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 23 [20480/50000]	Loss: 4.5045	LR: 1.000000
Training Epoch: 23 [20736/50000]	Loss: 4.5715	LR: 1.000000
Training Epoch: 23 [20992/50000]	Loss: 4.5295	LR: 1.000000
Training Epoch: 23 [21248/50000]	Loss: 4.5527	LR: 1.000000
Training Epoch: 23 [21504/50000]	Loss: 4.5283	LR: 1.000000
Training Epoch: 23 [21760/50000]	Loss: 4.5035	LR: 1.000000
Training Epoch: 23 [22016/50000]	Loss: 4.5051	LR: 1.000000
Training Epoch: 23 [22272/50000]	Loss: 4.5419	LR: 1.000000
Training Epoch: 23 [22528/50000]	Loss: 4.5458	LR: 1.000000
Training Epoch: 23 [22784/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 23 [23040/50000]	Loss: 4.5817	LR: 1.000000
Training Epoch: 23 [23296/50000]	Loss: 4.5265	LR: 1.000000
Training Epoch: 23 [23552/50000]	Loss: 4.4877	LR: 1.000000
Training Epoch: 23 [23808/50000]	Loss: 4.5360	LR: 1.000000
Training Epoch: 23 [24064/50000]	Loss: 4.5005	LR: 1.000000
Training Epoch: 23 [24320/50000]	Loss: 4.5453	LR: 1.000000
Training Epoch: 23 [24576/50000]	Loss: 4.5369	LR: 1.000000
Training Epoch: 23 [24832/50000]	Loss: 4.5146	LR: 1.000000
Training Epoch: 23 [25088/50000]	Loss: 4.5484	LR: 1.000000
Training Epoch: 23 [25344/50000]	Loss: 4.4966	LR: 1.000000
Training Epoch: 23 [25600/50000]	Loss: 4.5281	LR: 1.000000
Training Epoch: 23 [25856/50000]	Loss: 4.5151	LR: 1.000000
Training Epoch: 23 [26112/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 23 [26368/50000]	Loss: 4.4904	LR: 1.000000
Training Epoch: 23 [26624/50000]	Loss: 4.5138	LR: 1.000000
Training Epoch: 23 [26880/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 23 [27136/50000]	Loss: 4.4754	LR: 1.000000
Training Epoch: 23 [27392/50000]	Loss: 4.5304	LR: 1.000000
Training Epoch: 23 [27648/50000]	Loss: 4.5231	LR: 1.000000
Training Epoch: 23 [27904/50000]	Loss: 4.5333	LR: 1.000000
Training Epoch: 23 [28160/50000]	Loss: 4.5273	LR: 1.000000
Training Epoch: 23 [28416/50000]	Loss: 4.5263	LR: 1.000000
Training Epoch: 23 [28672/50000]	Loss: 4.4846	LR: 1.000000
Training Epoch: 23 [28928/50000]	Loss: 4.5239	LR: 1.000000
Training Epoch: 23 [29184/50000]	Loss: 4.5094	LR: 1.000000
Training Epoch: 23 [29440/50000]	Loss: 4.5284	LR: 1.000000
Training Epoch: 23 [29696/50000]	Loss: 4.5892	LR: 1.000000
Training Epoch: 23 [29952/50000]	Loss: 4.5241	LR: 1.000000
Training Epoch: 23 [30208/50000]	Loss: 4.4529	LR: 1.000000
Training Epoch: 23 [30464/50000]	Loss: 4.4904	LR: 1.000000
Training Epoch: 23 [30720/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 23 [30976/50000]	Loss: 5.0687	LR: 1.000000
Training Epoch: 23 [31232/50000]	Loss: 4.8088	LR: 1.000000
Training Epoch: 23 [31488/50000]	Loss: 4.5907	LR: 1.000000
Training Epoch: 23 [31744/50000]	Loss: 4.6340	LR: 1.000000
Training Epoch: 23 [32000/50000]	Loss: 4.6583	LR: 1.000000
Training Epoch: 23 [32256/50000]	Loss: 4.6375	LR: 1.000000
Training Epoch: 23 [32512/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 23 [32768/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 23 [33024/50000]	Loss: 4.6470	LR: 1.000000
Training Epoch: 23 [33280/50000]	Loss: 4.6315	LR: 1.000000
Training Epoch: 23 [33536/50000]	Loss: 4.6370	LR: 1.000000
Training Epoch: 23 [33792/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 23 [34048/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 23 [34304/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 23 [34560/50000]	Loss: 4.6371	LR: 1.000000
Training Epoch: 23 [34816/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 23 [35072/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 23 [35328/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 23 [35584/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 23 [35840/50000]	Loss: 4.6357	LR: 1.000000
Training Epoch: 23 [36096/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 23 [36352/50000]	Loss: 4.5946	LR: 1.000000
Training Epoch: 23 [36608/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 23 [36864/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 23 [37120/50000]	Loss: 4.6300	LR: 1.000000
Training Epoch: 23 [37376/50000]	Loss: 4.5808	LR: 1.000000
Training Epoch: 23 [37632/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 23 [37888/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 23 [38144/50000]	Loss: 4.5896	LR: 1.000000
Training Epoch: 23 [38400/50000]	Loss: 4.5936	LR: 1.000000
Training Epoch: 23 [38656/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 23 [38912/50000]	Loss: 4.5906	LR: 1.000000
Training Epoch: 23 [39168/50000]	Loss: 4.6008	LR: 1.000000
Training Epoch: 23 [39424/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 23 [39680/50000]	Loss: 4.5904	LR: 1.000000
Training Epoch: 23 [39936/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 23 [40192/50000]	Loss: 4.5810	LR: 1.000000
Training Epoch: 23 [40448/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 23 [40704/50000]	Loss: 4.5681	LR: 1.000000
Training Epoch: 23 [40960/50000]	Loss: 4.5516	LR: 1.000000
Training Epoch: 23 [41216/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 23 [41472/50000]	Loss: 4.5729	LR: 1.000000
Training Epoch: 23 [41728/50000]	Loss: 4.5789	LR: 1.000000
Training Epoch: 23 [41984/50000]	Loss: 4.5909	LR: 1.000000
Training Epoch: 23 [42240/50000]	Loss: 4.5587	LR: 1.000000
Training Epoch: 23 [42496/50000]	Loss: 4.5689	LR: 1.000000
Training Epoch: 23 [42752/50000]	Loss: 4.5689	LR: 1.000000
Training Epoch: 23 [43008/50000]	Loss: 4.5644	LR: 1.000000
Training Epoch: 23 [43264/50000]	Loss: 4.5762	LR: 1.000000
Training Epoch: 23 [43520/50000]	Loss: 4.5831	LR: 1.000000
Training Epoch: 23 [43776/50000]	Loss: 4.5593	LR: 1.000000
Training Epoch: 23 [44032/50000]	Loss: 4.5505	LR: 1.000000
Training Epoch: 23 [44288/50000]	Loss: 4.5370	LR: 1.000000
Training Epoch: 23 [44544/50000]	Loss: 4.5522	LR: 1.000000
Training Epoch: 23 [44800/50000]	Loss: 4.5746	LR: 1.000000
Training Epoch: 23 [45056/50000]	Loss: 4.5354	LR: 1.000000
Training Epoch: 23 [45312/50000]	Loss: 4.5469	LR: 1.000000
Training Epoch: 23 [45568/50000]	Loss: 4.5466	LR: 1.000000
Training Epoch: 23 [45824/50000]	Loss: 4.5441	LR: 1.000000
Training Epoch: 23 [46080/50000]	Loss: 4.5673	LR: 1.000000
Training Epoch: 23 [46336/50000]	Loss: 4.5758	LR: 1.000000
Training Epoch: 23 [46592/50000]	Loss: 4.5499	LR: 1.000000
Training Epoch: 23 [46848/50000]	Loss: 4.5619	LR: 1.000000
Training Epoch: 23 [47104/50000]	Loss: 4.5611	LR: 1.000000
Training Epoch: 23 [47360/50000]	Loss: 4.5369	LR: 1.000000
Training Epoch: 23 [47616/50000]	Loss: 4.5678	LR: 1.000000
Training Epoch: 23 [47872/50000]	Loss: 4.5548	LR: 1.000000
Training Epoch: 23 [48128/50000]	Loss: 4.5246	LR: 1.000000
Training Epoch: 23 [48384/50000]	Loss: 4.5361	LR: 1.000000
Training Epoch: 23 [48640/50000]	Loss: 4.5850	LR: 1.000000
Training Epoch: 23 [48896/50000]	Loss: 4.5000	LR: 1.000000
Training Epoch: 23 [49152/50000]	Loss: 4.5191	LR: 1.000000
Training Epoch: 23 [49408/50000]	Loss: 4.5313	LR: 1.000000
Training Epoch: 23 [49664/50000]	Loss: 4.5450	LR: 1.000000
Training Epoch: 23 [49920/50000]	Loss: 4.5532	LR: 1.000000
Training Epoch: 23 [50000/50000]	Loss: 4.5334	LR: 1.000000
epoch 23 training time consumed: 21.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   34505 GB |   34504 GB |
|       from large pool |  400448 KB |    1770 MB |   34474 GB |   34473 GB |
|       from small pool |    3549 KB |       9 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   34505 GB |   34504 GB |
|       from large pool |  400448 KB |    1770 MB |   34474 GB |   34473 GB |
|       from small pool |    3549 KB |       9 MB |      31 GB |      31 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   21231 GB |   21231 GB |
|       from large pool |  244672 KB |  473024 KB |   21196 GB |   21195 GB |
|       from small pool |    2594 KB |    4843 KB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1473 K  |    1472 K  |
|       from large pool |      36    |      77    |     712 K  |     712 K  |
|       from small pool |     186    |     224    |     760 K  |     760 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1473 K  |    1472 K  |
|       from large pool |      36    |      77    |     712 K  |     712 K  |
|       from small pool |     186    |     224    |     760 K  |     760 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      32    |  748844    |  748823    |
|       from large pool |      10    |      11    |  294961    |  294951    |
|       from small pool |      11    |      22    |  453883    |  453872    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 23, Average loss: 0.0183, Accuracy: 0.0127, Time consumed:1.45s

Training Epoch: 24 [256/50000]	Loss: 4.4949	LR: 1.000000
Training Epoch: 24 [512/50000]	Loss: 4.5582	LR: 1.000000
Training Epoch: 24 [768/50000]	Loss: 4.5844	LR: 1.000000
Training Epoch: 24 [1024/50000]	Loss: 4.5243	LR: 1.000000
Training Epoch: 24 [1280/50000]	Loss: 4.5377	LR: 1.000000
Training Epoch: 24 [1536/50000]	Loss: 4.5160	LR: 1.000000
Training Epoch: 24 [1792/50000]	Loss: 4.5899	LR: 1.000000
Training Epoch: 24 [2048/50000]	Loss: 4.5535	LR: 1.000000
Training Epoch: 24 [2304/50000]	Loss: 4.5124	LR: 1.000000
Training Epoch: 24 [2560/50000]	Loss: 4.5591	LR: 1.000000
Training Epoch: 24 [2816/50000]	Loss: 4.5502	LR: 1.000000
Training Epoch: 24 [3072/50000]	Loss: 4.5124	LR: 1.000000
Training Epoch: 24 [3328/50000]	Loss: 4.5210	LR: 1.000000
Training Epoch: 24 [3584/50000]	Loss: 4.5725	LR: 1.000000
Training Epoch: 24 [3840/50000]	Loss: 4.5427	LR: 1.000000
Training Epoch: 24 [4096/50000]	Loss: 4.5363	LR: 1.000000
Training Epoch: 24 [4352/50000]	Loss: 4.5000	LR: 1.000000
Training Epoch: 24 [4608/50000]	Loss: 4.5457	LR: 1.000000
Training Epoch: 24 [4864/50000]	Loss: 4.4726	LR: 1.000000
Training Epoch: 24 [5120/50000]	Loss: 4.4377	LR: 1.000000
Training Epoch: 24 [5376/50000]	Loss: 4.6391	LR: 1.000000
Training Epoch: 24 [5632/50000]	Loss: 4.5323	LR: 1.000000
Training Epoch: 24 [5888/50000]	Loss: 4.5205	LR: 1.000000
Training Epoch: 24 [6144/50000]	Loss: 4.4648	LR: 1.000000
Training Epoch: 24 [6400/50000]	Loss: 4.5615	LR: 1.000000
Training Epoch: 24 [6656/50000]	Loss: 4.5447	LR: 1.000000
Training Epoch: 24 [6912/50000]	Loss: 4.5200	LR: 1.000000
Training Epoch: 24 [7168/50000]	Loss: 4.5661	LR: 1.000000
Training Epoch: 24 [7424/50000]	Loss: 4.5204	LR: 1.000000
Training Epoch: 24 [7680/50000]	Loss: 4.5152	LR: 1.000000
Training Epoch: 24 [7936/50000]	Loss: 4.5431	LR: 1.000000
Training Epoch: 24 [8192/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 24 [8448/50000]	Loss: 4.5374	LR: 1.000000
Training Epoch: 24 [8704/50000]	Loss: 4.5416	LR: 1.000000
Training Epoch: 24 [8960/50000]	Loss: 4.5136	LR: 1.000000
Training Epoch: 24 [9216/50000]	Loss: 4.5791	LR: 1.000000
Training Epoch: 24 [9472/50000]	Loss: 4.5022	LR: 1.000000
Training Epoch: 24 [9728/50000]	Loss: 4.5149	LR: 1.000000
Training Epoch: 24 [9984/50000]	Loss: 4.5408	LR: 1.000000
Training Epoch: 24 [10240/50000]	Loss: 4.4940	LR: 1.000000
Training Epoch: 24 [10496/50000]	Loss: 4.5268	LR: 1.000000
Training Epoch: 24 [10752/50000]	Loss: 4.5512	LR: 1.000000
Training Epoch: 24 [11008/50000]	Loss: 4.5110	LR: 1.000000
Training Epoch: 24 [11264/50000]	Loss: 4.4851	LR: 1.000000
Training Epoch: 24 [11520/50000]	Loss: 4.4939	LR: 1.000000
Training Epoch: 24 [11776/50000]	Loss: 4.4930	LR: 1.000000
Training Epoch: 24 [12032/50000]	Loss: 4.4837	LR: 1.000000
Training Epoch: 24 [12288/50000]	Loss: 4.5084	LR: 1.000000
Training Epoch: 24 [12544/50000]	Loss: 4.4490	LR: 1.000000
Training Epoch: 24 [12800/50000]	Loss: 4.5216	LR: 1.000000
Training Epoch: 24 [13056/50000]	Loss: 4.5435	LR: 1.000000
Training Epoch: 24 [13312/50000]	Loss: 4.5321	LR: 1.000000
Training Epoch: 24 [13568/50000]	Loss: 4.5004	LR: 1.000000
Training Epoch: 24 [13824/50000]	Loss: 4.5383	LR: 1.000000
Training Epoch: 24 [14080/50000]	Loss: 4.5576	LR: 1.000000
Training Epoch: 24 [14336/50000]	Loss: 4.5341	LR: 1.000000
Training Epoch: 24 [14592/50000]	Loss: 4.5379	LR: 1.000000
Training Epoch: 24 [14848/50000]	Loss: 4.5247	LR: 1.000000
Training Epoch: 24 [15104/50000]	Loss: 4.4881	LR: 1.000000
Training Epoch: 24 [15360/50000]	Loss: 4.4943	LR: 1.000000
Training Epoch: 24 [15616/50000]	Loss: 4.5600	LR: 1.000000
Training Epoch: 24 [15872/50000]	Loss: 4.5611	LR: 1.000000
Training Epoch: 24 [16128/50000]	Loss: 4.5393	LR: 1.000000
Training Epoch: 24 [16384/50000]	Loss: 4.5353	LR: 1.000000
Training Epoch: 24 [16640/50000]	Loss: 4.4839	LR: 1.000000
Training Epoch: 24 [16896/50000]	Loss: 4.5399	LR: 1.000000
Training Epoch: 24 [17152/50000]	Loss: 4.5248	LR: 1.000000
Training Epoch: 24 [17408/50000]	Loss: 4.5341	LR: 1.000000
Training Epoch: 24 [17664/50000]	Loss: 4.5599	LR: 1.000000
Training Epoch: 24 [17920/50000]	Loss: 4.5428	LR: 1.000000
Training Epoch: 24 [18176/50000]	Loss: 4.5751	LR: 1.000000
Training Epoch: 24 [18432/50000]	Loss: 4.5820	LR: 1.000000
Training Epoch: 24 [18688/50000]	Loss: 4.5624	LR: 1.000000
Training Epoch: 24 [18944/50000]	Loss: 4.5788	LR: 1.000000
Training Epoch: 24 [19200/50000]	Loss: 4.5627	LR: 1.000000
Training Epoch: 24 [19456/50000]	Loss: 4.5281	LR: 1.000000
Training Epoch: 24 [19712/50000]	Loss: 4.5455	LR: 1.000000
Training Epoch: 24 [19968/50000]	Loss: 4.5347	LR: 1.000000
Training Epoch: 24 [20224/50000]	Loss: 4.5159	LR: 1.000000
Training Epoch: 24 [20480/50000]	Loss: 4.5261	LR: 1.000000
Training Epoch: 24 [20736/50000]	Loss: 4.5186	LR: 1.000000
Training Epoch: 24 [20992/50000]	Loss: 4.5036	LR: 1.000000
Training Epoch: 24 [21248/50000]	Loss: 4.5167	LR: 1.000000
Training Epoch: 24 [21504/50000]	Loss: 4.5450	LR: 1.000000
Training Epoch: 24 [21760/50000]	Loss: 4.4820	LR: 1.000000
Training Epoch: 24 [22016/50000]	Loss: 4.5187	LR: 1.000000
Training Epoch: 24 [22272/50000]	Loss: 4.5015	LR: 1.000000
Training Epoch: 24 [22528/50000]	Loss: 4.5597	LR: 1.000000
Training Epoch: 24 [22784/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 24 [23040/50000]	Loss: 4.4896	LR: 1.000000
Training Epoch: 24 [23296/50000]	Loss: 4.4857	LR: 1.000000
Training Epoch: 24 [23552/50000]	Loss: 4.5126	LR: 1.000000
Training Epoch: 24 [23808/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 24 [24064/50000]	Loss: 4.5414	LR: 1.000000
Training Epoch: 24 [24320/50000]	Loss: 4.4610	LR: 1.000000
Training Epoch: 24 [24576/50000]	Loss: 4.5159	LR: 1.000000
Training Epoch: 24 [24832/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 24 [25088/50000]	Loss: 4.5255	LR: 1.000000
Training Epoch: 24 [25344/50000]	Loss: 4.5230	LR: 1.000000
Training Epoch: 24 [25600/50000]	Loss: 4.5221	LR: 1.000000
Training Epoch: 24 [25856/50000]	Loss: 4.4848	LR: 1.000000
Training Epoch: 24 [26112/50000]	Loss: 4.4922	LR: 1.000000
Training Epoch: 24 [26368/50000]	Loss: 4.5193	LR: 1.000000
Training Epoch: 24 [26624/50000]	Loss: 4.5456	LR: 1.000000
Training Epoch: 24 [26880/50000]	Loss: 4.5146	LR: 1.000000
Training Epoch: 24 [27136/50000]	Loss: 4.4924	LR: 1.000000
Training Epoch: 24 [27392/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 24 [27648/50000]	Loss: 4.4709	LR: 1.000000
Training Epoch: 24 [27904/50000]	Loss: 4.5323	LR: 1.000000
Training Epoch: 24 [28160/50000]	Loss: 4.4852	LR: 1.000000
Training Epoch: 24 [28416/50000]	Loss: 4.4867	LR: 1.000000
Training Epoch: 24 [28672/50000]	Loss: 4.4824	LR: 1.000000
Training Epoch: 24 [28928/50000]	Loss: 4.5182	LR: 1.000000
Training Epoch: 24 [29184/50000]	Loss: 4.4671	LR: 1.000000
Training Epoch: 24 [29440/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 24 [29696/50000]	Loss: 4.5731	LR: 1.000000
Training Epoch: 24 [29952/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 24 [30208/50000]	Loss: 4.5207	LR: 1.000000
Training Epoch: 24 [30464/50000]	Loss: 4.4751	LR: 1.000000
Training Epoch: 24 [30720/50000]	Loss: 4.4766	LR: 1.000000
Training Epoch: 24 [30976/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 24 [31232/50000]	Loss: 4.5202	LR: 1.000000
Training Epoch: 24 [31488/50000]	Loss: 4.5011	LR: 1.000000
Training Epoch: 24 [31744/50000]	Loss: 4.5298	LR: 1.000000
Training Epoch: 24 [32000/50000]	Loss: 4.5108	LR: 1.000000
Training Epoch: 24 [32256/50000]	Loss: 4.5240	LR: 1.000000
Training Epoch: 24 [32512/50000]	Loss: 4.4447	LR: 1.000000
Training Epoch: 24 [32768/50000]	Loss: 4.4698	LR: 1.000000
Training Epoch: 24 [33024/50000]	Loss: 4.5224	LR: 1.000000
Training Epoch: 24 [33280/50000]	Loss: 4.5085	LR: 1.000000
Training Epoch: 24 [33536/50000]	Loss: 4.4992	LR: 1.000000
Training Epoch: 24 [33792/50000]	Loss: 4.5318	LR: 1.000000
Training Epoch: 24 [34048/50000]	Loss: 4.4912	LR: 1.000000
Training Epoch: 24 [34304/50000]	Loss: 4.5182	LR: 1.000000
Training Epoch: 24 [34560/50000]	Loss: 4.5559	LR: 1.000000
Training Epoch: 24 [34816/50000]	Loss: 4.5485	LR: 1.000000
Training Epoch: 24 [35072/50000]	Loss: 4.4933	LR: 1.000000
Training Epoch: 24 [35328/50000]	Loss: 4.4635	LR: 1.000000
Training Epoch: 24 [35584/50000]	Loss: 4.4739	LR: 1.000000
Training Epoch: 24 [35840/50000]	Loss: 4.4970	LR: 1.000000
Training Epoch: 24 [36096/50000]	Loss: 4.4530	LR: 1.000000
Training Epoch: 24 [36352/50000]	Loss: 4.4599	LR: 1.000000
Training Epoch: 24 [36608/50000]	Loss: 4.5339	LR: 1.000000
Training Epoch: 24 [36864/50000]	Loss: 4.9746	LR: 1.000000
Training Epoch: 24 [37120/50000]	Loss: 4.5882	LR: 1.000000
Training Epoch: 24 [37376/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 24 [37632/50000]	Loss: 4.5129	LR: 1.000000
Training Epoch: 24 [37888/50000]	Loss: 4.5834	LR: 1.000000
Training Epoch: 24 [38144/50000]	Loss: 4.5899	LR: 1.000000
Training Epoch: 24 [38400/50000]	Loss: 4.5792	LR: 1.000000
Training Epoch: 24 [38656/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 24 [38912/50000]	Loss: 4.5739	LR: 1.000000
Training Epoch: 24 [39168/50000]	Loss: 4.5921	LR: 1.000000
Training Epoch: 24 [39424/50000]	Loss: 4.5844	LR: 1.000000
Training Epoch: 24 [39680/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 24 [39936/50000]	Loss: 4.5488	LR: 1.000000
Training Epoch: 24 [40192/50000]	Loss: 4.5563	LR: 1.000000
Training Epoch: 24 [40448/50000]	Loss: 4.5716	LR: 1.000000
Training Epoch: 24 [40704/50000]	Loss: 4.5406	LR: 1.000000
Training Epoch: 24 [40960/50000]	Loss: 4.5285	LR: 1.000000
Training Epoch: 24 [41216/50000]	Loss: 4.5137	LR: 1.000000
Training Epoch: 24 [41472/50000]	Loss: 4.5034	LR: 1.000000
Training Epoch: 24 [41728/50000]	Loss: 4.5083	LR: 1.000000
Training Epoch: 24 [41984/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 24 [42240/50000]	Loss: 4.5784	LR: 1.000000
Training Epoch: 24 [42496/50000]	Loss: 4.5548	LR: 1.000000
Training Epoch: 24 [42752/50000]	Loss: 4.5412	LR: 1.000000
Training Epoch: 24 [43008/50000]	Loss: 4.5653	LR: 1.000000
Training Epoch: 24 [43264/50000]	Loss: 4.5633	LR: 1.000000
Training Epoch: 24 [43520/50000]	Loss: 4.5898	LR: 1.000000
Training Epoch: 24 [43776/50000]	Loss: 4.5910	LR: 1.000000
Training Epoch: 24 [44032/50000]	Loss: 4.5822	LR: 1.000000
Training Epoch: 24 [44288/50000]	Loss: 4.5824	LR: 1.000000
Training Epoch: 24 [44544/50000]	Loss: 4.5808	LR: 1.000000
Training Epoch: 24 [44800/50000]	Loss: 4.5726	LR: 1.000000
Training Epoch: 24 [45056/50000]	Loss: 4.5762	LR: 1.000000
Training Epoch: 24 [45312/50000]	Loss: 4.5716	LR: 1.000000
Training Epoch: 24 [45568/50000]	Loss: 4.5647	LR: 1.000000
Training Epoch: 24 [45824/50000]	Loss: 4.5355	LR: 1.000000
Training Epoch: 24 [46080/50000]	Loss: 4.5075	LR: 1.000000
Training Epoch: 24 [46336/50000]	Loss: 4.5392	LR: 1.000000
Training Epoch: 24 [46592/50000]	Loss: 4.5226	LR: 1.000000
Training Epoch: 24 [46848/50000]	Loss: 4.5409	LR: 1.000000
Training Epoch: 24 [47104/50000]	Loss: 4.5290	LR: 1.000000
Training Epoch: 24 [47360/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 24 [47616/50000]	Loss: 4.5170	LR: 1.000000
Training Epoch: 24 [47872/50000]	Loss: 4.5256	LR: 1.000000
Training Epoch: 24 [48128/50000]	Loss: 4.5084	LR: 1.000000
Training Epoch: 24 [48384/50000]	Loss: 4.5502	LR: 1.000000
Training Epoch: 24 [48640/50000]	Loss: 4.4815	LR: 1.000000
Training Epoch: 24 [48896/50000]	Loss: 4.5767	LR: 1.000000
Training Epoch: 24 [49152/50000]	Loss: 4.5601	LR: 1.000000
Training Epoch: 24 [49408/50000]	Loss: 4.5569	LR: 1.000000
Training Epoch: 24 [49664/50000]	Loss: 4.5368	LR: 1.000000
Training Epoch: 24 [49920/50000]	Loss: 4.5305	LR: 1.000000
Training Epoch: 24 [50000/50000]	Loss: 4.5006	LR: 1.000000
epoch 24 training time consumed: 21.92s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   36005 GB |   36005 GB |
|       from large pool |  400448 KB |    1770 MB |   35972 GB |   35972 GB |
|       from small pool |    3549 KB |       9 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   36005 GB |   36005 GB |
|       from large pool |  400448 KB |    1770 MB |   35972 GB |   35972 GB |
|       from small pool |    3549 KB |       9 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   22154 GB |   22154 GB |
|       from large pool |  244672 KB |  473024 KB |   22117 GB |   22117 GB |
|       from small pool |    2594 KB |    4843 KB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1537 K  |    1536 K  |
|       from large pool |      36    |      77    |     743 K  |     743 K  |
|       from small pool |     186    |     224    |     793 K  |     793 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1537 K  |    1536 K  |
|       from large pool |      36    |      77    |     743 K  |     743 K  |
|       from small pool |     186    |     224    |     793 K  |     793 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      32    |     781 K  |     781 K  |
|       from large pool |      10    |      11    |     307 K  |     307 K  |
|       from small pool |      10    |      22    |     473 K  |     473 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 24, Average loss: 0.0182, Accuracy: 0.0156, Time consumed:1.45s

Training Epoch: 25 [256/50000]	Loss: 4.5023	LR: 1.000000
Training Epoch: 25 [512/50000]	Loss: 4.4859	LR: 1.000000
Training Epoch: 25 [768/50000]	Loss: 4.5111	LR: 1.000000
Training Epoch: 25 [1024/50000]	Loss: 4.4902	LR: 1.000000
Training Epoch: 25 [1280/50000]	Loss: 4.5434	LR: 1.000000
Training Epoch: 25 [1536/50000]	Loss: 4.5414	LR: 1.000000
Training Epoch: 25 [1792/50000]	Loss: 4.5101	LR: 1.000000
Training Epoch: 25 [2048/50000]	Loss: 4.5204	LR: 1.000000
Training Epoch: 25 [2304/50000]	Loss: 4.4839	LR: 1.000000
Training Epoch: 25 [2560/50000]	Loss: 4.5019	LR: 1.000000
Training Epoch: 25 [2816/50000]	Loss: 4.5535	LR: 1.000000
Training Epoch: 25 [3072/50000]	Loss: 4.4932	LR: 1.000000
Training Epoch: 25 [3328/50000]	Loss: 4.4833	LR: 1.000000
Training Epoch: 25 [3584/50000]	Loss: 4.5397	LR: 1.000000
Training Epoch: 25 [3840/50000]	Loss: 4.4962	LR: 1.000000
Training Epoch: 25 [4096/50000]	Loss: 4.4828	LR: 1.000000
Training Epoch: 25 [4352/50000]	Loss: 4.5011	LR: 1.000000
Training Epoch: 25 [4608/50000]	Loss: 4.5221	LR: 1.000000
Training Epoch: 25 [4864/50000]	Loss: 4.4744	LR: 1.000000
Training Epoch: 25 [5120/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 25 [5376/50000]	Loss: 4.5190	LR: 1.000000
Training Epoch: 25 [5632/50000]	Loss: 4.5148	LR: 1.000000
Training Epoch: 25 [5888/50000]	Loss: 4.5453	LR: 1.000000
Training Epoch: 25 [6144/50000]	Loss: 4.5177	LR: 1.000000
Training Epoch: 25 [6400/50000]	Loss: 4.5454	LR: 1.000000
Training Epoch: 25 [6656/50000]	Loss: 4.5369	LR: 1.000000
Training Epoch: 25 [6912/50000]	Loss: 4.4969	LR: 1.000000
Training Epoch: 25 [7168/50000]	Loss: 4.4984	LR: 1.000000
Training Epoch: 25 [7424/50000]	Loss: 4.5077	LR: 1.000000
Training Epoch: 25 [7680/50000]	Loss: 4.5133	LR: 1.000000
Training Epoch: 25 [7936/50000]	Loss: 4.5338	LR: 1.000000
Training Epoch: 25 [8192/50000]	Loss: 4.5097	LR: 1.000000
Training Epoch: 25 [8448/50000]	Loss: 4.5063	LR: 1.000000
Training Epoch: 25 [8704/50000]	Loss: 4.5032	LR: 1.000000
Training Epoch: 25 [8960/50000]	Loss: 4.5102	LR: 1.000000
Training Epoch: 25 [9216/50000]	Loss: 4.5095	LR: 1.000000
Training Epoch: 25 [9472/50000]	Loss: 4.4758	LR: 1.000000
Training Epoch: 25 [9728/50000]	Loss: 4.5879	LR: 1.000000
Training Epoch: 25 [9984/50000]	Loss: 4.5679	LR: 1.000000
Training Epoch: 25 [10240/50000]	Loss: 4.5303	LR: 1.000000
Training Epoch: 25 [10496/50000]	Loss: 4.6541	LR: 1.000000
Training Epoch: 25 [10752/50000]	Loss: 4.5213	LR: 1.000000
Training Epoch: 25 [11008/50000]	Loss: 4.5181	LR: 1.000000
Training Epoch: 25 [11264/50000]	Loss: 4.5037	LR: 1.000000
Training Epoch: 25 [11520/50000]	Loss: 4.5287	LR: 1.000000
Training Epoch: 25 [11776/50000]	Loss: 4.5508	LR: 1.000000
Training Epoch: 25 [12032/50000]	Loss: 4.5307	LR: 1.000000
Training Epoch: 25 [12288/50000]	Loss: 4.5502	LR: 1.000000
Training Epoch: 25 [12544/50000]	Loss: 4.5530	LR: 1.000000
Training Epoch: 25 [12800/50000]	Loss: 4.5495	LR: 1.000000
Training Epoch: 25 [13056/50000]	Loss: 4.5183	LR: 1.000000
Training Epoch: 25 [13312/50000]	Loss: 4.5145	LR: 1.000000
Training Epoch: 25 [13568/50000]	Loss: 4.5417	LR: 1.000000
Training Epoch: 25 [13824/50000]	Loss: 4.5213	LR: 1.000000
Training Epoch: 25 [14080/50000]	Loss: 4.4967	LR: 1.000000
Training Epoch: 25 [14336/50000]	Loss: 4.4646	LR: 1.000000
Training Epoch: 25 [14592/50000]	Loss: 4.5115	LR: 1.000000
Training Epoch: 25 [14848/50000]	Loss: 4.4911	LR: 1.000000
Training Epoch: 25 [15104/50000]	Loss: 4.4739	LR: 1.000000
Training Epoch: 25 [15360/50000]	Loss: 4.4948	LR: 1.000000
Training Epoch: 25 [15616/50000]	Loss: 4.4905	LR: 1.000000
Training Epoch: 25 [15872/50000]	Loss: 4.5563	LR: 1.000000
Training Epoch: 25 [16128/50000]	Loss: 4.5019	LR: 1.000000
Training Epoch: 25 [16384/50000]	Loss: 4.5392	LR: 1.000000
Training Epoch: 25 [16640/50000]	Loss: 4.4915	LR: 1.000000
Training Epoch: 25 [16896/50000]	Loss: 4.4876	LR: 1.000000
Training Epoch: 25 [17152/50000]	Loss: 4.5257	LR: 1.000000
Training Epoch: 25 [17408/50000]	Loss: 4.5228	LR: 1.000000
Training Epoch: 25 [17664/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 25 [17920/50000]	Loss: 4.5345	LR: 1.000000
Training Epoch: 25 [18176/50000]	Loss: 4.5052	LR: 1.000000
Training Epoch: 25 [18432/50000]	Loss: 4.4776	LR: 1.000000
Training Epoch: 25 [18688/50000]	Loss: 4.4994	LR: 1.000000
Training Epoch: 25 [18944/50000]	Loss: 4.4837	LR: 1.000000
Training Epoch: 25 [19200/50000]	Loss: 4.5123	LR: 1.000000
Training Epoch: 25 [19456/50000]	Loss: 4.5244	LR: 1.000000
Training Epoch: 25 [19712/50000]	Loss: 4.4616	LR: 1.000000
Training Epoch: 25 [19968/50000]	Loss: 4.4825	LR: 1.000000
Training Epoch: 25 [20224/50000]	Loss: 4.5381	LR: 1.000000
Training Epoch: 25 [20480/50000]	Loss: 4.5056	LR: 1.000000
Training Epoch: 25 [20736/50000]	Loss: 4.5111	LR: 1.000000
Training Epoch: 25 [20992/50000]	Loss: 4.4811	LR: 1.000000
Training Epoch: 25 [21248/50000]	Loss: 4.4809	LR: 1.000000
Training Epoch: 25 [21504/50000]	Loss: 4.5173	LR: 1.000000
Training Epoch: 25 [21760/50000]	Loss: 4.5545	LR: 1.000000
Training Epoch: 25 [22016/50000]	Loss: 4.4806	LR: 1.000000
Training Epoch: 25 [22272/50000]	Loss: 4.5103	LR: 1.000000
Training Epoch: 25 [22528/50000]	Loss: 4.4990	LR: 1.000000
Training Epoch: 25 [22784/50000]	Loss: 4.4981	LR: 1.000000
Training Epoch: 25 [23040/50000]	Loss: 4.5951	LR: 1.000000
Training Epoch: 25 [23296/50000]	Loss: 4.5298	LR: 1.000000
Training Epoch: 25 [23552/50000]	Loss: 4.4999	LR: 1.000000
Training Epoch: 25 [23808/50000]	Loss: 4.4842	LR: 1.000000
Training Epoch: 25 [24064/50000]	Loss: 4.5249	LR: 1.000000
Training Epoch: 25 [24320/50000]	Loss: 4.5053	LR: 1.000000
Training Epoch: 25 [24576/50000]	Loss: 4.5048	LR: 1.000000
Training Epoch: 25 [24832/50000]	Loss: 4.5277	LR: 1.000000
Training Epoch: 25 [25088/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 25 [25344/50000]	Loss: 4.5179	LR: 1.000000
Training Epoch: 25 [25600/50000]	Loss: 4.5338	LR: 1.000000
Training Epoch: 25 [25856/50000]	Loss: 4.4878	LR: 1.000000
Training Epoch: 25 [26112/50000]	Loss: 4.4999	LR: 1.000000
Training Epoch: 25 [26368/50000]	Loss: 4.4331	LR: 1.000000
Training Epoch: 25 [26624/50000]	Loss: 4.4994	LR: 1.000000
Training Epoch: 25 [26880/50000]	Loss: 4.5580	LR: 1.000000
Training Epoch: 25 [27136/50000]	Loss: 4.5119	LR: 1.000000
Training Epoch: 25 [27392/50000]	Loss: 4.5030	LR: 1.000000
Training Epoch: 25 [27648/50000]	Loss: 4.5827	LR: 1.000000
Training Epoch: 25 [27904/50000]	Loss: 4.5177	LR: 1.000000
Training Epoch: 25 [28160/50000]	Loss: 4.4571	LR: 1.000000
Training Epoch: 25 [28416/50000]	Loss: 4.5290	LR: 1.000000
Training Epoch: 25 [28672/50000]	Loss: 4.4547	LR: 1.000000
Training Epoch: 25 [28928/50000]	Loss: 4.4760	LR: 1.000000
Training Epoch: 25 [29184/50000]	Loss: 4.4719	LR: 1.000000
Training Epoch: 25 [29440/50000]	Loss: 4.4706	LR: 1.000000
Training Epoch: 25 [29696/50000]	Loss: 4.5143	LR: 1.000000
Training Epoch: 25 [29952/50000]	Loss: 4.5369	LR: 1.000000
Training Epoch: 25 [30208/50000]	Loss: 4.4648	LR: 1.000000
Training Epoch: 25 [30464/50000]	Loss: 4.4428	LR: 1.000000
Training Epoch: 25 [30720/50000]	Loss: 4.4879	LR: 1.000000
Training Epoch: 25 [30976/50000]	Loss: 4.4638	LR: 1.000000
Training Epoch: 25 [31232/50000]	Loss: 4.5106	LR: 1.000000
Training Epoch: 25 [31488/50000]	Loss: 4.4882	LR: 1.000000
Training Epoch: 25 [31744/50000]	Loss: 4.5229	LR: 1.000000
Training Epoch: 25 [32000/50000]	Loss: 4.5134	LR: 1.000000
Training Epoch: 25 [32256/50000]	Loss: 4.5049	LR: 1.000000
Training Epoch: 25 [32512/50000]	Loss: 4.4989	LR: 1.000000
Training Epoch: 25 [32768/50000]	Loss: 4.5067	LR: 1.000000
Training Epoch: 25 [33024/50000]	Loss: 4.4561	LR: 1.000000
Training Epoch: 25 [33280/50000]	Loss: 4.4940	LR: 1.000000
Training Epoch: 25 [33536/50000]	Loss: 4.5238	LR: 1.000000
Training Epoch: 25 [33792/50000]	Loss: 4.4777	LR: 1.000000
Training Epoch: 25 [34048/50000]	Loss: 4.4979	LR: 1.000000
Training Epoch: 25 [34304/50000]	Loss: 4.5293	LR: 1.000000
Training Epoch: 25 [34560/50000]	Loss: 4.5361	LR: 1.000000
Training Epoch: 25 [34816/50000]	Loss: 4.5408	LR: 1.000000
Training Epoch: 25 [35072/50000]	Loss: 4.5079	LR: 1.000000
Training Epoch: 25 [35328/50000]	Loss: 4.5650	LR: 1.000000
Training Epoch: 25 [35584/50000]	Loss: 4.5337	LR: 1.000000
Training Epoch: 25 [35840/50000]	Loss: 4.5428	LR: 1.000000
Training Epoch: 25 [36096/50000]	Loss: 4.5017	LR: 1.000000
Training Epoch: 25 [36352/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 25 [36608/50000]	Loss: 4.5161	LR: 1.000000
Training Epoch: 25 [36864/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 25 [37120/50000]	Loss: 4.4960	LR: 1.000000
Training Epoch: 25 [37376/50000]	Loss: 4.4892	LR: 1.000000
Training Epoch: 25 [37632/50000]	Loss: 4.4951	LR: 1.000000
Training Epoch: 25 [37888/50000]	Loss: 4.4892	LR: 1.000000
Training Epoch: 25 [38144/50000]	Loss: 4.4666	LR: 1.000000
Training Epoch: 25 [38400/50000]	Loss: 4.5490	LR: 1.000000
Training Epoch: 25 [38656/50000]	Loss: 4.4994	LR: 1.000000
Training Epoch: 25 [38912/50000]	Loss: 4.4851	LR: 1.000000
Training Epoch: 25 [39168/50000]	Loss: 4.4599	LR: 1.000000
Training Epoch: 25 [39424/50000]	Loss: 4.4998	LR: 1.000000
Training Epoch: 25 [39680/50000]	Loss: 4.5454	LR: 1.000000
Training Epoch: 25 [39936/50000]	Loss: 4.4536	LR: 1.000000
Training Epoch: 25 [40192/50000]	Loss: 4.5254	LR: 1.000000
Training Epoch: 25 [40448/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 25 [40704/50000]	Loss: 4.4923	LR: 1.000000
Training Epoch: 25 [40960/50000]	Loss: 4.5745	LR: 1.000000
Training Epoch: 25 [41216/50000]	Loss: 4.5565	LR: 1.000000
Training Epoch: 25 [41472/50000]	Loss: 4.5456	LR: 1.000000
Training Epoch: 25 [41728/50000]	Loss: 4.5162	LR: 1.000000
Training Epoch: 25 [41984/50000]	Loss: 4.5074	LR: 1.000000
Training Epoch: 25 [42240/50000]	Loss: 4.5728	LR: 1.000000
Training Epoch: 25 [42496/50000]	Loss: 4.5325	LR: 1.000000
Training Epoch: 25 [42752/50000]	Loss: 4.5252	LR: 1.000000
Training Epoch: 25 [43008/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 25 [43264/50000]	Loss: 4.4755	LR: 1.000000
Training Epoch: 25 [43520/50000]	Loss: 4.4952	LR: 1.000000
Training Epoch: 25 [43776/50000]	Loss: 4.5493	LR: 1.000000
Training Epoch: 25 [44032/50000]	Loss: 4.4817	LR: 1.000000
Training Epoch: 25 [44288/50000]	Loss: 4.4565	LR: 1.000000
Training Epoch: 25 [44544/50000]	Loss: 4.4761	LR: 1.000000
Training Epoch: 25 [44800/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 25 [45056/50000]	Loss: 4.5025	LR: 1.000000
Training Epoch: 25 [45312/50000]	Loss: 4.5154	LR: 1.000000
Training Epoch: 25 [45568/50000]	Loss: 4.5093	LR: 1.000000
Training Epoch: 25 [45824/50000]	Loss: 4.5157	LR: 1.000000
Training Epoch: 25 [46080/50000]	Loss: 4.5334	LR: 1.000000
Training Epoch: 25 [46336/50000]	Loss: 4.4768	LR: 1.000000
Training Epoch: 25 [46592/50000]	Loss: 4.4756	LR: 1.000000
Training Epoch: 25 [46848/50000]	Loss: 4.5039	LR: 1.000000
Training Epoch: 25 [47104/50000]	Loss: 4.5302	LR: 1.000000
Training Epoch: 25 [47360/50000]	Loss: 4.5272	LR: 1.000000
Training Epoch: 25 [47616/50000]	Loss: 4.4844	LR: 1.000000
Training Epoch: 25 [47872/50000]	Loss: 4.5233	LR: 1.000000
Training Epoch: 25 [48128/50000]	Loss: 4.5295	LR: 1.000000
Training Epoch: 25 [48384/50000]	Loss: 4.5358	LR: 1.000000
Training Epoch: 25 [48640/50000]	Loss: 4.5264	LR: 1.000000
Training Epoch: 25 [48896/50000]	Loss: 4.5576	LR: 1.000000
Training Epoch: 25 [49152/50000]	Loss: 4.5398	LR: 1.000000
Training Epoch: 25 [49408/50000]	Loss: 4.5344	LR: 1.000000
Training Epoch: 25 [49664/50000]	Loss: 4.4805	LR: 1.000000
Training Epoch: 25 [49920/50000]	Loss: 4.4834	LR: 1.000000
Training Epoch: 25 [50000/50000]	Loss: 4.4662	LR: 1.000000
epoch 25 training time consumed: 21.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   37505 GB |   37505 GB |
|       from large pool |  400448 KB |    1770 MB |   37471 GB |   37471 GB |
|       from small pool |    3549 KB |       9 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   37505 GB |   37505 GB |
|       from large pool |  400448 KB |    1770 MB |   37471 GB |   37471 GB |
|       from small pool |    3549 KB |       9 MB |      33 GB |      33 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   23077 GB |   23077 GB |
|       from large pool |  244672 KB |  473024 KB |   23039 GB |   23038 GB |
|       from small pool |    2594 KB |    4843 KB |      38 GB |      38 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1601 K  |    1600 K  |
|       from large pool |      36    |      77    |     774 K  |     774 K  |
|       from small pool |     186    |     224    |     827 K  |     826 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1601 K  |    1600 K  |
|       from large pool |      36    |      77    |     774 K  |     774 K  |
|       from small pool |     186    |     224    |     827 K  |     826 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      32    |     813 K  |     813 K  |
|       from large pool |      10    |      11    |     320 K  |     320 K  |
|       from small pool |      10    |      22    |     492 K  |     492 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 25, Average loss: 0.0179, Accuracy: 0.0170, Time consumed:1.43s

Training Epoch: 26 [256/50000]	Loss: 4.5032	LR: 1.000000
Training Epoch: 26 [512/50000]	Loss: 4.4831	LR: 1.000000
Training Epoch: 26 [768/50000]	Loss: 4.4691	LR: 1.000000
Training Epoch: 26 [1024/50000]	Loss: 4.4347	LR: 1.000000
Training Epoch: 26 [1280/50000]	Loss: 4.4754	LR: 1.000000
Training Epoch: 26 [1536/50000]	Loss: 4.5404	LR: 1.000000
Training Epoch: 26 [1792/50000]	Loss: 4.4874	LR: 1.000000
Training Epoch: 26 [2048/50000]	Loss: 4.4639	LR: 1.000000
Training Epoch: 26 [2304/50000]	Loss: 4.4850	LR: 1.000000
Training Epoch: 26 [2560/50000]	Loss: 4.4749	LR: 1.000000
Training Epoch: 26 [2816/50000]	Loss: 4.4877	LR: 1.000000
Training Epoch: 26 [3072/50000]	Loss: 4.5530	LR: 1.000000
Training Epoch: 26 [3328/50000]	Loss: 4.5361	LR: 1.000000
Training Epoch: 26 [3584/50000]	Loss: 4.4459	LR: 1.000000
Training Epoch: 26 [3840/50000]	Loss: 4.4785	LR: 1.000000
Training Epoch: 26 [4096/50000]	Loss: 4.4761	LR: 1.000000
Training Epoch: 26 [4352/50000]	Loss: 4.5291	LR: 1.000000
Training Epoch: 26 [4608/50000]	Loss: 4.4902	LR: 1.000000
Training Epoch: 26 [4864/50000]	Loss: 4.5276	LR: 1.000000
Training Epoch: 26 [5120/50000]	Loss: 4.4950	LR: 1.000000
Training Epoch: 26 [5376/50000]	Loss: 4.5227	LR: 1.000000
Training Epoch: 26 [5632/50000]	Loss: 4.5385	LR: 1.000000
Training Epoch: 26 [5888/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 26 [6144/50000]	Loss: 4.4549	LR: 1.000000
Training Epoch: 26 [6400/50000]	Loss: 4.4875	LR: 1.000000
Training Epoch: 26 [6656/50000]	Loss: 4.5028	LR: 1.000000
Training Epoch: 26 [6912/50000]	Loss: 4.4810	LR: 1.000000
Training Epoch: 26 [7168/50000]	Loss: 4.5224	LR: 1.000000
Training Epoch: 26 [7424/50000]	Loss: 4.5173	LR: 1.000000
Training Epoch: 26 [7680/50000]	Loss: 4.5112	LR: 1.000000
Training Epoch: 26 [7936/50000]	Loss: 4.5073	LR: 1.000000
Training Epoch: 26 [8192/50000]	Loss: 4.5060	LR: 1.000000
Training Epoch: 26 [8448/50000]	Loss: 4.5351	LR: 1.000000
Training Epoch: 26 [8704/50000]	Loss: 4.4748	LR: 1.000000
Training Epoch: 26 [8960/50000]	Loss: 4.5341	LR: 1.000000
Training Epoch: 26 [9216/50000]	Loss: 4.4957	LR: 1.000000
Training Epoch: 26 [9472/50000]	Loss: 4.5240	LR: 1.000000
Training Epoch: 26 [9728/50000]	Loss: 4.4721	LR: 1.000000
Training Epoch: 26 [9984/50000]	Loss: 4.4708	LR: 1.000000
Training Epoch: 26 [10240/50000]	Loss: 4.4386	LR: 1.000000
Training Epoch: 26 [10496/50000]	Loss: 4.5619	LR: 1.000000
Training Epoch: 26 [10752/50000]	Loss: 4.4741	LR: 1.000000
Training Epoch: 26 [11008/50000]	Loss: 4.4995	LR: 1.000000
Training Epoch: 26 [11264/50000]	Loss: 4.5080	LR: 1.000000
Training Epoch: 26 [11520/50000]	Loss: 4.5754	LR: 1.000000
Training Epoch: 26 [11776/50000]	Loss: 4.5636	LR: 1.000000
Training Epoch: 26 [12032/50000]	Loss: 4.5686	LR: 1.000000
Training Epoch: 26 [12288/50000]	Loss: 4.4986	LR: 1.000000
Training Epoch: 26 [12544/50000]	Loss: 4.4848	LR: 1.000000
Training Epoch: 26 [12800/50000]	Loss: 4.4960	LR: 1.000000
Training Epoch: 26 [13056/50000]	Loss: 4.5404	LR: 1.000000
Training Epoch: 26 [13312/50000]	Loss: 4.5495	LR: 1.000000
Training Epoch: 26 [13568/50000]	Loss: 4.5510	LR: 1.000000
Training Epoch: 26 [13824/50000]	Loss: 4.4822	LR: 1.000000
Training Epoch: 26 [14080/50000]	Loss: 4.5258	LR: 1.000000
Training Epoch: 26 [14336/50000]	Loss: 4.5799	LR: 1.000000
Training Epoch: 26 [14592/50000]	Loss: 4.4706	LR: 1.000000
Training Epoch: 26 [14848/50000]	Loss: 4.4734	LR: 1.000000
Training Epoch: 26 [15104/50000]	Loss: 4.4956	LR: 1.000000
Training Epoch: 26 [15360/50000]	Loss: 4.5112	LR: 1.000000
Training Epoch: 26 [15616/50000]	Loss: 4.5536	LR: 1.000000
Training Epoch: 26 [15872/50000]	Loss: 4.5370	LR: 1.000000
Training Epoch: 26 [16128/50000]	Loss: 4.5055	LR: 1.000000
Training Epoch: 26 [16384/50000]	Loss: 4.5410	LR: 1.000000
Training Epoch: 26 [16640/50000]	Loss: 4.5106	LR: 1.000000
Training Epoch: 26 [16896/50000]	Loss: 4.4932	LR: 1.000000
Training Epoch: 26 [17152/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 26 [17408/50000]	Loss: 4.5299	LR: 1.000000
Training Epoch: 26 [17664/50000]	Loss: 4.5007	LR: 1.000000
Training Epoch: 26 [17920/50000]	Loss: 4.4277	LR: 1.000000
Training Epoch: 26 [18176/50000]	Loss: 4.4285	LR: 1.000000
Training Epoch: 26 [18432/50000]	Loss: 4.5094	LR: 1.000000
Training Epoch: 26 [18688/50000]	Loss: 4.5500	LR: 1.000000
Training Epoch: 26 [18944/50000]	Loss: 4.5037	LR: 1.000000
Training Epoch: 26 [19200/50000]	Loss: 4.5092	LR: 1.000000
Training Epoch: 26 [19456/50000]	Loss: 4.4843	LR: 1.000000
Training Epoch: 26 [19712/50000]	Loss: 4.4746	LR: 1.000000
Training Epoch: 26 [19968/50000]	Loss: 4.4741	LR: 1.000000
Training Epoch: 26 [20224/50000]	Loss: 4.4950	LR: 1.000000
Training Epoch: 26 [20480/50000]	Loss: 4.5728	LR: 1.000000
Training Epoch: 26 [20736/50000]	Loss: 4.5022	LR: 1.000000
Training Epoch: 26 [20992/50000]	Loss: 4.5249	LR: 1.000000
Training Epoch: 26 [21248/50000]	Loss: 4.5005	LR: 1.000000
Training Epoch: 26 [21504/50000]	Loss: 4.4833	LR: 1.000000
Training Epoch: 26 [21760/50000]	Loss: 4.5014	LR: 1.000000
Training Epoch: 26 [22016/50000]	Loss: 4.5023	LR: 1.000000
Training Epoch: 26 [22272/50000]	Loss: 4.5633	LR: 1.000000
Training Epoch: 26 [22528/50000]	Loss: 4.4954	LR: 1.000000
Training Epoch: 26 [22784/50000]	Loss: 4.4849	LR: 1.000000
Training Epoch: 26 [23040/50000]	Loss: 4.4688	LR: 1.000000
Training Epoch: 26 [23296/50000]	Loss: 4.4945	LR: 1.000000
Training Epoch: 26 [23552/50000]	Loss: 4.5272	LR: 1.000000
Training Epoch: 26 [23808/50000]	Loss: 4.4876	LR: 1.000000
Training Epoch: 26 [24064/50000]	Loss: 4.4908	LR: 1.000000
Training Epoch: 26 [24320/50000]	Loss: 4.4651	LR: 1.000000
Training Epoch: 26 [24576/50000]	Loss: 4.4688	LR: 1.000000
Training Epoch: 26 [24832/50000]	Loss: 4.5395	LR: 1.000000
Training Epoch: 26 [25088/50000]	Loss: 4.4906	LR: 1.000000
Training Epoch: 26 [25344/50000]	Loss: 4.4837	LR: 1.000000
Training Epoch: 26 [25600/50000]	Loss: 4.5031	LR: 1.000000
Training Epoch: 26 [25856/50000]	Loss: 4.4942	LR: 1.000000
Training Epoch: 26 [26112/50000]	Loss: 4.4761	LR: 1.000000
Training Epoch: 26 [26368/50000]	Loss: 4.5035	LR: 1.000000
Training Epoch: 26 [26624/50000]	Loss: 4.5262	LR: 1.000000
Training Epoch: 26 [26880/50000]	Loss: 4.5067	LR: 1.000000
Training Epoch: 26 [27136/50000]	Loss: 4.4643	LR: 1.000000
Training Epoch: 26 [27392/50000]	Loss: 4.4766	LR: 1.000000
Training Epoch: 26 [27648/50000]	Loss: 4.4715	LR: 1.000000
Training Epoch: 26 [27904/50000]	Loss: 4.4943	LR: 1.000000
Training Epoch: 26 [28160/50000]	Loss: 4.4700	LR: 1.000000
Training Epoch: 26 [28416/50000]	Loss: 4.4606	LR: 1.000000
Training Epoch: 26 [28672/50000]	Loss: 4.5318	LR: 1.000000
Training Epoch: 26 [28928/50000]	Loss: 4.4714	LR: 1.000000
Training Epoch: 26 [29184/50000]	Loss: 4.4944	LR: 1.000000
Training Epoch: 26 [29440/50000]	Loss: 4.5032	LR: 1.000000
Training Epoch: 26 [29696/50000]	Loss: 4.4916	LR: 1.000000
Training Epoch: 26 [29952/50000]	Loss: 4.4912	LR: 1.000000
Training Epoch: 26 [30208/50000]	Loss: 4.5204	LR: 1.000000
Training Epoch: 26 [30464/50000]	Loss: 4.4857	LR: 1.000000
Training Epoch: 26 [30720/50000]	Loss: 4.5156	LR: 1.000000
Training Epoch: 26 [30976/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 26 [31232/50000]	Loss: 4.4671	LR: 1.000000
Training Epoch: 26 [31488/50000]	Loss: 4.4820	LR: 1.000000
Training Epoch: 26 [31744/50000]	Loss: 4.4419	LR: 1.000000
Training Epoch: 26 [32000/50000]	Loss: 4.4831	LR: 1.000000
Training Epoch: 26 [32256/50000]	Loss: 4.5071	LR: 1.000000
Training Epoch: 26 [32512/50000]	Loss: 4.4655	LR: 1.000000
Training Epoch: 26 [32768/50000]	Loss: 4.4718	LR: 1.000000
Training Epoch: 26 [33024/50000]	Loss: 4.4860	LR: 1.000000
Training Epoch: 26 [33280/50000]	Loss: 4.4488	LR: 1.000000
Training Epoch: 26 [33536/50000]	Loss: 4.5059	LR: 1.000000
Training Epoch: 26 [33792/50000]	Loss: 4.4565	LR: 1.000000
Training Epoch: 26 [34048/50000]	Loss: 4.4696	LR: 1.000000
Training Epoch: 26 [34304/50000]	Loss: 4.4534	LR: 1.000000
Training Epoch: 26 [34560/50000]	Loss: 4.4822	LR: 1.000000
Training Epoch: 26 [34816/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 26 [35072/50000]	Loss: 4.4706	LR: 1.000000
Training Epoch: 26 [35328/50000]	Loss: 4.4925	LR: 1.000000
Training Epoch: 26 [35584/50000]	Loss: 4.5605	LR: 1.000000
Training Epoch: 26 [35840/50000]	Loss: 4.4539	LR: 1.000000
Training Epoch: 26 [36096/50000]	Loss: 4.4785	LR: 1.000000
Training Epoch: 26 [36352/50000]	Loss: 4.4553	LR: 1.000000
Training Epoch: 26 [36608/50000]	Loss: 4.4739	LR: 1.000000
Training Epoch: 26 [36864/50000]	Loss: 4.5200	LR: 1.000000
Training Epoch: 26 [37120/50000]	Loss: 4.5098	LR: 1.000000
Training Epoch: 26 [37376/50000]	Loss: 4.5153	LR: 1.000000
Training Epoch: 26 [37632/50000]	Loss: 4.4726	LR: 1.000000
Training Epoch: 26 [37888/50000]	Loss: 4.4948	LR: 1.000000
Training Epoch: 26 [38144/50000]	Loss: 4.4931	LR: 1.000000
Training Epoch: 26 [38400/50000]	Loss: 4.5303	LR: 1.000000
Training Epoch: 26 [38656/50000]	Loss: 4.5215	LR: 1.000000
Training Epoch: 26 [38912/50000]	Loss: 4.4672	LR: 1.000000
Training Epoch: 26 [39168/50000]	Loss: 4.5048	LR: 1.000000
Training Epoch: 26 [39424/50000]	Loss: 4.5088	LR: 1.000000
Training Epoch: 26 [39680/50000]	Loss: 4.5282	LR: 1.000000
Training Epoch: 26 [39936/50000]	Loss: 4.4590	LR: 1.000000
Training Epoch: 26 [40192/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 26 [40448/50000]	Loss: 4.5005	LR: 1.000000
Training Epoch: 26 [40704/50000]	Loss: 4.5198	LR: 1.000000
Training Epoch: 26 [40960/50000]	Loss: 4.5054	LR: 1.000000
Training Epoch: 26 [41216/50000]	Loss: 4.4709	LR: 1.000000
Training Epoch: 26 [41472/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 26 [41728/50000]	Loss: 4.5574	LR: 1.000000
Training Epoch: 26 [41984/50000]	Loss: 4.4752	LR: 1.000000
Training Epoch: 26 [42240/50000]	Loss: 4.5225	LR: 1.000000
Training Epoch: 26 [42496/50000]	Loss: 4.4993	LR: 1.000000
Training Epoch: 26 [42752/50000]	Loss: 4.4974	LR: 1.000000
Training Epoch: 26 [43008/50000]	Loss: 4.4596	LR: 1.000000
Training Epoch: 26 [43264/50000]	Loss: 4.4911	LR: 1.000000
Training Epoch: 26 [43520/50000]	Loss: 4.5046	LR: 1.000000
Training Epoch: 26 [43776/50000]	Loss: 4.5368	LR: 1.000000
Training Epoch: 26 [44032/50000]	Loss: 4.5134	LR: 1.000000
Training Epoch: 26 [44288/50000]	Loss: 4.4925	LR: 1.000000
Training Epoch: 26 [44544/50000]	Loss: 4.4826	LR: 1.000000
Training Epoch: 26 [44800/50000]	Loss: 4.4971	LR: 1.000000
Training Epoch: 26 [45056/50000]	Loss: 4.4348	LR: 1.000000
Training Epoch: 26 [45312/50000]	Loss: 4.5068	LR: 1.000000
Training Epoch: 26 [45568/50000]	Loss: 4.5014	LR: 1.000000
Training Epoch: 26 [45824/50000]	Loss: 4.5630	LR: 1.000000
Training Epoch: 26 [46080/50000]	Loss: 4.4762	LR: 1.000000
Training Epoch: 26 [46336/50000]	Loss: 4.5156	LR: 1.000000
Training Epoch: 26 [46592/50000]	Loss: 4.4685	LR: 1.000000
Training Epoch: 26 [46848/50000]	Loss: 4.5489	LR: 1.000000
Training Epoch: 26 [47104/50000]	Loss: 4.4970	LR: 1.000000
Training Epoch: 26 [47360/50000]	Loss: 4.5485	LR: 1.000000
Training Epoch: 26 [47616/50000]	Loss: 4.5161	LR: 1.000000
Training Epoch: 26 [47872/50000]	Loss: 4.4986	LR: 1.000000
Training Epoch: 26 [48128/50000]	Loss: 4.5460	LR: 1.000000
Training Epoch: 26 [48384/50000]	Loss: 4.4858	LR: 1.000000
Training Epoch: 26 [48640/50000]	Loss: 4.5158	LR: 1.000000
Training Epoch: 26 [48896/50000]	Loss: 4.5631	LR: 1.000000
Training Epoch: 26 [49152/50000]	Loss: 4.5194	LR: 1.000000
Training Epoch: 26 [49408/50000]	Loss: 4.5149	LR: 1.000000
Training Epoch: 26 [49664/50000]	Loss: 4.5752	LR: 1.000000
Training Epoch: 26 [49920/50000]	Loss: 4.5237	LR: 1.000000
Training Epoch: 26 [50000/50000]	Loss: 4.5200	LR: 1.000000
epoch 26 training time consumed: 21.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   39005 GB |   39005 GB |
|       from large pool |  400448 KB |    1770 MB |   38970 GB |   38970 GB |
|       from small pool |    3549 KB |       9 MB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   39005 GB |   39005 GB |
|       from large pool |  400448 KB |    1770 MB |   38970 GB |   38970 GB |
|       from small pool |    3549 KB |       9 MB |      35 GB |      35 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   24000 GB |   24000 GB |
|       from large pool |  244672 KB |  473024 KB |   23960 GB |   23960 GB |
|       from small pool |    2594 KB |    4843 KB |      40 GB |      40 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1665 K  |    1664 K  |
|       from large pool |      36    |      77    |     805 K  |     804 K  |
|       from small pool |     186    |     224    |     860 K  |     859 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1665 K  |    1664 K  |
|       from large pool |      36    |      77    |     805 K  |     804 K  |
|       from small pool |     186    |     224    |     860 K  |     859 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      32    |     846 K  |     846 K  |
|       from large pool |      10    |      11    |     333 K  |     333 K  |
|       from small pool |      12    |      22    |     513 K  |     513 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 26, Average loss: 0.0183, Accuracy: 0.0112, Time consumed:1.46s

Training Epoch: 27 [256/50000]	Loss: 4.5540	LR: 1.000000
Training Epoch: 27 [512/50000]	Loss: 4.5306	LR: 1.000000
Training Epoch: 27 [768/50000]	Loss: 4.4837	LR: 1.000000
Training Epoch: 27 [1024/50000]	Loss: 4.5623	LR: 1.000000
Training Epoch: 27 [1280/50000]	Loss: 4.4671	LR: 1.000000
Training Epoch: 27 [1536/50000]	Loss: 4.4816	LR: 1.000000
Training Epoch: 27 [1792/50000]	Loss: 4.5323	LR: 1.000000
Training Epoch: 27 [2048/50000]	Loss: 4.5561	LR: 1.000000
Training Epoch: 27 [2304/50000]	Loss: 4.5343	LR: 1.000000
Training Epoch: 27 [2560/50000]	Loss: 4.4577	LR: 1.000000
Training Epoch: 27 [2816/50000]	Loss: 4.5010	LR: 1.000000
Training Epoch: 27 [3072/50000]	Loss: 4.4963	LR: 1.000000
Training Epoch: 27 [3328/50000]	Loss: 4.5225	LR: 1.000000
Training Epoch: 27 [3584/50000]	Loss: 4.5291	LR: 1.000000
Training Epoch: 27 [3840/50000]	Loss: 4.4973	LR: 1.000000
Training Epoch: 27 [4096/50000]	Loss: 4.4887	LR: 1.000000
Training Epoch: 27 [4352/50000]	Loss: 4.4923	LR: 1.000000
Training Epoch: 27 [4608/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 27 [4864/50000]	Loss: 4.4877	LR: 1.000000
Training Epoch: 27 [5120/50000]	Loss: 4.5351	LR: 1.000000
Training Epoch: 27 [5376/50000]	Loss: 4.5075	LR: 1.000000
Training Epoch: 27 [5632/50000]	Loss: 4.5386	LR: 1.000000
Training Epoch: 27 [5888/50000]	Loss: 4.4678	LR: 1.000000
Training Epoch: 27 [6144/50000]	Loss: 4.4911	LR: 1.000000
Training Epoch: 27 [6400/50000]	Loss: 4.5428	LR: 1.000000
Training Epoch: 27 [6656/50000]	Loss: 4.5131	LR: 1.000000
Training Epoch: 27 [6912/50000]	Loss: 4.4787	LR: 1.000000
Training Epoch: 27 [7168/50000]	Loss: 4.5113	LR: 1.000000
Training Epoch: 27 [7424/50000]	Loss: 4.5096	LR: 1.000000
Training Epoch: 27 [7680/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 27 [7936/50000]	Loss: 4.4804	LR: 1.000000
Training Epoch: 27 [8192/50000]	Loss: 4.4542	LR: 1.000000
Training Epoch: 27 [8448/50000]	Loss: 4.4971	LR: 1.000000
Training Epoch: 27 [8704/50000]	Loss: 4.4358	LR: 1.000000
Training Epoch: 27 [8960/50000]	Loss: 4.5045	LR: 1.000000
Training Epoch: 27 [9216/50000]	Loss: 4.4615	LR: 1.000000
Training Epoch: 27 [9472/50000]	Loss: 4.5050	LR: 1.000000
Training Epoch: 27 [9728/50000]	Loss: 4.4540	LR: 1.000000
Training Epoch: 27 [9984/50000]	Loss: 4.5584	LR: 1.000000
Training Epoch: 27 [10240/50000]	Loss: 4.4916	LR: 1.000000
Training Epoch: 27 [10496/50000]	Loss: 4.4903	LR: 1.000000
Training Epoch: 27 [10752/50000]	Loss: 4.4767	LR: 1.000000
Training Epoch: 27 [11008/50000]	Loss: 4.5310	LR: 1.000000
Training Epoch: 27 [11264/50000]	Loss: 4.5173	LR: 1.000000
Training Epoch: 27 [11520/50000]	Loss: 4.5302	LR: 1.000000
Training Epoch: 27 [11776/50000]	Loss: 4.5117	LR: 1.000000
Training Epoch: 27 [12032/50000]	Loss: 4.4961	LR: 1.000000
Training Epoch: 27 [12288/50000]	Loss: 4.4548	LR: 1.000000
Training Epoch: 27 [12544/50000]	Loss: 4.4953	LR: 1.000000
Training Epoch: 27 [12800/50000]	Loss: 4.5346	LR: 1.000000
Training Epoch: 27 [13056/50000]	Loss: 4.4434	LR: 1.000000
Training Epoch: 27 [13312/50000]	Loss: 4.5527	LR: 1.000000
Training Epoch: 27 [13568/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 27 [13824/50000]	Loss: 4.6024	LR: 1.000000
Training Epoch: 27 [14080/50000]	Loss: 4.6480	LR: 1.000000
Training Epoch: 27 [14336/50000]	Loss: 4.6008	LR: 1.000000
Training Epoch: 27 [14592/50000]	Loss: 4.5904	LR: 1.000000
Training Epoch: 27 [14848/50000]	Loss: 4.5821	LR: 1.000000
Training Epoch: 27 [15104/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 27 [15360/50000]	Loss: 4.5364	LR: 1.000000
Training Epoch: 27 [15616/50000]	Loss: 4.5457	LR: 1.000000
Training Epoch: 27 [15872/50000]	Loss: 4.5400	LR: 1.000000
Training Epoch: 27 [16128/50000]	Loss: 4.5681	LR: 1.000000
Training Epoch: 27 [16384/50000]	Loss: 4.4975	LR: 1.000000
Training Epoch: 27 [16640/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 27 [16896/50000]	Loss: 4.5131	LR: 1.000000
Training Epoch: 27 [17152/50000]	Loss: 4.5590	LR: 1.000000
Training Epoch: 27 [17408/50000]	Loss: 4.4503	LR: 1.000000
Training Epoch: 27 [17664/50000]	Loss: 4.5100	LR: 1.000000
Training Epoch: 27 [17920/50000]	Loss: 4.5349	LR: 1.000000
Training Epoch: 27 [18176/50000]	Loss: 4.5334	LR: 1.000000
Training Epoch: 27 [18432/50000]	Loss: 4.4929	LR: 1.000000
Training Epoch: 27 [18688/50000]	Loss: 4.5319	LR: 1.000000
Training Epoch: 27 [18944/50000]	Loss: 4.9725	LR: 1.000000
Training Epoch: 27 [19200/50000]	Loss: 4.5223	LR: 1.000000
Training Epoch: 27 [19456/50000]	Loss: 4.5438	LR: 1.000000
Training Epoch: 27 [19712/50000]	Loss: 4.5435	LR: 1.000000
Training Epoch: 27 [19968/50000]	Loss: 4.5750	LR: 1.000000
Training Epoch: 27 [20224/50000]	Loss: 4.5294	LR: 1.000000
Training Epoch: 27 [20480/50000]	Loss: 4.5468	LR: 1.000000
Training Epoch: 27 [20736/50000]	Loss: 4.5281	LR: 1.000000
Training Epoch: 27 [20992/50000]	Loss: 4.5406	LR: 1.000000
Training Epoch: 27 [21248/50000]	Loss: 4.5174	LR: 1.000000
Training Epoch: 27 [21504/50000]	Loss: 4.5259	LR: 1.000000
Training Epoch: 27 [21760/50000]	Loss: 4.5245	LR: 1.000000
Training Epoch: 27 [22016/50000]	Loss: 4.5188	LR: 1.000000
Training Epoch: 27 [22272/50000]	Loss: 4.5170	LR: 1.000000
Training Epoch: 27 [22528/50000]	Loss: 4.5257	LR: 1.000000
Training Epoch: 27 [22784/50000]	Loss: 4.5106	LR: 1.000000
Training Epoch: 27 [23040/50000]	Loss: 4.5440	LR: 1.000000
Training Epoch: 27 [23296/50000]	Loss: 4.5863	LR: 1.000000
Training Epoch: 27 [23552/50000]	Loss: 4.5872	LR: 1.000000
Training Epoch: 27 [23808/50000]	Loss: 4.5792	LR: 1.000000
Training Epoch: 27 [24064/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 27 [24320/50000]	Loss: 4.5835	LR: 1.000000
Training Epoch: 27 [24576/50000]	Loss: 4.5944	LR: 1.000000
Training Epoch: 27 [24832/50000]	Loss: 4.6299	LR: 1.000000
Training Epoch: 27 [25088/50000]	Loss: 4.5793	LR: 1.000000
Training Epoch: 27 [25344/50000]	Loss: 4.5510	LR: 1.000000
Training Epoch: 27 [25600/50000]	Loss: 4.5690	LR: 1.000000
Training Epoch: 27 [25856/50000]	Loss: 4.5982	LR: 1.000000
Training Epoch: 27 [26112/50000]	Loss: 4.5793	LR: 1.000000
Training Epoch: 27 [26368/50000]	Loss: 4.5800	LR: 1.000000
Training Epoch: 27 [26624/50000]	Loss: 4.5650	LR: 1.000000
Training Epoch: 27 [26880/50000]	Loss: 4.6421	LR: 1.000000
Training Epoch: 27 [27136/50000]	Loss: 4.5865	LR: 1.000000
Training Epoch: 27 [27392/50000]	Loss: 4.5728	LR: 1.000000
Training Epoch: 27 [27648/50000]	Loss: 4.5662	LR: 1.000000
Training Epoch: 27 [27904/50000]	Loss: 4.5896	LR: 1.000000
Training Epoch: 27 [28160/50000]	Loss: 4.5688	LR: 1.000000
Training Epoch: 27 [28416/50000]	Loss: 4.5444	LR: 1.000000
Training Epoch: 27 [28672/50000]	Loss: 4.5807	LR: 1.000000
Training Epoch: 27 [28928/50000]	Loss: 4.5934	LR: 1.000000
Training Epoch: 27 [29184/50000]	Loss: 4.5890	LR: 1.000000
Training Epoch: 27 [29440/50000]	Loss: 4.5830	LR: 1.000000
Training Epoch: 27 [29696/50000]	Loss: 4.5564	LR: 1.000000
Training Epoch: 27 [29952/50000]	Loss: 4.5820	LR: 1.000000
Training Epoch: 27 [30208/50000]	Loss: 4.5930	LR: 1.000000
Training Epoch: 27 [30464/50000]	Loss: 4.5465	LR: 1.000000
Training Epoch: 27 [30720/50000]	Loss: 4.5605	LR: 1.000000
Training Epoch: 27 [30976/50000]	Loss: 4.5787	LR: 1.000000
Training Epoch: 27 [31232/50000]	Loss: 4.5635	LR: 1.000000
Training Epoch: 27 [31488/50000]	Loss: 4.5692	LR: 1.000000
Training Epoch: 27 [31744/50000]	Loss: 4.5870	LR: 1.000000
Training Epoch: 27 [32000/50000]	Loss: 4.5994	LR: 1.000000
Training Epoch: 27 [32256/50000]	Loss: 4.5698	LR: 1.000000
Training Epoch: 27 [32512/50000]	Loss: 4.5656	LR: 1.000000
Training Epoch: 27 [32768/50000]	Loss: 4.5528	LR: 1.000000
Training Epoch: 27 [33024/50000]	Loss: 4.5500	LR: 1.000000
Training Epoch: 27 [33280/50000]	Loss: 4.5350	LR: 1.000000
Training Epoch: 27 [33536/50000]	Loss: 4.5587	LR: 1.000000
Training Epoch: 27 [33792/50000]	Loss: 4.5483	LR: 1.000000
Training Epoch: 27 [34048/50000]	Loss: 4.5875	LR: 1.000000
Training Epoch: 27 [34304/50000]	Loss: 4.5421	LR: 1.000000
Training Epoch: 27 [34560/50000]	Loss: 4.5453	LR: 1.000000
Training Epoch: 27 [34816/50000]	Loss: 4.5753	LR: 1.000000
Training Epoch: 27 [35072/50000]	Loss: 4.5454	LR: 1.000000
Training Epoch: 27 [35328/50000]	Loss: 4.5201	LR: 1.000000
Training Epoch: 27 [35584/50000]	Loss: 4.5371	LR: 1.000000
Training Epoch: 27 [35840/50000]	Loss: 4.5691	LR: 1.000000
Training Epoch: 27 [36096/50000]	Loss: 4.5608	LR: 1.000000
Training Epoch: 27 [36352/50000]	Loss: 4.5518	LR: 1.000000
Training Epoch: 27 [36608/50000]	Loss: 4.5184	LR: 1.000000
Training Epoch: 27 [36864/50000]	Loss: 4.5301	LR: 1.000000
Training Epoch: 27 [37120/50000]	Loss: 4.5314	LR: 1.000000
Training Epoch: 27 [37376/50000]	Loss: 4.5258	LR: 1.000000
Training Epoch: 27 [37632/50000]	Loss: 4.5343	LR: 1.000000
Training Epoch: 27 [37888/50000]	Loss: 4.5287	LR: 1.000000
Training Epoch: 27 [38144/50000]	Loss: 4.5278	LR: 1.000000
Training Epoch: 27 [38400/50000]	Loss: 4.5091	LR: 1.000000
Training Epoch: 27 [38656/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 27 [38912/50000]	Loss: 4.5328	LR: 1.000000
Training Epoch: 27 [39168/50000]	Loss: 4.5060	LR: 1.000000
Training Epoch: 27 [39424/50000]	Loss: 4.5009	LR: 1.000000
Training Epoch: 27 [39680/50000]	Loss: 4.5403	LR: 1.000000
Training Epoch: 27 [39936/50000]	Loss: 4.5507	LR: 1.000000
Training Epoch: 27 [40192/50000]	Loss: 4.5470	LR: 1.000000
Training Epoch: 27 [40448/50000]	Loss: 4.5205	LR: 1.000000
Training Epoch: 27 [40704/50000]	Loss: 4.5402	LR: 1.000000
Training Epoch: 27 [40960/50000]	Loss: 4.5287	LR: 1.000000
Training Epoch: 27 [41216/50000]	Loss: 4.5241	LR: 1.000000
Training Epoch: 27 [41472/50000]	Loss: 4.5077	LR: 1.000000
Training Epoch: 27 [41728/50000]	Loss: 4.5149	LR: 1.000000
Training Epoch: 27 [41984/50000]	Loss: 4.5114	LR: 1.000000
Training Epoch: 27 [42240/50000]	Loss: 4.5356	LR: 1.000000
Training Epoch: 27 [42496/50000]	Loss: 4.5158	LR: 1.000000
Training Epoch: 27 [42752/50000]	Loss: 4.5082	LR: 1.000000
Training Epoch: 27 [43008/50000]	Loss: 4.5328	LR: 1.000000
Training Epoch: 27 [43264/50000]	Loss: 4.5041	LR: 1.000000
Training Epoch: 27 [43520/50000]	Loss: 4.5079	LR: 1.000000
Training Epoch: 27 [43776/50000]	Loss: 4.5136	LR: 1.000000
Training Epoch: 27 [44032/50000]	Loss: 4.5048	LR: 1.000000
Training Epoch: 27 [44288/50000]	Loss: 4.5022	LR: 1.000000
Training Epoch: 27 [44544/50000]	Loss: 4.4696	LR: 1.000000
Training Epoch: 27 [44800/50000]	Loss: 4.5426	LR: 1.000000
Training Epoch: 27 [45056/50000]	Loss: 4.5604	LR: 1.000000
Training Epoch: 27 [45312/50000]	Loss: 4.5443	LR: 1.000000
Training Epoch: 27 [45568/50000]	Loss: 4.5672	LR: 1.000000
Training Epoch: 27 [45824/50000]	Loss: 4.5162	LR: 1.000000
Training Epoch: 27 [46080/50000]	Loss: 4.5363	LR: 1.000000
Training Epoch: 27 [46336/50000]	Loss: 4.4945	LR: 1.000000
Training Epoch: 27 [46592/50000]	Loss: 4.5131	LR: 1.000000
Training Epoch: 27 [46848/50000]	Loss: 4.5609	LR: 1.000000
Training Epoch: 27 [47104/50000]	Loss: 4.4978	LR: 1.000000
Training Epoch: 27 [47360/50000]	Loss: 4.5150	LR: 1.000000
Training Epoch: 27 [47616/50000]	Loss: 4.4862	LR: 1.000000
Training Epoch: 27 [47872/50000]	Loss: 4.4854	LR: 1.000000
Training Epoch: 27 [48128/50000]	Loss: 4.4590	LR: 1.000000
Training Epoch: 27 [48384/50000]	Loss: 4.5280	LR: 1.000000
Training Epoch: 27 [48640/50000]	Loss: 4.4846	LR: 1.000000
Training Epoch: 27 [48896/50000]	Loss: 4.5550	LR: 1.000000
Training Epoch: 27 [49152/50000]	Loss: 4.5191	LR: 1.000000
Training Epoch: 27 [49408/50000]	Loss: 4.5537	LR: 1.000000
Training Epoch: 27 [49664/50000]	Loss: 4.5411	LR: 1.000000
Training Epoch: 27 [49920/50000]	Loss: 4.4643	LR: 1.000000
Training Epoch: 27 [50000/50000]	Loss: 4.5307	LR: 1.000000
epoch 27 training time consumed: 22.00s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   40505 GB |   40505 GB |
|       from large pool |  400448 KB |    1770 MB |   40469 GB |   40469 GB |
|       from small pool |    3549 KB |       9 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   40505 GB |   40505 GB |
|       from large pool |  400448 KB |    1770 MB |   40469 GB |   40469 GB |
|       from small pool |    3549 KB |       9 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   24923 GB |   24923 GB |
|       from large pool |  244672 KB |  473024 KB |   24882 GB |   24881 GB |
|       from small pool |    2594 KB |    4843 KB |      41 GB |      41 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1729 K  |    1728 K  |
|       from large pool |      36    |      77    |     835 K  |     835 K  |
|       from small pool |     186    |     224    |     893 K  |     892 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1729 K  |    1728 K  |
|       from large pool |      36    |      77    |     835 K  |     835 K  |
|       from small pool |     186    |     224    |     893 K  |     892 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      32    |     879 K  |     879 K  |
|       from large pool |      10    |      11    |     346 K  |     346 K  |
|       from small pool |      10    |      22    |     533 K  |     533 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 27, Average loss: 0.0186, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 28 [256/50000]	Loss: 4.5138	LR: 1.000000
Training Epoch: 28 [512/50000]	Loss: 4.4624	LR: 1.000000
Training Epoch: 28 [768/50000]	Loss: 4.5019	LR: 1.000000
Training Epoch: 28 [1024/50000]	Loss: 4.5221	LR: 1.000000
Training Epoch: 28 [1280/50000]	Loss: 4.5201	LR: 1.000000
Training Epoch: 28 [1536/50000]	Loss: 4.5181	LR: 1.000000
Training Epoch: 28 [1792/50000]	Loss: 4.5209	LR: 1.000000
Training Epoch: 28 [2048/50000]	Loss: 4.4652	LR: 1.000000
Training Epoch: 28 [2304/50000]	Loss: 4.4704	LR: 1.000000
Training Epoch: 28 [2560/50000]	Loss: 4.5364	LR: 1.000000
Training Epoch: 28 [2816/50000]	Loss: 4.5366	LR: 1.000000
Training Epoch: 28 [3072/50000]	Loss: 4.5465	LR: 1.000000
Training Epoch: 28 [3328/50000]	Loss: 4.4992	LR: 1.000000
Training Epoch: 28 [3584/50000]	Loss: 4.5412	LR: 1.000000
Training Epoch: 28 [3840/50000]	Loss: 4.4470	LR: 1.000000
Training Epoch: 28 [4096/50000]	Loss: 4.5558	LR: 1.000000
Training Epoch: 28 [4352/50000]	Loss: 4.5712	LR: 1.000000
Training Epoch: 28 [4608/50000]	Loss: 4.5236	LR: 1.000000
Training Epoch: 28 [4864/50000]	Loss: 4.5063	LR: 1.000000
Training Epoch: 28 [5120/50000]	Loss: 4.5411	LR: 1.000000
Training Epoch: 28 [5376/50000]	Loss: 4.4936	LR: 1.000000
Training Epoch: 28 [5632/50000]	Loss: 4.5101	LR: 1.000000
Training Epoch: 28 [5888/50000]	Loss: 4.4872	LR: 1.000000
Training Epoch: 28 [6144/50000]	Loss: 4.5410	LR: 1.000000
Training Epoch: 28 [6400/50000]	Loss: 4.5692	LR: 1.000000
Training Epoch: 28 [6656/50000]	Loss: 4.5659	LR: 1.000000
Training Epoch: 28 [6912/50000]	Loss: 4.5226	LR: 1.000000
Training Epoch: 28 [7168/50000]	Loss: 4.4951	LR: 1.000000
Training Epoch: 28 [7424/50000]	Loss: 4.5217	LR: 1.000000
Training Epoch: 28 [7680/50000]	Loss: 4.5191	LR: 1.000000
Training Epoch: 28 [7936/50000]	Loss: 4.4836	LR: 1.000000
Training Epoch: 28 [8192/50000]	Loss: 4.4539	LR: 1.000000
Training Epoch: 28 [8448/50000]	Loss: 4.4756	LR: 1.000000
Training Epoch: 28 [8704/50000]	Loss: 4.5361	LR: 1.000000
Training Epoch: 28 [8960/50000]	Loss: 4.5365	LR: 1.000000
Training Epoch: 28 [9216/50000]	Loss: 4.5291	LR: 1.000000
Training Epoch: 28 [9472/50000]	Loss: 4.4928	LR: 1.000000
Training Epoch: 28 [9728/50000]	Loss: 4.4971	LR: 1.000000
Training Epoch: 28 [9984/50000]	Loss: 4.5439	LR: 1.000000
Training Epoch: 28 [10240/50000]	Loss: 4.5182	LR: 1.000000
Training Epoch: 28 [10496/50000]	Loss: 4.4668	LR: 1.000000
Training Epoch: 28 [10752/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 28 [11008/50000]	Loss: 4.5076	LR: 1.000000
Training Epoch: 28 [11264/50000]	Loss: 4.4956	LR: 1.000000
Training Epoch: 28 [11520/50000]	Loss: 4.5090	LR: 1.000000
Training Epoch: 28 [11776/50000]	Loss: 4.5093	LR: 1.000000
Training Epoch: 28 [12032/50000]	Loss: 4.4997	LR: 1.000000
Training Epoch: 28 [12288/50000]	Loss: 4.5443	LR: 1.000000
Training Epoch: 28 [12544/50000]	Loss: 4.5022	LR: 1.000000
Training Epoch: 28 [12800/50000]	Loss: 4.5477	LR: 1.000000
Training Epoch: 28 [13056/50000]	Loss: 4.5078	LR: 1.000000
Training Epoch: 28 [13312/50000]	Loss: 4.5470	LR: 1.000000
Training Epoch: 28 [13568/50000]	Loss: 4.5184	LR: 1.000000
Training Epoch: 28 [13824/50000]	Loss: 4.5146	LR: 1.000000
Training Epoch: 28 [14080/50000]	Loss: 4.5071	LR: 1.000000
Training Epoch: 28 [14336/50000]	Loss: 4.4625	LR: 1.000000
Training Epoch: 28 [14592/50000]	Loss: 4.4509	LR: 1.000000
Training Epoch: 28 [14848/50000]	Loss: 4.4732	LR: 1.000000
Training Epoch: 28 [15104/50000]	Loss: 4.4952	LR: 1.000000
Training Epoch: 28 [15360/50000]	Loss: 4.6024	LR: 1.000000
Training Epoch: 28 [15616/50000]	Loss: 4.5373	LR: 1.000000
Training Epoch: 28 [15872/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 28 [16128/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 28 [16384/50000]	Loss: 4.5040	LR: 1.000000
Training Epoch: 28 [16640/50000]	Loss: 4.4972	LR: 1.000000
Training Epoch: 28 [16896/50000]	Loss: 4.5283	LR: 1.000000
Training Epoch: 28 [17152/50000]	Loss: 4.5071	LR: 1.000000
Training Epoch: 28 [17408/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 28 [17664/50000]	Loss: 4.5547	LR: 1.000000
Training Epoch: 28 [17920/50000]	Loss: 4.5382	LR: 1.000000
Training Epoch: 28 [18176/50000]	Loss: 4.5151	LR: 1.000000
Training Epoch: 28 [18432/50000]	Loss: 4.5288	LR: 1.000000
Training Epoch: 28 [18688/50000]	Loss: 4.5183	LR: 1.000000
Training Epoch: 28 [18944/50000]	Loss: 4.5161	LR: 1.000000
Training Epoch: 28 [19200/50000]	Loss: 4.5391	LR: 1.000000
Training Epoch: 28 [19456/50000]	Loss: 4.5543	LR: 1.000000
Training Epoch: 28 [19712/50000]	Loss: 4.5069	LR: 1.000000
Training Epoch: 28 [19968/50000]	Loss: 4.4903	LR: 1.000000
Training Epoch: 28 [20224/50000]	Loss: 4.4883	LR: 1.000000
Training Epoch: 28 [20480/50000]	Loss: 4.4941	LR: 1.000000
Training Epoch: 28 [20736/50000]	Loss: 4.5259	LR: 1.000000
Training Epoch: 28 [20992/50000]	Loss: 4.5012	LR: 1.000000
Training Epoch: 28 [21248/50000]	Loss: 4.4599	LR: 1.000000
Training Epoch: 28 [21504/50000]	Loss: 4.4690	LR: 1.000000
Training Epoch: 28 [21760/50000]	Loss: 4.5185	LR: 1.000000
Training Epoch: 28 [22016/50000]	Loss: 4.5986	LR: 1.000000
Training Epoch: 28 [22272/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 28 [22528/50000]	Loss: 4.5246	LR: 1.000000
Training Epoch: 28 [22784/50000]	Loss: 4.5029	LR: 1.000000
Training Epoch: 28 [23040/50000]	Loss: 4.4847	LR: 1.000000
Training Epoch: 28 [23296/50000]	Loss: 4.5257	LR: 1.000000
Training Epoch: 28 [23552/50000]	Loss: 4.5202	LR: 1.000000
Training Epoch: 28 [23808/50000]	Loss: 4.5066	LR: 1.000000
Training Epoch: 28 [24064/50000]	Loss: 4.5511	LR: 1.000000
Training Epoch: 28 [24320/50000]	Loss: 4.5416	LR: 1.000000
Training Epoch: 28 [24576/50000]	Loss: 4.4938	LR: 1.000000
Training Epoch: 28 [24832/50000]	Loss: 4.4999	LR: 1.000000
Training Epoch: 28 [25088/50000]	Loss: 4.4930	LR: 1.000000
Training Epoch: 28 [25344/50000]	Loss: 4.4496	LR: 1.000000
Training Epoch: 28 [25600/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 28 [25856/50000]	Loss: 4.5646	LR: 1.000000
Training Epoch: 28 [26112/50000]	Loss: 4.5164	LR: 1.000000
Training Epoch: 28 [26368/50000]	Loss: 4.5013	LR: 1.000000
Training Epoch: 28 [26624/50000]	Loss: 4.4841	LR: 1.000000
Training Epoch: 28 [26880/50000]	Loss: 4.4815	LR: 1.000000
Training Epoch: 28 [27136/50000]	Loss: 4.5035	LR: 1.000000
Training Epoch: 28 [27392/50000]	Loss: 4.4785	LR: 1.000000
Training Epoch: 28 [27648/50000]	Loss: 4.4304	LR: 1.000000
Training Epoch: 28 [27904/50000]	Loss: 4.4609	LR: 1.000000
Training Epoch: 28 [28160/50000]	Loss: 4.5531	LR: 1.000000
Training Epoch: 28 [28416/50000]	Loss: 4.5319	LR: 1.000000
Training Epoch: 28 [28672/50000]	Loss: 4.5106	LR: 1.000000
Training Epoch: 28 [28928/50000]	Loss: 4.4723	LR: 1.000000
Training Epoch: 28 [29184/50000]	Loss: 4.5044	LR: 1.000000
Training Epoch: 28 [29440/50000]	Loss: 4.5361	LR: 1.000000
Training Epoch: 28 [29696/50000]	Loss: 4.4808	LR: 1.000000
Training Epoch: 28 [29952/50000]	Loss: 4.5468	LR: 1.000000
Training Epoch: 28 [30208/50000]	Loss: 4.5304	LR: 1.000000
Training Epoch: 28 [30464/50000]	Loss: 4.4371	LR: 1.000000
Training Epoch: 28 [30720/50000]	Loss: 4.4865	LR: 1.000000
Training Epoch: 28 [30976/50000]	Loss: 4.5059	LR: 1.000000
Training Epoch: 28 [31232/50000]	Loss: 4.5282	LR: 1.000000
Training Epoch: 28 [31488/50000]	Loss: 4.4565	LR: 1.000000
Training Epoch: 28 [31744/50000]	Loss: 4.5213	LR: 1.000000
Training Epoch: 28 [32000/50000]	Loss: 4.4727	LR: 1.000000
Training Epoch: 28 [32256/50000]	Loss: 4.4917	LR: 1.000000
Training Epoch: 28 [32512/50000]	Loss: 4.4915	LR: 1.000000
Training Epoch: 28 [32768/50000]	Loss: 4.4822	LR: 1.000000
Training Epoch: 28 [33024/50000]	Loss: 4.5342	LR: 1.000000
Training Epoch: 28 [33280/50000]	Loss: 4.4813	LR: 1.000000
Training Epoch: 28 [33536/50000]	Loss: 4.4751	LR: 1.000000
Training Epoch: 28 [33792/50000]	Loss: 4.4746	LR: 1.000000
Training Epoch: 28 [34048/50000]	Loss: 4.5806	LR: 1.000000
Training Epoch: 28 [34304/50000]	Loss: 4.5536	LR: 1.000000
Training Epoch: 28 [34560/50000]	Loss: 4.5104	LR: 1.000000
Training Epoch: 28 [34816/50000]	Loss: 4.5118	LR: 1.000000
Training Epoch: 28 [35072/50000]	Loss: 4.5104	LR: 1.000000
Training Epoch: 28 [35328/50000]	Loss: 4.4864	LR: 1.000000
Training Epoch: 28 [35584/50000]	Loss: 4.4912	LR: 1.000000
Training Epoch: 28 [35840/50000]	Loss: 4.5228	LR: 1.000000
Training Epoch: 28 [36096/50000]	Loss: 4.5439	LR: 1.000000
Training Epoch: 28 [36352/50000]	Loss: 4.5367	LR: 1.000000
Training Epoch: 28 [36608/50000]	Loss: 4.5063	LR: 1.000000
Training Epoch: 28 [36864/50000]	Loss: 4.5235	LR: 1.000000
Training Epoch: 28 [37120/50000]	Loss: 4.4740	LR: 1.000000
Training Epoch: 28 [37376/50000]	Loss: 4.5375	LR: 1.000000
Training Epoch: 28 [37632/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 28 [37888/50000]	Loss: 4.5246	LR: 1.000000
Training Epoch: 28 [38144/50000]	Loss: 4.4694	LR: 1.000000
Training Epoch: 28 [38400/50000]	Loss: 4.5042	LR: 1.000000
Training Epoch: 28 [38656/50000]	Loss: 4.4834	LR: 1.000000
Training Epoch: 28 [38912/50000]	Loss: 4.5287	LR: 1.000000
Training Epoch: 28 [39168/50000]	Loss: 4.5072	LR: 1.000000
Training Epoch: 28 [39424/50000]	Loss: 4.4478	LR: 1.000000
Training Epoch: 28 [39680/50000]	Loss: 4.4437	LR: 1.000000
Training Epoch: 28 [39936/50000]	Loss: 4.4616	LR: 1.000000
Training Epoch: 28 [40192/50000]	Loss: 4.5184	LR: 1.000000
Training Epoch: 28 [40448/50000]	Loss: 4.4811	LR: 1.000000
Training Epoch: 28 [40704/50000]	Loss: 4.4909	LR: 1.000000
Training Epoch: 28 [40960/50000]	Loss: 4.5366	LR: 1.000000
Training Epoch: 28 [41216/50000]	Loss: 4.5159	LR: 1.000000
Training Epoch: 28 [41472/50000]	Loss: 4.4813	LR: 1.000000
Training Epoch: 28 [41728/50000]	Loss: 4.4654	LR: 1.000000
Training Epoch: 28 [41984/50000]	Loss: 4.4381	LR: 1.000000
Training Epoch: 28 [42240/50000]	Loss: 4.5151	LR: 1.000000
Training Epoch: 28 [42496/50000]	Loss: 4.5296	LR: 1.000000
Training Epoch: 28 [42752/50000]	Loss: 4.5089	LR: 1.000000
Training Epoch: 28 [43008/50000]	Loss: 4.4702	LR: 1.000000
Training Epoch: 28 [43264/50000]	Loss: 4.4853	LR: 1.000000
Training Epoch: 28 [43520/50000]	Loss: 4.4658	LR: 1.000000
Training Epoch: 28 [43776/50000]	Loss: 4.4799	LR: 1.000000
Training Epoch: 28 [44032/50000]	Loss: 4.4955	LR: 1.000000
Training Epoch: 28 [44288/50000]	Loss: 4.5220	LR: 1.000000
Training Epoch: 28 [44544/50000]	Loss: 4.4476	LR: 1.000000
Training Epoch: 28 [44800/50000]	Loss: 4.4629	LR: 1.000000
Training Epoch: 28 [45056/50000]	Loss: 4.5093	LR: 1.000000
Training Epoch: 28 [45312/50000]	Loss: 4.4823	LR: 1.000000
Training Epoch: 28 [45568/50000]	Loss: 4.5317	LR: 1.000000
Training Epoch: 28 [45824/50000]	Loss: 4.5059	LR: 1.000000
Training Epoch: 28 [46080/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 28 [46336/50000]	Loss: 4.5229	LR: 1.000000
Training Epoch: 28 [46592/50000]	Loss: 4.5393	LR: 1.000000
Training Epoch: 28 [46848/50000]	Loss: 4.4889	LR: 1.000000
Training Epoch: 28 [47104/50000]	Loss: 4.4970	LR: 1.000000
Training Epoch: 28 [47360/50000]	Loss: 4.5563	LR: 1.000000
Training Epoch: 28 [47616/50000]	Loss: 4.5311	LR: 1.000000
Training Epoch: 28 [47872/50000]	Loss: 4.5130	LR: 1.000000
Training Epoch: 28 [48128/50000]	Loss: 4.4711	LR: 1.000000
Training Epoch: 28 [48384/50000]	Loss: 4.4841	LR: 1.000000
Training Epoch: 28 [48640/50000]	Loss: 4.5009	LR: 1.000000
Training Epoch: 28 [48896/50000]	Loss: 4.5136	LR: 1.000000
Training Epoch: 28 [49152/50000]	Loss: 4.4859	LR: 1.000000
Training Epoch: 28 [49408/50000]	Loss: 4.4898	LR: 1.000000
Training Epoch: 28 [49664/50000]	Loss: 4.4486	LR: 1.000000
Training Epoch: 28 [49920/50000]	Loss: 4.4443	LR: 1.000000
Training Epoch: 28 [50000/50000]	Loss: 4.5246	LR: 1.000000
epoch 28 training time consumed: 21.96s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   42006 GB |   42005 GB |
|       from large pool |  400448 KB |    1770 MB |   41968 GB |   41967 GB |
|       from small pool |    3549 KB |       9 MB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   42006 GB |   42005 GB |
|       from large pool |  400448 KB |    1770 MB |   41968 GB |   41967 GB |
|       from small pool |    3549 KB |       9 MB |      37 GB |      37 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   25846 GB |   25846 GB |
|       from large pool |  244672 KB |  473024 KB |   25803 GB |   25803 GB |
|       from small pool |    4642 KB |    4843 KB |      43 GB |      43 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1793 K  |    1792 K  |
|       from large pool |      36    |      77    |     866 K  |     866 K  |
|       from small pool |     186    |     224    |     926 K  |     926 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1793 K  |    1792 K  |
|       from large pool |      36    |      77    |     866 K  |     866 K  |
|       from small pool |     186    |     224    |     926 K  |     926 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      32    |     912 K  |     912 K  |
|       from large pool |      10    |      11    |     359 K  |     359 K  |
|       from small pool |      13    |      22    |     553 K  |     553 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 28, Average loss: 0.0179, Accuracy: 0.0182, Time consumed:1.46s

Training Epoch: 29 [256/50000]	Loss: 4.5210	LR: 1.000000
Training Epoch: 29 [512/50000]	Loss: 4.4886	LR: 1.000000
Training Epoch: 29 [768/50000]	Loss: 4.5286	LR: 1.000000
Training Epoch: 29 [1024/50000]	Loss: 4.4933	LR: 1.000000
Training Epoch: 29 [1280/50000]	Loss: 4.5125	LR: 1.000000
Training Epoch: 29 [1536/50000]	Loss: 4.5592	LR: 1.000000
Training Epoch: 29 [1792/50000]	Loss: 4.4993	LR: 1.000000
Training Epoch: 29 [2048/50000]	Loss: 4.5067	LR: 1.000000
Training Epoch: 29 [2304/50000]	Loss: 4.5363	LR: 1.000000
Training Epoch: 29 [2560/50000]	Loss: 4.4975	LR: 1.000000
Training Epoch: 29 [2816/50000]	Loss: 4.4913	LR: 1.000000
Training Epoch: 29 [3072/50000]	Loss: 4.5259	LR: 1.000000
Training Epoch: 29 [3328/50000]	Loss: 4.4965	LR: 1.000000
Training Epoch: 29 [3584/50000]	Loss: 4.5312	LR: 1.000000
Training Epoch: 29 [3840/50000]	Loss: 4.5043	LR: 1.000000
Training Epoch: 29 [4096/50000]	Loss: 4.4630	LR: 1.000000
Training Epoch: 29 [4352/50000]	Loss: 4.4740	LR: 1.000000
Training Epoch: 29 [4608/50000]	Loss: 4.5073	LR: 1.000000
Training Epoch: 29 [4864/50000]	Loss: 4.5274	LR: 1.000000
Training Epoch: 29 [5120/50000]	Loss: 4.5298	LR: 1.000000
Training Epoch: 29 [5376/50000]	Loss: 4.5424	LR: 1.000000
Training Epoch: 29 [5632/50000]	Loss: 4.5471	LR: 1.000000
Training Epoch: 29 [5888/50000]	Loss: 4.5360	LR: 1.000000
Training Epoch: 29 [6144/50000]	Loss: 4.5303	LR: 1.000000
Training Epoch: 29 [6400/50000]	Loss: 4.5294	LR: 1.000000
Training Epoch: 29 [6656/50000]	Loss: 4.5199	LR: 1.000000
Training Epoch: 29 [6912/50000]	Loss: 4.4967	LR: 1.000000
Training Epoch: 29 [7168/50000]	Loss: 4.4838	LR: 1.000000
Training Epoch: 29 [7424/50000]	Loss: 4.5070	LR: 1.000000
Training Epoch: 29 [7680/50000]	Loss: 4.5073	LR: 1.000000
Training Epoch: 29 [7936/50000]	Loss: 4.4886	LR: 1.000000
Training Epoch: 29 [8192/50000]	Loss: 4.5051	LR: 1.000000
Training Epoch: 29 [8448/50000]	Loss: 4.4979	LR: 1.000000
Training Epoch: 29 [8704/50000]	Loss: 4.4926	LR: 1.000000
Training Epoch: 29 [8960/50000]	Loss: 4.5421	LR: 1.000000
Training Epoch: 29 [9216/50000]	Loss: 4.5217	LR: 1.000000
Training Epoch: 29 [9472/50000]	Loss: 4.5522	LR: 1.000000
Training Epoch: 29 [9728/50000]	Loss: 4.4542	LR: 1.000000
Training Epoch: 29 [9984/50000]	Loss: 4.5068	LR: 1.000000
Training Epoch: 29 [10240/50000]	Loss: 4.5508	LR: 1.000000
Training Epoch: 29 [10496/50000]	Loss: 4.5398	LR: 1.000000
Training Epoch: 29 [10752/50000]	Loss: 4.4737	LR: 1.000000
Training Epoch: 29 [11008/50000]	Loss: 4.5359	LR: 1.000000
Training Epoch: 29 [11264/50000]	Loss: 4.5422	LR: 1.000000
Training Epoch: 29 [11520/50000]	Loss: 4.5698	LR: 1.000000
Training Epoch: 29 [11776/50000]	Loss: 4.5244	LR: 1.000000
Training Epoch: 29 [12032/50000]	Loss: 4.4895	LR: 1.000000
Training Epoch: 29 [12288/50000]	Loss: 4.4654	LR: 1.000000
Training Epoch: 29 [12544/50000]	Loss: 4.4741	LR: 1.000000
Training Epoch: 29 [12800/50000]	Loss: 4.4628	LR: 1.000000
Training Epoch: 29 [13056/50000]	Loss: 4.4900	LR: 1.000000
Training Epoch: 29 [13312/50000]	Loss: 4.4964	LR: 1.000000
Training Epoch: 29 [13568/50000]	Loss: 4.4764	LR: 1.000000
Training Epoch: 29 [13824/50000]	Loss: 4.5413	LR: 1.000000
Training Epoch: 29 [14080/50000]	Loss: 4.5389	LR: 1.000000
Training Epoch: 29 [14336/50000]	Loss: 4.5093	LR: 1.000000
Training Epoch: 29 [14592/50000]	Loss: 4.5240	LR: 1.000000
Training Epoch: 29 [14848/50000]	Loss: 4.4713	LR: 1.000000
Training Epoch: 29 [15104/50000]	Loss: 4.5385	LR: 1.000000
Training Epoch: 29 [15360/50000]	Loss: 4.4435	LR: 1.000000
Training Epoch: 29 [15616/50000]	Loss: 4.5144	LR: 1.000000
Training Epoch: 29 [15872/50000]	Loss: 4.5399	LR: 1.000000
Training Epoch: 29 [16128/50000]	Loss: 4.5458	LR: 1.000000
Training Epoch: 29 [16384/50000]	Loss: 4.4586	LR: 1.000000
Training Epoch: 29 [16640/50000]	Loss: 4.5288	LR: 1.000000
Training Epoch: 29 [16896/50000]	Loss: 4.4850	LR: 1.000000
Training Epoch: 29 [17152/50000]	Loss: 4.4845	LR: 1.000000
Training Epoch: 29 [17408/50000]	Loss: 4.4587	LR: 1.000000
Training Epoch: 29 [17664/50000]	Loss: 4.4891	LR: 1.000000
Training Epoch: 29 [17920/50000]	Loss: 4.5479	LR: 1.000000
Training Epoch: 29 [18176/50000]	Loss: 4.5239	LR: 1.000000
Training Epoch: 29 [18432/50000]	Loss: 4.5145	LR: 1.000000
Training Epoch: 29 [18688/50000]	Loss: 4.4971	LR: 1.000000
Training Epoch: 29 [18944/50000]	Loss: 4.4885	LR: 1.000000
Training Epoch: 29 [19200/50000]	Loss: 4.5163	LR: 1.000000
Training Epoch: 29 [19456/50000]	Loss: 4.5314	LR: 1.000000
Training Epoch: 29 [19712/50000]	Loss: 4.5617	LR: 1.000000
Training Epoch: 29 [19968/50000]	Loss: 4.5148	LR: 1.000000
Training Epoch: 29 [20224/50000]	Loss: 4.4819	LR: 1.000000
Training Epoch: 29 [20480/50000]	Loss: 4.4748	LR: 1.000000
Training Epoch: 29 [20736/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 29 [20992/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 29 [21248/50000]	Loss: 4.4907	LR: 1.000000
Training Epoch: 29 [21504/50000]	Loss: 4.5202	LR: 1.000000
Training Epoch: 29 [21760/50000]	Loss: 4.5129	LR: 1.000000
Training Epoch: 29 [22016/50000]	Loss: 4.5360	LR: 1.000000
Training Epoch: 29 [22272/50000]	Loss: 4.5434	LR: 1.000000
Training Epoch: 29 [22528/50000]	Loss: 4.5271	LR: 1.000000
Training Epoch: 29 [22784/50000]	Loss: 4.5480	LR: 1.000000
Training Epoch: 29 [23040/50000]	Loss: 4.5372	LR: 1.000000
Training Epoch: 29 [23296/50000]	Loss: 4.5469	LR: 1.000000
Training Epoch: 29 [23552/50000]	Loss: 4.5713	LR: 1.000000
Training Epoch: 29 [23808/50000]	Loss: 4.5513	LR: 1.000000
Training Epoch: 29 [24064/50000]	Loss: 4.5019	LR: 1.000000
Training Epoch: 29 [24320/50000]	Loss: 4.5367	LR: 1.000000
Training Epoch: 29 [24576/50000]	Loss: 4.5917	LR: 1.000000
Training Epoch: 29 [24832/50000]	Loss: 4.5485	LR: 1.000000
Training Epoch: 29 [25088/50000]	Loss: 4.5024	LR: 1.000000
Training Epoch: 29 [25344/50000]	Loss: 4.4683	LR: 1.000000
Training Epoch: 29 [25600/50000]	Loss: 4.4844	LR: 1.000000
Training Epoch: 29 [25856/50000]	Loss: 4.4351	LR: 1.000000
Training Epoch: 29 [26112/50000]	Loss: 4.4595	LR: 1.000000
Training Epoch: 29 [26368/50000]	Loss: 4.5176	LR: 1.000000
Training Epoch: 29 [26624/50000]	Loss: 4.5150	LR: 1.000000
Training Epoch: 29 [26880/50000]	Loss: 4.5566	LR: 1.000000
Training Epoch: 29 [27136/50000]	Loss: 4.4542	LR: 1.000000
Training Epoch: 29 [27392/50000]	Loss: 4.5315	LR: 1.000000
Training Epoch: 29 [27648/50000]	Loss: 4.5026	LR: 1.000000
Training Epoch: 29 [27904/50000]	Loss: 4.5747	LR: 1.000000
Training Epoch: 29 [28160/50000]	Loss: 4.5769	LR: 1.000000
Training Epoch: 29 [28416/50000]	Loss: 4.5472	LR: 1.000000
Training Epoch: 29 [28672/50000]	Loss: 4.5078	LR: 1.000000
Training Epoch: 29 [28928/50000]	Loss: 4.5150	LR: 1.000000
Training Epoch: 29 [29184/50000]	Loss: 4.5091	LR: 1.000000
Training Epoch: 29 [29440/50000]	Loss: 4.5107	LR: 1.000000
Training Epoch: 29 [29696/50000]	Loss: 4.5120	LR: 1.000000
Training Epoch: 29 [29952/50000]	Loss: 4.5335	LR: 1.000000
Training Epoch: 29 [30208/50000]	Loss: 4.5343	LR: 1.000000
Training Epoch: 29 [30464/50000]	Loss: 4.5144	LR: 1.000000
Training Epoch: 29 [30720/50000]	Loss: 4.4956	LR: 1.000000
Training Epoch: 29 [30976/50000]	Loss: 4.5123	LR: 1.000000
Training Epoch: 29 [31232/50000]	Loss: 4.5672	LR: 1.000000
Training Epoch: 29 [31488/50000]	Loss: 4.4846	LR: 1.000000
Training Epoch: 29 [31744/50000]	Loss: 4.5272	LR: 1.000000
Training Epoch: 29 [32000/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 29 [32256/50000]	Loss: 4.5278	LR: 1.000000
Training Epoch: 29 [32512/50000]	Loss: 4.4697	LR: 1.000000
Training Epoch: 29 [32768/50000]	Loss: 4.5383	LR: 1.000000
Training Epoch: 29 [33024/50000]	Loss: 4.4865	LR: 1.000000
Training Epoch: 29 [33280/50000]	Loss: 4.5501	LR: 1.000000
Training Epoch: 29 [33536/50000]	Loss: 4.5412	LR: 1.000000
Training Epoch: 29 [33792/50000]	Loss: 4.5200	LR: 1.000000
Training Epoch: 29 [34048/50000]	Loss: 4.5328	LR: 1.000000
Training Epoch: 29 [34304/50000]	Loss: 4.4856	LR: 1.000000
Training Epoch: 29 [34560/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 29 [34816/50000]	Loss: 4.5225	LR: 1.000000
Training Epoch: 29 [35072/50000]	Loss: 4.5553	LR: 1.000000
Training Epoch: 29 [35328/50000]	Loss: 4.5461	LR: 1.000000
Training Epoch: 29 [35584/50000]	Loss: 4.5610	LR: 1.000000
Training Epoch: 29 [35840/50000]	Loss: 4.5145	LR: 1.000000
Training Epoch: 29 [36096/50000]	Loss: 4.5669	LR: 1.000000
Training Epoch: 29 [36352/50000]	Loss: 4.5279	LR: 1.000000
Training Epoch: 29 [36608/50000]	Loss: 4.4949	LR: 1.000000
Training Epoch: 29 [36864/50000]	Loss: 4.5593	LR: 1.000000
Training Epoch: 29 [37120/50000]	Loss: 4.5753	LR: 1.000000
Training Epoch: 29 [37376/50000]	Loss: 4.5459	LR: 1.000000
Training Epoch: 29 [37632/50000]	Loss: 4.5259	LR: 1.000000
Training Epoch: 29 [37888/50000]	Loss: 4.5354	LR: 1.000000
Training Epoch: 29 [38144/50000]	Loss: 4.5046	LR: 1.000000
Training Epoch: 29 [38400/50000]	Loss: 4.4953	LR: 1.000000
Training Epoch: 29 [38656/50000]	Loss: 4.4893	LR: 1.000000
Training Epoch: 29 [38912/50000]	Loss: 4.4815	LR: 1.000000
Training Epoch: 29 [39168/50000]	Loss: 4.5221	LR: 1.000000
Training Epoch: 29 [39424/50000]	Loss: 4.4868	LR: 1.000000
Training Epoch: 29 [39680/50000]	Loss: 4.5309	LR: 1.000000
Training Epoch: 29 [39936/50000]	Loss: 4.4900	LR: 1.000000
Training Epoch: 29 [40192/50000]	Loss: 4.5048	LR: 1.000000
Training Epoch: 29 [40448/50000]	Loss: 4.4866	LR: 1.000000
Training Epoch: 29 [40704/50000]	Loss: 4.5385	LR: 1.000000
Training Epoch: 29 [40960/50000]	Loss: 4.4540	LR: 1.000000
Training Epoch: 29 [41216/50000]	Loss: 4.5048	LR: 1.000000
Training Epoch: 29 [41472/50000]	Loss: 4.4651	LR: 1.000000
Training Epoch: 29 [41728/50000]	Loss: 4.5385	LR: 1.000000
Training Epoch: 29 [41984/50000]	Loss: 4.5128	LR: 1.000000
Training Epoch: 29 [42240/50000]	Loss: 4.5482	LR: 1.000000
Training Epoch: 29 [42496/50000]	Loss: 4.4715	LR: 1.000000
Training Epoch: 29 [42752/50000]	Loss: 4.5091	LR: 1.000000
Training Epoch: 29 [43008/50000]	Loss: 4.5073	LR: 1.000000
Training Epoch: 29 [43264/50000]	Loss: 4.5438	LR: 1.000000
Training Epoch: 29 [43520/50000]	Loss: 4.4668	LR: 1.000000
Training Epoch: 29 [43776/50000]	Loss: 4.5685	LR: 1.000000
Training Epoch: 29 [44032/50000]	Loss: 4.4755	LR: 1.000000
Training Epoch: 29 [44288/50000]	Loss: 4.5008	LR: 1.000000
Training Epoch: 29 [44544/50000]	Loss: 4.4923	LR: 1.000000
Training Epoch: 29 [44800/50000]	Loss: 4.5268	LR: 1.000000
Training Epoch: 29 [45056/50000]	Loss: 4.5067	LR: 1.000000
Training Epoch: 29 [45312/50000]	Loss: 4.5006	LR: 1.000000
Training Epoch: 29 [45568/50000]	Loss: 4.5208	LR: 1.000000
Training Epoch: 29 [45824/50000]	Loss: 4.5081	LR: 1.000000
Training Epoch: 29 [46080/50000]	Loss: 4.5288	LR: 1.000000
Training Epoch: 29 [46336/50000]	Loss: 4.5331	LR: 1.000000
Training Epoch: 29 [46592/50000]	Loss: 4.5776	LR: 1.000000
Training Epoch: 29 [46848/50000]	Loss: 4.5513	LR: 1.000000
Training Epoch: 29 [47104/50000]	Loss: 4.5375	LR: 1.000000
Training Epoch: 29 [47360/50000]	Loss: 4.5629	LR: 1.000000
Training Epoch: 29 [47616/50000]	Loss: 4.5108	LR: 1.000000
Training Epoch: 29 [47872/50000]	Loss: 4.5030	LR: 1.000000
Training Epoch: 29 [48128/50000]	Loss: 4.5308	LR: 1.000000
Training Epoch: 29 [48384/50000]	Loss: 4.5061	LR: 1.000000
Training Epoch: 29 [48640/50000]	Loss: 4.5312	LR: 1.000000
Training Epoch: 29 [48896/50000]	Loss: 4.5133	LR: 1.000000
Training Epoch: 29 [49152/50000]	Loss: 4.5445	LR: 1.000000
Training Epoch: 29 [49408/50000]	Loss: 4.5585	LR: 1.000000
Training Epoch: 29 [49664/50000]	Loss: 4.5414	LR: 1.000000
Training Epoch: 29 [49920/50000]	Loss: 4.5547	LR: 1.000000
Training Epoch: 29 [50000/50000]	Loss: 4.4784	LR: 1.000000
epoch 29 training time consumed: 21.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   43506 GB |   43505 GB |
|       from large pool |  400448 KB |    1770 MB |   43467 GB |   43466 GB |
|       from small pool |    3549 KB |       9 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   43506 GB |   43505 GB |
|       from large pool |  400448 KB |    1770 MB |   43467 GB |   43466 GB |
|       from small pool |    3549 KB |       9 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   26769 GB |   26769 GB |
|       from large pool |  244672 KB |  473024 KB |   26725 GB |   26724 GB |
|       from small pool |    2594 KB |    4843 KB |      44 GB |      44 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1857 K  |    1856 K  |
|       from large pool |      36    |      77    |     897 K  |     897 K  |
|       from small pool |     186    |     224    |     959 K  |     959 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1857 K  |    1856 K  |
|       from large pool |      36    |      77    |     897 K  |     897 K  |
|       from small pool |     186    |     224    |     959 K  |     959 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      32    |     944 K  |     944 K  |
|       from large pool |      10    |      11    |     371 K  |     371 K  |
|       from small pool |      12    |      22    |     572 K  |     572 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 29, Average loss: 0.0183, Accuracy: 0.0118, Time consumed:1.43s

Training Epoch: 30 [256/50000]	Loss: 4.5325	LR: 1.000000
Training Epoch: 30 [512/50000]	Loss: 4.5457	LR: 1.000000
Training Epoch: 30 [768/50000]	Loss: 4.5929	LR: 1.000000
Training Epoch: 30 [1024/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 30 [1280/50000]	Loss: 4.5831	LR: 1.000000
Training Epoch: 30 [1536/50000]	Loss: 4.6001	LR: 1.000000
Training Epoch: 30 [1792/50000]	Loss: 4.5525	LR: 1.000000
Training Epoch: 30 [2048/50000]	Loss: 4.6400	LR: 1.000000
Training Epoch: 30 [2304/50000]	Loss: 4.6470	LR: 1.000000
Training Epoch: 30 [2560/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 30 [2816/50000]	Loss: 4.6017	LR: 1.000000
Training Epoch: 30 [3072/50000]	Loss: 4.5916	LR: 1.000000
Training Epoch: 30 [3328/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 30 [3584/50000]	Loss: 4.5981	LR: 1.000000
Training Epoch: 30 [3840/50000]	Loss: 4.5778	LR: 1.000000
Training Epoch: 30 [4096/50000]	Loss: 4.5833	LR: 1.000000
Training Epoch: 30 [4352/50000]	Loss: 4.5978	LR: 1.000000
Training Epoch: 30 [4608/50000]	Loss: 4.5933	LR: 1.000000
Training Epoch: 30 [4864/50000]	Loss: 4.5973	LR: 1.000000
Training Epoch: 30 [5120/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 30 [5376/50000]	Loss: 4.5894	LR: 1.000000
Training Epoch: 30 [5632/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 30 [5888/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 30 [6144/50000]	Loss: 4.5707	LR: 1.000000
Training Epoch: 30 [6400/50000]	Loss: 4.5701	LR: 1.000000
Training Epoch: 30 [6656/50000]	Loss: 4.5805	LR: 1.000000
Training Epoch: 30 [6912/50000]	Loss: 4.5793	LR: 1.000000
Training Epoch: 30 [7168/50000]	Loss: 4.5661	LR: 1.000000
Training Epoch: 30 [7424/50000]	Loss: 4.5693	LR: 1.000000
Training Epoch: 30 [7680/50000]	Loss: 4.5558	LR: 1.000000
Training Epoch: 30 [7936/50000]	Loss: 4.5220	LR: 1.000000
Training Epoch: 30 [8192/50000]	Loss: 4.5898	LR: 1.000000
Training Epoch: 30 [8448/50000]	Loss: 4.5974	LR: 1.000000
Training Epoch: 30 [8704/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 30 [8960/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 30 [9216/50000]	Loss: 4.5675	LR: 1.000000
Training Epoch: 30 [9472/50000]	Loss: 4.5580	LR: 1.000000
Training Epoch: 30 [9728/50000]	Loss: 4.5532	LR: 1.000000
Training Epoch: 30 [9984/50000]	Loss: 4.5695	LR: 1.000000
Training Epoch: 30 [10240/50000]	Loss: 4.5644	LR: 1.000000
Training Epoch: 30 [10496/50000]	Loss: 4.5719	LR: 1.000000
Training Epoch: 30 [10752/50000]	Loss: 4.5836	LR: 1.000000
Training Epoch: 30 [11008/50000]	Loss: 4.5349	LR: 1.000000
Training Epoch: 30 [11264/50000]	Loss: 4.5265	LR: 1.000000
Training Epoch: 30 [11520/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 30 [11776/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 30 [12032/50000]	Loss: 4.5990	LR: 1.000000
Training Epoch: 30 [12288/50000]	Loss: 4.5380	LR: 1.000000
Training Epoch: 30 [12544/50000]	Loss: 4.5018	LR: 1.000000
Training Epoch: 30 [12800/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 30 [13056/50000]	Loss: 4.5289	LR: 1.000000
Training Epoch: 30 [13312/50000]	Loss: 4.5551	LR: 1.000000
Training Epoch: 30 [13568/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 30 [13824/50000]	Loss: 4.5512	LR: 1.000000
Training Epoch: 30 [14080/50000]	Loss: 4.5515	LR: 1.000000
Training Epoch: 30 [14336/50000]	Loss: 4.4909	LR: 1.000000
Training Epoch: 30 [14592/50000]	Loss: 4.5356	LR: 1.000000
Training Epoch: 30 [14848/50000]	Loss: 4.5270	LR: 1.000000
Training Epoch: 30 [15104/50000]	Loss: 4.5218	LR: 1.000000
Training Epoch: 30 [15360/50000]	Loss: 4.5450	LR: 1.000000
Training Epoch: 30 [15616/50000]	Loss: 4.4738	LR: 1.000000
Training Epoch: 30 [15872/50000]	Loss: 4.5315	LR: 1.000000
Training Epoch: 30 [16128/50000]	Loss: 4.5101	LR: 1.000000
Training Epoch: 30 [16384/50000]	Loss: 4.5064	LR: 1.000000
Training Epoch: 30 [16640/50000]	Loss: 4.5330	LR: 1.000000
Training Epoch: 30 [16896/50000]	Loss: 4.5378	LR: 1.000000
Training Epoch: 30 [17152/50000]	Loss: 4.5372	LR: 1.000000
Training Epoch: 30 [17408/50000]	Loss: 4.5790	LR: 1.000000
Training Epoch: 30 [17664/50000]	Loss: 4.5172	LR: 1.000000
Training Epoch: 30 [17920/50000]	Loss: 4.4900	LR: 1.000000
Training Epoch: 30 [18176/50000]	Loss: 4.5023	LR: 1.000000
Training Epoch: 30 [18432/50000]	Loss: 4.4805	LR: 1.000000
Training Epoch: 30 [18688/50000]	Loss: 4.5018	LR: 1.000000
Training Epoch: 30 [18944/50000]	Loss: 4.5165	LR: 1.000000
Training Epoch: 30 [19200/50000]	Loss: 4.5254	LR: 1.000000
Training Epoch: 30 [19456/50000]	Loss: 4.5279	LR: 1.000000
Training Epoch: 30 [19712/50000]	Loss: 4.5463	LR: 1.000000
Training Epoch: 30 [19968/50000]	Loss: 4.5490	LR: 1.000000
Training Epoch: 30 [20224/50000]	Loss: 4.5260	LR: 1.000000
Training Epoch: 30 [20480/50000]	Loss: 4.5501	LR: 1.000000
Training Epoch: 30 [20736/50000]	Loss: 4.5145	LR: 1.000000
Training Epoch: 30 [20992/50000]	Loss: 4.5016	LR: 1.000000
Training Epoch: 30 [21248/50000]	Loss: 4.5164	LR: 1.000000
Training Epoch: 30 [21504/50000]	Loss: 4.4992	LR: 1.000000
Training Epoch: 30 [21760/50000]	Loss: 4.5047	LR: 1.000000
Training Epoch: 30 [22016/50000]	Loss: 4.4629	LR: 1.000000
Training Epoch: 30 [22272/50000]	Loss: 4.5588	LR: 1.000000
Training Epoch: 30 [22528/50000]	Loss: 4.5356	LR: 1.000000
Training Epoch: 30 [22784/50000]	Loss: 4.5102	LR: 1.000000
Training Epoch: 30 [23040/50000]	Loss: 4.5147	LR: 1.000000
Training Epoch: 30 [23296/50000]	Loss: 4.5252	LR: 1.000000
Training Epoch: 30 [23552/50000]	Loss: 4.5156	LR: 1.000000
Training Epoch: 30 [23808/50000]	Loss: 4.5313	LR: 1.000000
Training Epoch: 30 [24064/50000]	Loss: 4.5184	LR: 1.000000
Training Epoch: 30 [24320/50000]	Loss: 4.5501	LR: 1.000000
Training Epoch: 30 [24576/50000]	Loss: 4.5294	LR: 1.000000
Training Epoch: 30 [24832/50000]	Loss: 4.5269	LR: 1.000000
Training Epoch: 30 [25088/50000]	Loss: 4.4971	LR: 1.000000
Training Epoch: 30 [25344/50000]	Loss: 4.4928	LR: 1.000000
Training Epoch: 30 [25600/50000]	Loss: 4.5442	LR: 1.000000
Training Epoch: 30 [25856/50000]	Loss: 4.4484	LR: 1.000000
Training Epoch: 30 [26112/50000]	Loss: 4.5224	LR: 1.000000
Training Epoch: 30 [26368/50000]	Loss: 4.5203	LR: 1.000000
Training Epoch: 30 [26624/50000]	Loss: 4.5003	LR: 1.000000
Training Epoch: 30 [26880/50000]	Loss: 4.5131	LR: 1.000000
Training Epoch: 30 [27136/50000]	Loss: 4.4781	LR: 1.000000
Training Epoch: 30 [27392/50000]	Loss: 4.5154	LR: 1.000000
Training Epoch: 30 [27648/50000]	Loss: 4.4763	LR: 1.000000
Training Epoch: 30 [27904/50000]	Loss: 4.5465	LR: 1.000000
Training Epoch: 30 [28160/50000]	Loss: 4.4610	LR: 1.000000
Training Epoch: 30 [28416/50000]	Loss: 4.5002	LR: 1.000000
Training Epoch: 30 [28672/50000]	Loss: 4.5626	LR: 1.000000
Training Epoch: 30 [28928/50000]	Loss: 4.5781	LR: 1.000000
Training Epoch: 30 [29184/50000]	Loss: 4.4974	LR: 1.000000
Training Epoch: 30 [29440/50000]	Loss: 4.5135	LR: 1.000000
Training Epoch: 30 [29696/50000]	Loss: 4.5348	LR: 1.000000
Training Epoch: 30 [29952/50000]	Loss: 4.4978	LR: 1.000000
Training Epoch: 30 [30208/50000]	Loss: 4.5938	LR: 1.000000
Training Epoch: 30 [30464/50000]	Loss: 4.4746	LR: 1.000000
Training Epoch: 30 [30720/50000]	Loss: 4.4877	LR: 1.000000
Training Epoch: 30 [30976/50000]	Loss: 4.5430	LR: 1.000000
Training Epoch: 30 [31232/50000]	Loss: 4.5276	LR: 1.000000
Training Epoch: 30 [31488/50000]	Loss: 4.5059	LR: 1.000000
Training Epoch: 30 [31744/50000]	Loss: 4.5265	LR: 1.000000
Training Epoch: 30 [32000/50000]	Loss: 4.5202	LR: 1.000000
Training Epoch: 30 [32256/50000]	Loss: 4.5319	LR: 1.000000
Training Epoch: 30 [32512/50000]	Loss: 4.5117	LR: 1.000000
Training Epoch: 30 [32768/50000]	Loss: 4.4914	LR: 1.000000
Training Epoch: 30 [33024/50000]	Loss: 4.4903	LR: 1.000000
Training Epoch: 30 [33280/50000]	Loss: 4.4859	LR: 1.000000
Training Epoch: 30 [33536/50000]	Loss: 4.5161	LR: 1.000000
Training Epoch: 30 [33792/50000]	Loss: 4.4437	LR: 1.000000
Training Epoch: 30 [34048/50000]	Loss: 4.5260	LR: 1.000000
Training Epoch: 30 [34304/50000]	Loss: 4.5046	LR: 1.000000
Training Epoch: 30 [34560/50000]	Loss: 4.4884	LR: 1.000000
Training Epoch: 30 [34816/50000]	Loss: 4.4604	LR: 1.000000
Training Epoch: 30 [35072/50000]	Loss: 4.5225	LR: 1.000000
Training Epoch: 30 [35328/50000]	Loss: 4.5170	LR: 1.000000
Training Epoch: 30 [35584/50000]	Loss: 4.4697	LR: 1.000000
Training Epoch: 30 [35840/50000]	Loss: 4.4965	LR: 1.000000
Training Epoch: 30 [36096/50000]	Loss: 4.5267	LR: 1.000000
Training Epoch: 30 [36352/50000]	Loss: 4.5291	LR: 1.000000
Training Epoch: 30 [36608/50000]	Loss: 5.5640	LR: 1.000000
Training Epoch: 30 [36864/50000]	Loss: 4.5920	LR: 1.000000
Training Epoch: 30 [37120/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 30 [37376/50000]	Loss: 4.5951	LR: 1.000000
Training Epoch: 30 [37632/50000]	Loss: 4.6930	LR: 1.000000
Training Epoch: 30 [37888/50000]	Loss: 4.6958	LR: 1.000000
Training Epoch: 30 [38144/50000]	Loss: 4.6378	LR: 1.000000
Training Epoch: 30 [38400/50000]	Loss: 4.6943	LR: 1.000000
Training Epoch: 30 [38656/50000]	Loss: 4.6406	LR: 1.000000
Training Epoch: 30 [38912/50000]	Loss: 4.6314	LR: 1.000000
Training Epoch: 30 [39168/50000]	Loss: 4.6369	LR: 1.000000
Training Epoch: 30 [39424/50000]	Loss: 4.6404	LR: 1.000000
Training Epoch: 30 [39680/50000]	Loss: 4.6533	LR: 1.000000
Training Epoch: 30 [39936/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 30 [40192/50000]	Loss: 4.6436	LR: 1.000000
Training Epoch: 30 [40448/50000]	Loss: 4.6484	LR: 1.000000
Training Epoch: 30 [40704/50000]	Loss: 4.6551	LR: 1.000000
Training Epoch: 30 [40960/50000]	Loss: 4.6496	LR: 1.000000
Training Epoch: 30 [41216/50000]	Loss: 4.6472	LR: 1.000000
Training Epoch: 30 [41472/50000]	Loss: 4.6581	LR: 1.000000
Training Epoch: 30 [41728/50000]	Loss: 4.6320	LR: 1.000000
Training Epoch: 30 [41984/50000]	Loss: 4.6291	LR: 1.000000
Training Epoch: 30 [42240/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 30 [42496/50000]	Loss: 4.6307	LR: 1.000000
Training Epoch: 30 [42752/50000]	Loss: 4.6308	LR: 1.000000
Training Epoch: 30 [43008/50000]	Loss: 4.6308	LR: 1.000000
Training Epoch: 30 [43264/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 30 [43520/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 30 [43776/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 30 [44032/50000]	Loss: 4.6423	LR: 1.000000
Training Epoch: 30 [44288/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 30 [44544/50000]	Loss: 4.6307	LR: 1.000000
Training Epoch: 30 [44800/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 30 [45056/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 30 [45312/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 30 [45568/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 30 [45824/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 30 [46080/50000]	Loss: 4.6007	LR: 1.000000
Training Epoch: 30 [46336/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 30 [46592/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 30 [46848/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 30 [47104/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 30 [47360/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 30 [47616/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 30 [47872/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 30 [48128/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 30 [48384/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 30 [48640/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 30 [48896/50000]	Loss: 4.6285	LR: 1.000000
Training Epoch: 30 [49152/50000]	Loss: 4.6276	LR: 1.000000
Training Epoch: 30 [49408/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 30 [49664/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 30 [49920/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 30 [50000/50000]	Loss: 4.5908	LR: 1.000000
epoch 30 training time consumed: 21.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   45006 GB |   45006 GB |
|       from large pool |  400448 KB |    1770 MB |   44965 GB |   44965 GB |
|       from small pool |    3549 KB |       9 MB |      40 GB |      40 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   45006 GB |   45006 GB |
|       from large pool |  400448 KB |    1770 MB |   44965 GB |   44965 GB |
|       from small pool |    3549 KB |       9 MB |      40 GB |      40 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   27692 GB |   27692 GB |
|       from large pool |  244672 KB |  473024 KB |   27646 GB |   27646 GB |
|       from small pool |    4642 KB |    4843 KB |      46 GB |      46 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1921 K  |    1920 K  |
|       from large pool |      36    |      77    |     928 K  |     928 K  |
|       from small pool |     186    |     224    |     992 K  |     992 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1921 K  |    1920 K  |
|       from large pool |      36    |      77    |     928 K  |     928 K  |
|       from small pool |     186    |     224    |     992 K  |     992 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      32    |     977 K  |     977 K  |
|       from large pool |      10    |      11    |     384 K  |     384 K  |
|       from small pool |      12    |      22    |     592 K  |     592 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 30, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-30-regular.pth
Training Epoch: 31 [256/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 31 [512/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 31 [768/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 31 [1024/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 31 [1280/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 31 [1536/50000]	Loss: 4.5926	LR: 1.000000
Training Epoch: 31 [1792/50000]	Loss: 4.5948	LR: 1.000000
Training Epoch: 31 [2048/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 31 [2304/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 31 [2560/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 31 [2816/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 31 [3072/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 31 [3328/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 31 [3584/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 31 [3840/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 31 [4096/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 31 [4352/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 31 [4608/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 31 [4864/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 31 [5120/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 31 [5376/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 31 [5632/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 31 [5888/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 31 [6144/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 31 [6400/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 31 [6656/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 31 [6912/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 31 [7168/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 31 [7424/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 31 [7680/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 31 [7936/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 31 [8192/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 31 [8448/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 31 [8704/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 31 [8960/50000]	Loss: 4.6341	LR: 1.000000
Training Epoch: 31 [9216/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 31 [9472/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 31 [9728/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 31 [9984/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 31 [10240/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 31 [10496/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 31 [10752/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 31 [11008/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 31 [11264/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 31 [11520/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 31 [11776/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 31 [12032/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 31 [12288/50000]	Loss: 4.5978	LR: 1.000000
Training Epoch: 31 [12544/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 31 [12800/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 31 [13056/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 31 [13312/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 31 [13568/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 31 [13824/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 31 [14080/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 31 [14336/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 31 [14592/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 31 [14848/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 31 [15104/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 31 [15360/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 31 [15616/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 31 [15872/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 31 [16128/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 31 [16384/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 31 [16640/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 31 [16896/50000]	Loss: 4.5921	LR: 1.000000
Training Epoch: 31 [17152/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 31 [17408/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 31 [17664/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 31 [17920/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 31 [18176/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 31 [18432/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 31 [18688/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 31 [18944/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 31 [19200/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 31 [19456/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 31 [19712/50000]	Loss: 4.5986	LR: 1.000000
Training Epoch: 31 [19968/50000]	Loss: 4.5997	LR: 1.000000
Training Epoch: 31 [20224/50000]	Loss: 4.5784	LR: 1.000000
Training Epoch: 31 [20480/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 31 [20736/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 31 [20992/50000]	Loss: 4.5983	LR: 1.000000
Training Epoch: 31 [21248/50000]	Loss: 4.5928	LR: 1.000000
Training Epoch: 31 [21504/50000]	Loss: 4.5909	LR: 1.000000
Training Epoch: 31 [21760/50000]	Loss: 4.5972	LR: 1.000000
Training Epoch: 31 [22016/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 31 [22272/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 31 [22528/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 31 [22784/50000]	Loss: 4.5924	LR: 1.000000
Training Epoch: 31 [23040/50000]	Loss: 4.5935	LR: 1.000000
Training Epoch: 31 [23296/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 31 [23552/50000]	Loss: 4.5873	LR: 1.000000
Training Epoch: 31 [23808/50000]	Loss: 4.5935	LR: 1.000000
Training Epoch: 31 [24064/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 31 [24320/50000]	Loss: 4.5967	LR: 1.000000
Training Epoch: 31 [24576/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 31 [24832/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 31 [25088/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 31 [25344/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 31 [25600/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 31 [25856/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 31 [26112/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 31 [26368/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 31 [26624/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 31 [26880/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 31 [27136/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 31 [27392/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 31 [27648/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 31 [27904/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 31 [28160/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 31 [28416/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 31 [28672/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 31 [28928/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 31 [29184/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 31 [29440/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 31 [29696/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 31 [29952/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 31 [30208/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 31 [30464/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 31 [30720/50000]	Loss: 4.6335	LR: 1.000000
Training Epoch: 31 [30976/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 31 [31232/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 31 [31488/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 31 [31744/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 31 [32000/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 31 [32256/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 31 [32512/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 31 [32768/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 31 [33024/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 31 [33280/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 31 [33536/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 31 [33792/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 31 [34048/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 31 [34304/50000]	Loss: 4.6321	LR: 1.000000
Training Epoch: 31 [34560/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 31 [34816/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 31 [35072/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 31 [35328/50000]	Loss: 4.5963	LR: 1.000000
Training Epoch: 31 [35584/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 31 [35840/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 31 [36096/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 31 [36352/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 31 [36608/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 31 [36864/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 31 [37120/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 31 [37376/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 31 [37632/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 31 [37888/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 31 [38144/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 31 [38400/50000]	Loss: 4.6413	LR: 1.000000
Training Epoch: 31 [38656/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 31 [38912/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 31 [39168/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 31 [39424/50000]	Loss: 4.6315	LR: 1.000000
Training Epoch: 31 [39680/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 31 [39936/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 31 [40192/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 31 [40448/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 31 [40704/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 31 [40960/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 31 [41216/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 31 [41472/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 31 [41728/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 31 [41984/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 31 [42240/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 31 [42496/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 31 [42752/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 31 [43008/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 31 [43264/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 31 [43520/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 31 [43776/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 31 [44032/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 31 [44288/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 31 [44544/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 31 [44800/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 31 [45056/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 31 [45312/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 31 [45568/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 31 [45824/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 31 [46080/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 31 [46336/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 31 [46592/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 31 [46848/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 31 [47104/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 31 [47360/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 31 [47616/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 31 [47872/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 31 [48128/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 31 [48384/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 31 [48640/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 31 [48896/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 31 [49152/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 31 [49408/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 31 [49664/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 31 [49920/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 31 [50000/50000]	Loss: 4.6062	LR: 1.000000
epoch 31 training time consumed: 21.88s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   46506 GB |   46506 GB |
|       from large pool |  400448 KB |    1770 MB |   46464 GB |   46464 GB |
|       from small pool |    3549 KB |       9 MB |      41 GB |      41 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   46506 GB |   46506 GB |
|       from large pool |  400448 KB |    1770 MB |   46464 GB |   46464 GB |
|       from small pool |    3549 KB |       9 MB |      41 GB |      41 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   28615 GB |   28615 GB |
|       from large pool |  244672 KB |  473024 KB |   28568 GB |   28567 GB |
|       from small pool |    2594 KB |    4843 KB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    1985 K  |    1984 K  |
|       from large pool |      36    |      77    |     959 K  |     959 K  |
|       from small pool |     186    |     224    |    1025 K  |    1025 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    1985 K  |    1984 K  |
|       from large pool |      36    |      77    |     959 K  |     959 K  |
|       from small pool |     186    |     224    |    1025 K  |    1025 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      32    |    1010 K  |    1010 K  |
|       from large pool |      10    |      11    |     397 K  |     397 K  |
|       from small pool |      13    |      22    |     612 K  |     612 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 31, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 32 [256/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 32 [512/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 32 [768/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 32 [1024/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 32 [1280/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 32 [1536/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 32 [1792/50000]	Loss: 4.6294	LR: 1.000000
Training Epoch: 32 [2048/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 32 [2304/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 32 [2560/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 32 [2816/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 32 [3072/50000]	Loss: 4.5971	LR: 1.000000
Training Epoch: 32 [3328/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 32 [3584/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 32 [3840/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 32 [4096/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 32 [4352/50000]	Loss: 4.6357	LR: 1.000000
Training Epoch: 32 [4608/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 32 [4864/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 32 [5120/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 32 [5376/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 32 [5632/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 32 [5888/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 32 [6144/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 32 [6400/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 32 [6656/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 32 [6912/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 32 [7168/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 32 [7424/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 32 [7680/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 32 [7936/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 32 [8192/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 32 [8448/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 32 [8704/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 32 [8960/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 32 [9216/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 32 [9472/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 32 [9728/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 32 [9984/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 32 [10240/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 32 [10496/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 32 [10752/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 32 [11008/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 32 [11264/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 32 [11520/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 32 [11776/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 32 [12032/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 32 [12288/50000]	Loss: 4.5970	LR: 1.000000
Training Epoch: 32 [12544/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 32 [12800/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 32 [13056/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 32 [13312/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 32 [13568/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 32 [13824/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 32 [14080/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 32 [14336/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 32 [14592/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 32 [14848/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 32 [15104/50000]	Loss: 4.6299	LR: 1.000000
Training Epoch: 32 [15360/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 32 [15616/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 32 [15872/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 32 [16128/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 32 [16384/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 32 [16640/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 32 [16896/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 32 [17152/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 32 [17408/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 32 [17664/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 32 [17920/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 32 [18176/50000]	Loss: 4.6282	LR: 1.000000
Training Epoch: 32 [18432/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 32 [18688/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 32 [18944/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 32 [19200/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 32 [19456/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 32 [19712/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 32 [19968/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 32 [20224/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 32 [20480/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 32 [20736/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 32 [20992/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 32 [21248/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 32 [21504/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 32 [21760/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 32 [22016/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 32 [22272/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 32 [22528/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 32 [22784/50000]	Loss: 4.5983	LR: 1.000000
Training Epoch: 32 [23040/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 32 [23296/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 32 [23552/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 32 [23808/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 32 [24064/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 32 [24320/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 32 [24576/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 32 [24832/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 32 [25088/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 32 [25344/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 32 [25600/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 32 [25856/50000]	Loss: 4.6294	LR: 1.000000
Training Epoch: 32 [26112/50000]	Loss: 4.6015	LR: 1.000000
Training Epoch: 32 [26368/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 32 [26624/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 32 [26880/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 32 [27136/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 32 [27392/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 32 [27648/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 32 [27904/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 32 [28160/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 32 [28416/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 32 [28672/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 32 [28928/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 32 [29184/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 32 [29440/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 32 [29696/50000]	Loss: 4.5959	LR: 1.000000
Training Epoch: 32 [29952/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 32 [30208/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 32 [30464/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 32 [30720/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 32 [30976/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 32 [31232/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 32 [31488/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 32 [31744/50000]	Loss: 4.6294	LR: 1.000000
Training Epoch: 32 [32000/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 32 [32256/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 32 [32512/50000]	Loss: 4.6321	LR: 1.000000
Training Epoch: 32 [32768/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 32 [33024/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 32 [33280/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 32 [33536/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 32 [33792/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 32 [34048/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 32 [34304/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 32 [34560/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 32 [34816/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 32 [35072/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 32 [35328/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 32 [35584/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 32 [35840/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 32 [36096/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 32 [36352/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 32 [36608/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 32 [36864/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 32 [37120/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 32 [37376/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 32 [37632/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 32 [37888/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 32 [38144/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 32 [38400/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 32 [38656/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 32 [38912/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 32 [39168/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 32 [39424/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 32 [39680/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 32 [39936/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 32 [40192/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 32 [40448/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 32 [40704/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 32 [40960/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 32 [41216/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 32 [41472/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 32 [41728/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 32 [41984/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 32 [42240/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 32 [42496/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 32 [42752/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 32 [43008/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 32 [43264/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 32 [43520/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 32 [43776/50000]	Loss: 4.6003	LR: 1.000000
Training Epoch: 32 [44032/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 32 [44288/50000]	Loss: 4.6313	LR: 1.000000
Training Epoch: 32 [44544/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 32 [44800/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 32 [45056/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 32 [45312/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 32 [45568/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 32 [45824/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 32 [46080/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 32 [46336/50000]	Loss: 4.6317	LR: 1.000000
Training Epoch: 32 [46592/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 32 [46848/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 32 [47104/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 32 [47360/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 32 [47616/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 32 [47872/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 32 [48128/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 32 [48384/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 32 [48640/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 32 [48896/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 32 [49152/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 32 [49408/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 32 [49664/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 32 [49920/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 32 [50000/50000]	Loss: 4.5896	LR: 1.000000
epoch 32 training time consumed: 21.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   48006 GB |   48006 GB |
|       from large pool |  400448 KB |    1770 MB |   47963 GB |   47963 GB |
|       from small pool |    3549 KB |       9 MB |      43 GB |      43 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   48006 GB |   48006 GB |
|       from large pool |  400448 KB |    1770 MB |   47963 GB |   47963 GB |
|       from small pool |    3549 KB |       9 MB |      43 GB |      43 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   29539 GB |   29538 GB |
|       from large pool |  244672 KB |  473024 KB |   29489 GB |   29489 GB |
|       from small pool |    2594 KB |    4843 KB |      49 GB |      49 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2049 K  |    2049 K  |
|       from large pool |      36    |      77    |     990 K  |     990 K  |
|       from small pool |     186    |     224    |    1058 K  |    1058 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2049 K  |    2049 K  |
|       from large pool |      36    |      77    |     990 K  |     990 K  |
|       from small pool |     186    |     224    |    1058 K  |    1058 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      32    |    1043 K  |    1043 K  |
|       from large pool |      10    |      11    |     410 K  |     410 K  |
|       from small pool |      12    |      22    |     632 K  |     632 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 32, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.40s

Training Epoch: 33 [256/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 33 [512/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 33 [768/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 33 [1024/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 33 [1280/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 33 [1536/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 33 [1792/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 33 [2048/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 33 [2304/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 33 [2560/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 33 [2816/50000]	Loss: 4.5939	LR: 1.000000
Training Epoch: 33 [3072/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 33 [3328/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 33 [3584/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 33 [3840/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 33 [4096/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 33 [4352/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 33 [4608/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 33 [4864/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 33 [5120/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 33 [5376/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 33 [5632/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 33 [5888/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 33 [6144/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 33 [6400/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 33 [6656/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 33 [6912/50000]	Loss: 4.6327	LR: 1.000000
Training Epoch: 33 [7168/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 33 [7424/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 33 [7680/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 33 [7936/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 33 [8192/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 33 [8448/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 33 [8704/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 33 [8960/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 33 [9216/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 33 [9472/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 33 [9728/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 33 [9984/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 33 [10240/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 33 [10496/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 33 [10752/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 33 [11008/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 33 [11264/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 33 [11520/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 33 [11776/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 33 [12032/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 33 [12288/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 33 [12544/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 33 [12800/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 33 [13056/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 33 [13312/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 33 [13568/50000]	Loss: 4.6313	LR: 1.000000
Training Epoch: 33 [13824/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 33 [14080/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 33 [14336/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 33 [14592/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 33 [14848/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 33 [15104/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 33 [15360/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 33 [15616/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 33 [15872/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 33 [16128/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 33 [16384/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 33 [16640/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 33 [16896/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 33 [17152/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 33 [17408/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 33 [17664/50000]	Loss: 4.6307	LR: 1.000000
Training Epoch: 33 [17920/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 33 [18176/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 33 [18432/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 33 [18688/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 33 [18944/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 33 [19200/50000]	Loss: 4.6286	LR: 1.000000
Training Epoch: 33 [19456/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 33 [19712/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 33 [19968/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 33 [20224/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 33 [20480/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 33 [20736/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 33 [20992/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 33 [21248/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 33 [21504/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 33 [21760/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 33 [22016/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 33 [22272/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 33 [22528/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 33 [22784/50000]	Loss: 4.6330	LR: 1.000000
Training Epoch: 33 [23040/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 33 [23296/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 33 [23552/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 33 [23808/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 33 [24064/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 33 [24320/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 33 [24576/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 33 [24832/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 33 [25088/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 33 [25344/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 33 [25600/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 33 [25856/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 33 [26112/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 33 [26368/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 33 [26624/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 33 [26880/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 33 [27136/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 33 [27392/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 33 [27648/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 33 [27904/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 33 [28160/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 33 [28416/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 33 [28672/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 33 [28928/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 33 [29184/50000]	Loss: 4.5845	LR: 1.000000
Training Epoch: 33 [29440/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 33 [29696/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 33 [29952/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 33 [30208/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 33 [30464/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 33 [30720/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 33 [30976/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 33 [31232/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 33 [31488/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 33 [31744/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 33 [32000/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 33 [32256/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 33 [32512/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 33 [32768/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 33 [33024/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 33 [33280/50000]	Loss: 4.6353	LR: 1.000000
Training Epoch: 33 [33536/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 33 [33792/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 33 [34048/50000]	Loss: 4.6009	LR: 1.000000
Training Epoch: 33 [34304/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 33 [34560/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 33 [34816/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 33 [35072/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 33 [35328/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 33 [35584/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 33 [35840/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 33 [36096/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 33 [36352/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 33 [36608/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 33 [36864/50000]	Loss: 4.5980	LR: 1.000000
Training Epoch: 33 [37120/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 33 [37376/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 33 [37632/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 33 [37888/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 33 [38144/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 33 [38400/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 33 [38656/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 33 [38912/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 33 [39168/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 33 [39424/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 33 [39680/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 33 [39936/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 33 [40192/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 33 [40448/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 33 [40704/50000]	Loss: 4.6291	LR: 1.000000
Training Epoch: 33 [40960/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 33 [41216/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 33 [41472/50000]	Loss: 4.6336	LR: 1.000000
Training Epoch: 33 [41728/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 33 [41984/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 33 [42240/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 33 [42496/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 33 [42752/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 33 [43008/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 33 [43264/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 33 [43520/50000]	Loss: 4.6326	LR: 1.000000
Training Epoch: 33 [43776/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 33 [44032/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 33 [44288/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 33 [44544/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 33 [44800/50000]	Loss: 4.6420	LR: 1.000000
Training Epoch: 33 [45056/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 33 [45312/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 33 [45568/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 33 [45824/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 33 [46080/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 33 [46336/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 33 [46592/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 33 [46848/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 33 [47104/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 33 [47360/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 33 [47616/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 33 [47872/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 33 [48128/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 33 [48384/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 33 [48640/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 33 [48896/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 33 [49152/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 33 [49408/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 33 [49664/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 33 [49920/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 33 [50000/50000]	Loss: 4.5878	LR: 1.000000
epoch 33 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   49507 GB |   49506 GB |
|       from large pool |  400448 KB |    1770 MB |   49462 GB |   49462 GB |
|       from small pool |    3549 KB |       9 MB |      44 GB |      44 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   49507 GB |   49506 GB |
|       from large pool |  400448 KB |    1770 MB |   49462 GB |   49462 GB |
|       from small pool |    3549 KB |       9 MB |      44 GB |      44 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   30462 GB |   30461 GB |
|       from large pool |  244672 KB |  473024 KB |   30411 GB |   30410 GB |
|       from small pool |    2594 KB |    4843 KB |      50 GB |      50 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2113 K  |    2113 K  |
|       from large pool |      36    |      77    |    1021 K  |    1021 K  |
|       from small pool |     186    |     224    |    1091 K  |    1091 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2113 K  |    2113 K  |
|       from large pool |      36    |      77    |    1021 K  |    1021 K  |
|       from small pool |     186    |     224    |    1091 K  |    1091 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      32    |    1075 K  |    1075 K  |
|       from large pool |      10    |      11    |     423 K  |     423 K  |
|       from small pool |      11    |      22    |     652 K  |     652 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 33, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.41s

Training Epoch: 34 [256/50000]	Loss: 4.6299	LR: 1.000000
Training Epoch: 34 [512/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 34 [768/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 34 [1024/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 34 [1280/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 34 [1536/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 34 [1792/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 34 [2048/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 34 [2304/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 34 [2560/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 34 [2816/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 34 [3072/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 34 [3328/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 34 [3584/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 34 [3840/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 34 [4096/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 34 [4352/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 34 [4608/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 34 [4864/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 34 [5120/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 34 [5376/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 34 [5632/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 34 [5888/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 34 [6144/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 34 [6400/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 34 [6656/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 34 [6912/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 34 [7168/50000]	Loss: 4.5921	LR: 1.000000
Training Epoch: 34 [7424/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 34 [7680/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 34 [7936/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 34 [8192/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 34 [8448/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 34 [8704/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 34 [8960/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 34 [9216/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 34 [9472/50000]	Loss: 4.5933	LR: 1.000000
Training Epoch: 34 [9728/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 34 [9984/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 34 [10240/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 34 [10496/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 34 [10752/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 34 [11008/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 34 [11264/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 34 [11520/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 34 [11776/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 34 [12032/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 34 [12288/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 34 [12544/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 34 [12800/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 34 [13056/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 34 [13312/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 34 [13568/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 34 [13824/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 34 [14080/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 34 [14336/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 34 [14592/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 34 [14848/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 34 [15104/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 34 [15360/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 34 [15616/50000]	Loss: 4.6366	LR: 1.000000
Training Epoch: 34 [15872/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 34 [16128/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 34 [16384/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 34 [16640/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 34 [16896/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 34 [17152/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 34 [17408/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 34 [17664/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 34 [17920/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 34 [18176/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 34 [18432/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 34 [18688/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 34 [18944/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 34 [19200/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 34 [19456/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 34 [19712/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 34 [19968/50000]	Loss: 4.6307	LR: 1.000000
Training Epoch: 34 [20224/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 34 [20480/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 34 [20736/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 34 [20992/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 34 [21248/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 34 [21504/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 34 [21760/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 34 [22016/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 34 [22272/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 34 [22528/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 34 [22784/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 34 [23040/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 34 [23296/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 34 [23552/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 34 [23808/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 34 [24064/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 34 [24320/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 34 [24576/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 34 [24832/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 34 [25088/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 34 [25344/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 34 [25600/50000]	Loss: 4.6328	LR: 1.000000
Training Epoch: 34 [25856/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 34 [26112/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 34 [26368/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 34 [26624/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 34 [26880/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 34 [27136/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 34 [27392/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 34 [27648/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 34 [27904/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 34 [28160/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 34 [28416/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 34 [28672/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 34 [28928/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 34 [29184/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 34 [29440/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 34 [29696/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 34 [29952/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 34 [30208/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 34 [30464/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 34 [30720/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 34 [30976/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 34 [31232/50000]	Loss: 4.6030	LR: 1.000000
Training Epoch: 34 [31488/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 34 [31744/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 34 [32000/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 34 [32256/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 34 [32512/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 34 [32768/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 34 [33024/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 34 [33280/50000]	Loss: 4.6305	LR: 1.000000
Training Epoch: 34 [33536/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 34 [33792/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 34 [34048/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 34 [34304/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 34 [34560/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 34 [34816/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 34 [35072/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 34 [35328/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 34 [35584/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 34 [35840/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 34 [36096/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 34 [36352/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 34 [36608/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 34 [36864/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 34 [37120/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 34 [37376/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 34 [37632/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 34 [37888/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 34 [38144/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 34 [38400/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 34 [38656/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 34 [38912/50000]	Loss: 4.6306	LR: 1.000000
Training Epoch: 34 [39168/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 34 [39424/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 34 [39680/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 34 [39936/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 34 [40192/50000]	Loss: 4.5974	LR: 1.000000
Training Epoch: 34 [40448/50000]	Loss: 4.5994	LR: 1.000000
Training Epoch: 34 [40704/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 34 [40960/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 34 [41216/50000]	Loss: 4.5994	LR: 1.000000
Training Epoch: 34 [41472/50000]	Loss: 4.6337	LR: 1.000000
Training Epoch: 34 [41728/50000]	Loss: 4.5976	LR: 1.000000
Training Epoch: 34 [41984/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 34 [42240/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 34 [42496/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 34 [42752/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 34 [43008/50000]	Loss: 4.6344	LR: 1.000000
Training Epoch: 34 [43264/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 34 [43520/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 34 [43776/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 34 [44032/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 34 [44288/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 34 [44544/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 34 [44800/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 34 [45056/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 34 [45312/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 34 [45568/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 34 [45824/50000]	Loss: 4.6396	LR: 1.000000
Training Epoch: 34 [46080/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 34 [46336/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 34 [46592/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 34 [46848/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 34 [47104/50000]	Loss: 4.6276	LR: 1.000000
Training Epoch: 34 [47360/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 34 [47616/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 34 [47872/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 34 [48128/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 34 [48384/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 34 [48640/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 34 [48896/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 34 [49152/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 34 [49408/50000]	Loss: 4.6296	LR: 1.000000
Training Epoch: 34 [49664/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 34 [49920/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 34 [50000/50000]	Loss: 4.6385	LR: 1.000000
epoch 34 training time consumed: 21.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   51007 GB |   51006 GB |
|       from large pool |  400448 KB |    1770 MB |   50961 GB |   50960 GB |
|       from small pool |    3549 KB |       9 MB |      45 GB |      45 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   51007 GB |   51006 GB |
|       from large pool |  400448 KB |    1770 MB |   50961 GB |   50960 GB |
|       from small pool |    3549 KB |       9 MB |      45 GB |      45 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   31385 GB |   31384 GB |
|       from large pool |  244672 KB |  473024 KB |   31332 GB |   31332 GB |
|       from small pool |    2594 KB |    4843 KB |      52 GB |      52 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2177 K  |    2177 K  |
|       from large pool |      36    |      77    |    1052 K  |    1052 K  |
|       from small pool |     186    |     224    |    1124 K  |    1124 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2177 K  |    2177 K  |
|       from large pool |      36    |      77    |    1052 K  |    1052 K  |
|       from small pool |     186    |     224    |    1124 K  |    1124 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1108 K  |    1108 K  |
|       from large pool |      10    |      11    |     436 K  |     436 K  |
|       from small pool |      11    |      23    |     672 K  |     672 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 34, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 35 [256/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 35 [512/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 35 [768/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 35 [1024/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 35 [1280/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 35 [1536/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 35 [1792/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 35 [2048/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 35 [2304/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 35 [2560/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 35 [2816/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 35 [3072/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 35 [3328/50000]	Loss: 4.6325	LR: 1.000000
Training Epoch: 35 [3584/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 35 [3840/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 35 [4096/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 35 [4352/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 35 [4608/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 35 [4864/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 35 [5120/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 35 [5376/50000]	Loss: 4.6363	LR: 1.000000
Training Epoch: 35 [5632/50000]	Loss: 4.5989	LR: 1.000000
Training Epoch: 35 [5888/50000]	Loss: 4.6344	LR: 1.000000
Training Epoch: 35 [6144/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 35 [6400/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 35 [6656/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 35 [6912/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 35 [7168/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 35 [7424/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 35 [7680/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 35 [7936/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 35 [8192/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 35 [8448/50000]	Loss: 4.6347	LR: 1.000000
Training Epoch: 35 [8704/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 35 [8960/50000]	Loss: 4.6354	LR: 1.000000
Training Epoch: 35 [9216/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 35 [9472/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 35 [9728/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 35 [9984/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 35 [10240/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 35 [10496/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 35 [10752/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 35 [11008/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 35 [11264/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 35 [11520/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 35 [11776/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 35 [12032/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 35 [12288/50000]	Loss: 4.6329	LR: 1.000000
Training Epoch: 35 [12544/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 35 [12800/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 35 [13056/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 35 [13312/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 35 [13568/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 35 [13824/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 35 [14080/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 35 [14336/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 35 [14592/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 35 [14848/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 35 [15104/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 35 [15360/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 35 [15616/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 35 [15872/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 35 [16128/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 35 [16384/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 35 [16640/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 35 [16896/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 35 [17152/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 35 [17408/50000]	Loss: 4.5984	LR: 1.000000
Training Epoch: 35 [17664/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 35 [17920/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 35 [18176/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 35 [18432/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 35 [18688/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 35 [18944/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 35 [19200/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 35 [19456/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 35 [19712/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 35 [19968/50000]	Loss: 4.6319	LR: 1.000000
Training Epoch: 35 [20224/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 35 [20480/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 35 [20736/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 35 [20992/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 35 [21248/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 35 [21504/50000]	Loss: 4.5917	LR: 1.000000
Training Epoch: 35 [21760/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 35 [22016/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 35 [22272/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 35 [22528/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 35 [22784/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 35 [23040/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 35 [23296/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 35 [23552/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 35 [23808/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 35 [24064/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 35 [24320/50000]	Loss: 4.6356	LR: 1.000000
Training Epoch: 35 [24576/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 35 [24832/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 35 [25088/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 35 [25344/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 35 [25600/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 35 [25856/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 35 [26112/50000]	Loss: 4.6406	LR: 1.000000
Training Epoch: 35 [26368/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 35 [26624/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 35 [26880/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 35 [27136/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 35 [27392/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 35 [27648/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 35 [27904/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 35 [28160/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 35 [28416/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 35 [28672/50000]	Loss: 4.6001	LR: 1.000000
Training Epoch: 35 [28928/50000]	Loss: 4.6316	LR: 1.000000
Training Epoch: 35 [29184/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 35 [29440/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 35 [29696/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 35 [29952/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 35 [30208/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 35 [30464/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 35 [30720/50000]	Loss: 4.6351	LR: 1.000000
Training Epoch: 35 [30976/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 35 [31232/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 35 [31488/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 35 [31744/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 35 [32000/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 35 [32256/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 35 [32512/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 35 [32768/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 35 [33024/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 35 [33280/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 35 [33536/50000]	Loss: 4.5960	LR: 1.000000
Training Epoch: 35 [33792/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 35 [34048/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 35 [34304/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 35 [34560/50000]	Loss: 4.6005	LR: 1.000000
Training Epoch: 35 [34816/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 35 [35072/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 35 [35328/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 35 [35584/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 35 [35840/50000]	Loss: 4.6005	LR: 1.000000
Training Epoch: 35 [36096/50000]	Loss: 4.6007	LR: 1.000000
Training Epoch: 35 [36352/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 35 [36608/50000]	Loss: 4.6308	LR: 1.000000
Training Epoch: 35 [36864/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 35 [37120/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 35 [37376/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 35 [37632/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 35 [37888/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 35 [38144/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 35 [38400/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 35 [38656/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 35 [38912/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 35 [39168/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 35 [39424/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 35 [39680/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 35 [39936/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 35 [40192/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 35 [40448/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 35 [40704/50000]	Loss: 4.5998	LR: 1.000000
Training Epoch: 35 [40960/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 35 [41216/50000]	Loss: 4.6323	LR: 1.000000
Training Epoch: 35 [41472/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 35 [41728/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 35 [41984/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 35 [42240/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 35 [42496/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 35 [42752/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 35 [43008/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 35 [43264/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 35 [43520/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 35 [43776/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 35 [44032/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 35 [44288/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 35 [44544/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 35 [44800/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 35 [45056/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 35 [45312/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 35 [45568/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 35 [45824/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 35 [46080/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 35 [46336/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 35 [46592/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 35 [46848/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 35 [47104/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 35 [47360/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 35 [47616/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 35 [47872/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 35 [48128/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 35 [48384/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 35 [48640/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 35 [48896/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 35 [49152/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 35 [49408/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 35 [49664/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 35 [49920/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 35 [50000/50000]	Loss: 4.6264	LR: 1.000000
epoch 35 training time consumed: 21.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   52507 GB |   52506 GB |
|       from large pool |  400448 KB |    1770 MB |   52460 GB |   52459 GB |
|       from small pool |    3549 KB |       9 MB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   52507 GB |   52506 GB |
|       from large pool |  400448 KB |    1770 MB |   52460 GB |   52459 GB |
|       from small pool |    3549 KB |       9 MB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   32308 GB |   32307 GB |
|       from large pool |  244672 KB |  473024 KB |   32254 GB |   32253 GB |
|       from small pool |    4642 KB |    4843 KB |      54 GB |      54 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2241 K  |    2241 K  |
|       from large pool |      36    |      77    |    1083 K  |    1083 K  |
|       from small pool |     186    |     224    |    1157 K  |    1157 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2241 K  |    2241 K  |
|       from large pool |      36    |      77    |    1083 K  |    1083 K  |
|       from small pool |     186    |     224    |    1157 K  |    1157 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      33    |    1141 K  |    1141 K  |
|       from large pool |      10    |      11    |     448 K  |     448 K  |
|       from small pool |      14    |      23    |     692 K  |     692 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 35, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 36 [256/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 36 [512/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 36 [768/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 36 [1024/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 36 [1280/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 36 [1536/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 36 [1792/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 36 [2048/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 36 [2304/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 36 [2560/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 36 [2816/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 36 [3072/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 36 [3328/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 36 [3584/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 36 [3840/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 36 [4096/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 36 [4352/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 36 [4608/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 36 [4864/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 36 [5120/50000]	Loss: 4.6024	LR: 1.000000
Training Epoch: 36 [5376/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 36 [5632/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 36 [5888/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 36 [6144/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 36 [6400/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 36 [6656/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 36 [6912/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 36 [7168/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 36 [7424/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 36 [7680/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 36 [7936/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 36 [8192/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 36 [8448/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 36 [8704/50000]	Loss: 4.6358	LR: 1.000000
Training Epoch: 36 [8960/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 36 [9216/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 36 [9472/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 36 [9728/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 36 [9984/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 36 [10240/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 36 [10496/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 36 [10752/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 36 [11008/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 36 [11264/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 36 [11520/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 36 [11776/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 36 [12032/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 36 [12288/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 36 [12544/50000]	Loss: 4.6008	LR: 1.000000
Training Epoch: 36 [12800/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 36 [13056/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 36 [13312/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 36 [13568/50000]	Loss: 4.6294	LR: 1.000000
Training Epoch: 36 [13824/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 36 [14080/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 36 [14336/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 36 [14592/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 36 [14848/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 36 [15104/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 36 [15360/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 36 [15616/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 36 [15872/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 36 [16128/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 36 [16384/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 36 [16640/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 36 [16896/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 36 [17152/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 36 [17408/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 36 [17664/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 36 [17920/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 36 [18176/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 36 [18432/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 36 [18688/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 36 [18944/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 36 [19200/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 36 [19456/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 36 [19712/50000]	Loss: 4.6005	LR: 1.000000
Training Epoch: 36 [19968/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 36 [20224/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 36 [20480/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 36 [20736/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 36 [20992/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 36 [21248/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 36 [21504/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 36 [21760/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 36 [22016/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 36 [22272/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 36 [22528/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 36 [22784/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 36 [23040/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 36 [23296/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 36 [23552/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 36 [23808/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 36 [24064/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 36 [24320/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 36 [24576/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 36 [24832/50000]	Loss: 4.6296	LR: 1.000000
Training Epoch: 36 [25088/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 36 [25344/50000]	Loss: 4.5971	LR: 1.000000
Training Epoch: 36 [25600/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 36 [25856/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 36 [26112/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 36 [26368/50000]	Loss: 4.6290	LR: 1.000000
Training Epoch: 36 [26624/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 36 [26880/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 36 [27136/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 36 [27392/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 36 [27648/50000]	Loss: 4.6371	LR: 1.000000
Training Epoch: 36 [27904/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 36 [28160/50000]	Loss: 4.6307	LR: 1.000000
Training Epoch: 36 [28416/50000]	Loss: 4.5969	LR: 1.000000
Training Epoch: 36 [28672/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 36 [28928/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 36 [29184/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 36 [29440/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 36 [29696/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 36 [29952/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 36 [30208/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 36 [30464/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 36 [30720/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 36 [30976/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 36 [31232/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 36 [31488/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 36 [31744/50000]	Loss: 4.6399	LR: 1.000000
Training Epoch: 36 [32000/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 36 [32256/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 36 [32512/50000]	Loss: 4.6286	LR: 1.000000
Training Epoch: 36 [32768/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 36 [33024/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 36 [33280/50000]	Loss: 4.6321	LR: 1.000000
Training Epoch: 36 [33536/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 36 [33792/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 36 [34048/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 36 [34304/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 36 [34560/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 36 [34816/50000]	Loss: 4.5972	LR: 1.000000
Training Epoch: 36 [35072/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 36 [35328/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 36 [35584/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 36 [35840/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 36 [36096/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 36 [36352/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 36 [36608/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 36 [36864/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 36 [37120/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 36 [37376/50000]	Loss: 4.6284	LR: 1.000000
Training Epoch: 36 [37632/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 36 [37888/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 36 [38144/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 36 [38400/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 36 [38656/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 36 [38912/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 36 [39168/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 36 [39424/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 36 [39680/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 36 [39936/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 36 [40192/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 36 [40448/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 36 [40704/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 36 [40960/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 36 [41216/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 36 [41472/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 36 [41728/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 36 [41984/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 36 [42240/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 36 [42496/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 36 [42752/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 36 [43008/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 36 [43264/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 36 [43520/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 36 [43776/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 36 [44032/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 36 [44288/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 36 [44544/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 36 [44800/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 36 [45056/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 36 [45312/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 36 [45568/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 36 [45824/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 36 [46080/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 36 [46336/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 36 [46592/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 36 [46848/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 36 [47104/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 36 [47360/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 36 [47616/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 36 [47872/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 36 [48128/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 36 [48384/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 36 [48640/50000]	Loss: 4.6282	LR: 1.000000
Training Epoch: 36 [48896/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 36 [49152/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 36 [49408/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 36 [49664/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 36 [49920/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 36 [50000/50000]	Loss: 4.6239	LR: 1.000000
epoch 36 training time consumed: 21.88s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   54007 GB |   54007 GB |
|       from large pool |  400448 KB |    1770 MB |   53958 GB |   53958 GB |
|       from small pool |    3549 KB |       9 MB |      48 GB |      48 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   54007 GB |   54007 GB |
|       from large pool |  400448 KB |    1770 MB |   53958 GB |   53958 GB |
|       from small pool |    3549 KB |       9 MB |      48 GB |      48 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   33231 GB |   33230 GB |
|       from large pool |  244672 KB |  473024 KB |   33175 GB |   33175 GB |
|       from small pool |    2594 KB |    4843 KB |      55 GB |      55 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2305 K  |    2305 K  |
|       from large pool |      36    |      77    |    1114 K  |    1114 K  |
|       from small pool |     186    |     224    |    1190 K  |    1190 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2305 K  |    2305 K  |
|       from large pool |      36    |      77    |    1114 K  |    1114 K  |
|       from small pool |     186    |     224    |    1190 K  |    1190 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    1174 K  |    1174 K  |
|       from large pool |      10    |      11    |     461 K  |     461 K  |
|       from small pool |      13    |      23    |     712 K  |     712 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 36, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 37 [256/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 37 [512/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 37 [768/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 37 [1024/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 37 [1280/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 37 [1536/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 37 [1792/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 37 [2048/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 37 [2304/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 37 [2560/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 37 [2816/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 37 [3072/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 37 [3328/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 37 [3584/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 37 [3840/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 37 [4096/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 37 [4352/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 37 [4608/50000]	Loss: 4.6284	LR: 1.000000
Training Epoch: 37 [4864/50000]	Loss: 4.6393	LR: 1.000000
Training Epoch: 37 [5120/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 37 [5376/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 37 [5632/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 37 [5888/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 37 [6144/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 37 [6400/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 37 [6656/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 37 [6912/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 37 [7168/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 37 [7424/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 37 [7680/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 37 [7936/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 37 [8192/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 37 [8448/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 37 [8704/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 37 [8960/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 37 [9216/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 37 [9472/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 37 [9728/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 37 [9984/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 37 [10240/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 37 [10496/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 37 [10752/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 37 [11008/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 37 [11264/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 37 [11520/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 37 [11776/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 37 [12032/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 37 [12288/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 37 [12544/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 37 [12800/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 37 [13056/50000]	Loss: 4.6276	LR: 1.000000
Training Epoch: 37 [13312/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 37 [13568/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 37 [13824/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 37 [14080/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 37 [14336/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 37 [14592/50000]	Loss: 4.6297	LR: 1.000000
Training Epoch: 37 [14848/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 37 [15104/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 37 [15360/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 37 [15616/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 37 [15872/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 37 [16128/50000]	Loss: 4.5988	LR: 1.000000
Training Epoch: 37 [16384/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 37 [16640/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 37 [16896/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 37 [17152/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 37 [17408/50000]	Loss: 4.5998	LR: 1.000000
Training Epoch: 37 [17664/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 37 [17920/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 37 [18176/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 37 [18432/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 37 [18688/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 37 [18944/50000]	Loss: 4.6348	LR: 1.000000
Training Epoch: 37 [19200/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 37 [19456/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 37 [19712/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 37 [19968/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 37 [20224/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 37 [20480/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 37 [20736/50000]	Loss: 4.5919	LR: 1.000000
Training Epoch: 37 [20992/50000]	Loss: 4.6253	LR: 1.000000
Training Epoch: 37 [21248/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 37 [21504/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 37 [21760/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 37 [22016/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 37 [22272/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 37 [22528/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 37 [22784/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 37 [23040/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 37 [23296/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 37 [23552/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 37 [23808/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 37 [24064/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 37 [24320/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 37 [24576/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 37 [24832/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 37 [25088/50000]	Loss: 4.5948	LR: 1.000000
Training Epoch: 37 [25344/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 37 [25600/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 37 [25856/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 37 [26112/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 37 [26368/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 37 [26624/50000]	Loss: 4.6394	LR: 1.000000
Training Epoch: 37 [26880/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 37 [27136/50000]	Loss: 4.6317	LR: 1.000000
Training Epoch: 37 [27392/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 37 [27648/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 37 [27904/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 37 [28160/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 37 [28416/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 37 [28672/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 37 [28928/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 37 [29184/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 37 [29440/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 37 [29696/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 37 [29952/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 37 [30208/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 37 [30464/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 37 [30720/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 37 [30976/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 37 [31232/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 37 [31488/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 37 [31744/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 37 [32000/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 37 [32256/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 37 [32512/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 37 [32768/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 37 [33024/50000]	Loss: 4.6273	LR: 1.000000
Training Epoch: 37 [33280/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 37 [33536/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 37 [33792/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 37 [34048/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 37 [34304/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 37 [34560/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 37 [34816/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 37 [35072/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 37 [35328/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 37 [35584/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 37 [35840/50000]	Loss: 4.6273	LR: 1.000000
Training Epoch: 37 [36096/50000]	Loss: 4.5981	LR: 1.000000
Training Epoch: 37 [36352/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 37 [36608/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 37 [36864/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 37 [37120/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 37 [37376/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 37 [37632/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 37 [37888/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 37 [38144/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 37 [38400/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 37 [38656/50000]	Loss: 4.6328	LR: 1.000000
Training Epoch: 37 [38912/50000]	Loss: 4.5958	LR: 1.000000
Training Epoch: 37 [39168/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 37 [39424/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 37 [39680/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 37 [39936/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 37 [40192/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 37 [40448/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 37 [40704/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 37 [40960/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 37 [41216/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 37 [41472/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 37 [41728/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 37 [41984/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 37 [42240/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 37 [42496/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 37 [42752/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 37 [43008/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 37 [43264/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 37 [43520/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 37 [43776/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 37 [44032/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 37 [44288/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 37 [44544/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 37 [44800/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 37 [45056/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 37 [45312/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 37 [45568/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 37 [45824/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 37 [46080/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 37 [46336/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 37 [46592/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 37 [46848/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 37 [47104/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 37 [47360/50000]	Loss: 4.6006	LR: 1.000000
Training Epoch: 37 [47616/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 37 [47872/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 37 [48128/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 37 [48384/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 37 [48640/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 37 [48896/50000]	Loss: 4.5980	LR: 1.000000
Training Epoch: 37 [49152/50000]	Loss: 4.5968	LR: 1.000000
Training Epoch: 37 [49408/50000]	Loss: 4.6322	LR: 1.000000
Training Epoch: 37 [49664/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 37 [49920/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 37 [50000/50000]	Loss: 4.6228	LR: 1.000000
epoch 37 training time consumed: 21.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   55507 GB |   55507 GB |
|       from large pool |  400448 KB |    1770 MB |   55457 GB |   55457 GB |
|       from small pool |    3549 KB |       9 MB |      49 GB |      49 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   55507 GB |   55507 GB |
|       from large pool |  400448 KB |    1770 MB |   55457 GB |   55457 GB |
|       from small pool |    3549 KB |       9 MB |      49 GB |      49 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   34154 GB |   34153 GB |
|       from large pool |  244672 KB |  473024 KB |   34097 GB |   34096 GB |
|       from small pool |    2594 KB |    4843 KB |      57 GB |      57 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2369 K  |    2369 K  |
|       from large pool |      36    |      77    |    1145 K  |    1145 K  |
|       from small pool |     186    |     224    |    1223 K  |    1223 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2369 K  |    2369 K  |
|       from large pool |      36    |      77    |    1145 K  |    1145 K  |
|       from small pool |     186    |     224    |    1223 K  |    1223 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1206 K  |    1206 K  |
|       from large pool |      10    |      11    |     474 K  |     474 K  |
|       from small pool |      10    |      23    |     731 K  |     731 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 37, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 38 [256/50000]	Loss: 4.5983	LR: 1.000000
Training Epoch: 38 [512/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 38 [768/50000]	Loss: 4.6299	LR: 1.000000
Training Epoch: 38 [1024/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 38 [1280/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 38 [1536/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 38 [1792/50000]	Loss: 4.6350	LR: 1.000000
Training Epoch: 38 [2048/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 38 [2304/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 38 [2560/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 38 [2816/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 38 [3072/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 38 [3328/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 38 [3584/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 38 [3840/50000]	Loss: 4.5942	LR: 1.000000
Training Epoch: 38 [4096/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 38 [4352/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 38 [4608/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 38 [4864/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 38 [5120/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 38 [5376/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 38 [5632/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 38 [5888/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 38 [6144/50000]	Loss: 4.6030	LR: 1.000000
Training Epoch: 38 [6400/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 38 [6656/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 38 [6912/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 38 [7168/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 38 [7424/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 38 [7680/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 38 [7936/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 38 [8192/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 38 [8448/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 38 [8704/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 38 [8960/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 38 [9216/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 38 [9472/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 38 [9728/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 38 [9984/50000]	Loss: 4.6309	LR: 1.000000
Training Epoch: 38 [10240/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 38 [10496/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 38 [10752/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 38 [11008/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 38 [11264/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 38 [11520/50000]	Loss: 4.6299	LR: 1.000000
Training Epoch: 38 [11776/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 38 [12032/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 38 [12288/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 38 [12544/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 38 [12800/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 38 [13056/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 38 [13312/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 38 [13568/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 38 [13824/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 38 [14080/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 38 [14336/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 38 [14592/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 38 [14848/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 38 [15104/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 38 [15360/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 38 [15616/50000]	Loss: 4.6306	LR: 1.000000
Training Epoch: 38 [15872/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 38 [16128/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 38 [16384/50000]	Loss: 4.6285	LR: 1.000000
Training Epoch: 38 [16640/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 38 [16896/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 38 [17152/50000]	Loss: 4.6314	LR: 1.000000
Training Epoch: 38 [17408/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 38 [17664/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 38 [17920/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 38 [18176/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 38 [18432/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 38 [18688/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 38 [18944/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 38 [19200/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 38 [19456/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 38 [19712/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 38 [19968/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 38 [20224/50000]	Loss: 4.5978	LR: 1.000000
Training Epoch: 38 [20480/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 38 [20736/50000]	Loss: 4.5877	LR: 1.000000
Training Epoch: 38 [20992/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 38 [21248/50000]	Loss: 4.6312	LR: 1.000000
Training Epoch: 38 [21504/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 38 [21760/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 38 [22016/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 38 [22272/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 38 [22528/50000]	Loss: 4.6334	LR: 1.000000
Training Epoch: 38 [22784/50000]	Loss: 4.6304	LR: 1.000000
Training Epoch: 38 [23040/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 38 [23296/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 38 [23552/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 38 [23808/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 38 [24064/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 38 [24320/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 38 [24576/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 38 [24832/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 38 [25088/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 38 [25344/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 38 [25600/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 38 [25856/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 38 [26112/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 38 [26368/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 38 [26624/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 38 [26880/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 38 [27136/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 38 [27392/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 38 [27648/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 38 [27904/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 38 [28160/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 38 [28416/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 38 [28672/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 38 [28928/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 38 [29184/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 38 [29440/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 38 [29696/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 38 [29952/50000]	Loss: 4.6276	LR: 1.000000
Training Epoch: 38 [30208/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 38 [30464/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 38 [30720/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 38 [30976/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 38 [31232/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 38 [31488/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 38 [31744/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 38 [32000/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 38 [32256/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 38 [32512/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 38 [32768/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 38 [33024/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 38 [33280/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 38 [33536/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 38 [33792/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 38 [34048/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 38 [34304/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 38 [34560/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 38 [34816/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 38 [35072/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 38 [35328/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 38 [35584/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 38 [35840/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 38 [36096/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 38 [36352/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 38 [36608/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 38 [36864/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 38 [37120/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 38 [37376/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 38 [37632/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 38 [37888/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 38 [38144/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 38 [38400/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 38 [38656/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 38 [38912/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 38 [39168/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 38 [39424/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 38 [39680/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 38 [39936/50000]	Loss: 4.6360	LR: 1.000000
Training Epoch: 38 [40192/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 38 [40448/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 38 [40704/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 38 [40960/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 38 [41216/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 38 [41472/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 38 [41728/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 38 [41984/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 38 [42240/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 38 [42496/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 38 [42752/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 38 [43008/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 38 [43264/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 38 [43520/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 38 [43776/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 38 [44032/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 38 [44288/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 38 [44544/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 38 [44800/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 38 [45056/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 38 [45312/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 38 [45568/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 38 [45824/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 38 [46080/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 38 [46336/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 38 [46592/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 38 [46848/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 38 [47104/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 38 [47360/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 38 [47616/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 38 [47872/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 38 [48128/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 38 [48384/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 38 [48640/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 38 [48896/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 38 [49152/50000]	Loss: 4.5961	LR: 1.000000
Training Epoch: 38 [49408/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 38 [49664/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 38 [49920/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 38 [50000/50000]	Loss: 4.6153	LR: 1.000000
epoch 38 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   57007 GB |   57007 GB |
|       from large pool |  400448 KB |    1770 MB |   56956 GB |   56956 GB |
|       from small pool |    3549 KB |       9 MB |      51 GB |      51 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   57007 GB |   57007 GB |
|       from large pool |  400448 KB |    1770 MB |   56956 GB |   56956 GB |
|       from small pool |    3549 KB |       9 MB |      51 GB |      51 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   35077 GB |   35076 GB |
|       from large pool |  244672 KB |  473024 KB |   35018 GB |   35018 GB |
|       from small pool |    2594 KB |    4843 KB |      58 GB |      58 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2433 K  |    2433 K  |
|       from large pool |      36    |      77    |    1176 K  |    1176 K  |
|       from small pool |     186    |     224    |    1256 K  |    1256 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2433 K  |    2433 K  |
|       from large pool |      36    |      77    |    1176 K  |    1176 K  |
|       from small pool |     186    |     224    |    1256 K  |    1256 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1238 K  |    1238 K  |
|       from large pool |      10    |      11    |     487 K  |     487 K  |
|       from small pool |      11    |      23    |     751 K  |     751 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 38, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 39 [256/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 39 [512/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 39 [768/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 39 [1024/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 39 [1280/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 39 [1536/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 39 [1792/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 39 [2048/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 39 [2304/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 39 [2560/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 39 [2816/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 39 [3072/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 39 [3328/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 39 [3584/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 39 [3840/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 39 [4096/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 39 [4352/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 39 [4608/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 39 [4864/50000]	Loss: 4.5967	LR: 1.000000
Training Epoch: 39 [5120/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 39 [5376/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 39 [5632/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 39 [5888/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 39 [6144/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 39 [6400/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 39 [6656/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 39 [6912/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 39 [7168/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 39 [7424/50000]	Loss: 4.6347	LR: 1.000000
Training Epoch: 39 [7680/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 39 [7936/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 39 [8192/50000]	Loss: 4.6312	LR: 1.000000
Training Epoch: 39 [8448/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 39 [8704/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 39 [8960/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 39 [9216/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 39 [9472/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 39 [9728/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 39 [9984/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 39 [10240/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 39 [10496/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 39 [10752/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 39 [11008/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 39 [11264/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 39 [11520/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 39 [11776/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 39 [12032/50000]	Loss: 4.6305	LR: 1.000000
Training Epoch: 39 [12288/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 39 [12544/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 39 [12800/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 39 [13056/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 39 [13312/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 39 [13568/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 39 [13824/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 39 [14080/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 39 [14336/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 39 [14592/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 39 [14848/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 39 [15104/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 39 [15360/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 39 [15616/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 39 [15872/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 39 [16128/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 39 [16384/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 39 [16640/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 39 [16896/50000]	Loss: 4.6001	LR: 1.000000
Training Epoch: 39 [17152/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 39 [17408/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 39 [17664/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 39 [17920/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 39 [18176/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 39 [18432/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 39 [18688/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 39 [18944/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 39 [19200/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 39 [19456/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 39 [19712/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 39 [19968/50000]	Loss: 4.6004	LR: 1.000000
Training Epoch: 39 [20224/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 39 [20480/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 39 [20736/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 39 [20992/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 39 [21248/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 39 [21504/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 39 [21760/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 39 [22016/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 39 [22272/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 39 [22528/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 39 [22784/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 39 [23040/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 39 [23296/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 39 [23552/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 39 [23808/50000]	Loss: 4.6325	LR: 1.000000
Training Epoch: 39 [24064/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 39 [24320/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 39 [24576/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 39 [24832/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 39 [25088/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 39 [25344/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 39 [25600/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 39 [25856/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 39 [26112/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 39 [26368/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 39 [26624/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 39 [26880/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 39 [27136/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 39 [27392/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 39 [27648/50000]	Loss: 4.5987	LR: 1.000000
Training Epoch: 39 [27904/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 39 [28160/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 39 [28416/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 39 [28672/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 39 [28928/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 39 [29184/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 39 [29440/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 39 [29696/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 39 [29952/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 39 [30208/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 39 [30464/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 39 [30720/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 39 [30976/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 39 [31232/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 39 [31488/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 39 [31744/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 39 [32000/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 39 [32256/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 39 [32512/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 39 [32768/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 39 [33024/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 39 [33280/50000]	Loss: 4.6306	LR: 1.000000
Training Epoch: 39 [33536/50000]	Loss: 4.5964	LR: 1.000000
Training Epoch: 39 [33792/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 39 [34048/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 39 [34304/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 39 [34560/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 39 [34816/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 39 [35072/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 39 [35328/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 39 [35584/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 39 [35840/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 39 [36096/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 39 [36352/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 39 [36608/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 39 [36864/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 39 [37120/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 39 [37376/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 39 [37632/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 39 [37888/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 39 [38144/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 39 [38400/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 39 [38656/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 39 [38912/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 39 [39168/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 39 [39424/50000]	Loss: 4.6327	LR: 1.000000
Training Epoch: 39 [39680/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 39 [39936/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 39 [40192/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 39 [40448/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 39 [40704/50000]	Loss: 4.6299	LR: 1.000000
Training Epoch: 39 [40960/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 39 [41216/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 39 [41472/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 39 [41728/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 39 [41984/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 39 [42240/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 39 [42496/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 39 [42752/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 39 [43008/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 39 [43264/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 39 [43520/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 39 [43776/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 39 [44032/50000]	Loss: 4.6291	LR: 1.000000
Training Epoch: 39 [44288/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 39 [44544/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 39 [44800/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 39 [45056/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 39 [45312/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 39 [45568/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 39 [45824/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 39 [46080/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 39 [46336/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 39 [46592/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 39 [46848/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 39 [47104/50000]	Loss: 4.6316	LR: 1.000000
Training Epoch: 39 [47360/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 39 [47616/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 39 [47872/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 39 [48128/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 39 [48384/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 39 [48640/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 39 [48896/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 39 [49152/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 39 [49408/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 39 [49664/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 39 [49920/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 39 [50000/50000]	Loss: 4.6420	LR: 1.000000
epoch 39 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   58508 GB |   58507 GB |
|       from large pool |  400448 KB |    1770 MB |   58455 GB |   58455 GB |
|       from small pool |    3549 KB |       9 MB |      52 GB |      52 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   58508 GB |   58507 GB |
|       from large pool |  400448 KB |    1770 MB |   58455 GB |   58455 GB |
|       from small pool |    3549 KB |       9 MB |      52 GB |      52 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   36000 GB |   35999 GB |
|       from large pool |  244672 KB |  473024 KB |   35939 GB |   35939 GB |
|       from small pool |    2594 KB |    4843 KB |      60 GB |      60 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2497 K  |    2497 K  |
|       from large pool |      36    |      77    |    1207 K  |    1207 K  |
|       from small pool |     186    |     224    |    1289 K  |    1289 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2497 K  |    2497 K  |
|       from large pool |      36    |      77    |    1207 K  |    1207 K  |
|       from small pool |     186    |     224    |    1289 K  |    1289 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1270 K  |    1270 K  |
|       from large pool |      10    |      11    |     500 K  |     500 K  |
|       from small pool |      10    |      23    |     770 K  |     770 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 39, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 40 [256/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 40 [512/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 40 [768/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 40 [1024/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 40 [1280/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 40 [1536/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 40 [1792/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 40 [2048/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 40 [2304/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 40 [2560/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 40 [2816/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 40 [3072/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 40 [3328/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 40 [3584/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 40 [3840/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 40 [4096/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 40 [4352/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 40 [4608/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 40 [4864/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 40 [5120/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 40 [5376/50000]	Loss: 4.6024	LR: 1.000000
Training Epoch: 40 [5632/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 40 [5888/50000]	Loss: 4.6355	LR: 1.000000
Training Epoch: 40 [6144/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 40 [6400/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 40 [6656/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 40 [6912/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 40 [7168/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 40 [7424/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 40 [7680/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 40 [7936/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 40 [8192/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 40 [8448/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 40 [8704/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 40 [8960/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 40 [9216/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 40 [9472/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 40 [9728/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 40 [9984/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 40 [10240/50000]	Loss: 4.5963	LR: 1.000000
Training Epoch: 40 [10496/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 40 [10752/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 40 [11008/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 40 [11264/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 40 [11520/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 40 [11776/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 40 [12032/50000]	Loss: 4.5970	LR: 1.000000
Training Epoch: 40 [12288/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 40 [12544/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 40 [12800/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 40 [13056/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 40 [13312/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 40 [13568/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 40 [13824/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 40 [14080/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 40 [14336/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 40 [14592/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 40 [14848/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 40 [15104/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 40 [15360/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 40 [15616/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 40 [15872/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 40 [16128/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 40 [16384/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 40 [16640/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 40 [16896/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 40 [17152/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 40 [17408/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 40 [17664/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 40 [17920/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 40 [18176/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 40 [18432/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 40 [18688/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 40 [18944/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 40 [19200/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 40 [19456/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 40 [19712/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 40 [19968/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 40 [20224/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 40 [20480/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 40 [20736/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 40 [20992/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 40 [21248/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 40 [21504/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 40 [21760/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 40 [22016/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 40 [22272/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 40 [22528/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 40 [22784/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 40 [23040/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 40 [23296/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 40 [23552/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 40 [23808/50000]	Loss: 4.5962	LR: 1.000000
Training Epoch: 40 [24064/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 40 [24320/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 40 [24576/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 40 [24832/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 40 [25088/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 40 [25344/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 40 [25600/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 40 [25856/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 40 [26112/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 40 [26368/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 40 [26624/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 40 [26880/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 40 [27136/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 40 [27392/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 40 [27648/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 40 [27904/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 40 [28160/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 40 [28416/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 40 [28672/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 40 [28928/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 40 [29184/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 40 [29440/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 40 [29696/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 40 [29952/50000]	Loss: 4.5901	LR: 1.000000
Training Epoch: 40 [30208/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 40 [30464/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 40 [30720/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 40 [30976/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 40 [31232/50000]	Loss: 4.6352	LR: 1.000000
Training Epoch: 40 [31488/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 40 [31744/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 40 [32000/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 40 [32256/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 40 [32512/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 40 [32768/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 40 [33024/50000]	Loss: 4.6341	LR: 1.000000
Training Epoch: 40 [33280/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 40 [33536/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 40 [33792/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 40 [34048/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 40 [34304/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 40 [34560/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 40 [34816/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 40 [35072/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 40 [35328/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 40 [35584/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 40 [35840/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 40 [36096/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 40 [36352/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 40 [36608/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 40 [36864/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 40 [37120/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 40 [37376/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 40 [37632/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 40 [37888/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 40 [38144/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 40 [38400/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 40 [38656/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 40 [38912/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 40 [39168/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 40 [39424/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 40 [39680/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 40 [39936/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 40 [40192/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 40 [40448/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 40 [40704/50000]	Loss: 4.6320	LR: 1.000000
Training Epoch: 40 [40960/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 40 [41216/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 40 [41472/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 40 [41728/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 40 [41984/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 40 [42240/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 40 [42496/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 40 [42752/50000]	Loss: 4.5937	LR: 1.000000
Training Epoch: 40 [43008/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 40 [43264/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 40 [43520/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 40 [43776/50000]	Loss: 4.5963	LR: 1.000000
Training Epoch: 40 [44032/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 40 [44288/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 40 [44544/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 40 [44800/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 40 [45056/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 40 [45312/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 40 [45568/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 40 [45824/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 40 [46080/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 40 [46336/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 40 [46592/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 40 [46848/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 40 [47104/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 40 [47360/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 40 [47616/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 40 [47872/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 40 [48128/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 40 [48384/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 40 [48640/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 40 [48896/50000]	Loss: 4.5953	LR: 1.000000
Training Epoch: 40 [49152/50000]	Loss: 4.5987	LR: 1.000000
Training Epoch: 40 [49408/50000]	Loss: 4.6329	LR: 1.000000
Training Epoch: 40 [49664/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 40 [49920/50000]	Loss: 4.6290	LR: 1.000000
Training Epoch: 40 [50000/50000]	Loss: 4.6370	LR: 1.000000
epoch 40 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   60008 GB |   60007 GB |
|       from large pool |  400448 KB |    1770 MB |   59954 GB |   59953 GB |
|       from small pool |    3549 KB |       9 MB |      54 GB |      54 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   60008 GB |   60007 GB |
|       from large pool |  400448 KB |    1770 MB |   59954 GB |   59953 GB |
|       from small pool |    3549 KB |       9 MB |      54 GB |      54 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   36923 GB |   36922 GB |
|       from large pool |  244672 KB |  473024 KB |   36861 GB |   36861 GB |
|       from small pool |    2594 KB |    4843 KB |      61 GB |      61 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2561 K  |    2561 K  |
|       from large pool |      36    |      77    |    1238 K  |    1238 K  |
|       from small pool |     186    |     224    |    1322 K  |    1322 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2561 K  |    2561 K  |
|       from large pool |      36    |      77    |    1238 K  |    1238 K  |
|       from small pool |     186    |     224    |    1322 K  |    1322 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    1303 K  |    1303 K  |
|       from large pool |      10    |      11    |     512 K  |     512 K  |
|       from small pool |      13    |      23    |     790 K  |     790 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 40, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.43s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-40-regular.pth
Training Epoch: 41 [256/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 41 [512/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 41 [768/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 41 [1024/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 41 [1280/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 41 [1536/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 41 [1792/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 41 [2048/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 41 [2304/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 41 [2560/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 41 [2816/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 41 [3072/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 41 [3328/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 41 [3584/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 41 [3840/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 41 [4096/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 41 [4352/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 41 [4608/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 41 [4864/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 41 [5120/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 41 [5376/50000]	Loss: 4.6286	LR: 1.000000
Training Epoch: 41 [5632/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 41 [5888/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 41 [6144/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 41 [6400/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 41 [6656/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 41 [6912/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 41 [7168/50000]	Loss: 4.6359	LR: 1.000000
Training Epoch: 41 [7424/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 41 [7680/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 41 [7936/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 41 [8192/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 41 [8448/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 41 [8704/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 41 [8960/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 41 [9216/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 41 [9472/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 41 [9728/50000]	Loss: 4.5972	LR: 1.000000
Training Epoch: 41 [9984/50000]	Loss: 4.6328	LR: 1.000000
Training Epoch: 41 [10240/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 41 [10496/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 41 [10752/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 41 [11008/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 41 [11264/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 41 [11520/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 41 [11776/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 41 [12032/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 41 [12288/50000]	Loss: 4.6297	LR: 1.000000
Training Epoch: 41 [12544/50000]	Loss: 4.6356	LR: 1.000000
Training Epoch: 41 [12800/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 41 [13056/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 41 [13312/50000]	Loss: 4.6373	LR: 1.000000
Training Epoch: 41 [13568/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 41 [13824/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 41 [14080/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 41 [14336/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 41 [14592/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 41 [14848/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 41 [15104/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 41 [15360/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 41 [15616/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 41 [15872/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 41 [16128/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 41 [16384/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 41 [16640/50000]	Loss: 4.5954	LR: 1.000000
Training Epoch: 41 [16896/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 41 [17152/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 41 [17408/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 41 [17664/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 41 [17920/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 41 [18176/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 41 [18432/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 41 [18688/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 41 [18944/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 41 [19200/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 41 [19456/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 41 [19712/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 41 [19968/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 41 [20224/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 41 [20480/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 41 [20736/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 41 [20992/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 41 [21248/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 41 [21504/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 41 [21760/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 41 [22016/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 41 [22272/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 41 [22528/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 41 [22784/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 41 [23040/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 41 [23296/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 41 [23552/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 41 [23808/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 41 [24064/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 41 [24320/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 41 [24576/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 41 [24832/50000]	Loss: 4.5951	LR: 1.000000
Training Epoch: 41 [25088/50000]	Loss: 4.6291	LR: 1.000000
Training Epoch: 41 [25344/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 41 [25600/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 41 [25856/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 41 [26112/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 41 [26368/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 41 [26624/50000]	Loss: 4.6346	LR: 1.000000
Training Epoch: 41 [26880/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 41 [27136/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 41 [27392/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 41 [27648/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 41 [27904/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 41 [28160/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 41 [28416/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 41 [28672/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 41 [28928/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 41 [29184/50000]	Loss: 4.5953	LR: 1.000000
Training Epoch: 41 [29440/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 41 [29696/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 41 [29952/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 41 [30208/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 41 [30464/50000]	Loss: 4.5869	LR: 1.000000
Training Epoch: 41 [30720/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 41 [30976/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 41 [31232/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 41 [31488/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 41 [31744/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 41 [32000/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 41 [32256/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 41 [32512/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 41 [32768/50000]	Loss: 4.6334	LR: 1.000000
Training Epoch: 41 [33024/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 41 [33280/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 41 [33536/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 41 [33792/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 41 [34048/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 41 [34304/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 41 [34560/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 41 [34816/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 41 [35072/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 41 [35328/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 41 [35584/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 41 [35840/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 41 [36096/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 41 [36352/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 41 [36608/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 41 [36864/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 41 [37120/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 41 [37376/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 41 [37632/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 41 [37888/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 41 [38144/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 41 [38400/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 41 [38656/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 41 [38912/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 41 [39168/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 41 [39424/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 41 [39680/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 41 [39936/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 41 [40192/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 41 [40448/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 41 [40704/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 41 [40960/50000]	Loss: 4.5990	LR: 1.000000
Training Epoch: 41 [41216/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 41 [41472/50000]	Loss: 4.5971	LR: 1.000000
Training Epoch: 41 [41728/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 41 [41984/50000]	Loss: 4.5936	LR: 1.000000
Training Epoch: 41 [42240/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 41 [42496/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 41 [42752/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 41 [43008/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 41 [43264/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 41 [43520/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 41 [43776/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 41 [44032/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 41 [44288/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 41 [44544/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 41 [44800/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 41 [45056/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 41 [45312/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 41 [45568/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 41 [45824/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 41 [46080/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 41 [46336/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 41 [46592/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 41 [46848/50000]	Loss: 4.5990	LR: 1.000000
Training Epoch: 41 [47104/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 41 [47360/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 41 [47616/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 41 [47872/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 41 [48128/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 41 [48384/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 41 [48640/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 41 [48896/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 41 [49152/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 41 [49408/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 41 [49664/50000]	Loss: 4.6362	LR: 1.000000
Training Epoch: 41 [49920/50000]	Loss: 4.6377	LR: 1.000000
Training Epoch: 41 [50000/50000]	Loss: 4.6108	LR: 1.000000
epoch 41 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   61508 GB |   61508 GB |
|       from large pool |  400448 KB |    1770 MB |   61453 GB |   61452 GB |
|       from small pool |    3549 KB |       9 MB |      55 GB |      55 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   61508 GB |   61508 GB |
|       from large pool |  400448 KB |    1770 MB |   61453 GB |   61452 GB |
|       from small pool |    3549 KB |       9 MB |      55 GB |      55 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   37846 GB |   37846 GB |
|       from large pool |  244672 KB |  473024 KB |   37782 GB |   37782 GB |
|       from small pool |    2594 KB |    4843 KB |      63 GB |      63 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2625 K  |    2625 K  |
|       from large pool |      36    |      77    |    1269 K  |    1269 K  |
|       from small pool |     186    |     224    |    1356 K  |    1355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2625 K  |    2625 K  |
|       from large pool |      36    |      77    |    1269 K  |    1269 K  |
|       from small pool |     186    |     224    |    1356 K  |    1355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1336 K  |    1336 K  |
|       from large pool |      10    |      11    |     525 K  |     525 K  |
|       from small pool |      10    |      23    |     810 K  |     810 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 41, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 42 [256/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 42 [512/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 42 [768/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 42 [1024/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 42 [1280/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 42 [1536/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 42 [1792/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 42 [2048/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 42 [2304/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 42 [2560/50000]	Loss: 4.6349	LR: 1.000000
Training Epoch: 42 [2816/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 42 [3072/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 42 [3328/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 42 [3584/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 42 [3840/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 42 [4096/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 42 [4352/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 42 [4608/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 42 [4864/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 42 [5120/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 42 [5376/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 42 [5632/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 42 [5888/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 42 [6144/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 42 [6400/50000]	Loss: 4.6023	LR: 1.000000
Training Epoch: 42 [6656/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 42 [6912/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 42 [7168/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 42 [7424/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 42 [7680/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 42 [7936/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 42 [8192/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 42 [8448/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 42 [8704/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 42 [8960/50000]	Loss: 4.5985	LR: 1.000000
Training Epoch: 42 [9216/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 42 [9472/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 42 [9728/50000]	Loss: 4.6333	LR: 1.000000
Training Epoch: 42 [9984/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 42 [10240/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 42 [10496/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 42 [10752/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 42 [11008/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 42 [11264/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 42 [11520/50000]	Loss: 4.6323	LR: 1.000000
Training Epoch: 42 [11776/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 42 [12032/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 42 [12288/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 42 [12544/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 42 [12800/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 42 [13056/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 42 [13312/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 42 [13568/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 42 [13824/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 42 [14080/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 42 [14336/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 42 [14592/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 42 [14848/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 42 [15104/50000]	Loss: 4.5983	LR: 1.000000
Training Epoch: 42 [15360/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 42 [15616/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 42 [15872/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 42 [16128/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 42 [16384/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 42 [16640/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 42 [16896/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 42 [17152/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 42 [17408/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 42 [17664/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 42 [17920/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 42 [18176/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 42 [18432/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 42 [18688/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 42 [18944/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 42 [19200/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 42 [19456/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 42 [19712/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 42 [19968/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 42 [20224/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 42 [20480/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 42 [20736/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 42 [20992/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 42 [21248/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 42 [21504/50000]	Loss: 4.6017	LR: 1.000000
Training Epoch: 42 [21760/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 42 [22016/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 42 [22272/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 42 [22528/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 42 [22784/50000]	Loss: 4.6321	LR: 1.000000
Training Epoch: 42 [23040/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 42 [23296/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 42 [23552/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 42 [23808/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 42 [24064/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 42 [24320/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 42 [24576/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 42 [24832/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 42 [25088/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 42 [25344/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 42 [25600/50000]	Loss: 4.6291	LR: 1.000000
Training Epoch: 42 [25856/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 42 [26112/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 42 [26368/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 42 [26624/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 42 [26880/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 42 [27136/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 42 [27392/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 42 [27648/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 42 [27904/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 42 [28160/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 42 [28416/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 42 [28672/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 42 [28928/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 42 [29184/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 42 [29440/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 42 [29696/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 42 [29952/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 42 [30208/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 42 [30464/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 42 [30720/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 42 [30976/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 42 [31232/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 42 [31488/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 42 [31744/50000]	Loss: 4.6331	LR: 1.000000
Training Epoch: 42 [32000/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 42 [32256/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 42 [32512/50000]	Loss: 4.6285	LR: 1.000000
Training Epoch: 42 [32768/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 42 [33024/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 42 [33280/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 42 [33536/50000]	Loss: 4.6372	LR: 1.000000
Training Epoch: 42 [33792/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 42 [34048/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 42 [34304/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 42 [34560/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 42 [34816/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 42 [35072/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 42 [35328/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 42 [35584/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 42 [35840/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 42 [36096/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 42 [36352/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 42 [36608/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 42 [36864/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 42 [37120/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 42 [37376/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 42 [37632/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 42 [37888/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 42 [38144/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 42 [38400/50000]	Loss: 4.6001	LR: 1.000000
Training Epoch: 42 [38656/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 42 [38912/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 42 [39168/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 42 [39424/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 42 [39680/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 42 [39936/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 42 [40192/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 42 [40448/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 42 [40704/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 42 [40960/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 42 [41216/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 42 [41472/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 42 [41728/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 42 [41984/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 42 [42240/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 42 [42496/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 42 [42752/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 42 [43008/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 42 [43264/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 42 [43520/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 42 [43776/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 42 [44032/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 42 [44288/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 42 [44544/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 42 [44800/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 42 [45056/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 42 [45312/50000]	Loss: 4.6341	LR: 1.000000
Training Epoch: 42 [45568/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 42 [45824/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 42 [46080/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 42 [46336/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 42 [46592/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 42 [46848/50000]	Loss: 4.6305	LR: 1.000000
Training Epoch: 42 [47104/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 42 [47360/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 42 [47616/50000]	Loss: 4.5981	LR: 1.000000
Training Epoch: 42 [47872/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 42 [48128/50000]	Loss: 4.6005	LR: 1.000000
Training Epoch: 42 [48384/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 42 [48640/50000]	Loss: 4.5987	LR: 1.000000
Training Epoch: 42 [48896/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 42 [49152/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 42 [49408/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 42 [49664/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 42 [49920/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 42 [50000/50000]	Loss: 4.6058	LR: 1.000000
epoch 42 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   63008 GB |   63008 GB |
|       from large pool |  400448 KB |    1770 MB |   62951 GB |   62951 GB |
|       from small pool |    3549 KB |       9 MB |      56 GB |      56 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   63008 GB |   63008 GB |
|       from large pool |  400448 KB |    1770 MB |   62951 GB |   62951 GB |
|       from small pool |    3549 KB |       9 MB |      56 GB |      56 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   38769 GB |   38769 GB |
|       from large pool |  244672 KB |  473024 KB |   38704 GB |   38704 GB |
|       from small pool |    2594 KB |    4843 KB |      64 GB |      64 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2689 K  |    2689 K  |
|       from large pool |      36    |      77    |    1300 K  |    1300 K  |
|       from small pool |     186    |     224    |    1389 K  |    1388 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2689 K  |    2689 K  |
|       from large pool |      36    |      77    |    1300 K  |    1300 K  |
|       from small pool |     186    |     224    |    1389 K  |    1388 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    1368 K  |    1368 K  |
|       from large pool |      10    |      11    |     538 K  |     538 K  |
|       from small pool |      13    |      23    |     830 K  |     830 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 42, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 43 [256/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 43 [512/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 43 [768/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 43 [1024/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 43 [1280/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 43 [1536/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 43 [1792/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 43 [2048/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 43 [2304/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 43 [2560/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 43 [2816/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 43 [3072/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 43 [3328/50000]	Loss: 4.5947	LR: 1.000000
Training Epoch: 43 [3584/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 43 [3840/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 43 [4096/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 43 [4352/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 43 [4608/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 43 [4864/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 43 [5120/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 43 [5376/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 43 [5632/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 43 [5888/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 43 [6144/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 43 [6400/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 43 [6656/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 43 [6912/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 43 [7168/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 43 [7424/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 43 [7680/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 43 [7936/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 43 [8192/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 43 [8448/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 43 [8704/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 43 [8960/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 43 [9216/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 43 [9472/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 43 [9728/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 43 [9984/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 43 [10240/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 43 [10496/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 43 [10752/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 43 [11008/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 43 [11264/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 43 [11520/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 43 [11776/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 43 [12032/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 43 [12288/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 43 [12544/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 43 [12800/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 43 [13056/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 43 [13312/50000]	Loss: 4.6317	LR: 1.000000
Training Epoch: 43 [13568/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 43 [13824/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 43 [14080/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 43 [14336/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 43 [14592/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 43 [14848/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 43 [15104/50000]	Loss: 4.6322	LR: 1.000000
Training Epoch: 43 [15360/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 43 [15616/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 43 [15872/50000]	Loss: 4.6355	LR: 1.000000
Training Epoch: 43 [16128/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 43 [16384/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 43 [16640/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 43 [16896/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 43 [17152/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 43 [17408/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 43 [17664/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 43 [17920/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 43 [18176/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 43 [18432/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 43 [18688/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 43 [18944/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 43 [19200/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 43 [19456/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 43 [19712/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 43 [19968/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 43 [20224/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 43 [20480/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 43 [20736/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 43 [20992/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 43 [21248/50000]	Loss: 4.6314	LR: 1.000000
Training Epoch: 43 [21504/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 43 [21760/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 43 [22016/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 43 [22272/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 43 [22528/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 43 [22784/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 43 [23040/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 43 [23296/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 43 [23552/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 43 [23808/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 43 [24064/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 43 [24320/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 43 [24576/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 43 [24832/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 43 [25088/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 43 [25344/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 43 [25600/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 43 [25856/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 43 [26112/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 43 [26368/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 43 [26624/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 43 [26880/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 43 [27136/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 43 [27392/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 43 [27648/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 43 [27904/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 43 [28160/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 43 [28416/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 43 [28672/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 43 [28928/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 43 [29184/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 43 [29440/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 43 [29696/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 43 [29952/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 43 [30208/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 43 [30464/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 43 [30720/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 43 [30976/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 43 [31232/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 43 [31488/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 43 [31744/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 43 [32000/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 43 [32256/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 43 [32512/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 43 [32768/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 43 [33024/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 43 [33280/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 43 [33536/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 43 [33792/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 43 [34048/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 43 [34304/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 43 [34560/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 43 [34816/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 43 [35072/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 43 [35328/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 43 [35584/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 43 [35840/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 43 [36096/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 43 [36352/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 43 [36608/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 43 [36864/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 43 [37120/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 43 [37376/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 43 [37632/50000]	Loss: 4.6332	LR: 1.000000
Training Epoch: 43 [37888/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 43 [38144/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 43 [38400/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 43 [38656/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 43 [38912/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 43 [39168/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 43 [39424/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 43 [39680/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 43 [39936/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 43 [40192/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 43 [40448/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 43 [40704/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 43 [40960/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 43 [41216/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 43 [41472/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 43 [41728/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 43 [41984/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 43 [42240/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 43 [42496/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 43 [42752/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 43 [43008/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 43 [43264/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 43 [43520/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 43 [43776/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 43 [44032/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 43 [44288/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 43 [44544/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 43 [44800/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 43 [45056/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 43 [45312/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 43 [45568/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 43 [45824/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 43 [46080/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 43 [46336/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 43 [46592/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 43 [46848/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 43 [47104/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 43 [47360/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 43 [47616/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 43 [47872/50000]	Loss: 4.6023	LR: 1.000000
Training Epoch: 43 [48128/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 43 [48384/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 43 [48640/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 43 [48896/50000]	Loss: 4.6350	LR: 1.000000
Training Epoch: 43 [49152/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 43 [49408/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 43 [49664/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 43 [49920/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 43 [50000/50000]	Loss: 4.6261	LR: 1.000000
epoch 43 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   64508 GB |   64508 GB |
|       from large pool |  400448 KB |    1770 MB |   64450 GB |   64450 GB |
|       from small pool |    3549 KB |       9 MB |      58 GB |      58 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   64508 GB |   64508 GB |
|       from large pool |  400448 KB |    1770 MB |   64450 GB |   64450 GB |
|       from small pool |    3549 KB |       9 MB |      58 GB |      58 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   39692 GB |   39692 GB |
|       from large pool |  244672 KB |  473024 KB |   39625 GB |   39625 GB |
|       from small pool |    2594 KB |    4843 KB |      66 GB |      66 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2753 K  |    2753 K  |
|       from large pool |      36    |      77    |    1331 K  |    1331 K  |
|       from small pool |     186    |     224    |    1422 K  |    1421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2753 K  |    2753 K  |
|       from large pool |      36    |      77    |    1331 K  |    1331 K  |
|       from small pool |     186    |     224    |    1422 K  |    1421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    1401 K  |    1401 K  |
|       from large pool |      10    |      11    |     551 K  |     551 K  |
|       from small pool |      12    |      23    |     850 K  |     850 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 43, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 44 [256/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 44 [512/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 44 [768/50000]	Loss: 4.5997	LR: 1.000000
Training Epoch: 44 [1024/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 44 [1280/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 44 [1536/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 44 [1792/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 44 [2048/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 44 [2304/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 44 [2560/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 44 [2816/50000]	Loss: 4.6364	LR: 1.000000
Training Epoch: 44 [3072/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 44 [3328/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 44 [3584/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 44 [3840/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 44 [4096/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 44 [4352/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 44 [4608/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 44 [4864/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 44 [5120/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 44 [5376/50000]	Loss: 4.6338	LR: 1.000000
Training Epoch: 44 [5632/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 44 [5888/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 44 [6144/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 44 [6400/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 44 [6656/50000]	Loss: 4.5984	LR: 1.000000
Training Epoch: 44 [6912/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 44 [7168/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 44 [7424/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 44 [7680/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 44 [7936/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 44 [8192/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 44 [8448/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 44 [8704/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 44 [8960/50000]	Loss: 4.5945	LR: 1.000000
Training Epoch: 44 [9216/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 44 [9472/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 44 [9728/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 44 [9984/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 44 [10240/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 44 [10496/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 44 [10752/50000]	Loss: 4.6411	LR: 1.000000
Training Epoch: 44 [11008/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 44 [11264/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 44 [11520/50000]	Loss: 4.6008	LR: 1.000000
Training Epoch: 44 [11776/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 44 [12032/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 44 [12288/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 44 [12544/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 44 [12800/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 44 [13056/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 44 [13312/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 44 [13568/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 44 [13824/50000]	Loss: 4.6325	LR: 1.000000
Training Epoch: 44 [14080/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 44 [14336/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 44 [14592/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 44 [14848/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 44 [15104/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 44 [15360/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 44 [15616/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 44 [15872/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 44 [16128/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 44 [16384/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 44 [16640/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 44 [16896/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 44 [17152/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 44 [17408/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 44 [17664/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 44 [17920/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 44 [18176/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 44 [18432/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 44 [18688/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 44 [18944/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 44 [19200/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 44 [19456/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 44 [19712/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 44 [19968/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 44 [20224/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 44 [20480/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 44 [20736/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 44 [20992/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 44 [21248/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 44 [21504/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 44 [21760/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 44 [22016/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 44 [22272/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 44 [22528/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 44 [22784/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 44 [23040/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 44 [23296/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 44 [23552/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 44 [23808/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 44 [24064/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 44 [24320/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 44 [24576/50000]	Loss: 4.6340	LR: 1.000000
Training Epoch: 44 [24832/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 44 [25088/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 44 [25344/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 44 [25600/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 44 [25856/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 44 [26112/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 44 [26368/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 44 [26624/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 44 [26880/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 44 [27136/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 44 [27392/50000]	Loss: 4.5974	LR: 1.000000
Training Epoch: 44 [27648/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 44 [27904/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 44 [28160/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 44 [28416/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 44 [28672/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 44 [28928/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 44 [29184/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 44 [29440/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 44 [29696/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 44 [29952/50000]	Loss: 4.5970	LR: 1.000000
Training Epoch: 44 [30208/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 44 [30464/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 44 [30720/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 44 [30976/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 44 [31232/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 44 [31488/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 44 [31744/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 44 [32000/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 44 [32256/50000]	Loss: 4.6320	LR: 1.000000
Training Epoch: 44 [32512/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 44 [32768/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 44 [33024/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 44 [33280/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 44 [33536/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 44 [33792/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 44 [34048/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 44 [34304/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 44 [34560/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 44 [34816/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 44 [35072/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 44 [35328/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 44 [35584/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 44 [35840/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 44 [36096/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 44 [36352/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 44 [36608/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 44 [36864/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 44 [37120/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 44 [37376/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 44 [37632/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 44 [37888/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 44 [38144/50000]	Loss: 4.6399	LR: 1.000000
Training Epoch: 44 [38400/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 44 [38656/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 44 [38912/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 44 [39168/50000]	Loss: 4.5962	LR: 1.000000
Training Epoch: 44 [39424/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 44 [39680/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 44 [39936/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 44 [40192/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 44 [40448/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 44 [40704/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 44 [40960/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 44 [41216/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 44 [41472/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 44 [41728/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 44 [41984/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 44 [42240/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 44 [42496/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 44 [42752/50000]	Loss: 4.6342	LR: 1.000000
Training Epoch: 44 [43008/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 44 [43264/50000]	Loss: 4.6361	LR: 1.000000
Training Epoch: 44 [43520/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 44 [43776/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 44 [44032/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 44 [44288/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 44 [44544/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 44 [44800/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 44 [45056/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 44 [45312/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 44 [45568/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 44 [45824/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 44 [46080/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 44 [46336/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 44 [46592/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 44 [46848/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 44 [47104/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 44 [47360/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 44 [47616/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 44 [47872/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 44 [48128/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 44 [48384/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 44 [48640/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 44 [48896/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 44 [49152/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 44 [49408/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 44 [49664/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 44 [49920/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 44 [50000/50000]	Loss: 4.6132	LR: 1.000000
epoch 44 training time consumed: 21.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   66009 GB |   66008 GB |
|       from large pool |  400448 KB |    1770 MB |   65949 GB |   65949 GB |
|       from small pool |    3549 KB |       9 MB |      59 GB |      59 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   66009 GB |   66008 GB |
|       from large pool |  400448 KB |    1770 MB |   65949 GB |   65949 GB |
|       from small pool |    3549 KB |       9 MB |      59 GB |      59 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   40615 GB |   40615 GB |
|       from large pool |  244672 KB |  473024 KB |   40547 GB |   40547 GB |
|       from small pool |    2594 KB |    4843 KB |      67 GB |      67 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2817 K  |    2817 K  |
|       from large pool |      36    |      77    |    1362 K  |    1362 K  |
|       from small pool |     186    |     224    |    1455 K  |    1455 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2817 K  |    2817 K  |
|       from large pool |      36    |      77    |    1362 K  |    1362 K  |
|       from small pool |     186    |     224    |    1455 K  |    1455 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1434 K  |    1434 K  |
|       from large pool |      10    |      11    |     564 K  |     564 K  |
|       from small pool |      11    |      23    |     869 K  |     869 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 44, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 45 [256/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 45 [512/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 45 [768/50000]	Loss: 4.5960	LR: 1.000000
Training Epoch: 45 [1024/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 45 [1280/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 45 [1536/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 45 [1792/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 45 [2048/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 45 [2304/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 45 [2560/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 45 [2816/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 45 [3072/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 45 [3328/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 45 [3584/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 45 [3840/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 45 [4096/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 45 [4352/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 45 [4608/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 45 [4864/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 45 [5120/50000]	Loss: 4.5984	LR: 1.000000
Training Epoch: 45 [5376/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 45 [5632/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 45 [5888/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 45 [6144/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 45 [6400/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 45 [6656/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 45 [6912/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 45 [7168/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 45 [7424/50000]	Loss: 4.6456	LR: 1.000000
Training Epoch: 45 [7680/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 45 [7936/50000]	Loss: 4.6462	LR: 1.000000
Training Epoch: 45 [8192/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 45 [8448/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 45 [8704/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 45 [8960/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 45 [9216/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 45 [9472/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 45 [9728/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 45 [9984/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 45 [10240/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 45 [10496/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 45 [10752/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 45 [11008/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 45 [11264/50000]	Loss: 4.5960	LR: 1.000000
Training Epoch: 45 [11520/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 45 [11776/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 45 [12032/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 45 [12288/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 45 [12544/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 45 [12800/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 45 [13056/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 45 [13312/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 45 [13568/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 45 [13824/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 45 [14080/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 45 [14336/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 45 [14592/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 45 [14848/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 45 [15104/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 45 [15360/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 45 [15616/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 45 [15872/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 45 [16128/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 45 [16384/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 45 [16640/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 45 [16896/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 45 [17152/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 45 [17408/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 45 [17664/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 45 [17920/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 45 [18176/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 45 [18432/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 45 [18688/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 45 [18944/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 45 [19200/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 45 [19456/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 45 [19712/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 45 [19968/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 45 [20224/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 45 [20480/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 45 [20736/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 45 [20992/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 45 [21248/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 45 [21504/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 45 [21760/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 45 [22016/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 45 [22272/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 45 [22528/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 45 [22784/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 45 [23040/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 45 [23296/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 45 [23552/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 45 [23808/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 45 [24064/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 45 [24320/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 45 [24576/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 45 [24832/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 45 [25088/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 45 [25344/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 45 [25600/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 45 [25856/50000]	Loss: 4.6004	LR: 1.000000
Training Epoch: 45 [26112/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 45 [26368/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 45 [26624/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 45 [26880/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 45 [27136/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 45 [27392/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 45 [27648/50000]	Loss: 4.5989	LR: 1.000000
Training Epoch: 45 [27904/50000]	Loss: 4.6023	LR: 1.000000
Training Epoch: 45 [28160/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 45 [28416/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 45 [28672/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 45 [28928/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 45 [29184/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 45 [29440/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 45 [29696/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 45 [29952/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 45 [30208/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 45 [30464/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 45 [30720/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 45 [30976/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 45 [31232/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 45 [31488/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 45 [31744/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 45 [32000/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 45 [32256/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 45 [32512/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 45 [32768/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 45 [33024/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 45 [33280/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 45 [33536/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 45 [33792/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 45 [34048/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 45 [34304/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 45 [34560/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 45 [34816/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 45 [35072/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 45 [35328/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 45 [35584/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 45 [35840/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 45 [36096/50000]	Loss: 4.6435	LR: 1.000000
Training Epoch: 45 [36352/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 45 [36608/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 45 [36864/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 45 [37120/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 45 [37376/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 45 [37632/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 45 [37888/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 45 [38144/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 45 [38400/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 45 [38656/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 45 [38912/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 45 [39168/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 45 [39424/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 45 [39680/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 45 [39936/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 45 [40192/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 45 [40448/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 45 [40704/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 45 [40960/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 45 [41216/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 45 [41472/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 45 [41728/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 45 [41984/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 45 [42240/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 45 [42496/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 45 [42752/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 45 [43008/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 45 [43264/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 45 [43520/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 45 [43776/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 45 [44032/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 45 [44288/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 45 [44544/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 45 [44800/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 45 [45056/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 45 [45312/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 45 [45568/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 45 [45824/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 45 [46080/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 45 [46336/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 45 [46592/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 45 [46848/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 45 [47104/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 45 [47360/50000]	Loss: 4.6030	LR: 1.000000
Training Epoch: 45 [47616/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 45 [47872/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 45 [48128/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 45 [48384/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 45 [48640/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 45 [48896/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 45 [49152/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 45 [49408/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 45 [49664/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 45 [49920/50000]	Loss: 4.6356	LR: 1.000000
Training Epoch: 45 [50000/50000]	Loss: 4.6224	LR: 1.000000
epoch 45 training time consumed: 22.12s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   67509 GB |   67508 GB |
|       from large pool |  400448 KB |    1770 MB |   67448 GB |   67448 GB |
|       from small pool |    3549 KB |       9 MB |      60 GB |      60 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   67509 GB |   67508 GB |
|       from large pool |  400448 KB |    1770 MB |   67448 GB |   67448 GB |
|       from small pool |    3549 KB |       9 MB |      60 GB |      60 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   41538 GB |   41538 GB |
|       from large pool |  244672 KB |  473024 KB |   41468 GB |   41468 GB |
|       from small pool |    2594 KB |    4843 KB |      69 GB |      69 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2881 K  |    2881 K  |
|       from large pool |      36    |      77    |    1393 K  |    1393 K  |
|       from small pool |     186    |     224    |    1488 K  |    1488 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2881 K  |    2881 K  |
|       from large pool |      36    |      77    |    1393 K  |    1393 K  |
|       from small pool |     186    |     224    |    1488 K  |    1488 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1467 K  |    1467 K  |
|       from large pool |      10    |      11    |     577 K  |     577 K  |
|       from small pool |      11    |      23    |     889 K  |     889 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 45, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 46 [256/50000]	Loss: 4.5990	LR: 1.000000
Training Epoch: 46 [512/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 46 [768/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 46 [1024/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 46 [1280/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 46 [1536/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 46 [1792/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 46 [2048/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 46 [2304/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 46 [2560/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 46 [2816/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 46 [3072/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 46 [3328/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 46 [3584/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 46 [3840/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 46 [4096/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 46 [4352/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 46 [4608/50000]	Loss: 4.6009	LR: 1.000000
Training Epoch: 46 [4864/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 46 [5120/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 46 [5376/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 46 [5632/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 46 [5888/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 46 [6144/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 46 [6400/50000]	Loss: 4.6385	LR: 1.000000
Training Epoch: 46 [6656/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 46 [6912/50000]	Loss: 4.5987	LR: 1.000000
Training Epoch: 46 [7168/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 46 [7424/50000]	Loss: 4.5935	LR: 1.000000
Training Epoch: 46 [7680/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 46 [7936/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 46 [8192/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 46 [8448/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 46 [8704/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 46 [8960/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 46 [9216/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 46 [9472/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 46 [9728/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 46 [9984/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 46 [10240/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 46 [10496/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 46 [10752/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 46 [11008/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 46 [11264/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 46 [11520/50000]	Loss: 4.5921	LR: 1.000000
Training Epoch: 46 [11776/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 46 [12032/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 46 [12288/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 46 [12544/50000]	Loss: 4.5980	LR: 1.000000
Training Epoch: 46 [12800/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 46 [13056/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 46 [13312/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 46 [13568/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 46 [13824/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 46 [14080/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 46 [14336/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 46 [14592/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 46 [14848/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 46 [15104/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 46 [15360/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 46 [15616/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 46 [15872/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 46 [16128/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 46 [16384/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 46 [16640/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 46 [16896/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 46 [17152/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 46 [17408/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 46 [17664/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 46 [17920/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 46 [18176/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 46 [18432/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 46 [18688/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 46 [18944/50000]	Loss: 4.6313	LR: 1.000000
Training Epoch: 46 [19200/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 46 [19456/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 46 [19712/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 46 [19968/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 46 [20224/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 46 [20480/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 46 [20736/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 46 [20992/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 46 [21248/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 46 [21504/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 46 [21760/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 46 [22016/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 46 [22272/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 46 [22528/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 46 [22784/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 46 [23040/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 46 [23296/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 46 [23552/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 46 [23808/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 46 [24064/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 46 [24320/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 46 [24576/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 46 [24832/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 46 [25088/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 46 [25344/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 46 [25600/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 46 [25856/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 46 [26112/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 46 [26368/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 46 [26624/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 46 [26880/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 46 [27136/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 46 [27392/50000]	Loss: 4.6304	LR: 1.000000
Training Epoch: 46 [27648/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 46 [27904/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 46 [28160/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 46 [28416/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 46 [28672/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 46 [28928/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 46 [29184/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 46 [29440/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 46 [29696/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 46 [29952/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 46 [30208/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 46 [30464/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 46 [30720/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 46 [30976/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 46 [31232/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 46 [31488/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 46 [31744/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 46 [32000/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 46 [32256/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 46 [32512/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 46 [32768/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 46 [33024/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 46 [33280/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 46 [33536/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 46 [33792/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 46 [34048/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 46 [34304/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 46 [34560/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 46 [34816/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 46 [35072/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 46 [35328/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 46 [35584/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 46 [35840/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 46 [36096/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 46 [36352/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 46 [36608/50000]	Loss: 4.6282	LR: 1.000000
Training Epoch: 46 [36864/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 46 [37120/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 46 [37376/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 46 [37632/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 46 [37888/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 46 [38144/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 46 [38400/50000]	Loss: 4.6286	LR: 1.000000
Training Epoch: 46 [38656/50000]	Loss: 4.5985	LR: 1.000000
Training Epoch: 46 [38912/50000]	Loss: 4.6284	LR: 1.000000
Training Epoch: 46 [39168/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 46 [39424/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 46 [39680/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 46 [39936/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 46 [40192/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 46 [40448/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 46 [40704/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 46 [40960/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 46 [41216/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 46 [41472/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 46 [41728/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 46 [41984/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 46 [42240/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 46 [42496/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 46 [42752/50000]	Loss: 4.6009	LR: 1.000000
Training Epoch: 46 [43008/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 46 [43264/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 46 [43520/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 46 [43776/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 46 [44032/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 46 [44288/50000]	Loss: 4.5964	LR: 1.000000
Training Epoch: 46 [44544/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 46 [44800/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 46 [45056/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 46 [45312/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 46 [45568/50000]	Loss: 4.5985	LR: 1.000000
Training Epoch: 46 [45824/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 46 [46080/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 46 [46336/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 46 [46592/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 46 [46848/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 46 [47104/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 46 [47360/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 46 [47616/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 46 [47872/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 46 [48128/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 46 [48384/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 46 [48640/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 46 [48896/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 46 [49152/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 46 [49408/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 46 [49664/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 46 [49920/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 46 [50000/50000]	Loss: 4.6100	LR: 1.000000
epoch 46 training time consumed: 21.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   69009 GB |   69008 GB |
|       from large pool |  400448 KB |    1770 MB |   68947 GB |   68946 GB |
|       from small pool |    3549 KB |       9 MB |      62 GB |      62 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   69009 GB |   69008 GB |
|       from large pool |  400448 KB |    1770 MB |   68947 GB |   68946 GB |
|       from small pool |    3549 KB |       9 MB |      62 GB |      62 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   42461 GB |   42461 GB |
|       from large pool |  244672 KB |  473024 KB |   42390 GB |   42390 GB |
|       from small pool |    2594 KB |    4843 KB |      71 GB |      71 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    2945 K  |    2945 K  |
|       from large pool |      36    |      77    |    1424 K  |    1424 K  |
|       from small pool |     186    |     224    |    1521 K  |    1521 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    2945 K  |    2945 K  |
|       from large pool |      36    |      77    |    1424 K  |    1424 K  |
|       from small pool |     186    |     224    |    1521 K  |    1521 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    1499 K  |    1499 K  |
|       from large pool |      10    |      11    |     589 K  |     589 K  |
|       from small pool |      12    |      23    |     909 K  |     909 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 46, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 47 [256/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 47 [512/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 47 [768/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 47 [1024/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 47 [1280/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 47 [1536/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 47 [1792/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 47 [2048/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 47 [2304/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 47 [2560/50000]	Loss: 4.5984	LR: 1.000000
Training Epoch: 47 [2816/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 47 [3072/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 47 [3328/50000]	Loss: 4.6010	LR: 1.000000
Training Epoch: 47 [3584/50000]	Loss: 4.6297	LR: 1.000000
Training Epoch: 47 [3840/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 47 [4096/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 47 [4352/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 47 [4608/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 47 [4864/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 47 [5120/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 47 [5376/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 47 [5632/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 47 [5888/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 47 [6144/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 47 [6400/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 47 [6656/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 47 [6912/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 47 [7168/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 47 [7424/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 47 [7680/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 47 [7936/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 47 [8192/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 47 [8448/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 47 [8704/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 47 [8960/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 47 [9216/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 47 [9472/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 47 [9728/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 47 [9984/50000]	Loss: 4.5988	LR: 1.000000
Training Epoch: 47 [10240/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 47 [10496/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 47 [10752/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 47 [11008/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 47 [11264/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 47 [11520/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 47 [11776/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 47 [12032/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 47 [12288/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 47 [12544/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 47 [12800/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 47 [13056/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 47 [13312/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 47 [13568/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 47 [13824/50000]	Loss: 4.5964	LR: 1.000000
Training Epoch: 47 [14080/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 47 [14336/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 47 [14592/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 47 [14848/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 47 [15104/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 47 [15360/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 47 [15616/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 47 [15872/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 47 [16128/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 47 [16384/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 47 [16640/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 47 [16896/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 47 [17152/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 47 [17408/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 47 [17664/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 47 [17920/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 47 [18176/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 47 [18432/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 47 [18688/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 47 [18944/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 47 [19200/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 47 [19456/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 47 [19712/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 47 [19968/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 47 [20224/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 47 [20480/50000]	Loss: 4.6283	LR: 1.000000
Training Epoch: 47 [20736/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 47 [20992/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 47 [21248/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 47 [21504/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 47 [21760/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 47 [22016/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 47 [22272/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 47 [22528/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 47 [22784/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 47 [23040/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 47 [23296/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 47 [23552/50000]	Loss: 4.6357	LR: 1.000000
Training Epoch: 47 [23808/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 47 [24064/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 47 [24320/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 47 [24576/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 47 [24832/50000]	Loss: 4.6253	LR: 1.000000
Training Epoch: 47 [25088/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 47 [25344/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 47 [25600/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 47 [25856/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 47 [26112/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 47 [26368/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 47 [26624/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 47 [26880/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 47 [27136/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 47 [27392/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 47 [27648/50000]	Loss: 4.6302	LR: 1.000000
Training Epoch: 47 [27904/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 47 [28160/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 47 [28416/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 47 [28672/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 47 [28928/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 47 [29184/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 47 [29440/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 47 [29696/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 47 [29952/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 47 [30208/50000]	Loss: 4.5973	LR: 1.000000
Training Epoch: 47 [30464/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 47 [30720/50000]	Loss: 4.6282	LR: 1.000000
Training Epoch: 47 [30976/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 47 [31232/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 47 [31488/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 47 [31744/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 47 [32000/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 47 [32256/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 47 [32512/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 47 [32768/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 47 [33024/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 47 [33280/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 47 [33536/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 47 [33792/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 47 [34048/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 47 [34304/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 47 [34560/50000]	Loss: 4.5994	LR: 1.000000
Training Epoch: 47 [34816/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 47 [35072/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 47 [35328/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 47 [35584/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 47 [35840/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 47 [36096/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 47 [36352/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 47 [36608/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 47 [36864/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 47 [37120/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 47 [37376/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 47 [37632/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 47 [37888/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 47 [38144/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 47 [38400/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 47 [38656/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 47 [38912/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 47 [39168/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 47 [39424/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 47 [39680/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 47 [39936/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 47 [40192/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 47 [40448/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 47 [40704/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 47 [40960/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 47 [41216/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 47 [41472/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 47 [41728/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 47 [41984/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 47 [42240/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 47 [42496/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 47 [42752/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 47 [43008/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 47 [43264/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 47 [43520/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 47 [43776/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 47 [44032/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 47 [44288/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 47 [44544/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 47 [44800/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 47 [45056/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 47 [45312/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 47 [45568/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 47 [45824/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 47 [46080/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 47 [46336/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 47 [46592/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 47 [46848/50000]	Loss: 4.5978	LR: 1.000000
Training Epoch: 47 [47104/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 47 [47360/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 47 [47616/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 47 [47872/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 47 [48128/50000]	Loss: 4.6003	LR: 1.000000
Training Epoch: 47 [48384/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 47 [48640/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 47 [48896/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 47 [49152/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 47 [49408/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 47 [49664/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 47 [49920/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 47 [50000/50000]	Loss: 4.6242	LR: 1.000000
epoch 47 training time consumed: 21.88s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   70509 GB |   70509 GB |
|       from large pool |  400448 KB |    1770 MB |   70446 GB |   70445 GB |
|       from small pool |    3549 KB |       9 MB |      63 GB |      63 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   70509 GB |   70509 GB |
|       from large pool |  400448 KB |    1770 MB |   70446 GB |   70445 GB |
|       from small pool |    3549 KB |       9 MB |      63 GB |      63 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   43384 GB |   43384 GB |
|       from large pool |  244672 KB |  473024 KB |   43311 GB |   43311 GB |
|       from small pool |    2594 KB |    4843 KB |      72 GB |      72 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3009 K  |    3009 K  |
|       from large pool |      36    |      77    |    1455 K  |    1455 K  |
|       from small pool |     186    |     224    |    1554 K  |    1554 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3009 K  |    3009 K  |
|       from large pool |      36    |      77    |    1455 K  |    1455 K  |
|       from small pool |     186    |     224    |    1554 K  |    1554 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      33    |    1532 K  |    1532 K  |
|       from large pool |      10    |      11    |     602 K  |     602 K  |
|       from small pool |      16    |      23    |     930 K  |     930 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 47, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 48 [256/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 48 [512/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 48 [768/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 48 [1024/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 48 [1280/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 48 [1536/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 48 [1792/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 48 [2048/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 48 [2304/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 48 [2560/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 48 [2816/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 48 [3072/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 48 [3328/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 48 [3584/50000]	Loss: 4.6007	LR: 1.000000
Training Epoch: 48 [3840/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 48 [4096/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 48 [4352/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 48 [4608/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 48 [4864/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 48 [5120/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 48 [5376/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 48 [5632/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 48 [5888/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 48 [6144/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 48 [6400/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 48 [6656/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 48 [6912/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 48 [7168/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 48 [7424/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 48 [7680/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 48 [7936/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 48 [8192/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 48 [8448/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 48 [8704/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 48 [8960/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 48 [9216/50000]	Loss: 4.5984	LR: 1.000000
Training Epoch: 48 [9472/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 48 [9728/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 48 [9984/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 48 [10240/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 48 [10496/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 48 [10752/50000]	Loss: 4.5974	LR: 1.000000
Training Epoch: 48 [11008/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 48 [11264/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 48 [11520/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 48 [11776/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 48 [12032/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 48 [12288/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 48 [12544/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 48 [12800/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 48 [13056/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 48 [13312/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 48 [13568/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 48 [13824/50000]	Loss: 4.6364	LR: 1.000000
Training Epoch: 48 [14080/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 48 [14336/50000]	Loss: 4.5876	LR: 1.000000
Training Epoch: 48 [14592/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 48 [14848/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 48 [15104/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 48 [15360/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 48 [15616/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 48 [15872/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 48 [16128/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 48 [16384/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 48 [16640/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 48 [16896/50000]	Loss: 4.6416	LR: 1.000000
Training Epoch: 48 [17152/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 48 [17408/50000]	Loss: 4.6300	LR: 1.000000
Training Epoch: 48 [17664/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 48 [17920/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 48 [18176/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 48 [18432/50000]	Loss: 4.5985	LR: 1.000000
Training Epoch: 48 [18688/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 48 [18944/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 48 [19200/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 48 [19456/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 48 [19712/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 48 [19968/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 48 [20224/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 48 [20480/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 48 [20736/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 48 [20992/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 48 [21248/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 48 [21504/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 48 [21760/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 48 [22016/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 48 [22272/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 48 [22528/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 48 [22784/50000]	Loss: 4.6005	LR: 1.000000
Training Epoch: 48 [23040/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 48 [23296/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 48 [23552/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 48 [23808/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 48 [24064/50000]	Loss: 4.6313	LR: 1.000000
Training Epoch: 48 [24320/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 48 [24576/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 48 [24832/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 48 [25088/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 48 [25344/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 48 [25600/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 48 [25856/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 48 [26112/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 48 [26368/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 48 [26624/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 48 [26880/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 48 [27136/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 48 [27392/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 48 [27648/50000]	Loss: 4.5989	LR: 1.000000
Training Epoch: 48 [27904/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 48 [28160/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 48 [28416/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 48 [28672/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 48 [28928/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 48 [29184/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 48 [29440/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 48 [29696/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 48 [29952/50000]	Loss: 4.6326	LR: 1.000000
Training Epoch: 48 [30208/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 48 [30464/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 48 [30720/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 48 [30976/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 48 [31232/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 48 [31488/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 48 [31744/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 48 [32000/50000]	Loss: 4.6332	LR: 1.000000
Training Epoch: 48 [32256/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 48 [32512/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 48 [32768/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 48 [33024/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 48 [33280/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 48 [33536/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 48 [33792/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 48 [34048/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 48 [34304/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 48 [34560/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 48 [34816/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 48 [35072/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 48 [35328/50000]	Loss: 4.6343	LR: 1.000000
Training Epoch: 48 [35584/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 48 [35840/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 48 [36096/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 48 [36352/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 48 [36608/50000]	Loss: 4.6337	LR: 1.000000
Training Epoch: 48 [36864/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 48 [37120/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 48 [37376/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 48 [37632/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 48 [37888/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 48 [38144/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 48 [38400/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 48 [38656/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 48 [38912/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 48 [39168/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 48 [39424/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 48 [39680/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 48 [39936/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 48 [40192/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 48 [40448/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 48 [40704/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 48 [40960/50000]	Loss: 4.5923	LR: 1.000000
Training Epoch: 48 [41216/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 48 [41472/50000]	Loss: 4.6284	LR: 1.000000
Training Epoch: 48 [41728/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 48 [41984/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 48 [42240/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 48 [42496/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 48 [42752/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 48 [43008/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 48 [43264/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 48 [43520/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 48 [43776/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 48 [44032/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 48 [44288/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 48 [44544/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 48 [44800/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 48 [45056/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 48 [45312/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 48 [45568/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 48 [45824/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 48 [46080/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 48 [46336/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 48 [46592/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 48 [46848/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 48 [47104/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 48 [47360/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 48 [47616/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 48 [47872/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 48 [48128/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 48 [48384/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 48 [48640/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 48 [48896/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 48 [49152/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 48 [49408/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 48 [49664/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 48 [49920/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 48 [50000/50000]	Loss: 4.6342	LR: 1.000000
epoch 48 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   72009 GB |   72009 GB |
|       from large pool |  400448 KB |    1770 MB |   71944 GB |   71944 GB |
|       from small pool |    3549 KB |       9 MB |      64 GB |      64 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   72009 GB |   72009 GB |
|       from large pool |  400448 KB |    1770 MB |   71944 GB |   71944 GB |
|       from small pool |    3549 KB |       9 MB |      64 GB |      64 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   44307 GB |   44307 GB |
|       from large pool |  244672 KB |  473024 KB |   44233 GB |   44233 GB |
|       from small pool |    2594 KB |    4843 KB |      74 GB |      74 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3073 K  |    3073 K  |
|       from large pool |      36    |      77    |    1486 K  |    1486 K  |
|       from small pool |     186    |     224    |    1587 K  |    1587 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3073 K  |    3073 K  |
|       from large pool |      36    |      77    |    1486 K  |    1486 K  |
|       from small pool |     186    |     224    |    1587 K  |    1587 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1565 K  |    1565 K  |
|       from large pool |      10    |      11    |     615 K  |     615 K  |
|       from small pool |      11    |      23    |     949 K  |     949 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 48, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 49 [256/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 49 [512/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 49 [768/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 49 [1024/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 49 [1280/50000]	Loss: 4.6010	LR: 1.000000
Training Epoch: 49 [1536/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 49 [1792/50000]	Loss: 4.6393	LR: 1.000000
Training Epoch: 49 [2048/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 49 [2304/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 49 [2560/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 49 [2816/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 49 [3072/50000]	Loss: 4.6322	LR: 1.000000
Training Epoch: 49 [3328/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 49 [3584/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 49 [3840/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 49 [4096/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 49 [4352/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 49 [4608/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 49 [4864/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 49 [5120/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 49 [5376/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 49 [5632/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 49 [5888/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 49 [6144/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 49 [6400/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 49 [6656/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 49 [6912/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 49 [7168/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 49 [7424/50000]	Loss: 4.5955	LR: 1.000000
Training Epoch: 49 [7680/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 49 [7936/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 49 [8192/50000]	Loss: 4.6325	LR: 1.000000
Training Epoch: 49 [8448/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 49 [8704/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 49 [8960/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 49 [9216/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 49 [9472/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 49 [9728/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 49 [9984/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 49 [10240/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 49 [10496/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 49 [10752/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 49 [11008/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 49 [11264/50000]	Loss: 4.6354	LR: 1.000000
Training Epoch: 49 [11520/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 49 [11776/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 49 [12032/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 49 [12288/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 49 [12544/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 49 [12800/50000]	Loss: 4.6256	LR: 1.000000
Training Epoch: 49 [13056/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 49 [13312/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 49 [13568/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 49 [13824/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 49 [14080/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 49 [14336/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 49 [14592/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 49 [14848/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 49 [15104/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 49 [15360/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 49 [15616/50000]	Loss: 4.5986	LR: 1.000000
Training Epoch: 49 [15872/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 49 [16128/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 49 [16384/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 49 [16640/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 49 [16896/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 49 [17152/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 49 [17408/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 49 [17664/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 49 [17920/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 49 [18176/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 49 [18432/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 49 [18688/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 49 [18944/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 49 [19200/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 49 [19456/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 49 [19712/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 49 [19968/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 49 [20224/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 49 [20480/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 49 [20736/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 49 [20992/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 49 [21248/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 49 [21504/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 49 [21760/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 49 [22016/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 49 [22272/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 49 [22528/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 49 [22784/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 49 [23040/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 49 [23296/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 49 [23552/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 49 [23808/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 49 [24064/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 49 [24320/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 49 [24576/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 49 [24832/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 49 [25088/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 49 [25344/50000]	Loss: 4.5997	LR: 1.000000
Training Epoch: 49 [25600/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 49 [25856/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 49 [26112/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 49 [26368/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 49 [26624/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 49 [26880/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 49 [27136/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 49 [27392/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 49 [27648/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 49 [27904/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 49 [28160/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 49 [28416/50000]	Loss: 4.5969	LR: 1.000000
Training Epoch: 49 [28672/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 49 [28928/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 49 [29184/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 49 [29440/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 49 [29696/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 49 [29952/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 49 [30208/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 49 [30464/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 49 [30720/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 49 [30976/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 49 [31232/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 49 [31488/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 49 [31744/50000]	Loss: 4.5982	LR: 1.000000
Training Epoch: 49 [32000/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 49 [32256/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 49 [32512/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 49 [32768/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 49 [33024/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 49 [33280/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 49 [33536/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 49 [33792/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 49 [34048/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 49 [34304/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 49 [34560/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 49 [34816/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 49 [35072/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 49 [35328/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 49 [35584/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 49 [35840/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 49 [36096/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 49 [36352/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 49 [36608/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 49 [36864/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 49 [37120/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 49 [37376/50000]	Loss: 4.6308	LR: 1.000000
Training Epoch: 49 [37632/50000]	Loss: 4.6290	LR: 1.000000
Training Epoch: 49 [37888/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 49 [38144/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 49 [38400/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 49 [38656/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 49 [38912/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 49 [39168/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 49 [39424/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 49 [39680/50000]	Loss: 4.6356	LR: 1.000000
Training Epoch: 49 [39936/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 49 [40192/50000]	Loss: 4.6334	LR: 1.000000
Training Epoch: 49 [40448/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 49 [40704/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 49 [40960/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 49 [41216/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 49 [41472/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 49 [41728/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 49 [41984/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 49 [42240/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 49 [42496/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 49 [42752/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 49 [43008/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 49 [43264/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 49 [43520/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 49 [43776/50000]	Loss: 4.5929	LR: 1.000000
Training Epoch: 49 [44032/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 49 [44288/50000]	Loss: 4.6291	LR: 1.000000
Training Epoch: 49 [44544/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 49 [44800/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 49 [45056/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 49 [45312/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 49 [45568/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 49 [45824/50000]	Loss: 4.6296	LR: 1.000000
Training Epoch: 49 [46080/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 49 [46336/50000]	Loss: 4.6346	LR: 1.000000
Training Epoch: 49 [46592/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 49 [46848/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 49 [47104/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 49 [47360/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 49 [47616/50000]	Loss: 4.5930	LR: 1.000000
Training Epoch: 49 [47872/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 49 [48128/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 49 [48384/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 49 [48640/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 49 [48896/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 49 [49152/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 49 [49408/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 49 [49664/50000]	Loss: 4.5970	LR: 1.000000
Training Epoch: 49 [49920/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 49 [50000/50000]	Loss: 4.6071	LR: 1.000000
epoch 49 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   73509 GB |   73509 GB |
|       from large pool |  400448 KB |    1770 MB |   73443 GB |   73443 GB |
|       from small pool |    3549 KB |       9 MB |      66 GB |      66 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   73509 GB |   73509 GB |
|       from large pool |  400448 KB |    1770 MB |   73443 GB |   73443 GB |
|       from small pool |    3549 KB |       9 MB |      66 GB |      66 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   45230 GB |   45230 GB |
|       from large pool |  244672 KB |  473024 KB |   45154 GB |   45154 GB |
|       from small pool |    4642 KB |    4843 KB |      75 GB |      75 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3137 K  |    3137 K  |
|       from large pool |      36    |      77    |    1517 K  |    1517 K  |
|       from small pool |     186    |     224    |    1620 K  |    1620 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3137 K  |    3137 K  |
|       from large pool |      36    |      77    |    1517 K  |    1517 K  |
|       from small pool |     186    |     224    |    1620 K  |    1620 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      33    |    1598 K  |    1598 K  |
|       from large pool |      10    |      11    |     628 K  |     628 K  |
|       from small pool |      14    |      23    |     969 K  |     969 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 49, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 50 [256/50000]	Loss: 4.6329	LR: 1.000000
Training Epoch: 50 [512/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 50 [768/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 50 [1024/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 50 [1280/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 50 [1536/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 50 [1792/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 50 [2048/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 50 [2304/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 50 [2560/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 50 [2816/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 50 [3072/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 50 [3328/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 50 [3584/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 50 [3840/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 50 [4096/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 50 [4352/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 50 [4608/50000]	Loss: 4.6004	LR: 1.000000
Training Epoch: 50 [4864/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 50 [5120/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 50 [5376/50000]	Loss: 4.6005	LR: 1.000000
Training Epoch: 50 [5632/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 50 [5888/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 50 [6144/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 50 [6400/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 50 [6656/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 50 [6912/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 50 [7168/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 50 [7424/50000]	Loss: 4.6283	LR: 1.000000
Training Epoch: 50 [7680/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 50 [7936/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 50 [8192/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 50 [8448/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 50 [8704/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 50 [8960/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 50 [9216/50000]	Loss: 4.6025	LR: 1.000000
Training Epoch: 50 [9472/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 50 [9728/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 50 [9984/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 50 [10240/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 50 [10496/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 50 [10752/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 50 [11008/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 50 [11264/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 50 [11520/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 50 [11776/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 50 [12032/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 50 [12288/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 50 [12544/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 50 [12800/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 50 [13056/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 50 [13312/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 50 [13568/50000]	Loss: 4.6326	LR: 1.000000
Training Epoch: 50 [13824/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 50 [14080/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 50 [14336/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 50 [14592/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 50 [14848/50000]	Loss: 4.5955	LR: 1.000000
Training Epoch: 50 [15104/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 50 [15360/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 50 [15616/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 50 [15872/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 50 [16128/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 50 [16384/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 50 [16640/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 50 [16896/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 50 [17152/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 50 [17408/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 50 [17664/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 50 [17920/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 50 [18176/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 50 [18432/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 50 [18688/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 50 [18944/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 50 [19200/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 50 [19456/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 50 [19712/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 50 [19968/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 50 [20224/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 50 [20480/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 50 [20736/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 50 [20992/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 50 [21248/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 50 [21504/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 50 [21760/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 50 [22016/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 50 [22272/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 50 [22528/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 50 [22784/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 50 [23040/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 50 [23296/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 50 [23552/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 50 [23808/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 50 [24064/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 50 [24320/50000]	Loss: 4.6008	LR: 1.000000
Training Epoch: 50 [24576/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 50 [24832/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 50 [25088/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 50 [25344/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 50 [25600/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 50 [25856/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 50 [26112/50000]	Loss: 4.6386	LR: 1.000000
Training Epoch: 50 [26368/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 50 [26624/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 50 [26880/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 50 [27136/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 50 [27392/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 50 [27648/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 50 [27904/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 50 [28160/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 50 [28416/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 50 [28672/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 50 [28928/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 50 [29184/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 50 [29440/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 50 [29696/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 50 [29952/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 50 [30208/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 50 [30464/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 50 [30720/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 50 [30976/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 50 [31232/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 50 [31488/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 50 [31744/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 50 [32000/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 50 [32256/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 50 [32512/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 50 [32768/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 50 [33024/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 50 [33280/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 50 [33536/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 50 [33792/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 50 [34048/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 50 [34304/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 50 [34560/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 50 [34816/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 50 [35072/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 50 [35328/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 50 [35584/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 50 [35840/50000]	Loss: 4.5969	LR: 1.000000
Training Epoch: 50 [36096/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 50 [36352/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 50 [36608/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 50 [36864/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 50 [37120/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 50 [37376/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 50 [37632/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 50 [37888/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 50 [38144/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 50 [38400/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 50 [38656/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 50 [38912/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 50 [39168/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 50 [39424/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 50 [39680/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 50 [39936/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 50 [40192/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 50 [40448/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 50 [40704/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 50 [40960/50000]	Loss: 4.6367	LR: 1.000000
Training Epoch: 50 [41216/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 50 [41472/50000]	Loss: 4.6370	LR: 1.000000
Training Epoch: 50 [41728/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 50 [41984/50000]	Loss: 4.5976	LR: 1.000000
Training Epoch: 50 [42240/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 50 [42496/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 50 [42752/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 50 [43008/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 50 [43264/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 50 [43520/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 50 [43776/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 50 [44032/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 50 [44288/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 50 [44544/50000]	Loss: 4.6273	LR: 1.000000
Training Epoch: 50 [44800/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 50 [45056/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 50 [45312/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 50 [45568/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 50 [45824/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 50 [46080/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 50 [46336/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 50 [46592/50000]	Loss: 4.6368	LR: 1.000000
Training Epoch: 50 [46848/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 50 [47104/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 50 [47360/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 50 [47616/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 50 [47872/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 50 [48128/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 50 [48384/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 50 [48640/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 50 [48896/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 50 [49152/50000]	Loss: 4.5890	LR: 1.000000
Training Epoch: 50 [49408/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 50 [49664/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 50 [49920/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 50 [50000/50000]	Loss: 4.6064	LR: 1.000000
epoch 50 training time consumed: 21.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   75010 GB |   75009 GB |
|       from large pool |  400448 KB |    1770 MB |   74942 GB |   74942 GB |
|       from small pool |    3549 KB |       9 MB |      67 GB |      67 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   75010 GB |   75009 GB |
|       from large pool |  400448 KB |    1770 MB |   74942 GB |   74942 GB |
|       from small pool |    3549 KB |       9 MB |      67 GB |      67 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   46153 GB |   46153 GB |
|       from large pool |  244672 KB |  473024 KB |   46076 GB |   46076 GB |
|       from small pool |    2594 KB |    4843 KB |      77 GB |      77 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3201 K  |    3201 K  |
|       from large pool |      36    |      77    |    1548 K  |    1547 K  |
|       from small pool |     186    |     224    |    1653 K  |    1653 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3201 K  |    3201 K  |
|       from large pool |      36    |      77    |    1548 K  |    1547 K  |
|       from small pool |     186    |     224    |    1653 K  |    1653 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1631 K  |    1631 K  |
|       from large pool |      10    |      11    |     641 K  |     641 K  |
|       from small pool |      10    |      23    |     989 K  |     989 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 50, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.44s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-50-regular.pth
Training Epoch: 51 [256/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 51 [512/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 51 [768/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 51 [1024/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 51 [1280/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 51 [1536/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 51 [1792/50000]	Loss: 4.6003	LR: 1.000000
Training Epoch: 51 [2048/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 51 [2304/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 51 [2560/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 51 [2816/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 51 [3072/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 51 [3328/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 51 [3584/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 51 [3840/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 51 [4096/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 51 [4352/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 51 [4608/50000]	Loss: 4.6382	LR: 1.000000
Training Epoch: 51 [4864/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 51 [5120/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 51 [5376/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 51 [5632/50000]	Loss: 4.6355	LR: 1.000000
Training Epoch: 51 [5888/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 51 [6144/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 51 [6400/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 51 [6656/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 51 [6912/50000]	Loss: 4.6030	LR: 1.000000
Training Epoch: 51 [7168/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 51 [7424/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 51 [7680/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 51 [7936/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 51 [8192/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 51 [8448/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 51 [8704/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 51 [8960/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 51 [9216/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 51 [9472/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 51 [9728/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 51 [9984/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 51 [10240/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 51 [10496/50000]	Loss: 4.6337	LR: 1.000000
Training Epoch: 51 [10752/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 51 [11008/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 51 [11264/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 51 [11520/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 51 [11776/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 51 [12032/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 51 [12288/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 51 [12544/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 51 [12800/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 51 [13056/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 51 [13312/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 51 [13568/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 51 [13824/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 51 [14080/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 51 [14336/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 51 [14592/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 51 [14848/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 51 [15104/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 51 [15360/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 51 [15616/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 51 [15872/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 51 [16128/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 51 [16384/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 51 [16640/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 51 [16896/50000]	Loss: 4.5985	LR: 1.000000
Training Epoch: 51 [17152/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 51 [17408/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 51 [17664/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 51 [17920/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 51 [18176/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 51 [18432/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 51 [18688/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 51 [18944/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 51 [19200/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 51 [19456/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 51 [19712/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 51 [19968/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 51 [20224/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 51 [20480/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 51 [20736/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 51 [20992/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 51 [21248/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 51 [21504/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 51 [21760/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 51 [22016/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 51 [22272/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 51 [22528/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 51 [22784/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 51 [23040/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 51 [23296/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 51 [23552/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 51 [23808/50000]	Loss: 4.6397	LR: 1.000000
Training Epoch: 51 [24064/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 51 [24320/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 51 [24576/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 51 [24832/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 51 [25088/50000]	Loss: 4.5988	LR: 1.000000
Training Epoch: 51 [25344/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 51 [25600/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 51 [25856/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 51 [26112/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 51 [26368/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 51 [26624/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 51 [26880/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 51 [27136/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 51 [27392/50000]	Loss: 4.6304	LR: 1.000000
Training Epoch: 51 [27648/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 51 [27904/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 51 [28160/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 51 [28416/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 51 [28672/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 51 [28928/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 51 [29184/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 51 [29440/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 51 [29696/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 51 [29952/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 51 [30208/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 51 [30464/50000]	Loss: 4.6343	LR: 1.000000
Training Epoch: 51 [30720/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 51 [30976/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 51 [31232/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 51 [31488/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 51 [31744/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 51 [32000/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 51 [32256/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 51 [32512/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 51 [32768/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 51 [33024/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 51 [33280/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 51 [33536/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 51 [33792/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 51 [34048/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 51 [34304/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 51 [34560/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 51 [34816/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 51 [35072/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 51 [35328/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 51 [35584/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 51 [35840/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 51 [36096/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 51 [36352/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 51 [36608/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 51 [36864/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 51 [37120/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 51 [37376/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 51 [37632/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 51 [37888/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 51 [38144/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 51 [38400/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 51 [38656/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 51 [38912/50000]	Loss: 4.6311	LR: 1.000000
Training Epoch: 51 [39168/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 51 [39424/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 51 [39680/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 51 [39936/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 51 [40192/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 51 [40448/50000]	Loss: 4.6253	LR: 1.000000
Training Epoch: 51 [40704/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 51 [40960/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 51 [41216/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 51 [41472/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 51 [41728/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 51 [41984/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 51 [42240/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 51 [42496/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 51 [42752/50000]	Loss: 4.6312	LR: 1.000000
Training Epoch: 51 [43008/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 51 [43264/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 51 [43520/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 51 [43776/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 51 [44032/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 51 [44288/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 51 [44544/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 51 [44800/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 51 [45056/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 51 [45312/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 51 [45568/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 51 [45824/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 51 [46080/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 51 [46336/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 51 [46592/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 51 [46848/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 51 [47104/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 51 [47360/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 51 [47616/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 51 [47872/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 51 [48128/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 51 [48384/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 51 [48640/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 51 [48896/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 51 [49152/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 51 [49408/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 51 [49664/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 51 [49920/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 51 [50000/50000]	Loss: 4.6073	LR: 1.000000
epoch 51 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   76510 GB |   76509 GB |
|       from large pool |  400448 KB |    1770 MB |   76441 GB |   76441 GB |
|       from small pool |    3549 KB |       9 MB |      68 GB |      68 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   76510 GB |   76509 GB |
|       from large pool |  400448 KB |    1770 MB |   76441 GB |   76441 GB |
|       from small pool |    3549 KB |       9 MB |      68 GB |      68 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   47076 GB |   47076 GB |
|       from large pool |  244672 KB |  473024 KB |   46997 GB |   46997 GB |
|       from small pool |    2594 KB |    4843 KB |      78 GB |      78 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3265 K  |    3265 K  |
|       from large pool |      36    |      77    |    1578 K  |    1578 K  |
|       from small pool |     186    |     224    |    1686 K  |    1686 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3265 K  |    3265 K  |
|       from large pool |      36    |      77    |    1578 K  |    1578 K  |
|       from small pool |     186    |     224    |    1686 K  |    1686 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    1663 K  |    1663 K  |
|       from large pool |      10    |      11    |     654 K  |     653 K  |
|       from small pool |       9    |      23    |    1009 K  |    1009 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 51, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 52 [256/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 52 [512/50000]	Loss: 4.6021	LR: 1.000000
Training Epoch: 52 [768/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 52 [1024/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 52 [1280/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 52 [1536/50000]	Loss: 4.6004	LR: 1.000000
Training Epoch: 52 [1792/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 52 [2048/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 52 [2304/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 52 [2560/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 52 [2816/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 52 [3072/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 52 [3328/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 52 [3584/50000]	Loss: 4.6226	LR: 1.000000
Training Epoch: 52 [3840/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 52 [4096/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 52 [4352/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 52 [4608/50000]	Loss: 4.6011	LR: 1.000000
Training Epoch: 52 [4864/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 52 [5120/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 52 [5376/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 52 [5632/50000]	Loss: 4.6359	LR: 1.000000
Training Epoch: 52 [5888/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 52 [6144/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 52 [6400/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 52 [6656/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 52 [6912/50000]	Loss: 4.6364	LR: 1.000000
Training Epoch: 52 [7168/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 52 [7424/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 52 [7680/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 52 [7936/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 52 [8192/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 52 [8448/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 52 [8704/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 52 [8960/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 52 [9216/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 52 [9472/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 52 [9728/50000]	Loss: 4.6250	LR: 1.000000
Training Epoch: 52 [9984/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 52 [10240/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 52 [10496/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 52 [10752/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 52 [11008/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 52 [11264/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 52 [11520/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 52 [11776/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 52 [12032/50000]	Loss: 4.6273	LR: 1.000000
Training Epoch: 52 [12288/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 52 [12544/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 52 [12800/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 52 [13056/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 52 [13312/50000]	Loss: 4.6031	LR: 1.000000
Training Epoch: 52 [13568/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 52 [13824/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 52 [14080/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 52 [14336/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 52 [14592/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 52 [14848/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 52 [15104/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 52 [15360/50000]	Loss: 4.6383	LR: 1.000000
Training Epoch: 52 [15616/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 52 [15872/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 52 [16128/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 52 [16384/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 52 [16640/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 52 [16896/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 52 [17152/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 52 [17408/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 52 [17664/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 52 [17920/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 52 [18176/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 52 [18432/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 52 [18688/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 52 [18944/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 52 [19200/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 52 [19456/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 52 [19712/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 52 [19968/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 52 [20224/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 52 [20480/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 52 [20736/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 52 [20992/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 52 [21248/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 52 [21504/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 52 [21760/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 52 [22016/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 52 [22272/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 52 [22528/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 52 [22784/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 52 [23040/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 52 [23296/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 52 [23552/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 52 [23808/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 52 [24064/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 52 [24320/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 52 [24576/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 52 [24832/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 52 [25088/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 52 [25344/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 52 [25600/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 52 [25856/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 52 [26112/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 52 [26368/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 52 [26624/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 52 [26880/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 52 [27136/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 52 [27392/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 52 [27648/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 52 [27904/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 52 [28160/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 52 [28416/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 52 [28672/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 52 [28928/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 52 [29184/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 52 [29440/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 52 [29696/50000]	Loss: 4.5912	LR: 1.000000
Training Epoch: 52 [29952/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 52 [30208/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 52 [30464/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 52 [30720/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 52 [30976/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 52 [31232/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 52 [31488/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 52 [31744/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 52 [32000/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 52 [32256/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 52 [32512/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 52 [32768/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 52 [33024/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 52 [33280/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 52 [33536/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 52 [33792/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 52 [34048/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 52 [34304/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 52 [34560/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 52 [34816/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 52 [35072/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 52 [35328/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 52 [35584/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 52 [35840/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 52 [36096/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 52 [36352/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 52 [36608/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 52 [36864/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 52 [37120/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 52 [37376/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 52 [37632/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 52 [37888/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 52 [38144/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 52 [38400/50000]	Loss: 4.6328	LR: 1.000000
Training Epoch: 52 [38656/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 52 [38912/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 52 [39168/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 52 [39424/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 52 [39680/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 52 [39936/50000]	Loss: 4.6335	LR: 1.000000
Training Epoch: 52 [40192/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 52 [40448/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 52 [40704/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 52 [40960/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 52 [41216/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 52 [41472/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 52 [41728/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 52 [41984/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 52 [42240/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 52 [42496/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 52 [42752/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 52 [43008/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 52 [43264/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 52 [43520/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 52 [43776/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 52 [44032/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 52 [44288/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 52 [44544/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 52 [44800/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 52 [45056/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 52 [45312/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 52 [45568/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 52 [45824/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 52 [46080/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 52 [46336/50000]	Loss: 4.6253	LR: 1.000000
Training Epoch: 52 [46592/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 52 [46848/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 52 [47104/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 52 [47360/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 52 [47616/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 52 [47872/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 52 [48128/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 52 [48384/50000]	Loss: 4.6004	LR: 1.000000
Training Epoch: 52 [48640/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 52 [48896/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 52 [49152/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 52 [49408/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 52 [49664/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 52 [49920/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 52 [50000/50000]	Loss: 4.6236	LR: 1.000000
epoch 52 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   78010 GB |   78010 GB |
|       from large pool |  400448 KB |    1770 MB |   77940 GB |   77939 GB |
|       from small pool |    3549 KB |       9 MB |      70 GB |      70 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   78010 GB |   78010 GB |
|       from large pool |  400448 KB |    1770 MB |   77940 GB |   77939 GB |
|       from small pool |    3549 KB |       9 MB |      70 GB |      70 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   47999 GB |   47999 GB |
|       from large pool |  244672 KB |  473024 KB |   47919 GB |   47919 GB |
|       from small pool |    2594 KB |    4843 KB |      80 GB |      80 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3329 K  |    3329 K  |
|       from large pool |      36    |      77    |    1609 K  |    1609 K  |
|       from small pool |     186    |     224    |    1719 K  |    1719 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3329 K  |    3329 K  |
|       from large pool |      36    |      77    |    1609 K  |    1609 K  |
|       from small pool |     186    |     224    |    1719 K  |    1719 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    1696 K  |    1695 K  |
|       from large pool |      10    |      11    |     666 K  |     666 K  |
|       from small pool |      13    |      23    |    1029 K  |    1029 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 52, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 53 [256/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 53 [512/50000]	Loss: 4.6049	LR: 1.000000
Training Epoch: 53 [768/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 53 [1024/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 53 [1280/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 53 [1536/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 53 [1792/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 53 [2048/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 53 [2304/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 53 [2560/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 53 [2816/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 53 [3072/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 53 [3328/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 53 [3584/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 53 [3840/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 53 [4096/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 53 [4352/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 53 [4608/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 53 [4864/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 53 [5120/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 53 [5376/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 53 [5632/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 53 [5888/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 53 [6144/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 53 [6400/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 53 [6656/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 53 [6912/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 53 [7168/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 53 [7424/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 53 [7680/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 53 [7936/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 53 [8192/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 53 [8448/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 53 [8704/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 53 [8960/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 53 [9216/50000]	Loss: 4.5961	LR: 1.000000
Training Epoch: 53 [9472/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 53 [9728/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 53 [9984/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 53 [10240/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 53 [10496/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 53 [10752/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 53 [11008/50000]	Loss: 4.6309	LR: 1.000000
Training Epoch: 53 [11264/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 53 [11520/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 53 [11776/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 53 [12032/50000]	Loss: 4.6273	LR: 1.000000
Training Epoch: 53 [12288/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 53 [12544/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 53 [12800/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 53 [13056/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 53 [13312/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 53 [13568/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 53 [13824/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 53 [14080/50000]	Loss: 4.6002	LR: 1.000000
Training Epoch: 53 [14336/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 53 [14592/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 53 [14848/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 53 [15104/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 53 [15360/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 53 [15616/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 53 [15872/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 53 [16128/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 53 [16384/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 53 [16640/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 53 [16896/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 53 [17152/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 53 [17408/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 53 [17664/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 53 [17920/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 53 [18176/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 53 [18432/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 53 [18688/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 53 [18944/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 53 [19200/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 53 [19456/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 53 [19712/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 53 [19968/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 53 [20224/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 53 [20480/50000]	Loss: 4.6063	LR: 1.000000
Training Epoch: 53 [20736/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 53 [20992/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 53 [21248/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 53 [21504/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 53 [21760/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 53 [22016/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 53 [22272/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 53 [22528/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 53 [22784/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 53 [23040/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 53 [23296/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 53 [23552/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 53 [23808/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 53 [24064/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 53 [24320/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 53 [24576/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 53 [24832/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 53 [25088/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 53 [25344/50000]	Loss: 4.6254	LR: 1.000000
Training Epoch: 53 [25600/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 53 [25856/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 53 [26112/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 53 [26368/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 53 [26624/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 53 [26880/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 53 [27136/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 53 [27392/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 53 [27648/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 53 [27904/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 53 [28160/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 53 [28416/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 53 [28672/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 53 [28928/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 53 [29184/50000]	Loss: 4.5976	LR: 1.000000
Training Epoch: 53 [29440/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 53 [29696/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 53 [29952/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 53 [30208/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 53 [30464/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 53 [30720/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 53 [30976/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 53 [31232/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 53 [31488/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 53 [31744/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 53 [32000/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 53 [32256/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 53 [32512/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 53 [32768/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 53 [33024/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 53 [33280/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 53 [33536/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 53 [33792/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 53 [34048/50000]	Loss: 4.5978	LR: 1.000000
Training Epoch: 53 [34304/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 53 [34560/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 53 [34816/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 53 [35072/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 53 [35328/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 53 [35584/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 53 [35840/50000]	Loss: 4.6003	LR: 1.000000
Training Epoch: 53 [36096/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 53 [36352/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 53 [36608/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 53 [36864/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 53 [37120/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 53 [37376/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 53 [37632/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 53 [37888/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 53 [38144/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 53 [38400/50000]	Loss: 4.6068	LR: 1.000000
Training Epoch: 53 [38656/50000]	Loss: 4.6069	LR: 1.000000
Training Epoch: 53 [38912/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 53 [39168/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 53 [39424/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 53 [39680/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 53 [39936/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 53 [40192/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 53 [40448/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 53 [40704/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 53 [40960/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 53 [41216/50000]	Loss: 4.6083	LR: 1.000000
Training Epoch: 53 [41472/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 53 [41728/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 53 [41984/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 53 [42240/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 53 [42496/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 53 [42752/50000]	Loss: 4.5977	LR: 1.000000
Training Epoch: 53 [43008/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 53 [43264/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 53 [43520/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 53 [43776/50000]	Loss: 4.6078	LR: 1.000000
Training Epoch: 53 [44032/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 53 [44288/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 53 [44544/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 53 [44800/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 53 [45056/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 53 [45312/50000]	Loss: 4.6360	LR: 1.000000
Training Epoch: 53 [45568/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 53 [45824/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 53 [46080/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 53 [46336/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 53 [46592/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 53 [46848/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 53 [47104/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 53 [47360/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 53 [47616/50000]	Loss: 4.6048	LR: 1.000000
Training Epoch: 53 [47872/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 53 [48128/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 53 [48384/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 53 [48640/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 53 [48896/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 53 [49152/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 53 [49408/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 53 [49664/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 53 [49920/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 53 [50000/50000]	Loss: 4.6185	LR: 1.000000
epoch 53 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   79510 GB |   79510 GB |
|       from large pool |  400448 KB |    1770 MB |   79439 GB |   79438 GB |
|       from small pool |    3549 KB |       9 MB |      71 GB |      71 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   79510 GB |   79510 GB |
|       from large pool |  400448 KB |    1770 MB |   79439 GB |   79438 GB |
|       from small pool |    3549 KB |       9 MB |      71 GB |      71 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   48922 GB |   48922 GB |
|       from large pool |  244672 KB |  473024 KB |   48840 GB |   48840 GB |
|       from small pool |    2594 KB |    4843 KB |      81 GB |      81 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3393 K  |    3393 K  |
|       from large pool |      36    |      77    |    1640 K  |    1640 K  |
|       from small pool |     186    |     224    |    1752 K  |    1752 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3393 K  |    3393 K  |
|       from large pool |      36    |      77    |    1640 K  |    1640 K  |
|       from small pool |     186    |     224    |    1752 K  |    1752 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1727 K  |    1727 K  |
|       from large pool |      10    |      11    |     679 K  |     679 K  |
|       from small pool |      11    |      23    |    1048 K  |    1048 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 53, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 54 [256/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 54 [512/50000]	Loss: 4.5997	LR: 1.000000
Training Epoch: 54 [768/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 54 [1024/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 54 [1280/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 54 [1536/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 54 [1792/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 54 [2048/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 54 [2304/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 54 [2560/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 54 [2816/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 54 [3072/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 54 [3328/50000]	Loss: 4.6042	LR: 1.000000
Training Epoch: 54 [3584/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 54 [3840/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 54 [4096/50000]	Loss: 4.5958	LR: 1.000000
Training Epoch: 54 [4352/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 54 [4608/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 54 [4864/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 54 [5120/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 54 [5376/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 54 [5632/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 54 [5888/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 54 [6144/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 54 [6400/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 54 [6656/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 54 [6912/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 54 [7168/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 54 [7424/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 54 [7680/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 54 [7936/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 54 [8192/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 54 [8448/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 54 [8704/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 54 [8960/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 54 [9216/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 54 [9472/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 54 [9728/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 54 [9984/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 54 [10240/50000]	Loss: 4.6017	LR: 1.000000
Training Epoch: 54 [10496/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 54 [10752/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 54 [11008/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 54 [11264/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 54 [11520/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 54 [11776/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 54 [12032/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 54 [12288/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 54 [12544/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 54 [12800/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 54 [13056/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 54 [13312/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 54 [13568/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 54 [13824/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 54 [14080/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 54 [14336/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 54 [14592/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 54 [14848/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 54 [15104/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 54 [15360/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 54 [15616/50000]	Loss: 4.6003	LR: 1.000000
Training Epoch: 54 [15872/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 54 [16128/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 54 [16384/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 54 [16640/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 54 [16896/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 54 [17152/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 54 [17408/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 54 [17664/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 54 [17920/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 54 [18176/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 54 [18432/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 54 [18688/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 54 [18944/50000]	Loss: 4.6316	LR: 1.000000
Training Epoch: 54 [19200/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 54 [19456/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 54 [19712/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 54 [19968/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 54 [20224/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 54 [20480/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 54 [20736/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 54 [20992/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 54 [21248/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 54 [21504/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 54 [21760/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 54 [22016/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 54 [22272/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 54 [22528/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 54 [22784/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 54 [23040/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 54 [23296/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 54 [23552/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 54 [23808/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 54 [24064/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 54 [24320/50000]	Loss: 4.6036	LR: 1.000000
Training Epoch: 54 [24576/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 54 [24832/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 54 [25088/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 54 [25344/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 54 [25600/50000]	Loss: 4.6253	LR: 1.000000
Training Epoch: 54 [25856/50000]	Loss: 4.5989	LR: 1.000000
Training Epoch: 54 [26112/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 54 [26368/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 54 [26624/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 54 [26880/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 54 [27136/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 54 [27392/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 54 [27648/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 54 [27904/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 54 [28160/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 54 [28416/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 54 [28672/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 54 [28928/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 54 [29184/50000]	Loss: 4.6001	LR: 1.000000
Training Epoch: 54 [29440/50000]	Loss: 4.5972	LR: 1.000000
Training Epoch: 54 [29696/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 54 [29952/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 54 [30208/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 54 [30464/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 54 [30720/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 54 [30976/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 54 [31232/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 54 [31488/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 54 [31744/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 54 [32000/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 54 [32256/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 54 [32512/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 54 [32768/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 54 [33024/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 54 [33280/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 54 [33536/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 54 [33792/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 54 [34048/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 54 [34304/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 54 [34560/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 54 [34816/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 54 [35072/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 54 [35328/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 54 [35584/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 54 [35840/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 54 [36096/50000]	Loss: 4.6075	LR: 1.000000
Training Epoch: 54 [36352/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 54 [36608/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 54 [36864/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 54 [37120/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 54 [37376/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 54 [37632/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 54 [37888/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 54 [38144/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 54 [38400/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 54 [38656/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 54 [38912/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 54 [39168/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 54 [39424/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 54 [39680/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 54 [39936/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 54 [40192/50000]	Loss: 4.5993	LR: 1.000000
Training Epoch: 54 [40448/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 54 [40704/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 54 [40960/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 54 [41216/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 54 [41472/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 54 [41728/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 54 [41984/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 54 [42240/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 54 [42496/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 54 [42752/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 54 [43008/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 54 [43264/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 54 [43520/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 54 [43776/50000]	Loss: 4.6311	LR: 1.000000
Training Epoch: 54 [44032/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 54 [44288/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 54 [44544/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 54 [44800/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 54 [45056/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 54 [45312/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 54 [45568/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 54 [45824/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 54 [46080/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 54 [46336/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 54 [46592/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 54 [46848/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 54 [47104/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 54 [47360/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 54 [47616/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 54 [47872/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 54 [48128/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 54 [48384/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 54 [48640/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 54 [48896/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 54 [49152/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 54 [49408/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 54 [49664/50000]	Loss: 4.6009	LR: 1.000000
Training Epoch: 54 [49920/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 54 [50000/50000]	Loss: 4.6164	LR: 1.000000
epoch 54 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   81010 GB |   81010 GB |
|       from large pool |  400448 KB |    1770 MB |   80937 GB |   80937 GB |
|       from small pool |    3549 KB |       9 MB |      72 GB |      72 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   81010 GB |   81010 GB |
|       from large pool |  400448 KB |    1770 MB |   80937 GB |   80937 GB |
|       from small pool |    3549 KB |       9 MB |      72 GB |      72 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   49845 GB |   49845 GB |
|       from large pool |  244672 KB |  473024 KB |   49762 GB |   49762 GB |
|       from small pool |    2594 KB |    4843 KB |      83 GB |      83 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3457 K  |    3457 K  |
|       from large pool |      36    |      77    |    1671 K  |    1671 K  |
|       from small pool |     186    |     224    |    1785 K  |    1785 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3457 K  |    3457 K  |
|       from large pool |      36    |      77    |    1671 K  |    1671 K  |
|       from small pool |     186    |     224    |    1785 K  |    1785 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    1761 K  |    1761 K  |
|       from large pool |      10    |      11    |     692 K  |     692 K  |
|       from small pool |      13    |      23    |    1068 K  |    1068 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 54, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 55 [256/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 55 [512/50000]	Loss: 4.6061	LR: 1.000000
Training Epoch: 55 [768/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 55 [1024/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 55 [1280/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 55 [1536/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 55 [1792/50000]	Loss: 4.6004	LR: 1.000000
Training Epoch: 55 [2048/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 55 [2304/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 55 [2560/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 55 [2816/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 55 [3072/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 55 [3328/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 55 [3584/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 55 [3840/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 55 [4096/50000]	Loss: 4.6007	LR: 1.000000
Training Epoch: 55 [4352/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 55 [4608/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 55 [4864/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 55 [5120/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 55 [5376/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 55 [5632/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 55 [5888/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 55 [6144/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 55 [6400/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 55 [6656/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 55 [6912/50000]	Loss: 4.6295	LR: 1.000000
Training Epoch: 55 [7168/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 55 [7424/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 55 [7680/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 55 [7936/50000]	Loss: 4.6317	LR: 1.000000
Training Epoch: 55 [8192/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 55 [8448/50000]	Loss: 4.6315	LR: 1.000000
Training Epoch: 55 [8704/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 55 [8960/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 55 [9216/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 55 [9472/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 55 [9728/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 55 [9984/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 55 [10240/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 55 [10496/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 55 [10752/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 55 [11008/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 55 [11264/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 55 [11520/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 55 [11776/50000]	Loss: 4.5985	LR: 1.000000
Training Epoch: 55 [12032/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 55 [12288/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 55 [12544/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 55 [12800/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 55 [13056/50000]	Loss: 4.6282	LR: 1.000000
Training Epoch: 55 [13312/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 55 [13568/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 55 [13824/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 55 [14080/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 55 [14336/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 55 [14592/50000]	Loss: 4.6362	LR: 1.000000
Training Epoch: 55 [14848/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 55 [15104/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 55 [15360/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 55 [15616/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 55 [15872/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 55 [16128/50000]	Loss: 4.6388	LR: 1.000000
Training Epoch: 55 [16384/50000]	Loss: 4.6354	LR: 1.000000
Training Epoch: 55 [16640/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 55 [16896/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 55 [17152/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 55 [17408/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 55 [17664/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 55 [17920/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 55 [18176/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 55 [18432/50000]	Loss: 4.6013	LR: 1.000000
Training Epoch: 55 [18688/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 55 [18944/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 55 [19200/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 55 [19456/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 55 [19712/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 55 [19968/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 55 [20224/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 55 [20480/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 55 [20736/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 55 [20992/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 55 [21248/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 55 [21504/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 55 [21760/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 55 [22016/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 55 [22272/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 55 [22528/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 55 [22784/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 55 [23040/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 55 [23296/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 55 [23552/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 55 [23808/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 55 [24064/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 55 [24320/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 55 [24576/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 55 [24832/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 55 [25088/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 55 [25344/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 55 [25600/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 55 [25856/50000]	Loss: 4.6346	LR: 1.000000
Training Epoch: 55 [26112/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 55 [26368/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 55 [26624/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 55 [26880/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 55 [27136/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 55 [27392/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 55 [27648/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 55 [27904/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 55 [28160/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 55 [28416/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 55 [28672/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 55 [28928/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 55 [29184/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 55 [29440/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 55 [29696/50000]	Loss: 4.6060	LR: 1.000000
Training Epoch: 55 [29952/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 55 [30208/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 55 [30464/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 55 [30720/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 55 [30976/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 55 [31232/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 55 [31488/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 55 [31744/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 55 [32000/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 55 [32256/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 55 [32512/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 55 [32768/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 55 [33024/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 55 [33280/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 55 [33536/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 55 [33792/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 55 [34048/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 55 [34304/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 55 [34560/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 55 [34816/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 55 [35072/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 55 [35328/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 55 [35584/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 55 [35840/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 55 [36096/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 55 [36352/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 55 [36608/50000]	Loss: 4.6030	LR: 1.000000
Training Epoch: 55 [36864/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 55 [37120/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 55 [37376/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 55 [37632/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 55 [37888/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 55 [38144/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 55 [38400/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 55 [38656/50000]	Loss: 4.6359	LR: 1.000000
Training Epoch: 55 [38912/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 55 [39168/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 55 [39424/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 55 [39680/50000]	Loss: 4.5962	LR: 1.000000
Training Epoch: 55 [39936/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 55 [40192/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 55 [40448/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 55 [40704/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 55 [40960/50000]	Loss: 4.6009	LR: 1.000000
Training Epoch: 55 [41216/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 55 [41472/50000]	Loss: 4.6100	LR: 1.000000
Training Epoch: 55 [41728/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 55 [41984/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 55 [42240/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 55 [42496/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 55 [42752/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 55 [43008/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 55 [43264/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 55 [43520/50000]	Loss: 4.6264	LR: 1.000000
Training Epoch: 55 [43776/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 55 [44032/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 55 [44288/50000]	Loss: 4.5973	LR: 1.000000
Training Epoch: 55 [44544/50000]	Loss: 4.5937	LR: 1.000000
Training Epoch: 55 [44800/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 55 [45056/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 55 [45312/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 55 [45568/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 55 [45824/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 55 [46080/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 55 [46336/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 55 [46592/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 55 [46848/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 55 [47104/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 55 [47360/50000]	Loss: 4.6221	LR: 1.000000
Training Epoch: 55 [47616/50000]	Loss: 4.6223	LR: 1.000000
Training Epoch: 55 [47872/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 55 [48128/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 55 [48384/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 55 [48640/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 55 [48896/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 55 [49152/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 55 [49408/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 55 [49664/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 55 [49920/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 55 [50000/50000]	Loss: 4.5890	LR: 1.000000
epoch 55 training time consumed: 21.73s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   82510 GB |   82510 GB |
|       from large pool |  400448 KB |    1770 MB |   82436 GB |   82436 GB |
|       from small pool |    3549 KB |       9 MB |      74 GB |      74 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   82510 GB |   82510 GB |
|       from large pool |  400448 KB |    1770 MB |   82436 GB |   82436 GB |
|       from small pool |    3549 KB |       9 MB |      74 GB |      74 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   50768 GB |   50768 GB |
|       from large pool |  244672 KB |  473024 KB |   50683 GB |   50683 GB |
|       from small pool |    2594 KB |    4843 KB |      84 GB |      84 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3521 K  |    3521 K  |
|       from large pool |      36    |      77    |    1702 K  |    1702 K  |
|       from small pool |     186    |     224    |    1818 K  |    1818 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3521 K  |    3521 K  |
|       from large pool |      36    |      77    |    1702 K  |    1702 K  |
|       from small pool |     186    |     224    |    1818 K  |    1818 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    1793 K  |    1793 K  |
|       from large pool |      10    |      11    |     705 K  |     705 K  |
|       from small pool |      11    |      23    |    1088 K  |    1088 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 55, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 56 [256/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 56 [512/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 56 [768/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 56 [1024/50000]	Loss: 4.5899	LR: 1.000000
Training Epoch: 56 [1280/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 56 [1536/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 56 [1792/50000]	Loss: 4.6350	LR: 1.000000
Training Epoch: 56 [2048/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 56 [2304/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 56 [2560/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 56 [2816/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 56 [3072/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 56 [3328/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 56 [3584/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 56 [3840/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 56 [4096/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 56 [4352/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 56 [4608/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 56 [4864/50000]	Loss: 4.6265	LR: 1.000000
Training Epoch: 56 [5120/50000]	Loss: 4.6286	LR: 1.000000
Training Epoch: 56 [5376/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 56 [5632/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 56 [5888/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 56 [6144/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 56 [6400/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 56 [6656/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 56 [6912/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 56 [7168/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 56 [7424/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 56 [7680/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 56 [7936/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 56 [8192/50000]	Loss: 4.6046	LR: 1.000000
Training Epoch: 56 [8448/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 56 [8704/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 56 [8960/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 56 [9216/50000]	Loss: 4.6241	LR: 1.000000
Training Epoch: 56 [9472/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 56 [9728/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 56 [9984/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 56 [10240/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 56 [10496/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 56 [10752/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 56 [11008/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 56 [11264/50000]	Loss: 4.6267	LR: 1.000000
Training Epoch: 56 [11520/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 56 [11776/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 56 [12032/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 56 [12288/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 56 [12544/50000]	Loss: 4.5934	LR: 1.000000
Training Epoch: 56 [12800/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 56 [13056/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 56 [13312/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 56 [13568/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 56 [13824/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 56 [14080/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 56 [14336/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 56 [14592/50000]	Loss: 4.6044	LR: 1.000000
Training Epoch: 56 [14848/50000]	Loss: 4.6300	LR: 1.000000
Training Epoch: 56 [15104/50000]	Loss: 4.6024	LR: 1.000000
Training Epoch: 56 [15360/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 56 [15616/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 56 [15872/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 56 [16128/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 56 [16384/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 56 [16640/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 56 [16896/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 56 [17152/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 56 [17408/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 56 [17664/50000]	Loss: 4.6230	LR: 1.000000
Training Epoch: 56 [17920/50000]	Loss: 4.5976	LR: 1.000000
Training Epoch: 56 [18176/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 56 [18432/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 56 [18688/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 56 [18944/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 56 [19200/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 56 [19456/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 56 [19712/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 56 [19968/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 56 [20224/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 56 [20480/50000]	Loss: 4.6309	LR: 1.000000
Training Epoch: 56 [20736/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 56 [20992/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 56 [21248/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 56 [21504/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 56 [21760/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 56 [22016/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 56 [22272/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 56 [22528/50000]	Loss: 4.5994	LR: 1.000000
Training Epoch: 56 [22784/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 56 [23040/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 56 [23296/50000]	Loss: 4.6355	LR: 1.000000
Training Epoch: 56 [23552/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 56 [23808/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 56 [24064/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 56 [24320/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 56 [24576/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 56 [24832/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 56 [25088/50000]	Loss: 4.6293	LR: 1.000000
Training Epoch: 56 [25344/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 56 [25600/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 56 [25856/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 56 [26112/50000]	Loss: 4.6409	LR: 1.000000
Training Epoch: 56 [26368/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 56 [26624/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 56 [26880/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 56 [27136/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 56 [27392/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 56 [27648/50000]	Loss: 4.6207	LR: 1.000000
Training Epoch: 56 [27904/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 56 [28160/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 56 [28416/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 56 [28672/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 56 [28928/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 56 [29184/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 56 [29440/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 56 [29696/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 56 [29952/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 56 [30208/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 56 [30464/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 56 [30720/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 56 [30976/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 56 [31232/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 56 [31488/50000]	Loss: 4.6327	LR: 1.000000
Training Epoch: 56 [31744/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 56 [32000/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 56 [32256/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 56 [32512/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 56 [32768/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 56 [33024/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 56 [33280/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 56 [33536/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 56 [33792/50000]	Loss: 4.6383	LR: 1.000000
Training Epoch: 56 [34048/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 56 [34304/50000]	Loss: 4.6008	LR: 1.000000
Training Epoch: 56 [34560/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 56 [34816/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 56 [35072/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 56 [35328/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 56 [35584/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 56 [35840/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 56 [36096/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 56 [36352/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 56 [36608/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 56 [36864/50000]	Loss: 4.6287	LR: 1.000000
Training Epoch: 56 [37120/50000]	Loss: 4.6259	LR: 1.000000
Training Epoch: 56 [37376/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 56 [37632/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 56 [37888/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 56 [38144/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 56 [38400/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 56 [38656/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 56 [38912/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 56 [39168/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 56 [39424/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 56 [39680/50000]	Loss: 4.6093	LR: 1.000000
Training Epoch: 56 [39936/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 56 [40192/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 56 [40448/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 56 [40704/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 56 [40960/50000]	Loss: 4.6165	LR: 1.000000
Training Epoch: 56 [41216/50000]	Loss: 4.6314	LR: 1.000000
Training Epoch: 56 [41472/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 56 [41728/50000]	Loss: 4.6128	LR: 1.000000
Training Epoch: 56 [41984/50000]	Loss: 4.6119	LR: 1.000000
Training Epoch: 56 [42240/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 56 [42496/50000]	Loss: 4.6136	LR: 1.000000
Training Epoch: 56 [42752/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 56 [43008/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 56 [43264/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 56 [43520/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 56 [43776/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 56 [44032/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 56 [44288/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 56 [44544/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 56 [44800/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 56 [45056/50000]	Loss: 4.6340	LR: 1.000000
Training Epoch: 56 [45312/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 56 [45568/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 56 [45824/50000]	Loss: 4.6261	LR: 1.000000
Training Epoch: 56 [46080/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 56 [46336/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 56 [46592/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 56 [46848/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 56 [47104/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 56 [47360/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 56 [47616/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 56 [47872/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 56 [48128/50000]	Loss: 4.6202	LR: 1.000000
Training Epoch: 56 [48384/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 56 [48640/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 56 [48896/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 56 [49152/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 56 [49408/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 56 [49664/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 56 [49920/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 56 [50000/50000]	Loss: 4.6283	LR: 1.000000
epoch 56 training time consumed: 21.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   84011 GB |   84010 GB |
|       from large pool |  400448 KB |    1770 MB |   83935 GB |   83935 GB |
|       from small pool |    3549 KB |       9 MB |      75 GB |      75 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   84011 GB |   84010 GB |
|       from large pool |  400448 KB |    1770 MB |   83935 GB |   83935 GB |
|       from small pool |    3549 KB |       9 MB |      75 GB |      75 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   51691 GB |   51691 GB |
|       from large pool |  244672 KB |  473024 KB |   51605 GB |   51605 GB |
|       from small pool |    2594 KB |    4843 KB |      86 GB |      86 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3585 K  |    3585 K  |
|       from large pool |      36    |      77    |    1733 K  |    1733 K  |
|       from small pool |     186    |     224    |    1852 K  |    1851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3585 K  |    3585 K  |
|       from large pool |      36    |      77    |    1733 K  |    1733 K  |
|       from small pool |     186    |     224    |    1852 K  |    1851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1826 K  |    1826 K  |
|       from large pool |      10    |      11    |     718 K  |     718 K  |
|       from small pool |      10    |      23    |    1108 K  |    1108 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 56, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 57 [256/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 57 [512/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 57 [768/50000]	Loss: 4.6203	LR: 1.000000
Training Epoch: 57 [1024/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 57 [1280/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 57 [1536/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 57 [1792/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 57 [2048/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 57 [2304/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 57 [2560/50000]	Loss: 4.6252	LR: 1.000000
Training Epoch: 57 [2816/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 57 [3072/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 57 [3328/50000]	Loss: 4.6014	LR: 1.000000
Training Epoch: 57 [3584/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 57 [3840/50000]	Loss: 4.6307	LR: 1.000000
Training Epoch: 57 [4096/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 57 [4352/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 57 [4608/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 57 [4864/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 57 [5120/50000]	Loss: 4.6282	LR: 1.000000
Training Epoch: 57 [5376/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 57 [5632/50000]	Loss: 4.6134	LR: 1.000000
Training Epoch: 57 [5888/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 57 [6144/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 57 [6400/50000]	Loss: 4.6290	LR: 1.000000
Training Epoch: 57 [6656/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 57 [6912/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 57 [7168/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 57 [7424/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 57 [7680/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 57 [7936/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 57 [8192/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 57 [8448/50000]	Loss: 4.6037	LR: 1.000000
Training Epoch: 57 [8704/50000]	Loss: 4.6131	LR: 1.000000
Training Epoch: 57 [8960/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 57 [9216/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 57 [9472/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 57 [9728/50000]	Loss: 4.6294	LR: 1.000000
Training Epoch: 57 [9984/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 57 [10240/50000]	Loss: 4.6296	LR: 1.000000
Training Epoch: 57 [10496/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 57 [10752/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 57 [11008/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 57 [11264/50000]	Loss: 4.6039	LR: 1.000000
Training Epoch: 57 [11520/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 57 [11776/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 57 [12032/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 57 [12288/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 57 [12544/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 57 [12800/50000]	Loss: 4.5983	LR: 1.000000
Training Epoch: 57 [13056/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 57 [13312/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 57 [13568/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 57 [13824/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 57 [14080/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 57 [14336/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 57 [14592/50000]	Loss: 4.6272	LR: 1.000000
Training Epoch: 57 [14848/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 57 [15104/50000]	Loss: 4.6204	LR: 1.000000
Training Epoch: 57 [15360/50000]	Loss: 4.6298	LR: 1.000000
Training Epoch: 57 [15616/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 57 [15872/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 57 [16128/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 57 [16384/50000]	Loss: 4.6239	LR: 1.000000
Training Epoch: 57 [16640/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 57 [16896/50000]	Loss: 4.6034	LR: 1.000000
Training Epoch: 57 [17152/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 57 [17408/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 57 [17664/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 57 [17920/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 57 [18176/50000]	Loss: 4.5935	LR: 1.000000
Training Epoch: 57 [18432/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 57 [18688/50000]	Loss: 4.6198	LR: 1.000000
Training Epoch: 57 [18944/50000]	Loss: 4.6026	LR: 1.000000
Training Epoch: 57 [19200/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 57 [19456/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 57 [19712/50000]	Loss: 4.6322	LR: 1.000000
Training Epoch: 57 [19968/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 57 [20224/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 57 [20480/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 57 [20736/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 57 [20992/50000]	Loss: 4.6038	LR: 1.000000
Training Epoch: 57 [21248/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 57 [21504/50000]	Loss: 4.5979	LR: 1.000000
Training Epoch: 57 [21760/50000]	Loss: 4.6188	LR: 1.000000
Training Epoch: 57 [22016/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 57 [22272/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 57 [22528/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 57 [22784/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 57 [23040/50000]	Loss: 4.6280	LR: 1.000000
Training Epoch: 57 [23296/50000]	Loss: 4.6274	LR: 1.000000
Training Epoch: 57 [23552/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 57 [23808/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 57 [24064/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 57 [24320/50000]	Loss: 4.6305	LR: 1.000000
Training Epoch: 57 [24576/50000]	Loss: 4.6228	LR: 1.000000
Training Epoch: 57 [24832/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 57 [25088/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 57 [25344/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 57 [25600/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 57 [25856/50000]	Loss: 4.6003	LR: 1.000000
Training Epoch: 57 [26112/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 57 [26368/50000]	Loss: 4.6310	LR: 1.000000
Training Epoch: 57 [26624/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 57 [26880/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 57 [27136/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 57 [27392/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 57 [27648/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 57 [27904/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 57 [28160/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 57 [28416/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 57 [28672/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 57 [28928/50000]	Loss: 4.6094	LR: 1.000000
Training Epoch: 57 [29184/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 57 [29440/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 57 [29696/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 57 [29952/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 57 [30208/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 57 [30464/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 57 [30720/50000]	Loss: 4.6040	LR: 1.000000
Training Epoch: 57 [30976/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 57 [31232/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 57 [31488/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 57 [31744/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 57 [32000/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 57 [32256/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 57 [32512/50000]	Loss: 4.6017	LR: 1.000000
Training Epoch: 57 [32768/50000]	Loss: 4.6172	LR: 1.000000
Training Epoch: 57 [33024/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 57 [33280/50000]	Loss: 4.5987	LR: 1.000000
Training Epoch: 57 [33536/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 57 [33792/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 57 [34048/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 57 [34304/50000]	Loss: 4.6067	LR: 1.000000
Training Epoch: 57 [34560/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 57 [34816/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 57 [35072/50000]	Loss: 4.6028	LR: 1.000000
Training Epoch: 57 [35328/50000]	Loss: 4.6329	LR: 1.000000
Training Epoch: 57 [35584/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 57 [35840/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 57 [36096/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 57 [36352/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 57 [36608/50000]	Loss: 4.6301	LR: 1.000000
Training Epoch: 57 [36864/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 57 [37120/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 57 [37376/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 57 [37632/50000]	Loss: 4.6006	LR: 1.000000
Training Epoch: 57 [37888/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 57 [38144/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 57 [38400/50000]	Loss: 4.6199	LR: 1.000000
Training Epoch: 57 [38656/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 57 [38912/50000]	Loss: 4.6214	LR: 1.000000
Training Epoch: 57 [39168/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 57 [39424/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 57 [39680/50000]	Loss: 4.6145	LR: 1.000000
Training Epoch: 57 [39936/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 57 [40192/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 57 [40448/50000]	Loss: 4.6232	LR: 1.000000
Training Epoch: 57 [40704/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 57 [40960/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 57 [41216/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 57 [41472/50000]	Loss: 4.5974	LR: 1.000000
Training Epoch: 57 [41728/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 57 [41984/50000]	Loss: 4.6111	LR: 1.000000
Training Epoch: 57 [42240/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 57 [42496/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 57 [42752/50000]	Loss: 4.6350	LR: 1.000000
Training Epoch: 57 [43008/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 57 [43264/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 57 [43520/50000]	Loss: 4.6116	LR: 1.000000
Training Epoch: 57 [43776/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 57 [44032/50000]	Loss: 4.6245	LR: 1.000000
Training Epoch: 57 [44288/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 57 [44544/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 57 [44800/50000]	Loss: 4.5999	LR: 1.000000
Training Epoch: 57 [45056/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 57 [45312/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 57 [45568/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 57 [45824/50000]	Loss: 4.6051	LR: 1.000000
Training Epoch: 57 [46080/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 57 [46336/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 57 [46592/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 57 [46848/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 57 [47104/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 57 [47360/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 57 [47616/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 57 [47872/50000]	Loss: 4.6052	LR: 1.000000
Training Epoch: 57 [48128/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 57 [48384/50000]	Loss: 4.6133	LR: 1.000000
Training Epoch: 57 [48640/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 57 [48896/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 57 [49152/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 57 [49408/50000]	Loss: 4.6058	LR: 1.000000
Training Epoch: 57 [49664/50000]	Loss: 4.6197	LR: 1.000000
Training Epoch: 57 [49920/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 57 [50000/50000]	Loss: 4.6026	LR: 1.000000
epoch 57 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   85511 GB |   85510 GB |
|       from large pool |  400448 KB |    1770 MB |   85434 GB |   85434 GB |
|       from small pool |    3549 KB |       9 MB |      76 GB |      76 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   85511 GB |   85510 GB |
|       from large pool |  400448 KB |    1770 MB |   85434 GB |   85434 GB |
|       from small pool |    3549 KB |       9 MB |      76 GB |      76 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   52614 GB |   52614 GB |
|       from large pool |  244672 KB |  473024 KB |   52526 GB |   52526 GB |
|       from small pool |    2594 KB |    4843 KB |      87 GB |      87 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3649 K  |    3649 K  |
|       from large pool |      36    |      77    |    1764 K  |    1764 K  |
|       from small pool |     186    |     224    |    1885 K  |    1884 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3649 K  |    3649 K  |
|       from large pool |      36    |      77    |    1764 K  |    1764 K  |
|       from small pool |     186    |     224    |    1885 K  |    1884 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1858 K  |    1858 K  |
|       from large pool |      10    |      11    |     730 K  |     730 K  |
|       from small pool |      10    |      23    |    1127 K  |    1127 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 57, Average loss: 0.0185, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 58 [256/50000]	Loss: 4.6181	LR: 1.000000
Training Epoch: 58 [512/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 58 [768/50000]	Loss: 4.6035	LR: 1.000000
Training Epoch: 58 [1024/50000]	Loss: 4.6144	LR: 1.000000
Training Epoch: 58 [1280/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 58 [1536/50000]	Loss: 4.6118	LR: 1.000000
Training Epoch: 58 [1792/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 58 [2048/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 58 [2304/50000]	Loss: 4.6137	LR: 1.000000
Training Epoch: 58 [2560/50000]	Loss: 4.6277	LR: 1.000000
Training Epoch: 58 [2816/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 58 [3072/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 58 [3328/50000]	Loss: 4.6266	LR: 1.000000
Training Epoch: 58 [3584/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 58 [3840/50000]	Loss: 4.5996	LR: 1.000000
Training Epoch: 58 [4096/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 58 [4352/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 58 [4608/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 58 [4864/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 58 [5120/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 58 [5376/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 58 [5632/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 58 [5888/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 58 [6144/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 58 [6400/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 58 [6656/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 58 [6912/50000]	Loss: 4.6283	LR: 1.000000
Training Epoch: 58 [7168/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 58 [7424/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 58 [7680/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 58 [7936/50000]	Loss: 4.6334	LR: 1.000000
Training Epoch: 58 [8192/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 58 [8448/50000]	Loss: 4.6271	LR: 1.000000
Training Epoch: 58 [8704/50000]	Loss: 4.6015	LR: 1.000000
Training Epoch: 58 [8960/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 58 [9216/50000]	Loss: 4.6045	LR: 1.000000
Training Epoch: 58 [9472/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 58 [9728/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 58 [9984/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 58 [10240/50000]	Loss: 4.6127	LR: 1.000000
Training Epoch: 58 [10496/50000]	Loss: 4.6108	LR: 1.000000
Training Epoch: 58 [10752/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 58 [11008/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 58 [11264/50000]	Loss: 4.6289	LR: 1.000000
Training Epoch: 58 [11520/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 58 [11776/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 58 [12032/50000]	Loss: 4.6209	LR: 1.000000
Training Epoch: 58 [12288/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 58 [12544/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 58 [12800/50000]	Loss: 4.6098	LR: 1.000000
Training Epoch: 58 [13056/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 58 [13312/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 58 [13568/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 58 [13824/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 58 [14080/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 58 [14336/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 58 [14592/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 58 [14848/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 58 [15104/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 58 [15360/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 58 [15616/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 58 [15872/50000]	Loss: 4.6166	LR: 1.000000
Training Epoch: 58 [16128/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 58 [16384/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 58 [16640/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 58 [16896/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 58 [17152/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 58 [17408/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 58 [17664/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 58 [17920/50000]	Loss: 4.6102	LR: 1.000000
Training Epoch: 58 [18176/50000]	Loss: 4.6177	LR: 1.000000
Training Epoch: 58 [18432/50000]	Loss: 4.6193	LR: 1.000000
Training Epoch: 58 [18688/50000]	Loss: 4.6334	LR: 1.000000
Training Epoch: 58 [18944/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 58 [19200/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 58 [19456/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 58 [19712/50000]	Loss: 4.6084	LR: 1.000000
Training Epoch: 58 [19968/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 58 [20224/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 58 [20480/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 58 [20736/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 58 [20992/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 58 [21248/50000]	Loss: 4.6318	LR: 1.000000
Training Epoch: 58 [21504/50000]	Loss: 4.6114	LR: 1.000000
Training Epoch: 58 [21760/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 58 [22016/50000]	Loss: 4.6298	LR: 1.000000
Training Epoch: 58 [22272/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 58 [22528/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 58 [22784/50000]	Loss: 4.6055	LR: 1.000000
Training Epoch: 58 [23040/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 58 [23296/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 58 [23552/50000]	Loss: 4.6201	LR: 1.000000
Training Epoch: 58 [23808/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 58 [24064/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 58 [24320/50000]	Loss: 4.6173	LR: 1.000000
Training Epoch: 58 [24576/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 58 [24832/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 58 [25088/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 58 [25344/50000]	Loss: 4.6288	LR: 1.000000
Training Epoch: 58 [25600/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 58 [25856/50000]	Loss: 4.6015	LR: 1.000000
Training Epoch: 58 [26112/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 58 [26368/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 58 [26624/50000]	Loss: 4.6235	LR: 1.000000
Training Epoch: 58 [26880/50000]	Loss: 4.6227	LR: 1.000000
Training Epoch: 58 [27136/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 58 [27392/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 58 [27648/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 58 [27904/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 58 [28160/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 58 [28416/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 58 [28672/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 58 [28928/50000]	Loss: 4.6237	LR: 1.000000
Training Epoch: 58 [29184/50000]	Loss: 4.5986	LR: 1.000000
Training Epoch: 58 [29440/50000]	Loss: 4.6294	LR: 1.000000
Training Epoch: 58 [29696/50000]	Loss: 4.6208	LR: 1.000000
Training Epoch: 58 [29952/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 58 [30208/50000]	Loss: 4.6130	LR: 1.000000
Training Epoch: 58 [30464/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 58 [30720/50000]	Loss: 4.6029	LR: 1.000000
Training Epoch: 58 [30976/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 58 [31232/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 58 [31488/50000]	Loss: 4.6053	LR: 1.000000
Training Epoch: 58 [31744/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 58 [32000/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 58 [32256/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 58 [32512/50000]	Loss: 4.6169	LR: 1.000000
Training Epoch: 58 [32768/50000]	Loss: 4.6080	LR: 1.000000
Training Epoch: 58 [33024/50000]	Loss: 4.6103	LR: 1.000000
Training Epoch: 58 [33280/50000]	Loss: 4.6160	LR: 1.000000
Training Epoch: 58 [33536/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 58 [33792/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 58 [34048/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 58 [34304/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 58 [34560/50000]	Loss: 4.6320	LR: 1.000000
Training Epoch: 58 [34816/50000]	Loss: 4.6043	LR: 1.000000
Training Epoch: 58 [35072/50000]	Loss: 4.6032	LR: 1.000000
Training Epoch: 58 [35328/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 58 [35584/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 58 [35840/50000]	Loss: 4.6174	LR: 1.000000
Training Epoch: 58 [36096/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 58 [36352/50000]	Loss: 4.6222	LR: 1.000000
Training Epoch: 58 [36608/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 58 [36864/50000]	Loss: 4.6249	LR: 1.000000
Training Epoch: 58 [37120/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 58 [37376/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 58 [37632/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 58 [37888/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 58 [38144/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 58 [38400/50000]	Loss: 4.6216	LR: 1.000000
Training Epoch: 58 [38656/50000]	Loss: 4.6225	LR: 1.000000
Training Epoch: 58 [38912/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 58 [39168/50000]	Loss: 4.6148	LR: 1.000000
Training Epoch: 58 [39424/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 58 [39680/50000]	Loss: 4.6332	LR: 1.000000
Training Epoch: 58 [39936/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 58 [40192/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 58 [40448/50000]	Loss: 4.6027	LR: 1.000000
Training Epoch: 58 [40704/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 58 [40960/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 58 [41216/50000]	Loss: 4.6150	LR: 1.000000
Training Epoch: 58 [41472/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 58 [41728/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 58 [41984/50000]	Loss: 4.6155	LR: 1.000000
Training Epoch: 58 [42240/50000]	Loss: 4.6263	LR: 1.000000
Training Epoch: 58 [42496/50000]	Loss: 4.6211	LR: 1.000000
Training Epoch: 58 [42752/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 58 [43008/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 58 [43264/50000]	Loss: 4.6149	LR: 1.000000
Training Epoch: 58 [43520/50000]	Loss: 4.6142	LR: 1.000000
Training Epoch: 58 [43776/50000]	Loss: 4.6132	LR: 1.000000
Training Epoch: 58 [44032/50000]	Loss: 4.6270	LR: 1.000000
Training Epoch: 58 [44288/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 58 [44544/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 58 [44800/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 58 [45056/50000]	Loss: 4.6117	LR: 1.000000
Training Epoch: 58 [45312/50000]	Loss: 4.6076	LR: 1.000000
Training Epoch: 58 [45568/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 58 [45824/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 58 [46080/50000]	Loss: 4.6182	LR: 1.000000
Training Epoch: 58 [46336/50000]	Loss: 4.5821	LR: 1.000000
Training Epoch: 58 [46592/50000]	Loss: 4.6275	LR: 1.000000
Training Epoch: 58 [46848/50000]	Loss: 4.6224	LR: 1.000000
Training Epoch: 58 [47104/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 58 [47360/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 58 [47616/50000]	Loss: 4.6020	LR: 1.000000
Training Epoch: 58 [47872/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 58 [48128/50000]	Loss: 4.6129	LR: 1.000000
Training Epoch: 58 [48384/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 58 [48640/50000]	Loss: 4.6106	LR: 1.000000
Training Epoch: 58 [48896/50000]	Loss: 4.6047	LR: 1.000000
Training Epoch: 58 [49152/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 58 [49408/50000]	Loss: 4.6158	LR: 1.000000
Training Epoch: 58 [49664/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 58 [49920/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 58 [50000/50000]	Loss: 4.6108	LR: 1.000000
epoch 58 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   87011 GB |   87011 GB |
|       from large pool |  400448 KB |    1770 MB |   86933 GB |   86932 GB |
|       from small pool |    3549 KB |       9 MB |      78 GB |      78 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   87011 GB |   87011 GB |
|       from large pool |  400448 KB |    1770 MB |   86933 GB |   86932 GB |
|       from small pool |    3549 KB |       9 MB |      78 GB |      78 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   53537 GB |   53537 GB |
|       from large pool |  244672 KB |  473024 KB |   53448 GB |   53447 GB |
|       from small pool |    2594 KB |    4843 KB |      89 GB |      89 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3713 K  |    3713 K  |
|       from large pool |      36    |      77    |    1795 K  |    1795 K  |
|       from small pool |     186    |     224    |    1918 K  |    1917 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3713 K  |    3713 K  |
|       from large pool |      36    |      77    |    1795 K  |    1795 K  |
|       from small pool |     186    |     224    |    1918 K  |    1917 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    1891 K  |    1891 K  |
|       from large pool |      10    |      11    |     743 K  |     743 K  |
|       from small pool |      12    |      23    |    1147 K  |    1147 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 58, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 59 [256/50000]	Loss: 4.6097	LR: 1.000000
Training Epoch: 59 [512/50000]	Loss: 4.6074	LR: 1.000000
Training Epoch: 59 [768/50000]	Loss: 4.6091	LR: 1.000000
Training Epoch: 59 [1024/50000]	Loss: 4.6189	LR: 1.000000
Training Epoch: 59 [1280/50000]	Loss: 4.6141	LR: 1.000000
Training Epoch: 59 [1536/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 59 [1792/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 59 [2048/50000]	Loss: 4.6105	LR: 1.000000
Training Epoch: 59 [2304/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 59 [2560/50000]	Loss: 4.6269	LR: 1.000000
Training Epoch: 59 [2816/50000]	Loss: 4.6206	LR: 1.000000
Training Epoch: 59 [3072/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 59 [3328/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 59 [3584/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 59 [3840/50000]	Loss: 4.6016	LR: 1.000000
Training Epoch: 59 [4096/50000]	Loss: 4.6212	LR: 1.000000
Training Epoch: 59 [4352/50000]	Loss: 4.6168	LR: 1.000000
Training Epoch: 59 [4608/50000]	Loss: 4.6115	LR: 1.000000
Training Epoch: 59 [4864/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 59 [5120/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 59 [5376/50000]	Loss: 4.6070	LR: 1.000000
Training Epoch: 59 [5632/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 59 [5888/50000]	Loss: 4.6095	LR: 1.000000
Training Epoch: 59 [6144/50000]	Loss: 4.6017	LR: 1.000000
Training Epoch: 59 [6400/50000]	Loss: 4.6123	LR: 1.000000
Training Epoch: 59 [6656/50000]	Loss: 4.6154	LR: 1.000000
Training Epoch: 59 [6912/50000]	Loss: 4.5966	LR: 1.000000
Training Epoch: 59 [7168/50000]	Loss: 4.6085	LR: 1.000000
Training Epoch: 59 [7424/50000]	Loss: 4.6236	LR: 1.000000
Training Epoch: 59 [7680/50000]	Loss: 4.6071	LR: 1.000000
Training Epoch: 59 [7936/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 59 [8192/50000]	Loss: 4.6219	LR: 1.000000
Training Epoch: 59 [8448/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 59 [8704/50000]	Loss: 4.6313	LR: 1.000000
Training Epoch: 59 [8960/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 59 [9216/50000]	Loss: 4.6244	LR: 1.000000
Training Epoch: 59 [9472/50000]	Loss: 4.6125	LR: 1.000000
Training Epoch: 59 [9728/50000]	Loss: 4.6442	LR: 1.000000
Training Epoch: 59 [9984/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 59 [10240/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 59 [10496/50000]	Loss: 4.6109	LR: 1.000000
Training Epoch: 59 [10752/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 59 [11008/50000]	Loss: 4.6140	LR: 1.000000
Training Epoch: 59 [11264/50000]	Loss: 4.6243	LR: 1.000000
Training Epoch: 59 [11520/50000]	Loss: 4.6297	LR: 1.000000
Training Epoch: 59 [11776/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 59 [12032/50000]	Loss: 4.6088	LR: 1.000000
Training Epoch: 59 [12288/50000]	Loss: 4.6072	LR: 1.000000
Training Epoch: 59 [12544/50000]	Loss: 4.6281	LR: 1.000000
Training Epoch: 59 [12800/50000]	Loss: 4.6086	LR: 1.000000
Training Epoch: 59 [13056/50000]	Loss: 4.6257	LR: 1.000000
Training Epoch: 59 [13312/50000]	Loss: 4.6099	LR: 1.000000
Training Epoch: 59 [13568/50000]	Loss: 4.6171	LR: 1.000000
Training Epoch: 59 [13824/50000]	Loss: 4.6309	LR: 1.000000
Training Epoch: 59 [14080/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 59 [14336/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 59 [14592/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 59 [14848/50000]	Loss: 4.6329	LR: 1.000000
Training Epoch: 59 [15104/50000]	Loss: 4.6262	LR: 1.000000
Training Epoch: 59 [15360/50000]	Loss: 4.6258	LR: 1.000000
Training Epoch: 59 [15616/50000]	Loss: 4.6164	LR: 1.000000
Training Epoch: 59 [15872/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 59 [16128/50000]	Loss: 4.6062	LR: 1.000000
Training Epoch: 59 [16384/50000]	Loss: 4.6292	LR: 1.000000
Training Epoch: 59 [16640/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 59 [16896/50000]	Loss: 4.6121	LR: 1.000000
Training Epoch: 59 [17152/50000]	Loss: 4.6308	LR: 1.000000
Training Epoch: 59 [17408/50000]	Loss: 4.6279	LR: 1.000000
Training Epoch: 59 [17664/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 59 [17920/50000]	Loss: 4.6238	LR: 1.000000
Training Epoch: 59 [18176/50000]	Loss: 4.5984	LR: 1.000000
Training Epoch: 59 [18432/50000]	Loss: 4.6064	LR: 1.000000
Training Epoch: 59 [18688/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 59 [18944/50000]	Loss: 4.6112	LR: 1.000000
Training Epoch: 59 [19200/50000]	Loss: 4.6089	LR: 1.000000
Training Epoch: 59 [19456/50000]	Loss: 4.6185	LR: 1.000000
Training Epoch: 59 [19712/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 59 [19968/50000]	Loss: 4.6000	LR: 1.000000
Training Epoch: 59 [20224/50000]	Loss: 4.6104	LR: 1.000000
Training Epoch: 59 [20480/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 59 [20736/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 59 [20992/50000]	Loss: 4.6303	LR: 1.000000
Training Epoch: 59 [21248/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 59 [21504/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 59 [21760/50000]	Loss: 4.6056	LR: 1.000000
Training Epoch: 59 [22016/50000]	Loss: 4.6139	LR: 1.000000
Training Epoch: 59 [22272/50000]	Loss: 4.6033	LR: 1.000000
Training Epoch: 59 [22528/50000]	Loss: 4.6276	LR: 1.000000
Training Epoch: 59 [22784/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 59 [23040/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 59 [23296/50000]	Loss: 4.6050	LR: 1.000000
Training Epoch: 59 [23552/50000]	Loss: 4.6113	LR: 1.000000
Training Epoch: 59 [23808/50000]	Loss: 4.5931	LR: 1.000000
Training Epoch: 59 [24064/50000]	Loss: 4.6205	LR: 1.000000
Training Epoch: 59 [24320/50000]	Loss: 4.6248	LR: 1.000000
Training Epoch: 59 [24576/50000]	Loss: 4.6218	LR: 1.000000
Training Epoch: 59 [24832/50000]	Loss: 4.6220	LR: 1.000000
Training Epoch: 59 [25088/50000]	Loss: 4.6179	LR: 1.000000
Training Epoch: 59 [25344/50000]	Loss: 4.6175	LR: 1.000000
Training Epoch: 59 [25600/50000]	Loss: 4.6213	LR: 1.000000
Training Epoch: 59 [25856/50000]	Loss: 4.6240	LR: 1.000000
Training Epoch: 59 [26112/50000]	Loss: 4.6319	LR: 1.000000
Training Epoch: 59 [26368/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 59 [26624/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 59 [26880/50000]	Loss: 4.6242	LR: 1.000000
Training Epoch: 59 [27136/50000]	Loss: 4.6176	LR: 1.000000
Training Epoch: 59 [27392/50000]	Loss: 4.6195	LR: 1.000000
Training Epoch: 59 [27648/50000]	Loss: 4.6019	LR: 1.000000
Training Epoch: 59 [27904/50000]	Loss: 4.6077	LR: 1.000000
Training Epoch: 59 [28160/50000]	Loss: 4.6192	LR: 1.000000
Training Epoch: 59 [28416/50000]	Loss: 4.6178	LR: 1.000000
Training Epoch: 59 [28672/50000]	Loss: 4.6190	LR: 1.000000
Training Epoch: 59 [28928/50000]	Loss: 4.6184	LR: 1.000000
Training Epoch: 59 [29184/50000]	Loss: 4.5980	LR: 1.000000
Training Epoch: 59 [29440/50000]	Loss: 4.6196	LR: 1.000000
Training Epoch: 59 [29696/50000]	Loss: 4.6087	LR: 1.000000
Training Epoch: 59 [29952/50000]	Loss: 4.6018	LR: 1.000000
Training Epoch: 59 [30208/50000]	Loss: 4.6146	LR: 1.000000
Training Epoch: 59 [30464/50000]	Loss: 4.6124	LR: 1.000000
Training Epoch: 59 [30720/50000]	Loss: 4.6054	LR: 1.000000
Training Epoch: 59 [30976/50000]	Loss: 4.6152	LR: 1.000000
Training Epoch: 59 [31232/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 59 [31488/50000]	Loss: 4.6147	LR: 1.000000
Training Epoch: 59 [31744/50000]	Loss: 4.6151	LR: 1.000000
Training Epoch: 59 [32000/50000]	Loss: 4.6122	LR: 1.000000
Training Epoch: 59 [32256/50000]	Loss: 4.6167	LR: 1.000000
Training Epoch: 59 [32512/50000]	Loss: 4.5991	LR: 1.000000
Training Epoch: 59 [32768/50000]	Loss: 4.6096	LR: 1.000000
Training Epoch: 59 [33024/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 59 [33280/50000]	Loss: 4.6246	LR: 1.000000
Training Epoch: 59 [33536/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 59 [33792/50000]	Loss: 4.6180	LR: 1.000000
Training Epoch: 59 [34048/50000]	Loss: 4.6163	LR: 1.000000
Training Epoch: 59 [34304/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 59 [34560/50000]	Loss: 4.6157	LR: 1.000000
Training Epoch: 59 [34816/50000]	Loss: 4.6156	LR: 1.000000
Training Epoch: 59 [35072/50000]	Loss: 4.6260	LR: 1.000000
Training Epoch: 59 [35328/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 59 [35584/50000]	Loss: 4.6231	LR: 1.000000
Training Epoch: 59 [35840/50000]	Loss: 4.6110	LR: 1.000000
Training Epoch: 59 [36096/50000]	Loss: 4.6057	LR: 1.000000
Training Epoch: 59 [36352/50000]	Loss: 4.6200	LR: 1.000000
Training Epoch: 59 [36608/50000]	Loss: 4.6073	LR: 1.000000
Training Epoch: 59 [36864/50000]	Loss: 4.6229	LR: 1.000000
Training Epoch: 59 [37120/50000]	Loss: 4.6059	LR: 1.000000
Training Epoch: 59 [37376/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 59 [37632/50000]	Loss: 4.6041	LR: 1.000000
Training Epoch: 59 [37888/50000]	Loss: 4.6313	LR: 1.000000
Training Epoch: 59 [38144/50000]	Loss: 4.6012	LR: 1.000000
Training Epoch: 59 [38400/50000]	Loss: 4.6153	LR: 1.000000
Training Epoch: 59 [38656/50000]	Loss: 4.6217	LR: 1.000000
Training Epoch: 59 [38912/50000]	Loss: 4.6278	LR: 1.000000
Training Epoch: 59 [39168/50000]	Loss: 4.6325	LR: 1.000000
Training Epoch: 59 [39424/50000]	Loss: 4.6000	LR: 1.000000
Training Epoch: 59 [39680/50000]	Loss: 4.6233	LR: 1.000000
Training Epoch: 59 [39936/50000]	Loss: 4.6162	LR: 1.000000
Training Epoch: 59 [40192/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 59 [40448/50000]	Loss: 4.5980	LR: 1.000000
Training Epoch: 59 [40704/50000]	Loss: 4.6161	LR: 1.000000
Training Epoch: 59 [40960/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 59 [41216/50000]	Loss: 4.6338	LR: 1.000000
Training Epoch: 59 [41472/50000]	Loss: 4.6143	LR: 1.000000
Training Epoch: 59 [41728/50000]	Loss: 4.6135	LR: 1.000000
Training Epoch: 59 [41984/50000]	Loss: 4.6065	LR: 1.000000
Training Epoch: 59 [42240/50000]	Loss: 4.6194	LR: 1.000000
Training Epoch: 59 [42496/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 59 [42752/50000]	Loss: 4.6210	LR: 1.000000
Training Epoch: 59 [43008/50000]	Loss: 4.6186	LR: 1.000000
Training Epoch: 59 [43264/50000]	Loss: 4.6326	LR: 1.000000
Training Epoch: 59 [43520/50000]	Loss: 4.6435	LR: 1.000000
Training Epoch: 59 [43776/50000]	Loss: 4.6022	LR: 1.000000
Training Epoch: 59 [44032/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 59 [44288/50000]	Loss: 4.6247	LR: 1.000000
Training Epoch: 59 [44544/50000]	Loss: 4.6092	LR: 1.000000
Training Epoch: 59 [44800/50000]	Loss: 4.6159	LR: 1.000000
Training Epoch: 59 [45056/50000]	Loss: 4.6187	LR: 1.000000
Training Epoch: 59 [45312/50000]	Loss: 4.6079	LR: 1.000000
Training Epoch: 59 [45568/50000]	Loss: 4.6234	LR: 1.000000
Training Epoch: 59 [45824/50000]	Loss: 4.6138	LR: 1.000000
Training Epoch: 59 [46080/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 59 [46336/50000]	Loss: 4.6120	LR: 1.000000
Training Epoch: 59 [46592/50000]	Loss: 4.6255	LR: 1.000000
Training Epoch: 59 [46848/50000]	Loss: 4.6082	LR: 1.000000
Training Epoch: 59 [47104/50000]	Loss: 4.6090	LR: 1.000000
Training Epoch: 59 [47360/50000]	Loss: 4.6251	LR: 1.000000
Training Epoch: 59 [47616/50000]	Loss: 4.6081	LR: 1.000000
Training Epoch: 59 [47872/50000]	Loss: 4.6066	LR: 1.000000
Training Epoch: 59 [48128/50000]	Loss: 4.6126	LR: 1.000000
Training Epoch: 59 [48384/50000]	Loss: 4.6191	LR: 1.000000
Training Epoch: 59 [48640/50000]	Loss: 4.6170	LR: 1.000000
Training Epoch: 59 [48896/50000]	Loss: 4.6101	LR: 1.000000
Training Epoch: 59 [49152/50000]	Loss: 4.6268	LR: 1.000000
Training Epoch: 59 [49408/50000]	Loss: 4.6107	LR: 1.000000
Training Epoch: 59 [49664/50000]	Loss: 4.6215	LR: 1.000000
Training Epoch: 59 [49920/50000]	Loss: 4.6183	LR: 1.000000
Training Epoch: 59 [50000/50000]	Loss: 4.6256	LR: 1.000000
epoch 59 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   88511 GB |   88511 GB |
|       from large pool |  400448 KB |    1770 MB |   88432 GB |   88431 GB |
|       from small pool |    3549 KB |       9 MB |      79 GB |      79 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   88511 GB |   88511 GB |
|       from large pool |  400448 KB |    1770 MB |   88432 GB |   88431 GB |
|       from small pool |    3549 KB |       9 MB |      79 GB |      79 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   54460 GB |   54460 GB |
|       from large pool |  244672 KB |  473024 KB |   54369 GB |   54369 GB |
|       from small pool |    2594 KB |    4843 KB |      91 GB |      91 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3777 K  |    3777 K  |
|       from large pool |      36    |      77    |    1826 K  |    1826 K  |
|       from small pool |     186    |     224    |    1951 K  |    1951 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3777 K  |    3777 K  |
|       from large pool |      36    |      77    |    1826 K  |    1826 K  |
|       from small pool |     186    |     224    |    1951 K  |    1951 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    1924 K  |    1924 K  |
|       from large pool |      10    |      11    |     756 K  |     756 K  |
|       from small pool |      10    |      23    |    1167 K  |    1167 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 59, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 60 [256/50000]	Loss: 4.6312	LR: 0.200000
Training Epoch: 60 [512/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [768/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 60 [1024/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 60 [1280/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 60 [1536/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 60 [1792/50000]	Loss: 4.6221	LR: 0.200000
Training Epoch: 60 [2048/50000]	Loss: 4.6203	LR: 0.200000
Training Epoch: 60 [2304/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 60 [2560/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 60 [2816/50000]	Loss: 4.5975	LR: 0.200000
Training Epoch: 60 [3072/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 60 [3328/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 60 [3584/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 60 [3840/50000]	Loss: 4.6179	LR: 0.200000
Training Epoch: 60 [4096/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 60 [4352/50000]	Loss: 4.6287	LR: 0.200000
Training Epoch: 60 [4608/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 60 [4864/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 60 [5120/50000]	Loss: 4.5953	LR: 0.200000
Training Epoch: 60 [5376/50000]	Loss: 4.6211	LR: 0.200000
Training Epoch: 60 [5632/50000]	Loss: 4.6260	LR: 0.200000
Training Epoch: 60 [5888/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 60 [6144/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 60 [6400/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 60 [6656/50000]	Loss: 4.6166	LR: 0.200000
Training Epoch: 60 [6912/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 60 [7168/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 60 [7424/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 60 [7680/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 60 [7936/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 60 [8192/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 60 [8448/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 60 [8704/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 60 [8960/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 60 [9216/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 60 [9472/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 60 [9728/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 60 [9984/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 60 [10240/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 60 [10496/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 60 [10752/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 60 [11008/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 60 [11264/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 60 [11520/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 60 [11776/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 60 [12032/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 60 [12288/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 60 [12544/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 60 [12800/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 60 [13056/50000]	Loss: 4.5988	LR: 0.200000
Training Epoch: 60 [13312/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 60 [13568/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 60 [13824/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 60 [14080/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 60 [14336/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 60 [14592/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 60 [14848/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 60 [15104/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 60 [15360/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 60 [15616/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [15872/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 60 [16128/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 60 [16384/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 60 [16640/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 60 [16896/50000]	Loss: 4.6164	LR: 0.200000
Training Epoch: 60 [17152/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 60 [17408/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 60 [17664/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 60 [17920/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 60 [18176/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 60 [18432/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 60 [18688/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 60 [18944/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 60 [19200/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 60 [19456/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 60 [19712/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 60 [19968/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 60 [20224/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 60 [20480/50000]	Loss: 4.5989	LR: 0.200000
Training Epoch: 60 [20736/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 60 [20992/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 60 [21248/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 60 [21504/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 60 [21760/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 60 [22016/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 60 [22272/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 60 [22528/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 60 [22784/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 60 [23040/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 60 [23296/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 60 [23552/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 60 [23808/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 60 [24064/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 60 [24320/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 60 [24576/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 60 [24832/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 60 [25088/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 60 [25344/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 60 [25600/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 60 [25856/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 60 [26112/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 60 [26368/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 60 [26624/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 60 [26880/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 60 [27136/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 60 [27392/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 60 [27648/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [27904/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 60 [28160/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 60 [28416/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 60 [28672/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 60 [28928/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 60 [29184/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 60 [29440/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 60 [29696/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 60 [29952/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 60 [30208/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 60 [30464/50000]	Loss: 4.6154	LR: 0.200000
Training Epoch: 60 [30720/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 60 [30976/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 60 [31232/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 60 [31488/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 60 [31744/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 60 [32000/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 60 [32256/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 60 [32512/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 60 [32768/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 60 [33024/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 60 [33280/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 60 [33536/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 60 [33792/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 60 [34048/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 60 [34304/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 60 [34560/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 60 [34816/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 60 [35072/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 60 [35328/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 60 [35584/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 60 [35840/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 60 [36096/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 60 [36352/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 60 [36608/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 60 [36864/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [37120/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 60 [37376/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 60 [37632/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 60 [37888/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 60 [38144/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 60 [38400/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 60 [38656/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 60 [38912/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 60 [39168/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 60 [39424/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 60 [39680/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [39936/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 60 [40192/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 60 [40448/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 60 [40704/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 60 [40960/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 60 [41216/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 60 [41472/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 60 [41728/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 60 [41984/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 60 [42240/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 60 [42496/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 60 [42752/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 60 [43008/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 60 [43264/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 60 [43520/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 60 [43776/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 60 [44032/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 60 [44288/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 60 [44544/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 60 [44800/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 60 [45056/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 60 [45312/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 60 [45568/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 60 [45824/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [46080/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 60 [46336/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 60 [46592/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [46848/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 60 [47104/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 60 [47360/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 60 [47616/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 60 [47872/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 60 [48128/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 60 [48384/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 60 [48640/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 60 [48896/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 60 [49152/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 60 [49408/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 60 [49664/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 60 [49920/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 60 [50000/50000]	Loss: 4.6085	LR: 0.200000
epoch 60 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   90011 GB |   90011 GB |
|       from large pool |  400448 KB |    1770 MB |   89930 GB |   89930 GB |
|       from small pool |    3549 KB |       9 MB |      80 GB |      80 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   90011 GB |   90011 GB |
|       from large pool |  400448 KB |    1770 MB |   89930 GB |   89930 GB |
|       from small pool |    3549 KB |       9 MB |      80 GB |      80 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   55383 GB |   55383 GB |
|       from large pool |  244672 KB |  473024 KB |   55291 GB |   55290 GB |
|       from small pool |    4642 KB |    4843 KB |      92 GB |      92 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3841 K  |    3841 K  |
|       from large pool |      36    |      77    |    1857 K  |    1857 K  |
|       from small pool |     186    |     224    |    1984 K  |    1984 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3841 K  |    3841 K  |
|       from large pool |      36    |      77    |    1857 K  |    1857 K  |
|       from small pool |     186    |     224    |    1984 K  |    1984 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    1957 K  |    1957 K  |
|       from large pool |      10    |      11    |     769 K  |     769 K  |
|       from small pool |      13    |      23    |    1187 K  |    1187 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 60, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-60-regular.pth
Training Epoch: 61 [256/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 61 [512/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 61 [768/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 61 [1024/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 61 [1280/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 61 [1536/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 61 [1792/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 61 [2048/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 61 [2304/50000]	Loss: 4.5971	LR: 0.200000
Training Epoch: 61 [2560/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 61 [2816/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 61 [3072/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 61 [3328/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 61 [3584/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 61 [3840/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 61 [4096/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 61 [4352/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 61 [4608/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 61 [4864/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 61 [5120/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 61 [5376/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 61 [5632/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 61 [5888/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 61 [6144/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 61 [6400/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 61 [6656/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 61 [6912/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 61 [7168/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 61 [7424/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 61 [7680/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 61 [7936/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 61 [8192/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 61 [8448/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 61 [8704/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 61 [8960/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 61 [9216/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 61 [9472/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 61 [9728/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 61 [9984/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 61 [10240/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 61 [10496/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 61 [10752/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 61 [11008/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 61 [11264/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 61 [11520/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 61 [11776/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 61 [12032/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 61 [12288/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 61 [12544/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 61 [12800/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 61 [13056/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 61 [13312/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 61 [13568/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 61 [13824/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 61 [14080/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 61 [14336/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 61 [14592/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 61 [14848/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 61 [15104/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 61 [15360/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 61 [15616/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 61 [15872/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 61 [16128/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 61 [16384/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 61 [16640/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 61 [16896/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 61 [17152/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 61 [17408/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 61 [17664/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 61 [17920/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 61 [18176/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 61 [18432/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 61 [18688/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 61 [18944/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 61 [19200/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 61 [19456/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 61 [19712/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 61 [19968/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 61 [20224/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 61 [20480/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 61 [20736/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 61 [20992/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 61 [21248/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 61 [21504/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 61 [21760/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 61 [22016/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 61 [22272/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 61 [22528/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 61 [22784/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 61 [23040/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 61 [23296/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 61 [23552/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 61 [23808/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 61 [24064/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 61 [24320/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 61 [24576/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 61 [24832/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 61 [25088/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 61 [25344/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 61 [25600/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 61 [25856/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 61 [26112/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 61 [26368/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 61 [26624/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 61 [26880/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 61 [27136/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 61 [27392/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 61 [27648/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 61 [27904/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 61 [28160/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 61 [28416/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 61 [28672/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 61 [28928/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 61 [29184/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 61 [29440/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 61 [29696/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 61 [29952/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 61 [30208/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 61 [30464/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 61 [30720/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 61 [30976/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 61 [31232/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 61 [31488/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 61 [31744/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 61 [32000/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 61 [32256/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 61 [32512/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 61 [32768/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 61 [33024/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 61 [33280/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 61 [33536/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 61 [33792/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 61 [34048/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 61 [34304/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 61 [34560/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 61 [34816/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 61 [35072/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 61 [35328/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 61 [35584/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 61 [35840/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 61 [36096/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 61 [36352/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 61 [36608/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 61 [36864/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 61 [37120/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 61 [37376/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 61 [37632/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 61 [37888/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 61 [38144/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 61 [38400/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 61 [38656/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 61 [38912/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 61 [39168/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 61 [39424/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 61 [39680/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 61 [39936/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 61 [40192/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 61 [40448/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 61 [40704/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 61 [40960/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 61 [41216/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 61 [41472/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 61 [41728/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 61 [41984/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 61 [42240/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 61 [42496/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 61 [42752/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 61 [43008/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 61 [43264/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 61 [43520/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 61 [43776/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 61 [44032/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 61 [44288/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 61 [44544/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 61 [44800/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 61 [45056/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 61 [45312/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 61 [45568/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 61 [45824/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 61 [46080/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 61 [46336/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 61 [46592/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 61 [46848/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 61 [47104/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 61 [47360/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 61 [47616/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 61 [47872/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 61 [48128/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 61 [48384/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 61 [48640/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 61 [48896/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 61 [49152/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 61 [49408/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 61 [49664/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 61 [49920/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 61 [50000/50000]	Loss: 4.6146	LR: 0.200000
epoch 61 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   91512 GB |   91511 GB |
|       from large pool |  400448 KB |    1770 MB |   91429 GB |   91429 GB |
|       from small pool |    3549 KB |       9 MB |      82 GB |      82 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   91512 GB |   91511 GB |
|       from large pool |  400448 KB |    1770 MB |   91429 GB |   91429 GB |
|       from small pool |    3549 KB |       9 MB |      82 GB |      82 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   56306 GB |   56306 GB |
|       from large pool |  244672 KB |  473024 KB |   56212 GB |   56212 GB |
|       from small pool |    2594 KB |    4843 KB |      94 GB |      94 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3905 K  |    3905 K  |
|       from large pool |      36    |      77    |    1888 K  |    1888 K  |
|       from small pool |     186    |     224    |    2017 K  |    2017 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3905 K  |    3905 K  |
|       from large pool |      36    |      77    |    1888 K  |    1888 K  |
|       from small pool |     186    |     224    |    2017 K  |    2017 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    1989 K  |    1989 K  |
|       from large pool |      10    |      11    |     782 K  |     782 K  |
|       from small pool |      12    |      23    |    1207 K  |    1207 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 61, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 62 [256/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 62 [512/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 62 [768/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 62 [1024/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 62 [1280/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 62 [1536/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 62 [1792/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 62 [2048/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 62 [2304/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 62 [2560/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 62 [2816/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 62 [3072/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 62 [3328/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 62 [3584/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 62 [3840/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 62 [4096/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 62 [4352/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 62 [4608/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 62 [4864/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 62 [5120/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 62 [5376/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 62 [5632/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 62 [5888/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 62 [6144/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 62 [6400/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 62 [6656/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 62 [6912/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 62 [7168/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 62 [7424/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 62 [7680/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 62 [7936/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 62 [8192/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 62 [8448/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 62 [8704/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 62 [8960/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 62 [9216/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 62 [9472/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 62 [9728/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 62 [9984/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 62 [10240/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 62 [10496/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 62 [10752/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 62 [11008/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 62 [11264/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 62 [11520/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 62 [11776/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 62 [12032/50000]	Loss: 4.6164	LR: 0.200000
Training Epoch: 62 [12288/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 62 [12544/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 62 [12800/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 62 [13056/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 62 [13312/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 62 [13568/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 62 [13824/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 62 [14080/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 62 [14336/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 62 [14592/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 62 [14848/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 62 [15104/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 62 [15360/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 62 [15616/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 62 [15872/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 62 [16128/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 62 [16384/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 62 [16640/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 62 [16896/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 62 [17152/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 62 [17408/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 62 [17664/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 62 [17920/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 62 [18176/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 62 [18432/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 62 [18688/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 62 [18944/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 62 [19200/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 62 [19456/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 62 [19712/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 62 [19968/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 62 [20224/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 62 [20480/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 62 [20736/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 62 [20992/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 62 [21248/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 62 [21504/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 62 [21760/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 62 [22016/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 62 [22272/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 62 [22528/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 62 [22784/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 62 [23040/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 62 [23296/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 62 [23552/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 62 [23808/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 62 [24064/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 62 [24320/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 62 [24576/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 62 [24832/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 62 [25088/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 62 [25344/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 62 [25600/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 62 [25856/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 62 [26112/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 62 [26368/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 62 [26624/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 62 [26880/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 62 [27136/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 62 [27392/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 62 [27648/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 62 [27904/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 62 [28160/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 62 [28416/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 62 [28672/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 62 [28928/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 62 [29184/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 62 [29440/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 62 [29696/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 62 [29952/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 62 [30208/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 62 [30464/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 62 [30720/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 62 [30976/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 62 [31232/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 62 [31488/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 62 [31744/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 62 [32000/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 62 [32256/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 62 [32512/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 62 [32768/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 62 [33024/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 62 [33280/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 62 [33536/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 62 [33792/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 62 [34048/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 62 [34304/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 62 [34560/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 62 [34816/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 62 [35072/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 62 [35328/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 62 [35584/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 62 [35840/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 62 [36096/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 62 [36352/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 62 [36608/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 62 [36864/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 62 [37120/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 62 [37376/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 62 [37632/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 62 [37888/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 62 [38144/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 62 [38400/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 62 [38656/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 62 [38912/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 62 [39168/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 62 [39424/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 62 [39680/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 62 [39936/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 62 [40192/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 62 [40448/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 62 [40704/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 62 [40960/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 62 [41216/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 62 [41472/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 62 [41728/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 62 [41984/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 62 [42240/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 62 [42496/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 62 [42752/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 62 [43008/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 62 [43264/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 62 [43520/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 62 [43776/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 62 [44032/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 62 [44288/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 62 [44544/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 62 [44800/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 62 [45056/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 62 [45312/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 62 [45568/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 62 [45824/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 62 [46080/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 62 [46336/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 62 [46592/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 62 [46848/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 62 [47104/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 62 [47360/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 62 [47616/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 62 [47872/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 62 [48128/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 62 [48384/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 62 [48640/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 62 [48896/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 62 [49152/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 62 [49408/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 62 [49664/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 62 [49920/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 62 [50000/50000]	Loss: 4.6136	LR: 0.200000
epoch 62 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   93012 GB |   93011 GB |
|       from large pool |  400448 KB |    1770 MB |   92928 GB |   92928 GB |
|       from small pool |    3549 KB |       9 MB |      83 GB |      83 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   93012 GB |   93011 GB |
|       from large pool |  400448 KB |    1770 MB |   92928 GB |   92928 GB |
|       from small pool |    3549 KB |       9 MB |      83 GB |      83 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   57229 GB |   57229 GB |
|       from large pool |  244672 KB |  473024 KB |   57134 GB |   57133 GB |
|       from small pool |    4642 KB |    4843 KB |      95 GB |      95 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    3969 K  |    3969 K  |
|       from large pool |      36    |      77    |    1919 K  |    1919 K  |
|       from small pool |     186    |     224    |    2050 K  |    2050 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    3969 K  |    3969 K  |
|       from large pool |      36    |      77    |    1919 K  |    1919 K  |
|       from small pool |     186    |     224    |    2050 K  |    2050 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    2022 K  |    2022 K  |
|       from large pool |      10    |      11    |     795 K  |     795 K  |
|       from small pool |      12    |      23    |    1227 K  |    1227 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 62, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 63 [256/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 63 [512/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 63 [768/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 63 [1024/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 63 [1280/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 63 [1536/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 63 [1792/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 63 [2048/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 63 [2304/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 63 [2560/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 63 [2816/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 63 [3072/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 63 [3328/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 63 [3584/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 63 [3840/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 63 [4096/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 63 [4352/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 63 [4608/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 63 [4864/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 63 [5120/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 63 [5376/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 63 [5632/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 63 [5888/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 63 [6144/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 63 [6400/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 63 [6656/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 63 [6912/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 63 [7168/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [7424/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 63 [7680/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 63 [7936/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 63 [8192/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 63 [8448/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 63 [8704/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 63 [8960/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 63 [9216/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 63 [9472/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 63 [9728/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 63 [9984/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 63 [10240/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 63 [10496/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 63 [10752/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 63 [11008/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 63 [11264/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 63 [11520/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 63 [11776/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 63 [12032/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 63 [12288/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 63 [12544/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 63 [12800/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 63 [13056/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 63 [13312/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 63 [13568/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 63 [13824/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 63 [14080/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 63 [14336/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 63 [14592/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 63 [14848/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 63 [15104/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 63 [15360/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 63 [15616/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 63 [15872/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 63 [16128/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 63 [16384/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 63 [16640/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 63 [16896/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 63 [17152/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 63 [17408/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 63 [17664/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 63 [17920/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 63 [18176/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 63 [18432/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 63 [18688/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [18944/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 63 [19200/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 63 [19456/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 63 [19712/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 63 [19968/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 63 [20224/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 63 [20480/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 63 [20736/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 63 [20992/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 63 [21248/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 63 [21504/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 63 [21760/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 63 [22016/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 63 [22272/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 63 [22528/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 63 [22784/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 63 [23040/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 63 [23296/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 63 [23552/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 63 [23808/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 63 [24064/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 63 [24320/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 63 [24576/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 63 [24832/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 63 [25088/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 63 [25344/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 63 [25600/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 63 [25856/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 63 [26112/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 63 [26368/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 63 [26624/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 63 [26880/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 63 [27136/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 63 [27392/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 63 [27648/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [27904/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 63 [28160/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 63 [28416/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 63 [28672/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 63 [28928/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 63 [29184/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 63 [29440/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 63 [29696/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 63 [29952/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 63 [30208/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 63 [30464/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 63 [30720/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 63 [30976/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 63 [31232/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 63 [31488/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 63 [31744/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 63 [32000/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 63 [32256/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 63 [32512/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 63 [32768/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 63 [33024/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [33280/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 63 [33536/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 63 [33792/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 63 [34048/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 63 [34304/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 63 [34560/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 63 [34816/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 63 [35072/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 63 [35328/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 63 [35584/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 63 [35840/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 63 [36096/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [36352/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 63 [36608/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 63 [36864/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 63 [37120/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 63 [37376/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 63 [37632/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 63 [37888/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 63 [38144/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 63 [38400/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 63 [38656/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 63 [38912/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 63 [39168/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 63 [39424/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 63 [39680/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 63 [39936/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 63 [40192/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 63 [40448/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 63 [40704/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 63 [40960/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 63 [41216/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 63 [41472/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 63 [41728/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 63 [41984/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 63 [42240/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 63 [42496/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 63 [42752/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 63 [43008/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 63 [43264/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [43520/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 63 [43776/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 63 [44032/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [44288/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 63 [44544/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 63 [44800/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 63 [45056/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 63 [45312/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 63 [45568/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 63 [45824/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 63 [46080/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 63 [46336/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 63 [46592/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 63 [46848/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 63 [47104/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 63 [47360/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 63 [47616/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 63 [47872/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 63 [48128/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 63 [48384/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 63 [48640/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 63 [48896/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 63 [49152/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 63 [49408/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 63 [49664/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 63 [49920/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 63 [50000/50000]	Loss: 4.6085	LR: 0.200000
epoch 63 training time consumed: 21.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   94512 GB |   94512 GB |
|       from large pool |  400448 KB |    1770 MB |   94427 GB |   94427 GB |
|       from small pool |    3549 KB |       9 MB |      85 GB |      85 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   94512 GB |   94512 GB |
|       from large pool |  400448 KB |    1770 MB |   94427 GB |   94427 GB |
|       from small pool |    3549 KB |       9 MB |      85 GB |      85 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   58152 GB |   58152 GB |
|       from large pool |  244672 KB |  473024 KB |   58055 GB |   58055 GB |
|       from small pool |    2594 KB |    4843 KB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4033 K  |    4033 K  |
|       from large pool |      36    |      77    |    1950 K  |    1950 K  |
|       from small pool |     186    |     224    |    2083 K  |    2083 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4033 K  |    4033 K  |
|       from large pool |      36    |      77    |    1950 K  |    1950 K  |
|       from small pool |     186    |     224    |    2083 K  |    2083 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    2055 K  |    2055 K  |
|       from large pool |      10    |      11    |     807 K  |     807 K  |
|       from small pool |      13    |      23    |    1247 K  |    1247 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 63, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 64 [256/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 64 [512/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 64 [768/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 64 [1024/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 64 [1280/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 64 [1536/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 64 [1792/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 64 [2048/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 64 [2304/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 64 [2560/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 64 [2816/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 64 [3072/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 64 [3328/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 64 [3584/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 64 [3840/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 64 [4096/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 64 [4352/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 64 [4608/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 64 [4864/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 64 [5120/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 64 [5376/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 64 [5632/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 64 [5888/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 64 [6144/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 64 [6400/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 64 [6656/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 64 [6912/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 64 [7168/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 64 [7424/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 64 [7680/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 64 [7936/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 64 [8192/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 64 [8448/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 64 [8704/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 64 [8960/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 64 [9216/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 64 [9472/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 64 [9728/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 64 [9984/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 64 [10240/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 64 [10496/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 64 [10752/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 64 [11008/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 64 [11264/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 64 [11520/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 64 [11776/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 64 [12032/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 64 [12288/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 64 [12544/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 64 [12800/50000]	Loss: 4.6160	LR: 0.200000
Training Epoch: 64 [13056/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 64 [13312/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 64 [13568/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 64 [13824/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 64 [14080/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 64 [14336/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 64 [14592/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 64 [14848/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 64 [15104/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 64 [15360/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 64 [15616/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 64 [15872/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 64 [16128/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 64 [16384/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 64 [16640/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 64 [16896/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 64 [17152/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 64 [17408/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 64 [17664/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 64 [17920/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 64 [18176/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 64 [18432/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 64 [18688/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 64 [18944/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 64 [19200/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 64 [19456/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 64 [19712/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 64 [19968/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 64 [20224/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 64 [20480/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 64 [20736/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 64 [20992/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 64 [21248/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 64 [21504/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 64 [21760/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 64 [22016/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 64 [22272/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 64 [22528/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 64 [22784/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 64 [23040/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 64 [23296/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 64 [23552/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 64 [23808/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 64 [24064/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 64 [24320/50000]	Loss: 4.6011	LR: 0.200000
Training Epoch: 64 [24576/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 64 [24832/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 64 [25088/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 64 [25344/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 64 [25600/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 64 [25856/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 64 [26112/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 64 [26368/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 64 [26624/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 64 [26880/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 64 [27136/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 64 [27392/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 64 [27648/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 64 [27904/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 64 [28160/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 64 [28416/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 64 [28672/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 64 [28928/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 64 [29184/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 64 [29440/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 64 [29696/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 64 [29952/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 64 [30208/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 64 [30464/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 64 [30720/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 64 [30976/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 64 [31232/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 64 [31488/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 64 [31744/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 64 [32000/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 64 [32256/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 64 [32512/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 64 [32768/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 64 [33024/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 64 [33280/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 64 [33536/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 64 [33792/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 64 [34048/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 64 [34304/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 64 [34560/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 64 [34816/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 64 [35072/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 64 [35328/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 64 [35584/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 64 [35840/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 64 [36096/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 64 [36352/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 64 [36608/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 64 [36864/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 64 [37120/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 64 [37376/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 64 [37632/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 64 [37888/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 64 [38144/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 64 [38400/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 64 [38656/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 64 [38912/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 64 [39168/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 64 [39424/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 64 [39680/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 64 [39936/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 64 [40192/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 64 [40448/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 64 [40704/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 64 [40960/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 64 [41216/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 64 [41472/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 64 [41728/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 64 [41984/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 64 [42240/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 64 [42496/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 64 [42752/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 64 [43008/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 64 [43264/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 64 [43520/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 64 [43776/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 64 [44032/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 64 [44288/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 64 [44544/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 64 [44800/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 64 [45056/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 64 [45312/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 64 [45568/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 64 [45824/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 64 [46080/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 64 [46336/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 64 [46592/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 64 [46848/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 64 [47104/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 64 [47360/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 64 [47616/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 64 [47872/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 64 [48128/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 64 [48384/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 64 [48640/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 64 [48896/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 64 [49152/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 64 [49408/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 64 [49664/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 64 [49920/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 64 [50000/50000]	Loss: 4.6064	LR: 0.200000
epoch 64 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   96012 GB |   96012 GB |
|       from large pool |  400448 KB |    1770 MB |   95926 GB |   95925 GB |
|       from small pool |    3549 KB |       9 MB |      86 GB |      86 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   96012 GB |   96012 GB |
|       from large pool |  400448 KB |    1770 MB |   95926 GB |   95925 GB |
|       from small pool |    3549 KB |       9 MB |      86 GB |      86 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   59075 GB |   59075 GB |
|       from large pool |  244672 KB |  473024 KB |   58977 GB |   58976 GB |
|       from small pool |    2594 KB |    4843 KB |      98 GB |      98 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4097 K  |    4097 K  |
|       from large pool |      36    |      77    |    1981 K  |    1981 K  |
|       from small pool |     186    |     224    |    2116 K  |    2116 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4097 K  |    4097 K  |
|       from large pool |      36    |      77    |    1981 K  |    1981 K  |
|       from small pool |     186    |     224    |    2116 K  |    2116 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    2088 K  |    2088 K  |
|       from large pool |      10    |      11    |     820 K  |     820 K  |
|       from small pool |      12    |      23    |    1267 K  |    1267 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 64, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 65 [256/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 65 [512/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 65 [768/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 65 [1024/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 65 [1280/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 65 [1536/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 65 [1792/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 65 [2048/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 65 [2304/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 65 [2560/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 65 [2816/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 65 [3072/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 65 [3328/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 65 [3584/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 65 [3840/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 65 [4096/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 65 [4352/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 65 [4608/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 65 [4864/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 65 [5120/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 65 [5376/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 65 [5632/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 65 [5888/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 65 [6144/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 65 [6400/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 65 [6656/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [6912/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 65 [7168/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 65 [7424/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 65 [7680/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 65 [7936/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 65 [8192/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 65 [8448/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 65 [8704/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 65 [8960/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 65 [9216/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 65 [9472/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 65 [9728/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 65 [9984/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 65 [10240/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 65 [10496/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [10752/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 65 [11008/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 65 [11264/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 65 [11520/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 65 [11776/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 65 [12032/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 65 [12288/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 65 [12544/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 65 [12800/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 65 [13056/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 65 [13312/50000]	Loss: 4.5989	LR: 0.200000
Training Epoch: 65 [13568/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 65 [13824/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 65 [14080/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 65 [14336/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 65 [14592/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 65 [14848/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 65 [15104/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [15360/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 65 [15616/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 65 [15872/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 65 [16128/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 65 [16384/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 65 [16640/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 65 [16896/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 65 [17152/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 65 [17408/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 65 [17664/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 65 [17920/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 65 [18176/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 65 [18432/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 65 [18688/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 65 [18944/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [19200/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 65 [19456/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 65 [19712/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 65 [19968/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 65 [20224/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 65 [20480/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 65 [20736/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 65 [20992/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 65 [21248/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 65 [21504/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 65 [21760/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 65 [22016/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 65 [22272/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 65 [22528/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 65 [22784/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 65 [23040/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 65 [23296/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 65 [23552/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 65 [23808/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 65 [24064/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 65 [24320/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 65 [24576/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 65 [24832/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 65 [25088/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 65 [25344/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 65 [25600/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 65 [25856/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [26112/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 65 [26368/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 65 [26624/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 65 [26880/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 65 [27136/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 65 [27392/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 65 [27648/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 65 [27904/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 65 [28160/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 65 [28416/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 65 [28672/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 65 [28928/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 65 [29184/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 65 [29440/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 65 [29696/50000]	Loss: 4.6176	LR: 0.200000
Training Epoch: 65 [29952/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 65 [30208/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 65 [30464/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 65 [30720/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 65 [30976/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 65 [31232/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 65 [31488/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 65 [31744/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 65 [32000/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 65 [32256/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 65 [32512/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 65 [32768/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 65 [33024/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 65 [33280/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 65 [33536/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 65 [33792/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 65 [34048/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 65 [34304/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 65 [34560/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 65 [34816/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 65 [35072/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 65 [35328/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 65 [35584/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 65 [35840/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 65 [36096/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 65 [36352/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 65 [36608/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 65 [36864/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 65 [37120/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 65 [37376/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 65 [37632/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 65 [37888/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 65 [38144/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 65 [38400/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 65 [38656/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 65 [38912/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 65 [39168/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 65 [39424/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 65 [39680/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 65 [39936/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 65 [40192/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 65 [40448/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 65 [40704/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 65 [40960/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 65 [41216/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 65 [41472/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 65 [41728/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 65 [41984/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 65 [42240/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 65 [42496/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 65 [42752/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 65 [43008/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 65 [43264/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 65 [43520/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 65 [43776/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 65 [44032/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 65 [44288/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 65 [44544/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 65 [44800/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 65 [45056/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 65 [45312/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 65 [45568/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [45824/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 65 [46080/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 65 [46336/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 65 [46592/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 65 [46848/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 65 [47104/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 65 [47360/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 65 [47616/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 65 [47872/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 65 [48128/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 65 [48384/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 65 [48640/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 65 [48896/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 65 [49152/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 65 [49408/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 65 [49664/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 65 [49920/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 65 [50000/50000]	Loss: 4.6036	LR: 0.200000
epoch 65 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   97512 GB |   97512 GB |
|       from large pool |  400448 KB |    1770 MB |   97425 GB |   97424 GB |
|       from small pool |    3549 KB |       9 MB |      87 GB |      87 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   97512 GB |   97512 GB |
|       from large pool |  400448 KB |    1770 MB |   97425 GB |   97424 GB |
|       from small pool |    3549 KB |       9 MB |      87 GB |      87 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   59998 GB |   59998 GB |
|       from large pool |  244672 KB |  473024 KB |   59898 GB |   59898 GB |
|       from small pool |    2594 KB |    4843 KB |     100 GB |     100 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4162 K  |    4161 K  |
|       from large pool |      36    |      77    |    2012 K  |    2012 K  |
|       from small pool |     186    |     224    |    2149 K  |    2149 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4162 K  |    4161 K  |
|       from large pool |      36    |      77    |    2012 K  |    2012 K  |
|       from small pool |     186    |     224    |    2149 K  |    2149 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2120 K  |    2120 K  |
|       from large pool |      10    |      11    |     833 K  |     833 K  |
|       from small pool |      11    |      23    |    1287 K  |    1287 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 65, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 66 [256/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 66 [512/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 66 [768/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 66 [1024/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 66 [1280/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 66 [1536/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 66 [1792/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 66 [2048/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 66 [2304/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 66 [2560/50000]	Loss: 4.6011	LR: 0.200000
Training Epoch: 66 [2816/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 66 [3072/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 66 [3328/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 66 [3584/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 66 [3840/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 66 [4096/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 66 [4352/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 66 [4608/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 66 [4864/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 66 [5120/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 66 [5376/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 66 [5632/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 66 [5888/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 66 [6144/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 66 [6400/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 66 [6656/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 66 [6912/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 66 [7168/50000]	Loss: 4.6002	LR: 0.200000
Training Epoch: 66 [7424/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 66 [7680/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 66 [7936/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 66 [8192/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 66 [8448/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 66 [8704/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 66 [8960/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 66 [9216/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 66 [9472/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 66 [9728/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 66 [9984/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 66 [10240/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 66 [10496/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 66 [10752/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 66 [11008/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 66 [11264/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 66 [11520/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 66 [11776/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 66 [12032/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 66 [12288/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 66 [12544/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 66 [12800/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 66 [13056/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 66 [13312/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 66 [13568/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 66 [13824/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 66 [14080/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 66 [14336/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 66 [14592/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 66 [14848/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 66 [15104/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 66 [15360/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 66 [15616/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 66 [15872/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 66 [16128/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 66 [16384/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 66 [16640/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 66 [16896/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 66 [17152/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 66 [17408/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 66 [17664/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 66 [17920/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 66 [18176/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 66 [18432/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 66 [18688/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 66 [18944/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 66 [19200/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 66 [19456/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 66 [19712/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 66 [19968/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 66 [20224/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 66 [20480/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 66 [20736/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 66 [20992/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 66 [21248/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 66 [21504/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 66 [21760/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 66 [22016/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 66 [22272/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 66 [22528/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 66 [22784/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 66 [23040/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 66 [23296/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 66 [23552/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 66 [23808/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 66 [24064/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 66 [24320/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 66 [24576/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 66 [24832/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 66 [25088/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 66 [25344/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 66 [25600/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 66 [25856/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 66 [26112/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 66 [26368/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 66 [26624/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 66 [26880/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 66 [27136/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 66 [27392/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 66 [27648/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 66 [27904/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 66 [28160/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 66 [28416/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 66 [28672/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 66 [28928/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 66 [29184/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 66 [29440/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 66 [29696/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 66 [29952/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 66 [30208/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 66 [30464/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 66 [30720/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 66 [30976/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 66 [31232/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 66 [31488/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 66 [31744/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 66 [32000/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 66 [32256/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 66 [32512/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 66 [32768/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 66 [33024/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 66 [33280/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 66 [33536/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 66 [33792/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 66 [34048/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 66 [34304/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 66 [34560/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 66 [34816/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 66 [35072/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 66 [35328/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 66 [35584/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 66 [35840/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 66 [36096/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 66 [36352/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 66 [36608/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 66 [36864/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 66 [37120/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 66 [37376/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 66 [37632/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 66 [37888/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 66 [38144/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 66 [38400/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 66 [38656/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 66 [38912/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 66 [39168/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 66 [39424/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 66 [39680/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 66 [39936/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 66 [40192/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 66 [40448/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 66 [40704/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 66 [40960/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 66 [41216/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 66 [41472/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 66 [41728/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 66 [41984/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 66 [42240/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 66 [42496/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 66 [42752/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 66 [43008/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 66 [43264/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 66 [43520/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 66 [43776/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 66 [44032/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 66 [44288/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 66 [44544/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 66 [44800/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 66 [45056/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 66 [45312/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 66 [45568/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 66 [45824/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 66 [46080/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 66 [46336/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 66 [46592/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 66 [46848/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 66 [47104/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 66 [47360/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 66 [47616/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 66 [47872/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 66 [48128/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 66 [48384/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 66 [48640/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 66 [48896/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 66 [49152/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 66 [49408/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 66 [49664/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 66 [49920/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 66 [50000/50000]	Loss: 4.6001	LR: 0.200000
epoch 66 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |   99012 GB |   99012 GB |
|       from large pool |  400448 KB |    1770 MB |   98923 GB |   98923 GB |
|       from small pool |    3549 KB |       9 MB |      89 GB |      89 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |   99012 GB |   99012 GB |
|       from large pool |  400448 KB |    1770 MB |   98923 GB |   98923 GB |
|       from small pool |    3549 KB |       9 MB |      89 GB |      89 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   60921 GB |   60921 GB |
|       from large pool |  244672 KB |  473024 KB |   60820 GB |   60819 GB |
|       from small pool |    2594 KB |    4843 KB |     101 GB |     101 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4226 K  |    4225 K  |
|       from large pool |      36    |      77    |    2043 K  |    2043 K  |
|       from small pool |     186    |     224    |    2182 K  |    2182 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4226 K  |    4225 K  |
|       from large pool |      36    |      77    |    2043 K  |    2043 K  |
|       from small pool |     186    |     224    |    2182 K  |    2182 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2153 K  |    2153 K  |
|       from large pool |      10    |      11    |     846 K  |     846 K  |
|       from small pool |      11    |      23    |    1307 K  |    1307 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 66, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.41s

Training Epoch: 67 [256/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 67 [512/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 67 [768/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 67 [1024/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 67 [1280/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 67 [1536/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 67 [1792/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 67 [2048/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 67 [2304/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 67 [2560/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 67 [2816/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 67 [3072/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 67 [3328/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 67 [3584/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 67 [3840/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 67 [4096/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 67 [4352/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 67 [4608/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 67 [4864/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 67 [5120/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 67 [5376/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 67 [5632/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 67 [5888/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 67 [6144/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 67 [6400/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 67 [6656/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 67 [6912/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 67 [7168/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 67 [7424/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 67 [7680/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 67 [7936/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 67 [8192/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 67 [8448/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 67 [8704/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 67 [8960/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 67 [9216/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 67 [9472/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 67 [9728/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 67 [9984/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 67 [10240/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 67 [10496/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 67 [10752/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 67 [11008/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 67 [11264/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 67 [11520/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 67 [11776/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 67 [12032/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 67 [12288/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 67 [12544/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 67 [12800/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 67 [13056/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 67 [13312/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 67 [13568/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 67 [13824/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 67 [14080/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 67 [14336/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 67 [14592/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 67 [14848/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 67 [15104/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 67 [15360/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 67 [15616/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 67 [15872/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 67 [16128/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 67 [16384/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 67 [16640/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 67 [16896/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 67 [17152/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 67 [17408/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 67 [17664/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 67 [17920/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 67 [18176/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 67 [18432/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 67 [18688/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 67 [18944/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 67 [19200/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 67 [19456/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 67 [19712/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 67 [19968/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 67 [20224/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 67 [20480/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 67 [20736/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 67 [20992/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 67 [21248/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 67 [21504/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 67 [21760/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 67 [22016/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 67 [22272/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 67 [22528/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 67 [22784/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 67 [23040/50000]	Loss: 4.6163	LR: 0.200000
Training Epoch: 67 [23296/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 67 [23552/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 67 [23808/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 67 [24064/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 67 [24320/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 67 [24576/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 67 [24832/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 67 [25088/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 67 [25344/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 67 [25600/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 67 [25856/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 67 [26112/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 67 [26368/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 67 [26624/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 67 [26880/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 67 [27136/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 67 [27392/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 67 [27648/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 67 [27904/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 67 [28160/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 67 [28416/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 67 [28672/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 67 [28928/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 67 [29184/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 67 [29440/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 67 [29696/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 67 [29952/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 67 [30208/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 67 [30464/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 67 [30720/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 67 [30976/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 67 [31232/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 67 [31488/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 67 [31744/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 67 [32000/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 67 [32256/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 67 [32512/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 67 [32768/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 67 [33024/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 67 [33280/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 67 [33536/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 67 [33792/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 67 [34048/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 67 [34304/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 67 [34560/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 67 [34816/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 67 [35072/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 67 [35328/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 67 [35584/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 67 [35840/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 67 [36096/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 67 [36352/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 67 [36608/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 67 [36864/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 67 [37120/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 67 [37376/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 67 [37632/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 67 [37888/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 67 [38144/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 67 [38400/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 67 [38656/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 67 [38912/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 67 [39168/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 67 [39424/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 67 [39680/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 67 [39936/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 67 [40192/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 67 [40448/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 67 [40704/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 67 [40960/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 67 [41216/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 67 [41472/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 67 [41728/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 67 [41984/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 67 [42240/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 67 [42496/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 67 [42752/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 67 [43008/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 67 [43264/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 67 [43520/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 67 [43776/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 67 [44032/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 67 [44288/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 67 [44544/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 67 [44800/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 67 [45056/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 67 [45312/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 67 [45568/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 67 [45824/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 67 [46080/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 67 [46336/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 67 [46592/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 67 [46848/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 67 [47104/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 67 [47360/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 67 [47616/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 67 [47872/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 67 [48128/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 67 [48384/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 67 [48640/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 67 [48896/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 67 [49152/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 67 [49408/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 67 [49664/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 67 [49920/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 67 [50000/50000]	Loss: 4.6128	LR: 0.200000
epoch 67 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  100513 GB |  100512 GB |
|       from large pool |  400448 KB |    1770 MB |  100422 GB |  100422 GB |
|       from small pool |    3549 KB |       9 MB |      90 GB |      90 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  100513 GB |  100512 GB |
|       from large pool |  400448 KB |    1770 MB |  100422 GB |  100422 GB |
|       from small pool |    3549 KB |       9 MB |      90 GB |      90 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   61844 GB |   61844 GB |
|       from large pool |  244672 KB |  473024 KB |   61741 GB |   61741 GB |
|       from small pool |    4642 KB |    4843 KB |     103 GB |     103 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4290 K  |    4289 K  |
|       from large pool |      36    |      77    |    2074 K  |    2074 K  |
|       from small pool |     186    |     224    |    2215 K  |    2215 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4290 K  |    4289 K  |
|       from large pool |      36    |      77    |    2074 K  |    2074 K  |
|       from small pool |     186    |     224    |    2215 K  |    2215 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      33    |    2186 K  |    2186 K  |
|       from large pool |      10    |      11    |     859 K  |     859 K  |
|       from small pool |      14    |      23    |    1327 K  |    1327 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 67, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 68 [256/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 68 [512/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 68 [768/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 68 [1024/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 68 [1280/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 68 [1536/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 68 [1792/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [2048/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 68 [2304/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [2560/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 68 [2816/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 68 [3072/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 68 [3328/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 68 [3584/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 68 [3840/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 68 [4096/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 68 [4352/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 68 [4608/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 68 [4864/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 68 [5120/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 68 [5376/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 68 [5632/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 68 [5888/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 68 [6144/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 68 [6400/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 68 [6656/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 68 [6912/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 68 [7168/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 68 [7424/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [7680/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 68 [7936/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 68 [8192/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 68 [8448/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 68 [8704/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 68 [8960/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 68 [9216/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 68 [9472/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 68 [9728/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 68 [9984/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 68 [10240/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 68 [10496/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [10752/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 68 [11008/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 68 [11264/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 68 [11520/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 68 [11776/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 68 [12032/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 68 [12288/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 68 [12544/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 68 [12800/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 68 [13056/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 68 [13312/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 68 [13568/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 68 [13824/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 68 [14080/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 68 [14336/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 68 [14592/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [14848/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 68 [15104/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 68 [15360/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 68 [15616/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 68 [15872/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 68 [16128/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 68 [16384/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 68 [16640/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 68 [16896/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 68 [17152/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 68 [17408/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 68 [17664/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 68 [17920/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 68 [18176/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 68 [18432/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 68 [18688/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 68 [18944/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 68 [19200/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 68 [19456/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 68 [19712/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 68 [19968/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 68 [20224/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 68 [20480/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 68 [20736/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 68 [20992/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 68 [21248/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 68 [21504/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 68 [21760/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [22016/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 68 [22272/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 68 [22528/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 68 [22784/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 68 [23040/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 68 [23296/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 68 [23552/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 68 [23808/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 68 [24064/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [24320/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 68 [24576/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 68 [24832/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 68 [25088/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 68 [25344/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 68 [25600/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 68 [25856/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 68 [26112/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 68 [26368/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 68 [26624/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 68 [26880/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 68 [27136/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 68 [27392/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 68 [27648/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 68 [27904/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 68 [28160/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 68 [28416/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [28672/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 68 [28928/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 68 [29184/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 68 [29440/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 68 [29696/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 68 [29952/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [30208/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 68 [30464/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 68 [30720/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 68 [30976/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 68 [31232/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 68 [31488/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 68 [31744/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 68 [32000/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 68 [32256/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 68 [32512/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 68 [32768/50000]	Loss: 4.6164	LR: 0.200000
Training Epoch: 68 [33024/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [33280/50000]	Loss: 4.6161	LR: 0.200000
Training Epoch: 68 [33536/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 68 [33792/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 68 [34048/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [34304/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 68 [34560/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 68 [34816/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 68 [35072/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 68 [35328/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 68 [35584/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [35840/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 68 [36096/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 68 [36352/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 68 [36608/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 68 [36864/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 68 [37120/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 68 [37376/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 68 [37632/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 68 [37888/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 68 [38144/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 68 [38400/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 68 [38656/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 68 [38912/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 68 [39168/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 68 [39424/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 68 [39680/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [39936/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 68 [40192/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 68 [40448/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 68 [40704/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 68 [40960/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 68 [41216/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 68 [41472/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 68 [41728/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [41984/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 68 [42240/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 68 [42496/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 68 [42752/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 68 [43008/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 68 [43264/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 68 [43520/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [43776/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 68 [44032/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 68 [44288/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 68 [44544/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 68 [44800/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 68 [45056/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 68 [45312/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 68 [45568/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 68 [45824/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 68 [46080/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 68 [46336/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 68 [46592/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 68 [46848/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 68 [47104/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 68 [47360/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 68 [47616/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 68 [47872/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 68 [48128/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 68 [48384/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 68 [48640/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 68 [48896/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 68 [49152/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 68 [49408/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 68 [49664/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 68 [49920/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 68 [50000/50000]	Loss: 4.6034	LR: 0.200000
epoch 68 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  102013 GB |  102012 GB |
|       from large pool |  400448 KB |    1770 MB |  101921 GB |  101921 GB |
|       from small pool |    3549 KB |       9 MB |      91 GB |      91 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  102013 GB |  102012 GB |
|       from large pool |  400448 KB |    1770 MB |  101921 GB |  101921 GB |
|       from small pool |    3549 KB |       9 MB |      91 GB |      91 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   62768 GB |   62767 GB |
|       from large pool |  244672 KB |  473024 KB |   62663 GB |   62662 GB |
|       from small pool |    2594 KB |    4843 KB |     104 GB |     104 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4354 K  |    4353 K  |
|       from large pool |      36    |      77    |    2105 K  |    2105 K  |
|       from small pool |     186    |     224    |    2248 K  |    2248 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4354 K  |    4353 K  |
|       from large pool |      36    |      77    |    2105 K  |    2105 K  |
|       from small pool |     186    |     224    |    2248 K  |    2248 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    2219 K  |    2219 K  |
|       from large pool |      10    |      11    |     871 K  |     871 K  |
|       from small pool |      13    |      23    |    1347 K  |    1347 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 68, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 69 [256/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [512/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 69 [768/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 69 [1024/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 69 [1280/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 69 [1536/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 69 [1792/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 69 [2048/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 69 [2304/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 69 [2560/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 69 [2816/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 69 [3072/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 69 [3328/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 69 [3584/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 69 [3840/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 69 [4096/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 69 [4352/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [4608/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 69 [4864/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 69 [5120/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 69 [5376/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 69 [5632/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 69 [5888/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 69 [6144/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 69 [6400/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 69 [6656/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 69 [6912/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 69 [7168/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 69 [7424/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 69 [7680/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 69 [7936/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 69 [8192/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 69 [8448/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [8704/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 69 [8960/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 69 [9216/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 69 [9472/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 69 [9728/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 69 [9984/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 69 [10240/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 69 [10496/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 69 [10752/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 69 [11008/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 69 [11264/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 69 [11520/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 69 [11776/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [12032/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 69 [12288/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 69 [12544/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 69 [12800/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 69 [13056/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 69 [13312/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 69 [13568/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 69 [13824/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [14080/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 69 [14336/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 69 [14592/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 69 [14848/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 69 [15104/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 69 [15360/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 69 [15616/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 69 [15872/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 69 [16128/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 69 [16384/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 69 [16640/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 69 [16896/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 69 [17152/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 69 [17408/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 69 [17664/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 69 [17920/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 69 [18176/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 69 [18432/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 69 [18688/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 69 [18944/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 69 [19200/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 69 [19456/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 69 [19712/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 69 [19968/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 69 [20224/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 69 [20480/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 69 [20736/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 69 [20992/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 69 [21248/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 69 [21504/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 69 [21760/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 69 [22016/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 69 [22272/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 69 [22528/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 69 [22784/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 69 [23040/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 69 [23296/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 69 [23552/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [23808/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 69 [24064/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 69 [24320/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 69 [24576/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 69 [24832/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 69 [25088/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 69 [25344/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 69 [25600/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 69 [25856/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 69 [26112/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 69 [26368/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 69 [26624/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 69 [26880/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 69 [27136/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 69 [27392/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 69 [27648/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 69 [27904/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 69 [28160/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 69 [28416/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 69 [28672/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 69 [28928/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 69 [29184/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 69 [29440/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 69 [29696/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 69 [29952/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 69 [30208/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 69 [30464/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 69 [30720/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 69 [30976/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 69 [31232/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 69 [31488/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 69 [31744/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 69 [32000/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 69 [32256/50000]	Loss: 4.6001	LR: 0.200000
Training Epoch: 69 [32512/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 69 [32768/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 69 [33024/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 69 [33280/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 69 [33536/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 69 [33792/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 69 [34048/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 69 [34304/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 69 [34560/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 69 [34816/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 69 [35072/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 69 [35328/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 69 [35584/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 69 [35840/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 69 [36096/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 69 [36352/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 69 [36608/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 69 [36864/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 69 [37120/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 69 [37376/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 69 [37632/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 69 [37888/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 69 [38144/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 69 [38400/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 69 [38656/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 69 [38912/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 69 [39168/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 69 [39424/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 69 [39680/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 69 [39936/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 69 [40192/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 69 [40448/50000]	Loss: 4.6011	LR: 0.200000
Training Epoch: 69 [40704/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 69 [40960/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 69 [41216/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 69 [41472/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 69 [41728/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 69 [41984/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 69 [42240/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 69 [42496/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 69 [42752/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 69 [43008/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 69 [43264/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 69 [43520/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 69 [43776/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 69 [44032/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 69 [44288/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 69 [44544/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 69 [44800/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 69 [45056/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 69 [45312/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 69 [45568/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 69 [45824/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 69 [46080/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 69 [46336/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 69 [46592/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 69 [46848/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 69 [47104/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 69 [47360/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 69 [47616/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 69 [47872/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 69 [48128/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 69 [48384/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 69 [48640/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 69 [48896/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 69 [49152/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 69 [49408/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 69 [49664/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 69 [49920/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 69 [50000/50000]	Loss: 4.6058	LR: 0.200000
epoch 69 training time consumed: 21.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  103513 GB |  103513 GB |
|       from large pool |  400448 KB |    1770 MB |  103420 GB |  103419 GB |
|       from small pool |    3549 KB |       9 MB |      93 GB |      93 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  103513 GB |  103513 GB |
|       from large pool |  400448 KB |    1770 MB |  103420 GB |  103419 GB |
|       from small pool |    3549 KB |       9 MB |      93 GB |      93 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   63691 GB |   63690 GB |
|       from large pool |  244672 KB |  473024 KB |   63584 GB |   63584 GB |
|       from small pool |    2594 KB |    4843 KB |     106 GB |     106 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4418 K  |    4417 K  |
|       from large pool |      36    |      77    |    2136 K  |    2136 K  |
|       from small pool |     186    |     224    |    2281 K  |    2281 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4418 K  |    4417 K  |
|       from large pool |      36    |      77    |    2136 K  |    2136 K  |
|       from small pool |     186    |     224    |    2281 K  |    2281 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2251 K  |    2251 K  |
|       from large pool |      10    |      11    |     884 K  |     884 K  |
|       from small pool |      10    |      23    |    1366 K  |    1366 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 69, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 70 [256/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 70 [512/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 70 [768/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 70 [1024/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 70 [1280/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 70 [1536/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 70 [1792/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 70 [2048/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 70 [2304/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 70 [2560/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 70 [2816/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 70 [3072/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 70 [3328/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 70 [3584/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 70 [3840/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 70 [4096/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 70 [4352/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 70 [4608/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 70 [4864/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 70 [5120/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 70 [5376/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 70 [5632/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 70 [5888/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 70 [6144/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 70 [6400/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 70 [6656/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 70 [6912/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 70 [7168/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 70 [7424/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 70 [7680/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 70 [7936/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 70 [8192/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 70 [8448/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 70 [8704/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 70 [8960/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 70 [9216/50000]	Loss: 4.6002	LR: 0.200000
Training Epoch: 70 [9472/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 70 [9728/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 70 [9984/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 70 [10240/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 70 [10496/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 70 [10752/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 70 [11008/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 70 [11264/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 70 [11520/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 70 [11776/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 70 [12032/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 70 [12288/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 70 [12544/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 70 [12800/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 70 [13056/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 70 [13312/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 70 [13568/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 70 [13824/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 70 [14080/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 70 [14336/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 70 [14592/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 70 [14848/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 70 [15104/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 70 [15360/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 70 [15616/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 70 [15872/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 70 [16128/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 70 [16384/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 70 [16640/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 70 [16896/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 70 [17152/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 70 [17408/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 70 [17664/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 70 [17920/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 70 [18176/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 70 [18432/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 70 [18688/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 70 [18944/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 70 [19200/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 70 [19456/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 70 [19712/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 70 [19968/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 70 [20224/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 70 [20480/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 70 [20736/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 70 [20992/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 70 [21248/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 70 [21504/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 70 [21760/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 70 [22016/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 70 [22272/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 70 [22528/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 70 [22784/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 70 [23040/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 70 [23296/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 70 [23552/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 70 [23808/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 70 [24064/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 70 [24320/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 70 [24576/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 70 [24832/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 70 [25088/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 70 [25344/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 70 [25600/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 70 [25856/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 70 [26112/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 70 [26368/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 70 [26624/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 70 [26880/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 70 [27136/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 70 [27392/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 70 [27648/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 70 [27904/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 70 [28160/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 70 [28416/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 70 [28672/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 70 [28928/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 70 [29184/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 70 [29440/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 70 [29696/50000]	Loss: 4.6159	LR: 0.200000
Training Epoch: 70 [29952/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 70 [30208/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 70 [30464/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 70 [30720/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 70 [30976/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 70 [31232/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 70 [31488/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 70 [31744/50000]	Loss: 4.6155	LR: 0.200000
Training Epoch: 70 [32000/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 70 [32256/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 70 [32512/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 70 [32768/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 70 [33024/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 70 [33280/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 70 [33536/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 70 [33792/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 70 [34048/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 70 [34304/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 70 [34560/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 70 [34816/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 70 [35072/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 70 [35328/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 70 [35584/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 70 [35840/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 70 [36096/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 70 [36352/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 70 [36608/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 70 [36864/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 70 [37120/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 70 [37376/50000]	Loss: 4.6011	LR: 0.200000
Training Epoch: 70 [37632/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 70 [37888/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 70 [38144/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 70 [38400/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 70 [38656/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 70 [38912/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 70 [39168/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 70 [39424/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 70 [39680/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 70 [39936/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 70 [40192/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 70 [40448/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 70 [40704/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 70 [40960/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 70 [41216/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 70 [41472/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 70 [41728/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 70 [41984/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 70 [42240/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 70 [42496/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 70 [42752/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 70 [43008/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 70 [43264/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 70 [43520/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 70 [43776/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 70 [44032/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 70 [44288/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 70 [44544/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 70 [44800/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 70 [45056/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 70 [45312/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 70 [45568/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 70 [45824/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 70 [46080/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 70 [46336/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 70 [46592/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 70 [46848/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 70 [47104/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 70 [47360/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 70 [47616/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 70 [47872/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 70 [48128/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 70 [48384/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 70 [48640/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 70 [48896/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 70 [49152/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 70 [49408/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 70 [49664/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 70 [49920/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 70 [50000/50000]	Loss: 4.6071	LR: 0.200000
epoch 70 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  105013 GB |  105013 GB |
|       from large pool |  400448 KB |    1770 MB |  104919 GB |  104918 GB |
|       from small pool |    3549 KB |       9 MB |      94 GB |      94 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  105013 GB |  105013 GB |
|       from large pool |  400448 KB |    1770 MB |  104919 GB |  104918 GB |
|       from small pool |    3549 KB |       9 MB |      94 GB |      94 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   64614 GB |   64613 GB |
|       from large pool |  244672 KB |  473024 KB |   64506 GB |   64505 GB |
|       from small pool |    2594 KB |    4843 KB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4482 K  |    4481 K  |
|       from large pool |      36    |      77    |    2167 K  |    2167 K  |
|       from small pool |     186    |     224    |    2314 K  |    2314 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4482 K  |    4481 K  |
|       from large pool |      36    |      77    |    2167 K  |    2167 K  |
|       from small pool |     186    |     224    |    2314 K  |    2314 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2283 K  |    2283 K  |
|       from large pool |      10    |      11    |     897 K  |     897 K  |
|       from small pool |      11    |      23    |    1386 K  |    1386 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 70, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-70-regular.pth
Training Epoch: 71 [256/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 71 [512/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 71 [768/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [1024/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 71 [1280/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 71 [1536/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 71 [1792/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 71 [2048/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 71 [2304/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 71 [2560/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 71 [2816/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 71 [3072/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 71 [3328/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 71 [3584/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 71 [3840/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 71 [4096/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 71 [4352/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 71 [4608/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 71 [4864/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 71 [5120/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 71 [5376/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 71 [5632/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 71 [5888/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 71 [6144/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 71 [6400/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 71 [6656/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 71 [6912/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 71 [7168/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 71 [7424/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 71 [7680/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 71 [7936/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 71 [8192/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 71 [8448/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 71 [8704/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 71 [8960/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 71 [9216/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 71 [9472/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 71 [9728/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 71 [9984/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 71 [10240/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 71 [10496/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 71 [10752/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 71 [11008/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [11264/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 71 [11520/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 71 [11776/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 71 [12032/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 71 [12288/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 71 [12544/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 71 [12800/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 71 [13056/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 71 [13312/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 71 [13568/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 71 [13824/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 71 [14080/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 71 [14336/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 71 [14592/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 71 [14848/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 71 [15104/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 71 [15360/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 71 [15616/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 71 [15872/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 71 [16128/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 71 [16384/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 71 [16640/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 71 [16896/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 71 [17152/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 71 [17408/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 71 [17664/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 71 [17920/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 71 [18176/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 71 [18432/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 71 [18688/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 71 [18944/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 71 [19200/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 71 [19456/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 71 [19712/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 71 [19968/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 71 [20224/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 71 [20480/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 71 [20736/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 71 [20992/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 71 [21248/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 71 [21504/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 71 [21760/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 71 [22016/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 71 [22272/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 71 [22528/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 71 [22784/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 71 [23040/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 71 [23296/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 71 [23552/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 71 [23808/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 71 [24064/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 71 [24320/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 71 [24576/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 71 [24832/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 71 [25088/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 71 [25344/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 71 [25600/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 71 [25856/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [26112/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 71 [26368/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 71 [26624/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 71 [26880/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 71 [27136/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 71 [27392/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 71 [27648/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 71 [27904/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 71 [28160/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 71 [28416/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 71 [28672/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 71 [28928/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 71 [29184/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 71 [29440/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 71 [29696/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 71 [29952/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 71 [30208/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 71 [30464/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 71 [30720/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 71 [30976/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 71 [31232/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 71 [31488/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 71 [31744/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 71 [32000/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [32256/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 71 [32512/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 71 [32768/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 71 [33024/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 71 [33280/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 71 [33536/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 71 [33792/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 71 [34048/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 71 [34304/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 71 [34560/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 71 [34816/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 71 [35072/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 71 [35328/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 71 [35584/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 71 [35840/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 71 [36096/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 71 [36352/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 71 [36608/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 71 [36864/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 71 [37120/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 71 [37376/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [37632/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 71 [37888/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 71 [38144/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 71 [38400/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 71 [38656/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 71 [38912/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 71 [39168/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 71 [39424/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 71 [39680/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 71 [39936/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 71 [40192/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 71 [40448/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 71 [40704/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 71 [40960/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 71 [41216/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 71 [41472/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 71 [41728/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 71 [41984/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 71 [42240/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 71 [42496/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 71 [42752/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 71 [43008/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 71 [43264/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 71 [43520/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 71 [43776/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 71 [44032/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 71 [44288/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 71 [44544/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 71 [44800/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 71 [45056/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 71 [45312/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 71 [45568/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 71 [45824/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 71 [46080/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 71 [46336/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 71 [46592/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [46848/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 71 [47104/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 71 [47360/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 71 [47616/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 71 [47872/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 71 [48128/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 71 [48384/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 71 [48640/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 71 [48896/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 71 [49152/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 71 [49408/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 71 [49664/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 71 [49920/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 71 [50000/50000]	Loss: 4.6015	LR: 0.200000
epoch 71 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  106513 GB |  106513 GB |
|       from large pool |  400448 KB |    1770 MB |  106418 GB |  106417 GB |
|       from small pool |    3549 KB |       9 MB |      95 GB |      95 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  106513 GB |  106513 GB |
|       from large pool |  400448 KB |    1770 MB |  106418 GB |  106417 GB |
|       from small pool |    3549 KB |       9 MB |      95 GB |      95 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   65537 GB |   65536 GB |
|       from large pool |  244672 KB |  473024 KB |   65427 GB |   65427 GB |
|       from small pool |    2594 KB |    4843 KB |     109 GB |     109 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4546 K  |    4545 K  |
|       from large pool |      36    |      77    |    2198 K  |    2198 K  |
|       from small pool |     186    |     224    |    2347 K  |    2347 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4546 K  |    4545 K  |
|       from large pool |      36    |      77    |    2198 K  |    2198 K  |
|       from small pool |     186    |     224    |    2347 K  |    2347 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2315 K  |    2315 K  |
|       from large pool |      10    |      11    |     910 K  |     910 K  |
|       from small pool |      10    |      23    |    1405 K  |    1405 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 71, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 72 [256/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 72 [512/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 72 [768/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 72 [1024/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 72 [1280/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 72 [1536/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 72 [1792/50000]	Loss: 4.6002	LR: 0.200000
Training Epoch: 72 [2048/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 72 [2304/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 72 [2560/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 72 [2816/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 72 [3072/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 72 [3328/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 72 [3584/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 72 [3840/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 72 [4096/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 72 [4352/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 72 [4608/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 72 [4864/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 72 [5120/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 72 [5376/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 72 [5632/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 72 [5888/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 72 [6144/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 72 [6400/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 72 [6656/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 72 [6912/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 72 [7168/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 72 [7424/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 72 [7680/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 72 [7936/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 72 [8192/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 72 [8448/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 72 [8704/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 72 [8960/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 72 [9216/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 72 [9472/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 72 [9728/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 72 [9984/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 72 [10240/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 72 [10496/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 72 [10752/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 72 [11008/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 72 [11264/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 72 [11520/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 72 [11776/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 72 [12032/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 72 [12288/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 72 [12544/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 72 [12800/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 72 [13056/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 72 [13312/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 72 [13568/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 72 [13824/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 72 [14080/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 72 [14336/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 72 [14592/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 72 [14848/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 72 [15104/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 72 [15360/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 72 [15616/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 72 [15872/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 72 [16128/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 72 [16384/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 72 [16640/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 72 [16896/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 72 [17152/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 72 [17408/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 72 [17664/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 72 [17920/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 72 [18176/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 72 [18432/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 72 [18688/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 72 [18944/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 72 [19200/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 72 [19456/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 72 [19712/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 72 [19968/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 72 [20224/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 72 [20480/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 72 [20736/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 72 [20992/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 72 [21248/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 72 [21504/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 72 [21760/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 72 [22016/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 72 [22272/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 72 [22528/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 72 [22784/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 72 [23040/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 72 [23296/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 72 [23552/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 72 [23808/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 72 [24064/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 72 [24320/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 72 [24576/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 72 [24832/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 72 [25088/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 72 [25344/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 72 [25600/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 72 [25856/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 72 [26112/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 72 [26368/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 72 [26624/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 72 [26880/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 72 [27136/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 72 [27392/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 72 [27648/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 72 [27904/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 72 [28160/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 72 [28416/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 72 [28672/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 72 [28928/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 72 [29184/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 72 [29440/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 72 [29696/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 72 [29952/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 72 [30208/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 72 [30464/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 72 [30720/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 72 [30976/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 72 [31232/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 72 [31488/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 72 [31744/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 72 [32000/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 72 [32256/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 72 [32512/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 72 [32768/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 72 [33024/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 72 [33280/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 72 [33536/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 72 [33792/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 72 [34048/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 72 [34304/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 72 [34560/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 72 [34816/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 72 [35072/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 72 [35328/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 72 [35584/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 72 [35840/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 72 [36096/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 72 [36352/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 72 [36608/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 72 [36864/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 72 [37120/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 72 [37376/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 72 [37632/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 72 [37888/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 72 [38144/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 72 [38400/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 72 [38656/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 72 [38912/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 72 [39168/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 72 [39424/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 72 [39680/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 72 [39936/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 72 [40192/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 72 [40448/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 72 [40704/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 72 [40960/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 72 [41216/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 72 [41472/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 72 [41728/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 72 [41984/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 72 [42240/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 72 [42496/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 72 [42752/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 72 [43008/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 72 [43264/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 72 [43520/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 72 [43776/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 72 [44032/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 72 [44288/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 72 [44544/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 72 [44800/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 72 [45056/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 72 [45312/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 72 [45568/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 72 [45824/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 72 [46080/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 72 [46336/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 72 [46592/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 72 [46848/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 72 [47104/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 72 [47360/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 72 [47616/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 72 [47872/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 72 [48128/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 72 [48384/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 72 [48640/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 72 [48896/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 72 [49152/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 72 [49408/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 72 [49664/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 72 [49920/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 72 [50000/50000]	Loss: 4.6043	LR: 0.200000
epoch 72 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  108014 GB |  108013 GB |
|       from large pool |  400448 KB |    1770 MB |  107916 GB |  107916 GB |
|       from small pool |    3549 KB |       9 MB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  108014 GB |  108013 GB |
|       from large pool |  400448 KB |    1770 MB |  107916 GB |  107916 GB |
|       from small pool |    3549 KB |       9 MB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   66460 GB |   66459 GB |
|       from large pool |  244672 KB |  473024 KB |   66349 GB |   66348 GB |
|       from small pool |    2594 KB |    4843 KB |     111 GB |     111 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4610 K  |    4609 K  |
|       from large pool |      36    |      77    |    2229 K  |    2229 K  |
|       from small pool |     186    |     224    |    2381 K  |    2380 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4610 K  |    4609 K  |
|       from large pool |      36    |      77    |    2229 K  |    2229 K  |
|       from small pool |     186    |     224    |    2381 K  |    2380 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    2348 K  |    2348 K  |
|       from large pool |      10    |      11    |     923 K  |     923 K  |
|       from small pool |      13    |      23    |    1425 K  |    1425 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 72, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 73 [256/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 73 [512/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [768/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 73 [1024/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 73 [1280/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 73 [1536/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 73 [1792/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 73 [2048/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 73 [2304/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 73 [2560/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 73 [2816/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 73 [3072/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 73 [3328/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 73 [3584/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 73 [3840/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [4096/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 73 [4352/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 73 [4608/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 73 [4864/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 73 [5120/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 73 [5376/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 73 [5632/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 73 [5888/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 73 [6144/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 73 [6400/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 73 [6656/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 73 [6912/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 73 [7168/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 73 [7424/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 73 [7680/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 73 [7936/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 73 [8192/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 73 [8448/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 73 [8704/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 73 [8960/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 73 [9216/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 73 [9472/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 73 [9728/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 73 [9984/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 73 [10240/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 73 [10496/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 73 [10752/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 73 [11008/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 73 [11264/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 73 [11520/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 73 [11776/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 73 [12032/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 73 [12288/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [12544/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 73 [12800/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 73 [13056/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 73 [13312/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 73 [13568/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 73 [13824/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 73 [14080/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 73 [14336/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 73 [14592/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 73 [14848/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 73 [15104/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 73 [15360/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 73 [15616/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 73 [15872/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 73 [16128/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 73 [16384/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 73 [16640/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 73 [16896/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 73 [17152/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 73 [17408/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 73 [17664/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 73 [17920/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 73 [18176/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 73 [18432/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 73 [18688/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 73 [18944/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [19200/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 73 [19456/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 73 [19712/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 73 [19968/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 73 [20224/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 73 [20480/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 73 [20736/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 73 [20992/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 73 [21248/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 73 [21504/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 73 [21760/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 73 [22016/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 73 [22272/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 73 [22528/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 73 [22784/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 73 [23040/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 73 [23296/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 73 [23552/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 73 [23808/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 73 [24064/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 73 [24320/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 73 [24576/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 73 [24832/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 73 [25088/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 73 [25344/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [25600/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 73 [25856/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 73 [26112/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 73 [26368/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 73 [26624/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 73 [26880/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 73 [27136/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 73 [27392/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 73 [27648/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 73 [27904/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 73 [28160/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 73 [28416/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 73 [28672/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 73 [28928/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 73 [29184/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 73 [29440/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 73 [29696/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 73 [29952/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 73 [30208/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 73 [30464/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 73 [30720/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 73 [30976/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 73 [31232/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 73 [31488/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 73 [31744/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 73 [32000/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 73 [32256/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 73 [32512/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 73 [32768/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 73 [33024/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 73 [33280/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 73 [33536/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 73 [33792/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 73 [34048/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 73 [34304/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 73 [34560/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 73 [34816/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 73 [35072/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 73 [35328/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 73 [35584/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 73 [35840/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 73 [36096/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 73 [36352/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 73 [36608/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 73 [36864/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 73 [37120/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 73 [37376/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 73 [37632/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 73 [37888/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 73 [38144/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 73 [38400/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 73 [38656/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 73 [38912/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 73 [39168/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 73 [39424/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 73 [39680/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 73 [39936/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 73 [40192/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 73 [40448/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 73 [40704/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 73 [40960/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 73 [41216/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 73 [41472/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 73 [41728/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 73 [41984/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 73 [42240/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 73 [42496/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 73 [42752/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 73 [43008/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 73 [43264/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [43520/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 73 [43776/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 73 [44032/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 73 [44288/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 73 [44544/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 73 [44800/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 73 [45056/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 73 [45312/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 73 [45568/50000]	Loss: 4.6169	LR: 0.200000
Training Epoch: 73 [45824/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 73 [46080/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 73 [46336/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 73 [46592/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 73 [46848/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 73 [47104/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 73 [47360/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 73 [47616/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 73 [47872/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 73 [48128/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 73 [48384/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 73 [48640/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 73 [48896/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 73 [49152/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 73 [49408/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 73 [49664/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 73 [49920/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 73 [50000/50000]	Loss: 4.6137	LR: 0.200000
epoch 73 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  109514 GB |  109513 GB |
|       from large pool |  400448 KB |    1770 MB |  109415 GB |  109415 GB |
|       from small pool |    3549 KB |       9 MB |      98 GB |      98 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  109514 GB |  109513 GB |
|       from large pool |  400448 KB |    1770 MB |  109415 GB |  109415 GB |
|       from small pool |    3549 KB |       9 MB |      98 GB |      98 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   67383 GB |   67382 GB |
|       from large pool |  244672 KB |  473024 KB |   67270 GB |   67270 GB |
|       from small pool |    2594 KB |    4843 KB |     112 GB |     112 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4674 K  |    4673 K  |
|       from large pool |      36    |      77    |    2260 K  |    2260 K  |
|       from small pool |     186    |     224    |    2414 K  |    2413 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4674 K  |    4673 K  |
|       from large pool |      36    |      77    |    2260 K  |    2260 K  |
|       from small pool |     186    |     224    |    2414 K  |    2413 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2381 K  |    2380 K  |
|       from large pool |      10    |      11    |     936 K  |     936 K  |
|       from small pool |      10    |      23    |    1444 K  |    1444 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 73, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 74 [256/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 74 [512/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 74 [768/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 74 [1024/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 74 [1280/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 74 [1536/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 74 [1792/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 74 [2048/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 74 [2304/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 74 [2560/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 74 [2816/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 74 [3072/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 74 [3328/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 74 [3584/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 74 [3840/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 74 [4096/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 74 [4352/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 74 [4608/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 74 [4864/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 74 [5120/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 74 [5376/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 74 [5632/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 74 [5888/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 74 [6144/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 74 [6400/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 74 [6656/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 74 [6912/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 74 [7168/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 74 [7424/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 74 [7680/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 74 [7936/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 74 [8192/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 74 [8448/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 74 [8704/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 74 [8960/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 74 [9216/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 74 [9472/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 74 [9728/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 74 [9984/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 74 [10240/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 74 [10496/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 74 [10752/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 74 [11008/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 74 [11264/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 74 [11520/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 74 [11776/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 74 [12032/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 74 [12288/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 74 [12544/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 74 [12800/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 74 [13056/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 74 [13312/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 74 [13568/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 74 [13824/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 74 [14080/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 74 [14336/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 74 [14592/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 74 [14848/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 74 [15104/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 74 [15360/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 74 [15616/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 74 [15872/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 74 [16128/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 74 [16384/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 74 [16640/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 74 [16896/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 74 [17152/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 74 [17408/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 74 [17664/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 74 [17920/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 74 [18176/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 74 [18432/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 74 [18688/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 74 [18944/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 74 [19200/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 74 [19456/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 74 [19712/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 74 [19968/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 74 [20224/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 74 [20480/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 74 [20736/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 74 [20992/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 74 [21248/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 74 [21504/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 74 [21760/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 74 [22016/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 74 [22272/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 74 [22528/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 74 [22784/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 74 [23040/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 74 [23296/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 74 [23552/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 74 [23808/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 74 [24064/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 74 [24320/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 74 [24576/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 74 [24832/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 74 [25088/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 74 [25344/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 74 [25600/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 74 [25856/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 74 [26112/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 74 [26368/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 74 [26624/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 74 [26880/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 74 [27136/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 74 [27392/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 74 [27648/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 74 [27904/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 74 [28160/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 74 [28416/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 74 [28672/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 74 [28928/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 74 [29184/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 74 [29440/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 74 [29696/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 74 [29952/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 74 [30208/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 74 [30464/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 74 [30720/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 74 [30976/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 74 [31232/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 74 [31488/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 74 [31744/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 74 [32000/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 74 [32256/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 74 [32512/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 74 [32768/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 74 [33024/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 74 [33280/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 74 [33536/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 74 [33792/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 74 [34048/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 74 [34304/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 74 [34560/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 74 [34816/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 74 [35072/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 74 [35328/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 74 [35584/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 74 [35840/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 74 [36096/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 74 [36352/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 74 [36608/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 74 [36864/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 74 [37120/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 74 [37376/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 74 [37632/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 74 [37888/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 74 [38144/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 74 [38400/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 74 [38656/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 74 [38912/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 74 [39168/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 74 [39424/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 74 [39680/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 74 [39936/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 74 [40192/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 74 [40448/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 74 [40704/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 74 [40960/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 74 [41216/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 74 [41472/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 74 [41728/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 74 [41984/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 74 [42240/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 74 [42496/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 74 [42752/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 74 [43008/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 74 [43264/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 74 [43520/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 74 [43776/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 74 [44032/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 74 [44288/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 74 [44544/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 74 [44800/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 74 [45056/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 74 [45312/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 74 [45568/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 74 [45824/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 74 [46080/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 74 [46336/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 74 [46592/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 74 [46848/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 74 [47104/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 74 [47360/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 74 [47616/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 74 [47872/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 74 [48128/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 74 [48384/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 74 [48640/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 74 [48896/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 74 [49152/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 74 [49408/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 74 [49664/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 74 [49920/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 74 [50000/50000]	Loss: 4.6040	LR: 0.200000
epoch 74 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  111014 GB |  111014 GB |
|       from large pool |  400448 KB |    1770 MB |  110914 GB |  110914 GB |
|       from small pool |    3549 KB |       9 MB |      99 GB |      99 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  111014 GB |  111014 GB |
|       from large pool |  400448 KB |    1770 MB |  110914 GB |  110914 GB |
|       from small pool |    3549 KB |       9 MB |      99 GB |      99 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   68306 GB |   68305 GB |
|       from large pool |  244672 KB |  473024 KB |   68191 GB |   68191 GB |
|       from small pool |    2594 KB |    4843 KB |     114 GB |     114 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4738 K  |    4738 K  |
|       from large pool |      36    |      77    |    2291 K  |    2291 K  |
|       from small pool |     186    |     224    |    2447 K  |    2447 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4738 K  |    4738 K  |
|       from large pool |      36    |      77    |    2291 K  |    2291 K  |
|       from small pool |     186    |     224    |    2447 K  |    2447 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    2413 K  |    2413 K  |
|       from large pool |      10    |      11    |     948 K  |     948 K  |
|       from small pool |      13    |      23    |    1464 K  |    1464 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 74, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 75 [256/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 75 [512/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 75 [768/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 75 [1024/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 75 [1280/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 75 [1536/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 75 [1792/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 75 [2048/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 75 [2304/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 75 [2560/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 75 [2816/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 75 [3072/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 75 [3328/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 75 [3584/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 75 [3840/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 75 [4096/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 75 [4352/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 75 [4608/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 75 [4864/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 75 [5120/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 75 [5376/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 75 [5632/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 75 [5888/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 75 [6144/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 75 [6400/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 75 [6656/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 75 [6912/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 75 [7168/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 75 [7424/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 75 [7680/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 75 [7936/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 75 [8192/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 75 [8448/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 75 [8704/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 75 [8960/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 75 [9216/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 75 [9472/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 75 [9728/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 75 [9984/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 75 [10240/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 75 [10496/50000]	Loss: 4.6178	LR: 0.200000
Training Epoch: 75 [10752/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 75 [11008/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 75 [11264/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 75 [11520/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 75 [11776/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 75 [12032/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 75 [12288/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 75 [12544/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 75 [12800/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 75 [13056/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 75 [13312/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 75 [13568/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 75 [13824/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 75 [14080/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 75 [14336/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 75 [14592/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 75 [14848/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 75 [15104/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 75 [15360/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 75 [15616/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 75 [15872/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 75 [16128/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 75 [16384/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 75 [16640/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 75 [16896/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 75 [17152/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 75 [17408/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 75 [17664/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 75 [17920/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 75 [18176/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 75 [18432/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 75 [18688/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 75 [18944/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 75 [19200/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 75 [19456/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 75 [19712/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 75 [19968/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 75 [20224/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 75 [20480/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 75 [20736/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 75 [20992/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 75 [21248/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 75 [21504/50000]	Loss: 4.6160	LR: 0.200000
Training Epoch: 75 [21760/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 75 [22016/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 75 [22272/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 75 [22528/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 75 [22784/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 75 [23040/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 75 [23296/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 75 [23552/50000]	Loss: 4.6159	LR: 0.200000
Training Epoch: 75 [23808/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 75 [24064/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 75 [24320/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 75 [24576/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 75 [24832/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 75 [25088/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 75 [25344/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 75 [25600/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 75 [25856/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 75 [26112/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 75 [26368/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 75 [26624/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 75 [26880/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 75 [27136/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 75 [27392/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 75 [27648/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 75 [27904/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 75 [28160/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 75 [28416/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 75 [28672/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 75 [28928/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 75 [29184/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 75 [29440/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 75 [29696/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 75 [29952/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 75 [30208/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 75 [30464/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 75 [30720/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 75 [30976/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 75 [31232/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 75 [31488/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 75 [31744/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 75 [32000/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 75 [32256/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 75 [32512/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 75 [32768/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 75 [33024/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 75 [33280/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 75 [33536/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 75 [33792/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 75 [34048/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 75 [34304/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 75 [34560/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 75 [34816/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 75 [35072/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 75 [35328/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 75 [35584/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 75 [35840/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 75 [36096/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 75 [36352/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 75 [36608/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 75 [36864/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 75 [37120/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 75 [37376/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 75 [37632/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 75 [37888/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 75 [38144/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 75 [38400/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 75 [38656/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 75 [38912/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 75 [39168/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 75 [39424/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 75 [39680/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 75 [39936/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 75 [40192/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 75 [40448/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 75 [40704/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 75 [40960/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 75 [41216/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 75 [41472/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 75 [41728/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 75 [41984/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 75 [42240/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 75 [42496/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 75 [42752/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 75 [43008/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 75 [43264/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 75 [43520/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 75 [43776/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 75 [44032/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 75 [44288/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 75 [44544/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 75 [44800/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 75 [45056/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 75 [45312/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 75 [45568/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 75 [45824/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 75 [46080/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 75 [46336/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 75 [46592/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 75 [46848/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 75 [47104/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 75 [47360/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 75 [47616/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 75 [47872/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 75 [48128/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 75 [48384/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 75 [48640/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 75 [48896/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 75 [49152/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 75 [49408/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 75 [49664/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 75 [49920/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 75 [50000/50000]	Loss: 4.6039	LR: 0.200000
epoch 75 training time consumed: 21.72s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  112514 GB |  112514 GB |
|       from large pool |  400448 KB |    1770 MB |  112413 GB |  112412 GB |
|       from small pool |    3549 KB |       9 MB |     101 GB |     101 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  112514 GB |  112514 GB |
|       from large pool |  400448 KB |    1770 MB |  112413 GB |  112412 GB |
|       from small pool |    3549 KB |       9 MB |     101 GB |     101 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   69229 GB |   69228 GB |
|       from large pool |  244672 KB |  473024 KB |   69113 GB |   69113 GB |
|       from small pool |    2594 KB |    4843 KB |     115 GB |     115 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4802 K  |    4802 K  |
|       from large pool |      36    |      77    |    2321 K  |    2321 K  |
|       from small pool |     186    |     224    |    2480 K  |    2480 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4802 K  |    4802 K  |
|       from large pool |      36    |      77    |    2321 K  |    2321 K  |
|       from small pool |     186    |     224    |    2480 K  |    2480 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    2446 K  |    2446 K  |
|       from large pool |      10    |      11    |     961 K  |     961 K  |
|       from small pool |      12    |      23    |    1484 K  |    1484 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 75, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 76 [256/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 76 [512/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 76 [768/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 76 [1024/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 76 [1280/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 76 [1536/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 76 [1792/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 76 [2048/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 76 [2304/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 76 [2560/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 76 [2816/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 76 [3072/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [3328/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 76 [3584/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [3840/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [4096/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 76 [4352/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 76 [4608/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 76 [4864/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 76 [5120/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 76 [5376/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 76 [5632/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 76 [5888/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 76 [6144/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 76 [6400/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 76 [6656/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 76 [6912/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 76 [7168/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 76 [7424/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 76 [7680/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 76 [7936/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 76 [8192/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 76 [8448/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 76 [8704/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 76 [8960/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 76 [9216/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 76 [9472/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 76 [9728/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [9984/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [10240/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 76 [10496/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 76 [10752/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 76 [11008/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 76 [11264/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 76 [11520/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 76 [11776/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [12032/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 76 [12288/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 76 [12544/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 76 [12800/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 76 [13056/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 76 [13312/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 76 [13568/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [13824/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 76 [14080/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 76 [14336/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 76 [14592/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 76 [14848/50000]	Loss: 4.6168	LR: 0.200000
Training Epoch: 76 [15104/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 76 [15360/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 76 [15616/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 76 [15872/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 76 [16128/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 76 [16384/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 76 [16640/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 76 [16896/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 76 [17152/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 76 [17408/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 76 [17664/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [17920/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 76 [18176/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 76 [18432/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 76 [18688/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 76 [18944/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 76 [19200/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 76 [19456/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 76 [19712/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [19968/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 76 [20224/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 76 [20480/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 76 [20736/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [20992/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 76 [21248/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 76 [21504/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 76 [21760/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 76 [22016/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 76 [22272/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 76 [22528/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 76 [22784/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 76 [23040/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 76 [23296/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 76 [23552/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 76 [23808/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 76 [24064/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 76 [24320/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 76 [24576/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 76 [24832/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 76 [25088/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 76 [25344/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 76 [25600/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 76 [25856/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 76 [26112/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 76 [26368/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [26624/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 76 [26880/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 76 [27136/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 76 [27392/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 76 [27648/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 76 [27904/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 76 [28160/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [28416/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 76 [28672/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 76 [28928/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 76 [29184/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 76 [29440/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 76 [29696/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 76 [29952/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 76 [30208/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 76 [30464/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 76 [30720/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 76 [30976/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [31232/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 76 [31488/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 76 [31744/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [32000/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 76 [32256/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 76 [32512/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 76 [32768/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [33024/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 76 [33280/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 76 [33536/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 76 [33792/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 76 [34048/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 76 [34304/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 76 [34560/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [34816/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 76 [35072/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [35328/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [35584/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 76 [35840/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 76 [36096/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 76 [36352/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 76 [36608/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 76 [36864/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 76 [37120/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 76 [37376/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 76 [37632/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 76 [37888/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 76 [38144/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 76 [38400/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 76 [38656/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 76 [38912/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 76 [39168/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 76 [39424/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 76 [39680/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 76 [39936/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 76 [40192/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 76 [40448/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 76 [40704/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 76 [40960/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 76 [41216/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 76 [41472/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 76 [41728/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 76 [41984/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 76 [42240/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 76 [42496/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [42752/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 76 [43008/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [43264/50000]	Loss: 4.6001	LR: 0.200000
Training Epoch: 76 [43520/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 76 [43776/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 76 [44032/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 76 [44288/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 76 [44544/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 76 [44800/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 76 [45056/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 76 [45312/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 76 [45568/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 76 [45824/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 76 [46080/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 76 [46336/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 76 [46592/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 76 [46848/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 76 [47104/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 76 [47360/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 76 [47616/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 76 [47872/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 76 [48128/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 76 [48384/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 76 [48640/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 76 [48896/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 76 [49152/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 76 [49408/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 76 [49664/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 76 [49920/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 76 [50000/50000]	Loss: 4.6092	LR: 0.200000
epoch 76 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  114014 GB |  114014 GB |
|       from large pool |  400448 KB |    1770 MB |  113912 GB |  113911 GB |
|       from small pool |    3549 KB |       9 MB |     102 GB |     102 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  114014 GB |  114014 GB |
|       from large pool |  400448 KB |    1770 MB |  113912 GB |  113911 GB |
|       from small pool |    3549 KB |       9 MB |     102 GB |     102 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   70152 GB |   70152 GB |
|       from large pool |  244672 KB |  473024 KB |   70034 GB |   70034 GB |
|       from small pool |    2594 KB |    4843 KB |     117 GB |     117 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4866 K  |    4866 K  |
|       from large pool |      36    |      77    |    2352 K  |    2352 K  |
|       from small pool |     186    |     224    |    2513 K  |    2513 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4866 K  |    4866 K  |
|       from large pool |      36    |      77    |    2352 K  |    2352 K  |
|       from small pool |     186    |     224    |    2513 K  |    2513 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2479 K  |    2479 K  |
|       from large pool |      10    |      11    |     974 K  |     974 K  |
|       from small pool |      11    |      23    |    1504 K  |    1504 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 76, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 77 [256/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 77 [512/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 77 [768/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 77 [1024/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 77 [1280/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 77 [1536/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 77 [1792/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 77 [2048/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 77 [2304/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 77 [2560/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 77 [2816/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 77 [3072/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 77 [3328/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 77 [3584/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 77 [3840/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 77 [4096/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 77 [4352/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 77 [4608/50000]	Loss: 4.6001	LR: 0.200000
Training Epoch: 77 [4864/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 77 [5120/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 77 [5376/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 77 [5632/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 77 [5888/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 77 [6144/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 77 [6400/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 77 [6656/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 77 [6912/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 77 [7168/50000]	Loss: 4.5999	LR: 0.200000
Training Epoch: 77 [7424/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 77 [7680/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 77 [7936/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 77 [8192/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 77 [8448/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 77 [8704/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 77 [8960/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 77 [9216/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 77 [9472/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 77 [9728/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 77 [9984/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 77 [10240/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 77 [10496/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 77 [10752/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 77 [11008/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 77 [11264/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 77 [11520/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 77 [11776/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 77 [12032/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 77 [12288/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 77 [12544/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 77 [12800/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 77 [13056/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 77 [13312/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 77 [13568/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 77 [13824/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 77 [14080/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 77 [14336/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 77 [14592/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 77 [14848/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 77 [15104/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 77 [15360/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 77 [15616/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 77 [15872/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 77 [16128/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 77 [16384/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 77 [16640/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 77 [16896/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 77 [17152/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 77 [17408/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 77 [17664/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 77 [17920/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 77 [18176/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 77 [18432/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 77 [18688/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 77 [18944/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 77 [19200/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 77 [19456/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 77 [19712/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 77 [19968/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 77 [20224/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 77 [20480/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 77 [20736/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 77 [20992/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 77 [21248/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 77 [21504/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 77 [21760/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 77 [22016/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 77 [22272/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 77 [22528/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 77 [22784/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 77 [23040/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 77 [23296/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 77 [23552/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 77 [23808/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 77 [24064/50000]	Loss: 4.5992	LR: 0.200000
Training Epoch: 77 [24320/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 77 [24576/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 77 [24832/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 77 [25088/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 77 [25344/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 77 [25600/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 77 [25856/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 77 [26112/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 77 [26368/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 77 [26624/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 77 [26880/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 77 [27136/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 77 [27392/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 77 [27648/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 77 [27904/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 77 [28160/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 77 [28416/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 77 [28672/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 77 [28928/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 77 [29184/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 77 [29440/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 77 [29696/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 77 [29952/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 77 [30208/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 77 [30464/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 77 [30720/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 77 [30976/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 77 [31232/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 77 [31488/50000]	Loss: 4.5992	LR: 0.200000
Training Epoch: 77 [31744/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 77 [32000/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 77 [32256/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 77 [32512/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 77 [32768/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 77 [33024/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 77 [33280/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 77 [33536/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 77 [33792/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 77 [34048/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 77 [34304/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 77 [34560/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 77 [34816/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 77 [35072/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 77 [35328/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 77 [35584/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 77 [35840/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 77 [36096/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 77 [36352/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 77 [36608/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 77 [36864/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 77 [37120/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 77 [37376/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 77 [37632/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 77 [37888/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 77 [38144/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 77 [38400/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 77 [38656/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 77 [38912/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 77 [39168/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 77 [39424/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 77 [39680/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 77 [39936/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 77 [40192/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 77 [40448/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 77 [40704/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 77 [40960/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 77 [41216/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 77 [41472/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 77 [41728/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 77 [41984/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 77 [42240/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 77 [42496/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 77 [42752/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 77 [43008/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 77 [43264/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 77 [43520/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 77 [43776/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 77 [44032/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 77 [44288/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 77 [44544/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 77 [44800/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 77 [45056/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 77 [45312/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 77 [45568/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 77 [45824/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 77 [46080/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 77 [46336/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 77 [46592/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 77 [46848/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 77 [47104/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 77 [47360/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 77 [47616/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 77 [47872/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 77 [48128/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 77 [48384/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 77 [48640/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 77 [48896/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 77 [49152/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 77 [49408/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 77 [49664/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 77 [49920/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 77 [50000/50000]	Loss: 4.6098	LR: 0.200000
epoch 77 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  115514 GB |  115514 GB |
|       from large pool |  400448 KB |    1770 MB |  115411 GB |  115410 GB |
|       from small pool |    3549 KB |       9 MB |     103 GB |     103 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  115514 GB |  115514 GB |
|       from large pool |  400448 KB |    1770 MB |  115411 GB |  115410 GB |
|       from small pool |    3549 KB |       9 MB |     103 GB |     103 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   71075 GB |   71075 GB |
|       from large pool |  244672 KB |  473024 KB |   70956 GB |   70956 GB |
|       from small pool |    2594 KB |    4843 KB |     118 GB |     118 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4930 K  |    4930 K  |
|       from large pool |      36    |      77    |    2383 K  |    2383 K  |
|       from small pool |     186    |     224    |    2546 K  |    2546 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4930 K  |    4930 K  |
|       from large pool |      36    |      77    |    2383 K  |    2383 K  |
|       from small pool |     186    |     224    |    2546 K  |    2546 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2512 K  |    2511 K  |
|       from large pool |      10    |      11    |     987 K  |     987 K  |
|       from small pool |      11    |      23    |    1524 K  |    1524 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 77, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.40s

Training Epoch: 78 [256/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 78 [512/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 78 [768/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 78 [1024/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 78 [1280/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 78 [1536/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 78 [1792/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 78 [2048/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 78 [2304/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 78 [2560/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 78 [2816/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 78 [3072/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 78 [3328/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 78 [3584/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 78 [3840/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 78 [4096/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 78 [4352/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 78 [4608/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 78 [4864/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 78 [5120/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 78 [5376/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 78 [5632/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 78 [5888/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 78 [6144/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 78 [6400/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 78 [6656/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 78 [6912/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 78 [7168/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 78 [7424/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 78 [7680/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 78 [7936/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 78 [8192/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 78 [8448/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 78 [8704/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 78 [8960/50000]	Loss: 4.5981	LR: 0.200000
Training Epoch: 78 [9216/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 78 [9472/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 78 [9728/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 78 [9984/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 78 [10240/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 78 [10496/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 78 [10752/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 78 [11008/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 78 [11264/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 78 [11520/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 78 [11776/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 78 [12032/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 78 [12288/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 78 [12544/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 78 [12800/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 78 [13056/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 78 [13312/50000]	Loss: 4.5999	LR: 0.200000
Training Epoch: 78 [13568/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 78 [13824/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 78 [14080/50000]	Loss: 4.6173	LR: 0.200000
Training Epoch: 78 [14336/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 78 [14592/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 78 [14848/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 78 [15104/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 78 [15360/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 78 [15616/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 78 [15872/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 78 [16128/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 78 [16384/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 78 [16640/50000]	Loss: 4.6161	LR: 0.200000
Training Epoch: 78 [16896/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 78 [17152/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 78 [17408/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 78 [17664/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 78 [17920/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 78 [18176/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 78 [18432/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 78 [18688/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 78 [18944/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 78 [19200/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 78 [19456/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 78 [19712/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 78 [19968/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 78 [20224/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 78 [20480/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 78 [20736/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 78 [20992/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 78 [21248/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 78 [21504/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 78 [21760/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 78 [22016/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 78 [22272/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 78 [22528/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 78 [22784/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 78 [23040/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 78 [23296/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 78 [23552/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 78 [23808/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 78 [24064/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 78 [24320/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 78 [24576/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 78 [24832/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 78 [25088/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 78 [25344/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 78 [25600/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 78 [25856/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 78 [26112/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 78 [26368/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 78 [26624/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 78 [26880/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 78 [27136/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 78 [27392/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 78 [27648/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 78 [27904/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 78 [28160/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 78 [28416/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 78 [28672/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 78 [28928/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 78 [29184/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 78 [29440/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 78 [29696/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 78 [29952/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 78 [30208/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 78 [30464/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 78 [30720/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 78 [30976/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 78 [31232/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 78 [31488/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 78 [31744/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 78 [32000/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 78 [32256/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 78 [32512/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 78 [32768/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 78 [33024/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 78 [33280/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 78 [33536/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 78 [33792/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 78 [34048/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 78 [34304/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 78 [34560/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 78 [34816/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 78 [35072/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 78 [35328/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 78 [35584/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 78 [35840/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 78 [36096/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 78 [36352/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 78 [36608/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 78 [36864/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 78 [37120/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 78 [37376/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 78 [37632/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 78 [37888/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 78 [38144/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 78 [38400/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 78 [38656/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 78 [38912/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 78 [39168/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 78 [39424/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 78 [39680/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 78 [39936/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 78 [40192/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 78 [40448/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 78 [40704/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 78 [40960/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 78 [41216/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 78 [41472/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 78 [41728/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 78 [41984/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 78 [42240/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 78 [42496/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 78 [42752/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 78 [43008/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 78 [43264/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 78 [43520/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 78 [43776/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 78 [44032/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 78 [44288/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 78 [44544/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 78 [44800/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 78 [45056/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 78 [45312/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 78 [45568/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 78 [45824/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 78 [46080/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 78 [46336/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 78 [46592/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 78 [46848/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 78 [47104/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 78 [47360/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 78 [47616/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 78 [47872/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 78 [48128/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 78 [48384/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 78 [48640/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 78 [48896/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 78 [49152/50000]	Loss: 4.6159	LR: 0.200000
Training Epoch: 78 [49408/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 78 [49664/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 78 [49920/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 78 [50000/50000]	Loss: 4.6059	LR: 0.200000
epoch 78 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  117015 GB |  117014 GB |
|       from large pool |  400448 KB |    1770 MB |  116909 GB |  116909 GB |
|       from small pool |    3549 KB |       9 MB |     105 GB |     105 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  117015 GB |  117014 GB |
|       from large pool |  400448 KB |    1770 MB |  116909 GB |  116909 GB |
|       from small pool |    3549 KB |       9 MB |     105 GB |     105 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   71998 GB |   71998 GB |
|       from large pool |  244672 KB |  473024 KB |   71877 GB |   71877 GB |
|       from small pool |    2594 KB |    4843 KB |     120 GB |     120 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    4994 K  |    4994 K  |
|       from large pool |      36    |      77    |    2414 K  |    2414 K  |
|       from small pool |     186    |     224    |    2579 K  |    2579 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    4994 K  |    4994 K  |
|       from large pool |      36    |      77    |    2414 K  |    2414 K  |
|       from small pool |     186    |     224    |    2579 K  |    2579 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    2544 K  |    2544 K  |
|       from large pool |      10    |      11    |    1000 K  |    1000 K  |
|       from small pool |      12    |      23    |    1544 K  |    1544 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 78, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 79 [256/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 79 [512/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 79 [768/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 79 [1024/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 79 [1280/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 79 [1536/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 79 [1792/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 79 [2048/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 79 [2304/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 79 [2560/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 79 [2816/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 79 [3072/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 79 [3328/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 79 [3584/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 79 [3840/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 79 [4096/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 79 [4352/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 79 [4608/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 79 [4864/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 79 [5120/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 79 [5376/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 79 [5632/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 79 [5888/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 79 [6144/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 79 [6400/50000]	Loss: 4.5989	LR: 0.200000
Training Epoch: 79 [6656/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 79 [6912/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 79 [7168/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [7424/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 79 [7680/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 79 [7936/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 79 [8192/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 79 [8448/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [8704/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 79 [8960/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 79 [9216/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 79 [9472/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 79 [9728/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 79 [9984/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 79 [10240/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 79 [10496/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 79 [10752/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 79 [11008/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 79 [11264/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 79 [11520/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 79 [11776/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 79 [12032/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 79 [12288/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 79 [12544/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 79 [12800/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 79 [13056/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 79 [13312/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 79 [13568/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 79 [13824/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 79 [14080/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 79 [14336/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 79 [14592/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 79 [14848/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [15104/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 79 [15360/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 79 [15616/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 79 [15872/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 79 [16128/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 79 [16384/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 79 [16640/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 79 [16896/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 79 [17152/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 79 [17408/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 79 [17664/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 79 [17920/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 79 [18176/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 79 [18432/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 79 [18688/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 79 [18944/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 79 [19200/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 79 [19456/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 79 [19712/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 79 [19968/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 79 [20224/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 79 [20480/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 79 [20736/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 79 [20992/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 79 [21248/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 79 [21504/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 79 [21760/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 79 [22016/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 79 [22272/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 79 [22528/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 79 [22784/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 79 [23040/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 79 [23296/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 79 [23552/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 79 [23808/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 79 [24064/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 79 [24320/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 79 [24576/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 79 [24832/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 79 [25088/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 79 [25344/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 79 [25600/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 79 [25856/50000]	Loss: 4.5992	LR: 0.200000
Training Epoch: 79 [26112/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 79 [26368/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 79 [26624/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 79 [26880/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 79 [27136/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [27392/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 79 [27648/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 79 [27904/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 79 [28160/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 79 [28416/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 79 [28672/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 79 [28928/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 79 [29184/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 79 [29440/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 79 [29696/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 79 [29952/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 79 [30208/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 79 [30464/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 79 [30720/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 79 [30976/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 79 [31232/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 79 [31488/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 79 [31744/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 79 [32000/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 79 [32256/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 79 [32512/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 79 [32768/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 79 [33024/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 79 [33280/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 79 [33536/50000]	Loss: 4.6153	LR: 0.200000
Training Epoch: 79 [33792/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 79 [34048/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 79 [34304/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 79 [34560/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 79 [34816/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 79 [35072/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 79 [35328/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 79 [35584/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 79 [35840/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 79 [36096/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 79 [36352/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 79 [36608/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 79 [36864/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 79 [37120/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 79 [37376/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [37632/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 79 [37888/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 79 [38144/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 79 [38400/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 79 [38656/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 79 [38912/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 79 [39168/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 79 [39424/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 79 [39680/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 79 [39936/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 79 [40192/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 79 [40448/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 79 [40704/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 79 [40960/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 79 [41216/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 79 [41472/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 79 [41728/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 79 [41984/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 79 [42240/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 79 [42496/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 79 [42752/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 79 [43008/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 79 [43264/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 79 [43520/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 79 [43776/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 79 [44032/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 79 [44288/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 79 [44544/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 79 [44800/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 79 [45056/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 79 [45312/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 79 [45568/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 79 [45824/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 79 [46080/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 79 [46336/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 79 [46592/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 79 [46848/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 79 [47104/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 79 [47360/50000]	Loss: 4.6170	LR: 0.200000
Training Epoch: 79 [47616/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [47872/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 79 [48128/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 79 [48384/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 79 [48640/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 79 [48896/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 79 [49152/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 79 [49408/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 79 [49664/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 79 [49920/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 79 [50000/50000]	Loss: 4.6190	LR: 0.200000
epoch 79 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  118515 GB |  118514 GB |
|       from large pool |  400448 KB |    1770 MB |  118408 GB |  118408 GB |
|       from small pool |    3549 KB |       9 MB |     106 GB |     106 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  118515 GB |  118514 GB |
|       from large pool |  400448 KB |    1770 MB |  118408 GB |  118408 GB |
|       from small pool |    3549 KB |       9 MB |     106 GB |     106 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   72921 GB |   72921 GB |
|       from large pool |  244672 KB |  473024 KB |   72799 GB |   72799 GB |
|       from small pool |    2594 KB |    4843 KB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5058 K  |    5058 K  |
|       from large pool |      36    |      77    |    2445 K  |    2445 K  |
|       from small pool |     186    |     224    |    2612 K  |    2612 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5058 K  |    5058 K  |
|       from large pool |      36    |      77    |    2445 K  |    2445 K  |
|       from small pool |     186    |     224    |    2612 K  |    2612 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      33    |    2577 K  |    2577 K  |
|       from large pool |      10    |      11    |    1013 K  |    1013 K  |
|       from small pool |      16    |      23    |    1564 K  |    1564 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 79, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 80 [256/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 80 [512/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 80 [768/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 80 [1024/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 80 [1280/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 80 [1536/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 80 [1792/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 80 [2048/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 80 [2304/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 80 [2560/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 80 [2816/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 80 [3072/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 80 [3328/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 80 [3584/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [3840/50000]	Loss: 4.6171	LR: 0.200000
Training Epoch: 80 [4096/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 80 [4352/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 80 [4608/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 80 [4864/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 80 [5120/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 80 [5376/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 80 [5632/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 80 [5888/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 80 [6144/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 80 [6400/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 80 [6656/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 80 [6912/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 80 [7168/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 80 [7424/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 80 [7680/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 80 [7936/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 80 [8192/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 80 [8448/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 80 [8704/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 80 [8960/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 80 [9216/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 80 [9472/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 80 [9728/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 80 [9984/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 80 [10240/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 80 [10496/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 80 [10752/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [11008/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [11264/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 80 [11520/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 80 [11776/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 80 [12032/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [12288/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 80 [12544/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 80 [12800/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 80 [13056/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 80 [13312/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 80 [13568/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 80 [13824/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 80 [14080/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 80 [14336/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 80 [14592/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 80 [14848/50000]	Loss: 4.6155	LR: 0.200000
Training Epoch: 80 [15104/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 80 [15360/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 80 [15616/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 80 [15872/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 80 [16128/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 80 [16384/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 80 [16640/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 80 [16896/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 80 [17152/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 80 [17408/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 80 [17664/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 80 [17920/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 80 [18176/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 80 [18432/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 80 [18688/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 80 [18944/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 80 [19200/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 80 [19456/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 80 [19712/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 80 [19968/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 80 [20224/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 80 [20480/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [20736/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 80 [20992/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 80 [21248/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 80 [21504/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 80 [21760/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 80 [22016/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 80 [22272/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 80 [22528/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 80 [22784/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 80 [23040/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 80 [23296/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 80 [23552/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 80 [23808/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 80 [24064/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 80 [24320/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 80 [24576/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 80 [24832/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 80 [25088/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 80 [25344/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 80 [25600/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 80 [25856/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 80 [26112/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 80 [26368/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 80 [26624/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [26880/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 80 [27136/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 80 [27392/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 80 [27648/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 80 [27904/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 80 [28160/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 80 [28416/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 80 [28672/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 80 [28928/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 80 [29184/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 80 [29440/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 80 [29696/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 80 [29952/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 80 [30208/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [30464/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 80 [30720/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 80 [30976/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 80 [31232/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 80 [31488/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 80 [31744/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 80 [32000/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 80 [32256/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 80 [32512/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 80 [32768/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 80 [33024/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 80 [33280/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 80 [33536/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 80 [33792/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 80 [34048/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 80 [34304/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 80 [34560/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 80 [34816/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 80 [35072/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 80 [35328/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 80 [35584/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [35840/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 80 [36096/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 80 [36352/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 80 [36608/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 80 [36864/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 80 [37120/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 80 [37376/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 80 [37632/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 80 [37888/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 80 [38144/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [38400/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 80 [38656/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 80 [38912/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 80 [39168/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 80 [39424/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 80 [39680/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 80 [39936/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 80 [40192/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 80 [40448/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 80 [40704/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 80 [40960/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 80 [41216/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 80 [41472/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 80 [41728/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 80 [41984/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 80 [42240/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 80 [42496/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [42752/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 80 [43008/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 80 [43264/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 80 [43520/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 80 [43776/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 80 [44032/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 80 [44288/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 80 [44544/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 80 [44800/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [45056/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 80 [45312/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 80 [45568/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 80 [45824/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [46080/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 80 [46336/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 80 [46592/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 80 [46848/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [47104/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 80 [47360/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 80 [47616/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 80 [47872/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 80 [48128/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 80 [48384/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 80 [48640/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 80 [48896/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 80 [49152/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 80 [49408/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 80 [49664/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 80 [49920/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 80 [50000/50000]	Loss: 4.6061	LR: 0.200000
epoch 80 training time consumed: 21.88s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  120015 GB |  120015 GB |
|       from large pool |  400448 KB |    1770 MB |  119907 GB |  119907 GB |
|       from small pool |    3549 KB |       9 MB |     107 GB |     107 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  120015 GB |  120015 GB |
|       from large pool |  400448 KB |    1770 MB |  119907 GB |  119907 GB |
|       from small pool |    3549 KB |       9 MB |     107 GB |     107 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   73844 GB |   73844 GB |
|       from large pool |  244672 KB |  473024 KB |   73720 GB |   73720 GB |
|       from small pool |    2594 KB |    4843 KB |     123 GB |     123 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5122 K  |    5122 K  |
|       from large pool |      36    |      77    |    2476 K  |    2476 K  |
|       from small pool |     186    |     224    |    2645 K  |    2645 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5122 K  |    5122 K  |
|       from large pool |      36    |      77    |    2476 K  |    2476 K  |
|       from small pool |     186    |     224    |    2645 K  |    2645 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2610 K  |    2610 K  |
|       from large pool |      10    |      11    |    1025 K  |    1025 K  |
|       from small pool |      11    |      23    |    1584 K  |    1584 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 80, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.48s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-80-regular.pth
Training Epoch: 81 [256/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 81 [512/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 81 [768/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 81 [1024/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 81 [1280/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 81 [1536/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 81 [1792/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 81 [2048/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 81 [2304/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 81 [2560/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [2816/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 81 [3072/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 81 [3328/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [3584/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 81 [3840/50000]	Loss: 4.6001	LR: 0.200000
Training Epoch: 81 [4096/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 81 [4352/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 81 [4608/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [4864/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 81 [5120/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 81 [5376/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 81 [5632/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 81 [5888/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 81 [6144/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 81 [6400/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 81 [6656/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 81 [6912/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 81 [7168/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 81 [7424/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 81 [7680/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 81 [7936/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 81 [8192/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 81 [8448/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 81 [8704/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 81 [8960/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 81 [9216/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 81 [9472/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 81 [9728/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 81 [9984/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 81 [10240/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 81 [10496/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 81 [10752/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 81 [11008/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 81 [11264/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 81 [11520/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 81 [11776/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 81 [12032/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 81 [12288/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 81 [12544/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 81 [12800/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 81 [13056/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 81 [13312/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 81 [13568/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 81 [13824/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 81 [14080/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 81 [14336/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 81 [14592/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 81 [14848/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 81 [15104/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 81 [15360/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 81 [15616/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 81 [15872/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 81 [16128/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 81 [16384/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 81 [16640/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 81 [16896/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 81 [17152/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 81 [17408/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 81 [17664/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 81 [17920/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 81 [18176/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 81 [18432/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 81 [18688/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 81 [18944/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 81 [19200/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 81 [19456/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 81 [19712/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 81 [19968/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 81 [20224/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 81 [20480/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 81 [20736/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 81 [20992/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 81 [21248/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 81 [21504/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 81 [21760/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 81 [22016/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 81 [22272/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 81 [22528/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 81 [22784/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 81 [23040/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 81 [23296/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 81 [23552/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 81 [23808/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 81 [24064/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 81 [24320/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [24576/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 81 [24832/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 81 [25088/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 81 [25344/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 81 [25600/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 81 [25856/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 81 [26112/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 81 [26368/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 81 [26624/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 81 [26880/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 81 [27136/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 81 [27392/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 81 [27648/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 81 [27904/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 81 [28160/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 81 [28416/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 81 [28672/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [28928/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 81 [29184/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 81 [29440/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 81 [29696/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 81 [29952/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 81 [30208/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 81 [30464/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 81 [30720/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 81 [30976/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 81 [31232/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 81 [31488/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 81 [31744/50000]	Loss: 4.6153	LR: 0.200000
Training Epoch: 81 [32000/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 81 [32256/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 81 [32512/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 81 [32768/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 81 [33024/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 81 [33280/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 81 [33536/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 81 [33792/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 81 [34048/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 81 [34304/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 81 [34560/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 81 [34816/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 81 [35072/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 81 [35328/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 81 [35584/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 81 [35840/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 81 [36096/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 81 [36352/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 81 [36608/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 81 [36864/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 81 [37120/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 81 [37376/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 81 [37632/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 81 [37888/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 81 [38144/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 81 [38400/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 81 [38656/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 81 [38912/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 81 [39168/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 81 [39424/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 81 [39680/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 81 [39936/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 81 [40192/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 81 [40448/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 81 [40704/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [40960/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 81 [41216/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 81 [41472/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 81 [41728/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 81 [41984/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 81 [42240/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 81 [42496/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 81 [42752/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 81 [43008/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 81 [43264/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 81 [43520/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 81 [43776/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 81 [44032/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 81 [44288/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 81 [44544/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [44800/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 81 [45056/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 81 [45312/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 81 [45568/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 81 [45824/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 81 [46080/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 81 [46336/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 81 [46592/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 81 [46848/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 81 [47104/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 81 [47360/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 81 [47616/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 81 [47872/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 81 [48128/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 81 [48384/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 81 [48640/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 81 [48896/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 81 [49152/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 81 [49408/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 81 [49664/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 81 [49920/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 81 [50000/50000]	Loss: 4.6111	LR: 0.200000
epoch 81 training time consumed: 21.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  121515 GB |  121515 GB |
|       from large pool |  400448 KB |    1770 MB |  121406 GB |  121405 GB |
|       from small pool |    3549 KB |       9 MB |     109 GB |     109 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  121515 GB |  121515 GB |
|       from large pool |  400448 KB |    1770 MB |  121406 GB |  121405 GB |
|       from small pool |    3549 KB |       9 MB |     109 GB |     109 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   74767 GB |   74767 GB |
|       from large pool |  244672 KB |  473024 KB |   74642 GB |   74642 GB |
|       from small pool |    4642 KB |    4843 KB |     125 GB |     125 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5186 K  |    5186 K  |
|       from large pool |      36    |      77    |    2507 K  |    2507 K  |
|       from small pool |     186    |     224    |    2678 K  |    2678 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5186 K  |    5186 K  |
|       from large pool |      36    |      77    |    2507 K  |    2507 K  |
|       from small pool |     186    |     224    |    2678 K  |    2678 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      33    |    2643 K  |    2643 K  |
|       from large pool |      10    |      11    |    1038 K  |    1038 K  |
|       from small pool |      14    |      23    |    1604 K  |    1604 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 81, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.48s

Training Epoch: 82 [256/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 82 [512/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 82 [768/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 82 [1024/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 82 [1280/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 82 [1536/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 82 [1792/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 82 [2048/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 82 [2304/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 82 [2560/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 82 [2816/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 82 [3072/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 82 [3328/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 82 [3584/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [3840/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 82 [4096/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 82 [4352/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 82 [4608/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [4864/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 82 [5120/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 82 [5376/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 82 [5632/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 82 [5888/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 82 [6144/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 82 [6400/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 82 [6656/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 82 [6912/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 82 [7168/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 82 [7424/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 82 [7680/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 82 [7936/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 82 [8192/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 82 [8448/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 82 [8704/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 82 [8960/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 82 [9216/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 82 [9472/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 82 [9728/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 82 [9984/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 82 [10240/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 82 [10496/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 82 [10752/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 82 [11008/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 82 [11264/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 82 [11520/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 82 [11776/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 82 [12032/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 82 [12288/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 82 [12544/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 82 [12800/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 82 [13056/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 82 [13312/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 82 [13568/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 82 [13824/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 82 [14080/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 82 [14336/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 82 [14592/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 82 [14848/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 82 [15104/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 82 [15360/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 82 [15616/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 82 [15872/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 82 [16128/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 82 [16384/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 82 [16640/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 82 [16896/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 82 [17152/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 82 [17408/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 82 [17664/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [17920/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [18176/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 82 [18432/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 82 [18688/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 82 [18944/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 82 [19200/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 82 [19456/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 82 [19712/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 82 [19968/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 82 [20224/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 82 [20480/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 82 [20736/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 82 [20992/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 82 [21248/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 82 [21504/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 82 [21760/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 82 [22016/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 82 [22272/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 82 [22528/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 82 [22784/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 82 [23040/50000]	Loss: 4.5983	LR: 0.200000
Training Epoch: 82 [23296/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 82 [23552/50000]	Loss: 4.6177	LR: 0.200000
Training Epoch: 82 [23808/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 82 [24064/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 82 [24320/50000]	Loss: 4.6182	LR: 0.200000
Training Epoch: 82 [24576/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 82 [24832/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 82 [25088/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 82 [25344/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [25600/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 82 [25856/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 82 [26112/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 82 [26368/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 82 [26624/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 82 [26880/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 82 [27136/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 82 [27392/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [27648/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 82 [27904/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 82 [28160/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 82 [28416/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 82 [28672/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 82 [28928/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 82 [29184/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 82 [29440/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 82 [29696/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 82 [29952/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 82 [30208/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 82 [30464/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 82 [30720/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 82 [30976/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 82 [31232/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 82 [31488/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 82 [31744/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 82 [32000/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 82 [32256/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 82 [32512/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 82 [32768/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 82 [33024/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 82 [33280/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 82 [33536/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 82 [33792/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 82 [34048/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 82 [34304/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 82 [34560/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 82 [34816/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 82 [35072/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 82 [35328/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 82 [35584/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 82 [35840/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 82 [36096/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 82 [36352/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 82 [36608/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [36864/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 82 [37120/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 82 [37376/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 82 [37632/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 82 [37888/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 82 [38144/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 82 [38400/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 82 [38656/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 82 [38912/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 82 [39168/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 82 [39424/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 82 [39680/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 82 [39936/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 82 [40192/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 82 [40448/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 82 [40704/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [40960/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 82 [41216/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 82 [41472/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 82 [41728/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 82 [41984/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 82 [42240/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 82 [42496/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 82 [42752/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 82 [43008/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 82 [43264/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 82 [43520/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 82 [43776/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 82 [44032/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 82 [44288/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 82 [44544/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 82 [44800/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 82 [45056/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 82 [45312/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 82 [45568/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 82 [45824/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 82 [46080/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 82 [46336/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 82 [46592/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 82 [46848/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 82 [47104/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 82 [47360/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 82 [47616/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 82 [47872/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 82 [48128/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 82 [48384/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 82 [48640/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 82 [48896/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 82 [49152/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 82 [49408/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 82 [49664/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 82 [49920/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 82 [50000/50000]	Loss: 4.6014	LR: 0.200000
epoch 82 training time consumed: 21.73s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  123015 GB |  123015 GB |
|       from large pool |  400448 KB |    1770 MB |  122905 GB |  122904 GB |
|       from small pool |    3549 KB |       9 MB |     110 GB |     110 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  123015 GB |  123015 GB |
|       from large pool |  400448 KB |    1770 MB |  122905 GB |  122904 GB |
|       from small pool |    3549 KB |       9 MB |     110 GB |     110 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   75690 GB |   75690 GB |
|       from large pool |  244672 KB |  473024 KB |   75563 GB |   75563 GB |
|       from small pool |    2594 KB |    4843 KB |     126 GB |     126 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5250 K  |    5250 K  |
|       from large pool |      36    |      77    |    2538 K  |    2538 K  |
|       from small pool |     186    |     224    |    2711 K  |    2711 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5250 K  |    5250 K  |
|       from large pool |      36    |      77    |    2538 K  |    2538 K  |
|       from small pool |     186    |     224    |    2711 K  |    2711 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2675 K  |    2675 K  |
|       from large pool |      10    |      11    |    1051 K  |    1051 K  |
|       from small pool |      10    |      23    |    1624 K  |    1624 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 82, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 83 [256/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 83 [512/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 83 [768/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 83 [1024/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 83 [1280/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 83 [1536/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 83 [1792/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 83 [2048/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 83 [2304/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 83 [2560/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 83 [2816/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 83 [3072/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 83 [3328/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 83 [3584/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 83 [3840/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 83 [4096/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 83 [4352/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 83 [4608/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 83 [4864/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 83 [5120/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 83 [5376/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 83 [5632/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 83 [5888/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 83 [6144/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 83 [6400/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 83 [6656/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 83 [6912/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 83 [7168/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 83 [7424/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 83 [7680/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 83 [7936/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 83 [8192/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 83 [8448/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 83 [8704/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 83 [8960/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 83 [9216/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 83 [9472/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 83 [9728/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 83 [9984/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 83 [10240/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 83 [10496/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 83 [10752/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 83 [11008/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 83 [11264/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 83 [11520/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 83 [11776/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 83 [12032/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 83 [12288/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 83 [12544/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 83 [12800/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 83 [13056/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 83 [13312/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 83 [13568/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 83 [13824/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 83 [14080/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 83 [14336/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 83 [14592/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 83 [14848/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 83 [15104/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 83 [15360/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 83 [15616/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 83 [15872/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 83 [16128/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 83 [16384/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 83 [16640/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 83 [16896/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 83 [17152/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 83 [17408/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 83 [17664/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 83 [17920/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 83 [18176/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 83 [18432/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 83 [18688/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 83 [18944/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 83 [19200/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 83 [19456/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 83 [19712/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 83 [19968/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 83 [20224/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 83 [20480/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 83 [20736/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 83 [20992/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 83 [21248/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 83 [21504/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 83 [21760/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 83 [22016/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 83 [22272/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 83 [22528/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 83 [22784/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 83 [23040/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 83 [23296/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 83 [23552/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 83 [23808/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 83 [24064/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 83 [24320/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 83 [24576/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 83 [24832/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 83 [25088/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 83 [25344/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 83 [25600/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 83 [25856/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 83 [26112/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 83 [26368/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 83 [26624/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 83 [26880/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 83 [27136/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 83 [27392/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 83 [27648/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 83 [27904/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 83 [28160/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 83 [28416/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 83 [28672/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 83 [28928/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 83 [29184/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 83 [29440/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 83 [29696/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 83 [29952/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 83 [30208/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 83 [30464/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 83 [30720/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 83 [30976/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 83 [31232/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 83 [31488/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 83 [31744/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 83 [32000/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 83 [32256/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 83 [32512/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 83 [32768/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 83 [33024/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 83 [33280/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 83 [33536/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 83 [33792/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 83 [34048/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 83 [34304/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 83 [34560/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 83 [34816/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 83 [35072/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 83 [35328/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 83 [35584/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 83 [35840/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 83 [36096/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 83 [36352/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 83 [36608/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 83 [36864/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 83 [37120/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 83 [37376/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 83 [37632/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 83 [37888/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 83 [38144/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 83 [38400/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 83 [38656/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 83 [38912/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 83 [39168/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 83 [39424/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 83 [39680/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 83 [39936/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 83 [40192/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 83 [40448/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 83 [40704/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 83 [40960/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 83 [41216/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 83 [41472/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 83 [41728/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 83 [41984/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 83 [42240/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 83 [42496/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 83 [42752/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 83 [43008/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 83 [43264/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 83 [43520/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 83 [43776/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 83 [44032/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 83 [44288/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 83 [44544/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 83 [44800/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 83 [45056/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 83 [45312/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 83 [45568/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 83 [45824/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 83 [46080/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 83 [46336/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 83 [46592/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 83 [46848/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 83 [47104/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 83 [47360/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 83 [47616/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 83 [47872/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 83 [48128/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 83 [48384/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 83 [48640/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 83 [48896/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 83 [49152/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 83 [49408/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 83 [49664/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 83 [49920/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 83 [50000/50000]	Loss: 4.6115	LR: 0.200000
epoch 83 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  124516 GB |  124515 GB |
|       from large pool |  400448 KB |    1770 MB |  124404 GB |  124403 GB |
|       from small pool |    3549 KB |       9 MB |     112 GB |     112 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  124516 GB |  124515 GB |
|       from large pool |  400448 KB |    1770 MB |  124404 GB |  124403 GB |
|       from small pool |    3549 KB |       9 MB |     112 GB |     112 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   76613 GB |   76613 GB |
|       from large pool |  244672 KB |  473024 KB |   76485 GB |   76485 GB |
|       from small pool |    2594 KB |    4843 KB |     128 GB |     128 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5314 K  |    5314 K  |
|       from large pool |      36    |      77    |    2569 K  |    2569 K  |
|       from small pool |     186    |     224    |    2744 K  |    2744 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5314 K  |    5314 K  |
|       from large pool |      36    |      77    |    2569 K  |    2569 K  |
|       from small pool |     186    |     224    |    2744 K  |    2744 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    2707 K  |    2707 K  |
|       from large pool |      10    |      11    |    1064 K  |    1064 K  |
|       from small pool |       9    |      23    |    1643 K  |    1643 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 83, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.41s

Training Epoch: 84 [256/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 84 [512/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 84 [768/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 84 [1024/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 84 [1280/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 84 [1536/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 84 [1792/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 84 [2048/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 84 [2304/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 84 [2560/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 84 [2816/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 84 [3072/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 84 [3328/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 84 [3584/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 84 [3840/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 84 [4096/50000]	Loss: 4.6007	LR: 0.200000
Training Epoch: 84 [4352/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 84 [4608/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 84 [4864/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 84 [5120/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 84 [5376/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 84 [5632/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 84 [5888/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 84 [6144/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 84 [6400/50000]	Loss: 4.5990	LR: 0.200000
Training Epoch: 84 [6656/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 84 [6912/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 84 [7168/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 84 [7424/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 84 [7680/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 84 [7936/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 84 [8192/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 84 [8448/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 84 [8704/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 84 [8960/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 84 [9216/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 84 [9472/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 84 [9728/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 84 [9984/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 84 [10240/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 84 [10496/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 84 [10752/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 84 [11008/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 84 [11264/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 84 [11520/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 84 [11776/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 84 [12032/50000]	Loss: 4.6168	LR: 0.200000
Training Epoch: 84 [12288/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 84 [12544/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 84 [12800/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 84 [13056/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 84 [13312/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 84 [13568/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 84 [13824/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 84 [14080/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 84 [14336/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 84 [14592/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 84 [14848/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 84 [15104/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 84 [15360/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 84 [15616/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 84 [15872/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 84 [16128/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 84 [16384/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 84 [16640/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 84 [16896/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 84 [17152/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 84 [17408/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 84 [17664/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 84 [17920/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 84 [18176/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 84 [18432/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 84 [18688/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 84 [18944/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 84 [19200/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 84 [19456/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 84 [19712/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 84 [19968/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 84 [20224/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 84 [20480/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 84 [20736/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 84 [20992/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 84 [21248/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 84 [21504/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 84 [21760/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 84 [22016/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 84 [22272/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 84 [22528/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 84 [22784/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 84 [23040/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 84 [23296/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 84 [23552/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 84 [23808/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 84 [24064/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 84 [24320/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 84 [24576/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 84 [24832/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 84 [25088/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 84 [25344/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 84 [25600/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 84 [25856/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 84 [26112/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 84 [26368/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 84 [26624/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 84 [26880/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 84 [27136/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 84 [27392/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 84 [27648/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 84 [27904/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 84 [28160/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 84 [28416/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 84 [28672/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 84 [28928/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 84 [29184/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 84 [29440/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 84 [29696/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 84 [29952/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 84 [30208/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 84 [30464/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 84 [30720/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 84 [30976/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 84 [31232/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 84 [31488/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 84 [31744/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 84 [32000/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 84 [32256/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 84 [32512/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 84 [32768/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 84 [33024/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 84 [33280/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 84 [33536/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 84 [33792/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 84 [34048/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 84 [34304/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 84 [34560/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 84 [34816/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 84 [35072/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 84 [35328/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 84 [35584/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 84 [35840/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 84 [36096/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 84 [36352/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 84 [36608/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 84 [36864/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 84 [37120/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 84 [37376/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 84 [37632/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 84 [37888/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 84 [38144/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 84 [38400/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 84 [38656/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 84 [38912/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 84 [39168/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 84 [39424/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 84 [39680/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 84 [39936/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 84 [40192/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 84 [40448/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 84 [40704/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 84 [40960/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 84 [41216/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 84 [41472/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 84 [41728/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 84 [41984/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 84 [42240/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 84 [42496/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 84 [42752/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 84 [43008/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 84 [43264/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 84 [43520/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 84 [43776/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 84 [44032/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 84 [44288/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 84 [44544/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 84 [44800/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 84 [45056/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 84 [45312/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 84 [45568/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 84 [45824/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 84 [46080/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 84 [46336/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 84 [46592/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 84 [46848/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 84 [47104/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 84 [47360/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 84 [47616/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 84 [47872/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 84 [48128/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 84 [48384/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 84 [48640/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 84 [48896/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 84 [49152/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 84 [49408/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 84 [49664/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 84 [49920/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 84 [50000/50000]	Loss: 4.6058	LR: 0.200000
epoch 84 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  126016 GB |  126015 GB |
|       from large pool |  400448 KB |    1770 MB |  125902 GB |  125902 GB |
|       from small pool |    3549 KB |       9 MB |     113 GB |     113 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  126016 GB |  126015 GB |
|       from large pool |  400448 KB |    1770 MB |  125902 GB |  125902 GB |
|       from small pool |    3549 KB |       9 MB |     113 GB |     113 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   77536 GB |   77536 GB |
|       from large pool |  244672 KB |  473024 KB |   77406 GB |   77406 GB |
|       from small pool |    2594 KB |    4843 KB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5378 K  |    5378 K  |
|       from large pool |      36    |      77    |    2600 K  |    2600 K  |
|       from small pool |     186    |     224    |    2777 K  |    2777 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5378 K  |    5378 K  |
|       from large pool |      36    |      77    |    2600 K  |    2600 K  |
|       from small pool |     186    |     224    |    2777 K  |    2777 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    2740 K  |    2740 K  |
|       from large pool |      10    |      11    |    1077 K  |    1077 K  |
|       from small pool |      13    |      23    |    1663 K  |    1663 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 84, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 85 [256/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 85 [512/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 85 [768/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 85 [1024/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 85 [1280/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 85 [1536/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 85 [1792/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 85 [2048/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 85 [2304/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 85 [2560/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 85 [2816/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 85 [3072/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 85 [3328/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 85 [3584/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 85 [3840/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 85 [4096/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 85 [4352/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 85 [4608/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 85 [4864/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 85 [5120/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 85 [5376/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 85 [5632/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 85 [5888/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 85 [6144/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 85 [6400/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 85 [6656/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 85 [6912/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 85 [7168/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 85 [7424/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 85 [7680/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 85 [7936/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 85 [8192/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 85 [8448/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 85 [8704/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 85 [8960/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 85 [9216/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 85 [9472/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 85 [9728/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 85 [9984/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 85 [10240/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 85 [10496/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 85 [10752/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 85 [11008/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 85 [11264/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 85 [11520/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 85 [11776/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 85 [12032/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 85 [12288/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 85 [12544/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 85 [12800/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 85 [13056/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 85 [13312/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 85 [13568/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 85 [13824/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 85 [14080/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 85 [14336/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 85 [14592/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 85 [14848/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 85 [15104/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 85 [15360/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 85 [15616/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 85 [15872/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 85 [16128/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 85 [16384/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 85 [16640/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 85 [16896/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 85 [17152/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 85 [17408/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 85 [17664/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 85 [17920/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 85 [18176/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 85 [18432/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 85 [18688/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 85 [18944/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 85 [19200/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 85 [19456/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 85 [19712/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 85 [19968/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 85 [20224/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 85 [20480/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 85 [20736/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 85 [20992/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 85 [21248/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 85 [21504/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 85 [21760/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 85 [22016/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 85 [22272/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 85 [22528/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 85 [22784/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 85 [23040/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 85 [23296/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 85 [23552/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 85 [23808/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 85 [24064/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 85 [24320/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 85 [24576/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 85 [24832/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 85 [25088/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 85 [25344/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 85 [25600/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 85 [25856/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 85 [26112/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 85 [26368/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 85 [26624/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 85 [26880/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 85 [27136/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 85 [27392/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 85 [27648/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 85 [27904/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 85 [28160/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 85 [28416/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 85 [28672/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 85 [28928/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 85 [29184/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 85 [29440/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 85 [29696/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 85 [29952/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 85 [30208/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 85 [30464/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 85 [30720/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 85 [30976/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 85 [31232/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 85 [31488/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 85 [31744/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 85 [32000/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 85 [32256/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 85 [32512/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 85 [32768/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 85 [33024/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 85 [33280/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 85 [33536/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 85 [33792/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 85 [34048/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 85 [34304/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 85 [34560/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 85 [34816/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 85 [35072/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 85 [35328/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 85 [35584/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 85 [35840/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 85 [36096/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 85 [36352/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 85 [36608/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 85 [36864/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 85 [37120/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 85 [37376/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 85 [37632/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 85 [37888/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 85 [38144/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 85 [38400/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 85 [38656/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 85 [38912/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 85 [39168/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 85 [39424/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 85 [39680/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 85 [39936/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 85 [40192/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 85 [40448/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 85 [40704/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 85 [40960/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 85 [41216/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 85 [41472/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 85 [41728/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 85 [41984/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 85 [42240/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 85 [42496/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 85 [42752/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 85 [43008/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 85 [43264/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 85 [43520/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 85 [43776/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 85 [44032/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 85 [44288/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 85 [44544/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 85 [44800/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 85 [45056/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 85 [45312/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 85 [45568/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 85 [45824/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 85 [46080/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 85 [46336/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 85 [46592/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 85 [46848/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 85 [47104/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 85 [47360/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 85 [47616/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 85 [47872/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 85 [48128/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 85 [48384/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 85 [48640/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 85 [48896/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 85 [49152/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 85 [49408/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 85 [49664/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 85 [49920/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 85 [50000/50000]	Loss: 4.6178	LR: 0.200000
epoch 85 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  127516 GB |  127516 GB |
|       from large pool |  400448 KB |    1770 MB |  127401 GB |  127401 GB |
|       from small pool |    3549 KB |       9 MB |     114 GB |     114 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  127516 GB |  127516 GB |
|       from large pool |  400448 KB |    1770 MB |  127401 GB |  127401 GB |
|       from small pool |    3549 KB |       9 MB |     114 GB |     114 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   78459 GB |   78459 GB |
|       from large pool |  244672 KB |  473024 KB |   78328 GB |   78328 GB |
|       from small pool |    2594 KB |    4843 KB |     131 GB |     131 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5442 K  |    5442 K  |
|       from large pool |      36    |      77    |    2631 K  |    2631 K  |
|       from small pool |     186    |     224    |    2810 K  |    2810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5442 K  |    5442 K  |
|       from large pool |      36    |      77    |    2631 K  |    2631 K  |
|       from small pool |     186    |     224    |    2810 K  |    2810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2772 K  |    2772 K  |
|       from large pool |      10    |      11    |    1089 K  |    1089 K  |
|       from small pool |      11    |      23    |    1682 K  |    1682 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 85, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 86 [256/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 86 [512/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 86 [768/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 86 [1024/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 86 [1280/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 86 [1536/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 86 [1792/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 86 [2048/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 86 [2304/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 86 [2560/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 86 [2816/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 86 [3072/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 86 [3328/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 86 [3584/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 86 [3840/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 86 [4096/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 86 [4352/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 86 [4608/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 86 [4864/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 86 [5120/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 86 [5376/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 86 [5632/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 86 [5888/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 86 [6144/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 86 [6400/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 86 [6656/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 86 [6912/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 86 [7168/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 86 [7424/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 86 [7680/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 86 [7936/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 86 [8192/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 86 [8448/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 86 [8704/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 86 [8960/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 86 [9216/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 86 [9472/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 86 [9728/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 86 [9984/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 86 [10240/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 86 [10496/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 86 [10752/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 86 [11008/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 86 [11264/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 86 [11520/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 86 [11776/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 86 [12032/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 86 [12288/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 86 [12544/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 86 [12800/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 86 [13056/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 86 [13312/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 86 [13568/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 86 [13824/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 86 [14080/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 86 [14336/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 86 [14592/50000]	Loss: 4.6002	LR: 0.200000
Training Epoch: 86 [14848/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 86 [15104/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 86 [15360/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 86 [15616/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 86 [15872/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 86 [16128/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 86 [16384/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 86 [16640/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 86 [16896/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 86 [17152/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 86 [17408/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 86 [17664/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 86 [17920/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 86 [18176/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 86 [18432/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 86 [18688/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 86 [18944/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 86 [19200/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 86 [19456/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 86 [19712/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 86 [19968/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 86 [20224/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 86 [20480/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 86 [20736/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 86 [20992/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 86 [21248/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 86 [21504/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 86 [21760/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 86 [22016/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 86 [22272/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 86 [22528/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 86 [22784/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 86 [23040/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 86 [23296/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 86 [23552/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 86 [23808/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 86 [24064/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 86 [24320/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 86 [24576/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 86 [24832/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 86 [25088/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 86 [25344/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 86 [25600/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 86 [25856/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 86 [26112/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 86 [26368/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 86 [26624/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 86 [26880/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 86 [27136/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 86 [27392/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 86 [27648/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 86 [27904/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 86 [28160/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 86 [28416/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 86 [28672/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 86 [28928/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 86 [29184/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 86 [29440/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 86 [29696/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 86 [29952/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 86 [30208/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 86 [30464/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 86 [30720/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 86 [30976/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 86 [31232/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 86 [31488/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 86 [31744/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 86 [32000/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 86 [32256/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 86 [32512/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 86 [32768/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 86 [33024/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 86 [33280/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 86 [33536/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 86 [33792/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 86 [34048/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 86 [34304/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 86 [34560/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 86 [34816/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 86 [35072/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 86 [35328/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 86 [35584/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 86 [35840/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 86 [36096/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 86 [36352/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 86 [36608/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 86 [36864/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 86 [37120/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 86 [37376/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 86 [37632/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 86 [37888/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 86 [38144/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 86 [38400/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 86 [38656/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 86 [38912/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 86 [39168/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 86 [39424/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 86 [39680/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 86 [39936/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 86 [40192/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 86 [40448/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 86 [40704/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 86 [40960/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 86 [41216/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 86 [41472/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 86 [41728/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 86 [41984/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 86 [42240/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 86 [42496/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 86 [42752/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 86 [43008/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 86 [43264/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 86 [43520/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 86 [43776/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 86 [44032/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 86 [44288/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 86 [44544/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 86 [44800/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 86 [45056/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 86 [45312/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 86 [45568/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 86 [45824/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 86 [46080/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 86 [46336/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 86 [46592/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 86 [46848/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 86 [47104/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 86 [47360/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 86 [47616/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 86 [47872/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 86 [48128/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 86 [48384/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 86 [48640/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 86 [48896/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 86 [49152/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 86 [49408/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 86 [49664/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 86 [49920/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 86 [50000/50000]	Loss: 4.6097	LR: 0.200000
epoch 86 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  129016 GB |  129016 GB |
|       from large pool |  400448 KB |    1770 MB |  128900 GB |  128900 GB |
|       from small pool |    3549 KB |       9 MB |     116 GB |     116 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  129016 GB |  129016 GB |
|       from large pool |  400448 KB |    1770 MB |  128900 GB |  128900 GB |
|       from small pool |    3549 KB |       9 MB |     116 GB |     116 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   79382 GB |   79382 GB |
|       from large pool |  244672 KB |  473024 KB |   79249 GB |   79249 GB |
|       from small pool |    2594 KB |    4843 KB |     132 GB |     132 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5506 K  |    5506 K  |
|       from large pool |      36    |      77    |    2662 K  |    2662 K  |
|       from small pool |     186    |     224    |    2843 K  |    2843 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5506 K  |    5506 K  |
|       from large pool |      36    |      77    |    2662 K  |    2662 K  |
|       from small pool |     186    |     224    |    2843 K  |    2843 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    2806 K  |    2806 K  |
|       from large pool |      10    |      11    |    1102 K  |    1102 K  |
|       from small pool |      13    |      23    |    1703 K  |    1703 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 86, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 87 [256/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 87 [512/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 87 [768/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 87 [1024/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 87 [1280/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 87 [1536/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 87 [1792/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 87 [2048/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 87 [2304/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 87 [2560/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 87 [2816/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [3072/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 87 [3328/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 87 [3584/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 87 [3840/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 87 [4096/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 87 [4352/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 87 [4608/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 87 [4864/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 87 [5120/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 87 [5376/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 87 [5632/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [5888/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 87 [6144/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 87 [6400/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 87 [6656/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 87 [6912/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 87 [7168/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 87 [7424/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 87 [7680/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 87 [7936/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 87 [8192/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 87 [8448/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 87 [8704/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [8960/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 87 [9216/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 87 [9472/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 87 [9728/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 87 [9984/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 87 [10240/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 87 [10496/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 87 [10752/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 87 [11008/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 87 [11264/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 87 [11520/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 87 [11776/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 87 [12032/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 87 [12288/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 87 [12544/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [12800/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 87 [13056/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 87 [13312/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 87 [13568/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 87 [13824/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 87 [14080/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 87 [14336/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 87 [14592/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 87 [14848/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 87 [15104/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 87 [15360/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 87 [15616/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 87 [15872/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 87 [16128/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 87 [16384/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 87 [16640/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 87 [16896/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 87 [17152/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 87 [17408/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 87 [17664/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 87 [17920/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 87 [18176/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 87 [18432/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 87 [18688/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 87 [18944/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 87 [19200/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 87 [19456/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 87 [19712/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 87 [19968/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 87 [20224/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 87 [20480/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 87 [20736/50000]	Loss: 4.5983	LR: 0.200000
Training Epoch: 87 [20992/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 87 [21248/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 87 [21504/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 87 [21760/50000]	Loss: 4.6156	LR: 0.200000
Training Epoch: 87 [22016/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 87 [22272/50000]	Loss: 4.6160	LR: 0.200000
Training Epoch: 87 [22528/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 87 [22784/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 87 [23040/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 87 [23296/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 87 [23552/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 87 [23808/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 87 [24064/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 87 [24320/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 87 [24576/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 87 [24832/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 87 [25088/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 87 [25344/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 87 [25600/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 87 [25856/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 87 [26112/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 87 [26368/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 87 [26624/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 87 [26880/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 87 [27136/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 87 [27392/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 87 [27648/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 87 [27904/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 87 [28160/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 87 [28416/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 87 [28672/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 87 [28928/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 87 [29184/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 87 [29440/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 87 [29696/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 87 [29952/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 87 [30208/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 87 [30464/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 87 [30720/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 87 [30976/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 87 [31232/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 87 [31488/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 87 [31744/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 87 [32000/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 87 [32256/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 87 [32512/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 87 [32768/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 87 [33024/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 87 [33280/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 87 [33536/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [33792/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 87 [34048/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 87 [34304/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 87 [34560/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 87 [34816/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 87 [35072/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 87 [35328/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 87 [35584/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 87 [35840/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 87 [36096/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 87 [36352/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 87 [36608/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 87 [36864/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 87 [37120/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 87 [37376/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 87 [37632/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 87 [37888/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 87 [38144/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [38400/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 87 [38656/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 87 [38912/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 87 [39168/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 87 [39424/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 87 [39680/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 87 [39936/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 87 [40192/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 87 [40448/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 87 [40704/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 87 [40960/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 87 [41216/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 87 [41472/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 87 [41728/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 87 [41984/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 87 [42240/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 87 [42496/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 87 [42752/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 87 [43008/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 87 [43264/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 87 [43520/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 87 [43776/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 87 [44032/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 87 [44288/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 87 [44544/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 87 [44800/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 87 [45056/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 87 [45312/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 87 [45568/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 87 [45824/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 87 [46080/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 87 [46336/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 87 [46592/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 87 [46848/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 87 [47104/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 87 [47360/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 87 [47616/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 87 [47872/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 87 [48128/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 87 [48384/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 87 [48640/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 87 [48896/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 87 [49152/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 87 [49408/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 87 [49664/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 87 [49920/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 87 [50000/50000]	Loss: 4.6103	LR: 0.200000
epoch 87 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  130516 GB |  130516 GB |
|       from large pool |  400448 KB |    1770 MB |  130399 GB |  130398 GB |
|       from small pool |    3549 KB |       9 MB |     117 GB |     117 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  130516 GB |  130516 GB |
|       from large pool |  400448 KB |    1770 MB |  130399 GB |  130398 GB |
|       from small pool |    3549 KB |       9 MB |     117 GB |     117 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   80305 GB |   80305 GB |
|       from large pool |  244672 KB |  473024 KB |   80171 GB |   80171 GB |
|       from small pool |    2594 KB |    4843 KB |     134 GB |     134 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5570 K  |    5570 K  |
|       from large pool |      36    |      77    |    2693 K  |    2693 K  |
|       from small pool |     186    |     224    |    2877 K  |    2876 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5570 K  |    5570 K  |
|       from large pool |      36    |      77    |    2693 K  |    2693 K  |
|       from small pool |     186    |     224    |    2877 K  |    2876 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    2838 K  |    2838 K  |
|       from large pool |      10    |      11    |    1115 K  |    1115 K  |
|       from small pool |      11    |      23    |    1723 K  |    1723 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 87, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 88 [256/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 88 [512/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 88 [768/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 88 [1024/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 88 [1280/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 88 [1536/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 88 [1792/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 88 [2048/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 88 [2304/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 88 [2560/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 88 [2816/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 88 [3072/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 88 [3328/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 88 [3584/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 88 [3840/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 88 [4096/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 88 [4352/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 88 [4608/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 88 [4864/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 88 [5120/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 88 [5376/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 88 [5632/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 88 [5888/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 88 [6144/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 88 [6400/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 88 [6656/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 88 [6912/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 88 [7168/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 88 [7424/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 88 [7680/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 88 [7936/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 88 [8192/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 88 [8448/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 88 [8704/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 88 [8960/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 88 [9216/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 88 [9472/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 88 [9728/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 88 [9984/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 88 [10240/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 88 [10496/50000]	Loss: 4.6166	LR: 0.200000
Training Epoch: 88 [10752/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 88 [11008/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 88 [11264/50000]	Loss: 4.6177	LR: 0.200000
Training Epoch: 88 [11520/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 88 [11776/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 88 [12032/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 88 [12288/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 88 [12544/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 88 [12800/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 88 [13056/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 88 [13312/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 88 [13568/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 88 [13824/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 88 [14080/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 88 [14336/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 88 [14592/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 88 [14848/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 88 [15104/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 88 [15360/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 88 [15616/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 88 [15872/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 88 [16128/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 88 [16384/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 88 [16640/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 88 [16896/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 88 [17152/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 88 [17408/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 88 [17664/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 88 [17920/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 88 [18176/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 88 [18432/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 88 [18688/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 88 [18944/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 88 [19200/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 88 [19456/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 88 [19712/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 88 [19968/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 88 [20224/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 88 [20480/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 88 [20736/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 88 [20992/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 88 [21248/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 88 [21504/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 88 [21760/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 88 [22016/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 88 [22272/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 88 [22528/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 88 [22784/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 88 [23040/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 88 [23296/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 88 [23552/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 88 [23808/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 88 [24064/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 88 [24320/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 88 [24576/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 88 [24832/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 88 [25088/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 88 [25344/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 88 [25600/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 88 [25856/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 88 [26112/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 88 [26368/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 88 [26624/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 88 [26880/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 88 [27136/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 88 [27392/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 88 [27648/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 88 [27904/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 88 [28160/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 88 [28416/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 88 [28672/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 88 [28928/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 88 [29184/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 88 [29440/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 88 [29696/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 88 [29952/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 88 [30208/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 88 [30464/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 88 [30720/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 88 [30976/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 88 [31232/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 88 [31488/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 88 [31744/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 88 [32000/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 88 [32256/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 88 [32512/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 88 [32768/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 88 [33024/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 88 [33280/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 88 [33536/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 88 [33792/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 88 [34048/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 88 [34304/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 88 [34560/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 88 [34816/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 88 [35072/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 88 [35328/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 88 [35584/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 88 [35840/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 88 [36096/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 88 [36352/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 88 [36608/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 88 [36864/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 88 [37120/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 88 [37376/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 88 [37632/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 88 [37888/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 88 [38144/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 88 [38400/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 88 [38656/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 88 [38912/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 88 [39168/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 88 [39424/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 88 [39680/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 88 [39936/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 88 [40192/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 88 [40448/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 88 [40704/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 88 [40960/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 88 [41216/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 88 [41472/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 88 [41728/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 88 [41984/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 88 [42240/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 88 [42496/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 88 [42752/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 88 [43008/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 88 [43264/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 88 [43520/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 88 [43776/50000]	Loss: 4.6156	LR: 0.200000
Training Epoch: 88 [44032/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 88 [44288/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 88 [44544/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 88 [44800/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 88 [45056/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 88 [45312/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 88 [45568/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 88 [45824/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 88 [46080/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 88 [46336/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 88 [46592/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 88 [46848/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 88 [47104/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 88 [47360/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 88 [47616/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 88 [47872/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 88 [48128/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 88 [48384/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 88 [48640/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 88 [48896/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 88 [49152/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 88 [49408/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 88 [49664/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 88 [49920/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 88 [50000/50000]	Loss: 4.6104	LR: 0.200000
epoch 88 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  132016 GB |  132016 GB |
|       from large pool |  400448 KB |    1770 MB |  131898 GB |  131897 GB |
|       from small pool |    3549 KB |       9 MB |     118 GB |     118 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  132016 GB |  132016 GB |
|       from large pool |  400448 KB |    1770 MB |  131898 GB |  131897 GB |
|       from small pool |    3549 KB |       9 MB |     118 GB |     118 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   81228 GB |   81228 GB |
|       from large pool |  244672 KB |  473024 KB |   81092 GB |   81092 GB |
|       from small pool |    2594 KB |    4843 KB |     135 GB |     135 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5634 K  |    5634 K  |
|       from large pool |      36    |      77    |    2724 K  |    2724 K  |
|       from small pool |     186    |     224    |    2910 K  |    2909 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5634 K  |    5634 K  |
|       from large pool |      36    |      77    |    2724 K  |    2724 K  |
|       from small pool |     186    |     224    |    2910 K  |    2909 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2871 K  |    2871 K  |
|       from large pool |      10    |      11    |    1128 K  |    1128 K  |
|       from small pool |      10    |      23    |    1742 K  |    1742 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 88, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 89 [256/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 89 [512/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 89 [768/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 89 [1024/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 89 [1280/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 89 [1536/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 89 [1792/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 89 [2048/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 89 [2304/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 89 [2560/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 89 [2816/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 89 [3072/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 89 [3328/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 89 [3584/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 89 [3840/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 89 [4096/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 89 [4352/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 89 [4608/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 89 [4864/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 89 [5120/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 89 [5376/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 89 [5632/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 89 [5888/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 89 [6144/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 89 [6400/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 89 [6656/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 89 [6912/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 89 [7168/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 89 [7424/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 89 [7680/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 89 [7936/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 89 [8192/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 89 [8448/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 89 [8704/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 89 [8960/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 89 [9216/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 89 [9472/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 89 [9728/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 89 [9984/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 89 [10240/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 89 [10496/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 89 [10752/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 89 [11008/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 89 [11264/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 89 [11520/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 89 [11776/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 89 [12032/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 89 [12288/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 89 [12544/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 89 [12800/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 89 [13056/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 89 [13312/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 89 [13568/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 89 [13824/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 89 [14080/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 89 [14336/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 89 [14592/50000]	Loss: 4.6155	LR: 0.200000
Training Epoch: 89 [14848/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 89 [15104/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 89 [15360/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 89 [15616/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 89 [15872/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 89 [16128/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 89 [16384/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 89 [16640/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 89 [16896/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 89 [17152/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 89 [17408/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 89 [17664/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 89 [17920/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 89 [18176/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 89 [18432/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 89 [18688/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 89 [18944/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 89 [19200/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 89 [19456/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 89 [19712/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 89 [19968/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 89 [20224/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 89 [20480/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 89 [20736/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 89 [20992/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 89 [21248/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 89 [21504/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 89 [21760/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 89 [22016/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 89 [22272/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 89 [22528/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 89 [22784/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 89 [23040/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 89 [23296/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 89 [23552/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 89 [23808/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 89 [24064/50000]	Loss: 4.5997	LR: 0.200000
Training Epoch: 89 [24320/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 89 [24576/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 89 [24832/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 89 [25088/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 89 [25344/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 89 [25600/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 89 [25856/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 89 [26112/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 89 [26368/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 89 [26624/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 89 [26880/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 89 [27136/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 89 [27392/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 89 [27648/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 89 [27904/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 89 [28160/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 89 [28416/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 89 [28672/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 89 [28928/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 89 [29184/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 89 [29440/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 89 [29696/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 89 [29952/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 89 [30208/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 89 [30464/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 89 [30720/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 89 [30976/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 89 [31232/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 89 [31488/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 89 [31744/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 89 [32000/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 89 [32256/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 89 [32512/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 89 [32768/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 89 [33024/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 89 [33280/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 89 [33536/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 89 [33792/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 89 [34048/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 89 [34304/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 89 [34560/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 89 [34816/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 89 [35072/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 89 [35328/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 89 [35584/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 89 [35840/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 89 [36096/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 89 [36352/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 89 [36608/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 89 [36864/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 89 [37120/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 89 [37376/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 89 [37632/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 89 [37888/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 89 [38144/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 89 [38400/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 89 [38656/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 89 [38912/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 89 [39168/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 89 [39424/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 89 [39680/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 89 [39936/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 89 [40192/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 89 [40448/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 89 [40704/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 89 [40960/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 89 [41216/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 89 [41472/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 89 [41728/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 89 [41984/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 89 [42240/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 89 [42496/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 89 [42752/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 89 [43008/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 89 [43264/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 89 [43520/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 89 [43776/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 89 [44032/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 89 [44288/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 89 [44544/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 89 [44800/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 89 [45056/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 89 [45312/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 89 [45568/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 89 [45824/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 89 [46080/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 89 [46336/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 89 [46592/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 89 [46848/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 89 [47104/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 89 [47360/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 89 [47616/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 89 [47872/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 89 [48128/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 89 [48384/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 89 [48640/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 89 [48896/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 89 [49152/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 89 [49408/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 89 [49664/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 89 [49920/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 89 [50000/50000]	Loss: 4.6062	LR: 0.200000
epoch 89 training time consumed: 21.75s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  133517 GB |  133516 GB |
|       from large pool |  400448 KB |    1770 MB |  133397 GB |  133396 GB |
|       from small pool |    3549 KB |       9 MB |     120 GB |     120 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  133517 GB |  133516 GB |
|       from large pool |  400448 KB |    1770 MB |  133397 GB |  133396 GB |
|       from small pool |    3549 KB |       9 MB |     120 GB |     120 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   82151 GB |   82151 GB |
|       from large pool |  244672 KB |  473024 KB |   82014 GB |   82014 GB |
|       from small pool |    2594 KB |    4843 KB |     137 GB |     137 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5698 K  |    5698 K  |
|       from large pool |      36    |      77    |    2755 K  |    2755 K  |
|       from small pool |     186    |     224    |    2943 K  |    2942 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5698 K  |    5698 K  |
|       from large pool |      36    |      77    |    2755 K  |    2755 K  |
|       from small pool |     186    |     224    |    2943 K  |    2942 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2903 K  |    2903 K  |
|       from large pool |      10    |      11    |    1141 K  |    1141 K  |
|       from small pool |      10    |      23    |    1762 K  |    1762 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 89, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 90 [256/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 90 [512/50000]	Loss: 4.5987	LR: 0.200000
Training Epoch: 90 [768/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 90 [1024/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 90 [1280/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 90 [1536/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 90 [1792/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 90 [2048/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 90 [2304/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 90 [2560/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 90 [2816/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 90 [3072/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 90 [3328/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 90 [3584/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 90 [3840/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 90 [4096/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 90 [4352/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 90 [4608/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 90 [4864/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 90 [5120/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 90 [5376/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 90 [5632/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 90 [5888/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 90 [6144/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 90 [6400/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 90 [6656/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 90 [6912/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 90 [7168/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 90 [7424/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 90 [7680/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 90 [7936/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 90 [8192/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 90 [8448/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 90 [8704/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 90 [8960/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 90 [9216/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 90 [9472/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 90 [9728/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 90 [9984/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 90 [10240/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 90 [10496/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 90 [10752/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 90 [11008/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 90 [11264/50000]	Loss: 4.6155	LR: 0.200000
Training Epoch: 90 [11520/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 90 [11776/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 90 [12032/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 90 [12288/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 90 [12544/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 90 [12800/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 90 [13056/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 90 [13312/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 90 [13568/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 90 [13824/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 90 [14080/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 90 [14336/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 90 [14592/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 90 [14848/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 90 [15104/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 90 [15360/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 90 [15616/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 90 [15872/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 90 [16128/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 90 [16384/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 90 [16640/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 90 [16896/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 90 [17152/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 90 [17408/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 90 [17664/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 90 [17920/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 90 [18176/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 90 [18432/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 90 [18688/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 90 [18944/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 90 [19200/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 90 [19456/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 90 [19712/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 90 [19968/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 90 [20224/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 90 [20480/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 90 [20736/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 90 [20992/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 90 [21248/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 90 [21504/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 90 [21760/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 90 [22016/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 90 [22272/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 90 [22528/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 90 [22784/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 90 [23040/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 90 [23296/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 90 [23552/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 90 [23808/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 90 [24064/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 90 [24320/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 90 [24576/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 90 [24832/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 90 [25088/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 90 [25344/50000]	Loss: 4.5998	LR: 0.200000
Training Epoch: 90 [25600/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 90 [25856/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 90 [26112/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 90 [26368/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 90 [26624/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 90 [26880/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 90 [27136/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 90 [27392/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 90 [27648/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 90 [27904/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 90 [28160/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 90 [28416/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 90 [28672/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 90 [28928/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 90 [29184/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 90 [29440/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 90 [29696/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 90 [29952/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 90 [30208/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 90 [30464/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 90 [30720/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 90 [30976/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 90 [31232/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 90 [31488/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 90 [31744/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 90 [32000/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 90 [32256/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 90 [32512/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 90 [32768/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 90 [33024/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 90 [33280/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 90 [33536/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 90 [33792/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 90 [34048/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 90 [34304/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 90 [34560/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 90 [34816/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 90 [35072/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 90 [35328/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 90 [35584/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 90 [35840/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 90 [36096/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 90 [36352/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 90 [36608/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 90 [36864/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 90 [37120/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 90 [37376/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 90 [37632/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 90 [37888/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 90 [38144/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 90 [38400/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 90 [38656/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 90 [38912/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 90 [39168/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 90 [39424/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 90 [39680/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 90 [39936/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 90 [40192/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 90 [40448/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 90 [40704/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 90 [40960/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 90 [41216/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 90 [41472/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 90 [41728/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 90 [41984/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 90 [42240/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 90 [42496/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 90 [42752/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 90 [43008/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 90 [43264/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 90 [43520/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 90 [43776/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 90 [44032/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 90 [44288/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 90 [44544/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 90 [44800/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 90 [45056/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 90 [45312/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 90 [45568/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 90 [45824/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 90 [46080/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 90 [46336/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 90 [46592/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 90 [46848/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 90 [47104/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 90 [47360/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 90 [47616/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 90 [47872/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 90 [48128/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 90 [48384/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 90 [48640/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 90 [48896/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 90 [49152/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 90 [49408/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 90 [49664/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 90 [49920/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 90 [50000/50000]	Loss: 4.6073	LR: 0.200000
epoch 90 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  135017 GB |  135016 GB |
|       from large pool |  400448 KB |    1770 MB |  134895 GB |  134895 GB |
|       from small pool |    3549 KB |       9 MB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  135017 GB |  135016 GB |
|       from large pool |  400448 KB |    1770 MB |  134895 GB |  134895 GB |
|       from small pool |    3549 KB |       9 MB |     121 GB |     121 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   83074 GB |   83074 GB |
|       from large pool |  244672 KB |  473024 KB |   82935 GB |   82935 GB |
|       from small pool |    2594 KB |    4843 KB |     138 GB |     138 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5762 K  |    5762 K  |
|       from large pool |      36    |      77    |    2786 K  |    2786 K  |
|       from small pool |     186    |     224    |    2976 K  |    2976 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5762 K  |    5762 K  |
|       from large pool |      36    |      77    |    2786 K  |    2786 K  |
|       from small pool |     186    |     224    |    2976 K  |    2976 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    2936 K  |    2936 K  |
|       from large pool |      10    |      11    |    1154 K  |    1154 K  |
|       from small pool |      12    |      23    |    1782 K  |    1782 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 90, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-90-regular.pth
Training Epoch: 91 [256/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 91 [512/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 91 [768/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 91 [1024/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 91 [1280/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 91 [1536/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 91 [1792/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 91 [2048/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 91 [2304/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 91 [2560/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 91 [2816/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 91 [3072/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 91 [3328/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 91 [3584/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 91 [3840/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 91 [4096/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 91 [4352/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 91 [4608/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 91 [4864/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 91 [5120/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 91 [5376/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 91 [5632/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 91 [5888/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 91 [6144/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 91 [6400/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 91 [6656/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 91 [6912/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 91 [7168/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 91 [7424/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 91 [7680/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 91 [7936/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 91 [8192/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 91 [8448/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 91 [8704/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 91 [8960/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 91 [9216/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 91 [9472/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 91 [9728/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 91 [9984/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 91 [10240/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 91 [10496/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 91 [10752/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 91 [11008/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 91 [11264/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 91 [11520/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 91 [11776/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 91 [12032/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 91 [12288/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 91 [12544/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 91 [12800/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 91 [13056/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 91 [13312/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 91 [13568/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 91 [13824/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 91 [14080/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 91 [14336/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 91 [14592/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 91 [14848/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 91 [15104/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 91 [15360/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 91 [15616/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 91 [15872/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 91 [16128/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 91 [16384/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 91 [16640/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 91 [16896/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 91 [17152/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 91 [17408/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 91 [17664/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 91 [17920/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 91 [18176/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 91 [18432/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 91 [18688/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 91 [18944/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 91 [19200/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 91 [19456/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 91 [19712/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 91 [19968/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 91 [20224/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 91 [20480/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 91 [20736/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 91 [20992/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 91 [21248/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 91 [21504/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 91 [21760/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [22016/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 91 [22272/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 91 [22528/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 91 [22784/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 91 [23040/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 91 [23296/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 91 [23552/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 91 [23808/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 91 [24064/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 91 [24320/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [24576/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 91 [24832/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 91 [25088/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 91 [25344/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 91 [25600/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 91 [25856/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 91 [26112/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 91 [26368/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 91 [26624/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 91 [26880/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 91 [27136/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 91 [27392/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 91 [27648/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 91 [27904/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 91 [28160/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 91 [28416/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 91 [28672/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 91 [28928/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 91 [29184/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 91 [29440/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 91 [29696/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 91 [29952/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 91 [30208/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 91 [30464/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 91 [30720/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 91 [30976/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 91 [31232/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 91 [31488/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 91 [31744/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [32000/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 91 [32256/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 91 [32512/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 91 [32768/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 91 [33024/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [33280/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 91 [33536/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 91 [33792/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 91 [34048/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 91 [34304/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 91 [34560/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 91 [34816/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 91 [35072/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 91 [35328/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 91 [35584/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 91 [35840/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 91 [36096/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 91 [36352/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 91 [36608/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 91 [36864/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 91 [37120/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 91 [37376/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 91 [37632/50000]	Loss: 4.6174	LR: 0.200000
Training Epoch: 91 [37888/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 91 [38144/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 91 [38400/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 91 [38656/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 91 [38912/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 91 [39168/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 91 [39424/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [39680/50000]	Loss: 4.6156	LR: 0.200000
Training Epoch: 91 [39936/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 91 [40192/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 91 [40448/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 91 [40704/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 91 [40960/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 91 [41216/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 91 [41472/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 91 [41728/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 91 [41984/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 91 [42240/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 91 [42496/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 91 [42752/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [43008/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 91 [43264/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 91 [43520/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 91 [43776/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 91 [44032/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 91 [44288/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 91 [44544/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 91 [44800/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 91 [45056/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 91 [45312/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 91 [45568/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 91 [45824/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 91 [46080/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 91 [46336/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 91 [46592/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 91 [46848/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 91 [47104/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [47360/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 91 [47616/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 91 [47872/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 91 [48128/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 91 [48384/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 91 [48640/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 91 [48896/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 91 [49152/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 91 [49408/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 91 [49664/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 91 [49920/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 91 [50000/50000]	Loss: 4.6142	LR: 0.200000
epoch 91 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  136517 GB |  136517 GB |
|       from large pool |  400448 KB |    1770 MB |  136394 GB |  136394 GB |
|       from small pool |    3549 KB |       9 MB |     122 GB |     122 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  136517 GB |  136517 GB |
|       from large pool |  400448 KB |    1770 MB |  136394 GB |  136394 GB |
|       from small pool |    3549 KB |       9 MB |     122 GB |     122 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   83997 GB |   83997 GB |
|       from large pool |  244672 KB |  473024 KB |   83857 GB |   83856 GB |
|       from small pool |    2594 KB |    4843 KB |     140 GB |     140 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5826 K  |    5826 K  |
|       from large pool |      36    |      77    |    2817 K  |    2817 K  |
|       from small pool |     186    |     224    |    3009 K  |    3009 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5826 K  |    5826 K  |
|       from large pool |      36    |      77    |    2817 K  |    2817 K  |
|       from small pool |     186    |     224    |    3009 K  |    3009 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    2969 K  |    2969 K  |
|       from large pool |      10    |      11    |    1166 K  |    1166 K  |
|       from small pool |      10    |      23    |    1802 K  |    1802 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 91, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 92 [256/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 92 [512/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 92 [768/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 92 [1024/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 92 [1280/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 92 [1536/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 92 [1792/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 92 [2048/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 92 [2304/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 92 [2560/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 92 [2816/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 92 [3072/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 92 [3328/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 92 [3584/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 92 [3840/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 92 [4096/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 92 [4352/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 92 [4608/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 92 [4864/50000]	Loss: 4.5987	LR: 0.200000
Training Epoch: 92 [5120/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 92 [5376/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 92 [5632/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 92 [5888/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 92 [6144/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 92 [6400/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 92 [6656/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 92 [6912/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 92 [7168/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 92 [7424/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 92 [7680/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 92 [7936/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 92 [8192/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 92 [8448/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 92 [8704/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 92 [8960/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 92 [9216/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 92 [9472/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 92 [9728/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 92 [9984/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 92 [10240/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 92 [10496/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 92 [10752/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 92 [11008/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 92 [11264/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 92 [11520/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 92 [11776/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 92 [12032/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 92 [12288/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 92 [12544/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 92 [12800/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 92 [13056/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 92 [13312/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 92 [13568/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 92 [13824/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 92 [14080/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 92 [14336/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 92 [14592/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 92 [14848/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 92 [15104/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 92 [15360/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 92 [15616/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 92 [15872/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 92 [16128/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 92 [16384/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 92 [16640/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 92 [16896/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 92 [17152/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 92 [17408/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 92 [17664/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 92 [17920/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 92 [18176/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 92 [18432/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 92 [18688/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 92 [18944/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 92 [19200/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 92 [19456/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 92 [19712/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 92 [19968/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 92 [20224/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 92 [20480/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 92 [20736/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 92 [20992/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 92 [21248/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 92 [21504/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 92 [21760/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 92 [22016/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 92 [22272/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 92 [22528/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 92 [22784/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 92 [23040/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 92 [23296/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 92 [23552/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 92 [23808/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 92 [24064/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 92 [24320/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 92 [24576/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 92 [24832/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 92 [25088/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 92 [25344/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 92 [25600/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 92 [25856/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 92 [26112/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 92 [26368/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 92 [26624/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 92 [26880/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 92 [27136/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 92 [27392/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 92 [27648/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 92 [27904/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 92 [28160/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 92 [28416/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 92 [28672/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 92 [28928/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 92 [29184/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 92 [29440/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 92 [29696/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 92 [29952/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 92 [30208/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 92 [30464/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 92 [30720/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 92 [30976/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 92 [31232/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 92 [31488/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 92 [31744/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 92 [32000/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 92 [32256/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 92 [32512/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 92 [32768/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 92 [33024/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 92 [33280/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 92 [33536/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 92 [33792/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 92 [34048/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 92 [34304/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 92 [34560/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 92 [34816/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 92 [35072/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 92 [35328/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 92 [35584/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 92 [35840/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 92 [36096/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 92 [36352/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 92 [36608/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 92 [36864/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 92 [37120/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 92 [37376/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 92 [37632/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 92 [37888/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 92 [38144/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 92 [38400/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 92 [38656/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 92 [38912/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 92 [39168/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 92 [39424/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 92 [39680/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 92 [39936/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 92 [40192/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 92 [40448/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 92 [40704/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 92 [40960/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 92 [41216/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 92 [41472/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 92 [41728/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 92 [41984/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 92 [42240/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 92 [42496/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 92 [42752/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 92 [43008/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 92 [43264/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 92 [43520/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 92 [43776/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 92 [44032/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 92 [44288/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 92 [44544/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 92 [44800/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 92 [45056/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 92 [45312/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 92 [45568/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 92 [45824/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 92 [46080/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 92 [46336/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 92 [46592/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 92 [46848/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 92 [47104/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 92 [47360/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 92 [47616/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 92 [47872/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 92 [48128/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 92 [48384/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 92 [48640/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 92 [48896/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 92 [49152/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 92 [49408/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 92 [49664/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 92 [49920/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 92 [50000/50000]	Loss: 4.6067	LR: 0.200000
epoch 92 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  138017 GB |  138017 GB |
|       from large pool |  400448 KB |    1770 MB |  137893 GB |  137893 GB |
|       from small pool |    3549 KB |       9 MB |     124 GB |     124 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  138017 GB |  138017 GB |
|       from large pool |  400448 KB |    1770 MB |  137893 GB |  137893 GB |
|       from small pool |    3549 KB |       9 MB |     124 GB |     124 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   84920 GB |   84920 GB |
|       from large pool |  244672 KB |  473024 KB |   84778 GB |   84778 GB |
|       from small pool |    4642 KB |    4843 KB |     141 GB |     141 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5890 K  |    5890 K  |
|       from large pool |      36    |      77    |    2848 K  |    2848 K  |
|       from small pool |     186    |     224    |    3042 K  |    3042 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5890 K  |    5890 K  |
|       from large pool |      36    |      77    |    2848 K  |    2848 K  |
|       from small pool |     186    |     224    |    3042 K  |    3042 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3002 K  |    3002 K  |
|       from large pool |      10    |      11    |    1179 K  |    1179 K  |
|       from small pool |      13    |      23    |    1822 K  |    1822 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 92, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.49s

Training Epoch: 93 [256/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 93 [512/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 93 [768/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 93 [1024/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 93 [1280/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 93 [1536/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 93 [1792/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 93 [2048/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 93 [2304/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 93 [2560/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 93 [2816/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 93 [3072/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 93 [3328/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 93 [3584/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 93 [3840/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 93 [4096/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 93 [4352/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 93 [4608/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 93 [4864/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 93 [5120/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 93 [5376/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 93 [5632/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 93 [5888/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 93 [6144/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 93 [6400/50000]	Loss: 4.5985	LR: 0.200000
Training Epoch: 93 [6656/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 93 [6912/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 93 [7168/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 93 [7424/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 93 [7680/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 93 [7936/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 93 [8192/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 93 [8448/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 93 [8704/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 93 [8960/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 93 [9216/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 93 [9472/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 93 [9728/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 93 [9984/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 93 [10240/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 93 [10496/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 93 [10752/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 93 [11008/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 93 [11264/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 93 [11520/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 93 [11776/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 93 [12032/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 93 [12288/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 93 [12544/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 93 [12800/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 93 [13056/50000]	Loss: 4.6181	LR: 0.200000
Training Epoch: 93 [13312/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 93 [13568/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 93 [13824/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 93 [14080/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 93 [14336/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 93 [14592/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 93 [14848/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 93 [15104/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 93 [15360/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 93 [15616/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 93 [15872/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 93 [16128/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 93 [16384/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 93 [16640/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 93 [16896/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 93 [17152/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 93 [17408/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 93 [17664/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 93 [17920/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 93 [18176/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 93 [18432/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 93 [18688/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 93 [18944/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 93 [19200/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 93 [19456/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 93 [19712/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 93 [19968/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 93 [20224/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 93 [20480/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 93 [20736/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 93 [20992/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 93 [21248/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 93 [21504/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 93 [21760/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 93 [22016/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 93 [22272/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 93 [22528/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 93 [22784/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 93 [23040/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 93 [23296/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 93 [23552/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 93 [23808/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 93 [24064/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 93 [24320/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 93 [24576/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 93 [24832/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 93 [25088/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 93 [25344/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 93 [25600/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 93 [25856/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 93 [26112/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 93 [26368/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 93 [26624/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 93 [26880/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 93 [27136/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 93 [27392/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 93 [27648/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 93 [27904/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 93 [28160/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 93 [28416/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 93 [28672/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 93 [28928/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 93 [29184/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 93 [29440/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 93 [29696/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 93 [29952/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 93 [30208/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 93 [30464/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 93 [30720/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 93 [30976/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 93 [31232/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 93 [31488/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 93 [31744/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 93 [32000/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 93 [32256/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 93 [32512/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 93 [32768/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 93 [33024/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 93 [33280/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 93 [33536/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 93 [33792/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 93 [34048/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 93 [34304/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 93 [34560/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 93 [34816/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 93 [35072/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 93 [35328/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 93 [35584/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 93 [35840/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 93 [36096/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 93 [36352/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 93 [36608/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 93 [36864/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 93 [37120/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 93 [37376/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 93 [37632/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 93 [37888/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 93 [38144/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 93 [38400/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 93 [38656/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 93 [38912/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 93 [39168/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 93 [39424/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 93 [39680/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 93 [39936/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 93 [40192/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 93 [40448/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 93 [40704/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 93 [40960/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 93 [41216/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 93 [41472/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 93 [41728/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 93 [41984/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 93 [42240/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 93 [42496/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 93 [42752/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 93 [43008/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 93 [43264/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 93 [43520/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 93 [43776/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 93 [44032/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 93 [44288/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 93 [44544/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 93 [44800/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 93 [45056/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 93 [45312/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 93 [45568/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 93 [45824/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 93 [46080/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 93 [46336/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 93 [46592/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 93 [46848/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 93 [47104/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 93 [47360/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 93 [47616/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 93 [47872/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 93 [48128/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 93 [48384/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 93 [48640/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 93 [48896/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 93 [49152/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 93 [49408/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 93 [49664/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 93 [49920/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 93 [50000/50000]	Loss: 4.6054	LR: 0.200000
epoch 93 training time consumed: 21.75s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  139517 GB |  139517 GB |
|       from large pool |  400448 KB |    1770 MB |  139392 GB |  139391 GB |
|       from small pool |    3549 KB |       9 MB |     125 GB |     125 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  139517 GB |  139517 GB |
|       from large pool |  400448 KB |    1770 MB |  139392 GB |  139391 GB |
|       from small pool |    3549 KB |       9 MB |     125 GB |     125 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   85843 GB |   85843 GB |
|       from large pool |  244672 KB |  473024 KB |   85700 GB |   85699 GB |
|       from small pool |    2594 KB |    4843 KB |     143 GB |     143 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    5954 K  |    5954 K  |
|       from large pool |      36    |      77    |    2879 K  |    2879 K  |
|       from small pool |     186    |     224    |    3075 K  |    3075 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    5954 K  |    5954 K  |
|       from large pool |      36    |      77    |    2879 K  |    2879 K  |
|       from small pool |     186    |     224    |    3075 K  |    3075 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    3034 K  |    3034 K  |
|       from large pool |      10    |      11    |    1192 K  |    1192 K  |
|       from small pool |      12    |      23    |    1842 K  |    1842 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 93, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 94 [256/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 94 [512/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 94 [768/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 94 [1024/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [1280/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 94 [1536/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 94 [1792/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 94 [2048/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 94 [2304/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 94 [2560/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 94 [2816/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 94 [3072/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 94 [3328/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 94 [3584/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 94 [3840/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 94 [4096/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 94 [4352/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 94 [4608/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 94 [4864/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 94 [5120/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 94 [5376/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 94 [5632/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 94 [5888/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 94 [6144/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 94 [6400/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 94 [6656/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 94 [6912/50000]	Loss: 4.5968	LR: 0.200000
Training Epoch: 94 [7168/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 94 [7424/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 94 [7680/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 94 [7936/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 94 [8192/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 94 [8448/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 94 [8704/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 94 [8960/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [9216/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 94 [9472/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 94 [9728/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 94 [9984/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 94 [10240/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 94 [10496/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [10752/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [11008/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 94 [11264/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 94 [11520/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 94 [11776/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 94 [12032/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 94 [12288/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 94 [12544/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 94 [12800/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [13056/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 94 [13312/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 94 [13568/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 94 [13824/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 94 [14080/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 94 [14336/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [14592/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 94 [14848/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 94 [15104/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 94 [15360/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 94 [15616/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 94 [15872/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 94 [16128/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 94 [16384/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 94 [16640/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 94 [16896/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 94 [17152/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 94 [17408/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 94 [17664/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 94 [17920/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 94 [18176/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 94 [18432/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 94 [18688/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 94 [18944/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 94 [19200/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 94 [19456/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 94 [19712/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 94 [19968/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 94 [20224/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 94 [20480/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 94 [20736/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 94 [20992/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 94 [21248/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [21504/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 94 [21760/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 94 [22016/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 94 [22272/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 94 [22528/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 94 [22784/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 94 [23040/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 94 [23296/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 94 [23552/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [23808/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 94 [24064/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 94 [24320/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 94 [24576/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 94 [24832/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 94 [25088/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 94 [25344/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 94 [25600/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [25856/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 94 [26112/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 94 [26368/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 94 [26624/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 94 [26880/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 94 [27136/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 94 [27392/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 94 [27648/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 94 [27904/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 94 [28160/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 94 [28416/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 94 [28672/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 94 [28928/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 94 [29184/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 94 [29440/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [29696/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 94 [29952/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 94 [30208/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 94 [30464/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 94 [30720/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 94 [30976/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 94 [31232/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 94 [31488/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 94 [31744/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 94 [32000/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 94 [32256/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 94 [32512/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 94 [32768/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 94 [33024/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [33280/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 94 [33536/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 94 [33792/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 94 [34048/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 94 [34304/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 94 [34560/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 94 [34816/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 94 [35072/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 94 [35328/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 94 [35584/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 94 [35840/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 94 [36096/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 94 [36352/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 94 [36608/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 94 [36864/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 94 [37120/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 94 [37376/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 94 [37632/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 94 [37888/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 94 [38144/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 94 [38400/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 94 [38656/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 94 [38912/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 94 [39168/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 94 [39424/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 94 [39680/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 94 [39936/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 94 [40192/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [40448/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 94 [40704/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 94 [40960/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 94 [41216/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 94 [41472/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 94 [41728/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 94 [41984/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 94 [42240/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 94 [42496/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 94 [42752/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 94 [43008/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 94 [43264/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 94 [43520/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 94 [43776/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 94 [44032/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 94 [44288/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 94 [44544/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 94 [44800/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 94 [45056/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 94 [45312/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 94 [45568/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 94 [45824/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 94 [46080/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 94 [46336/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 94 [46592/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 94 [46848/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 94 [47104/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 94 [47360/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 94 [47616/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 94 [47872/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 94 [48128/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 94 [48384/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 94 [48640/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 94 [48896/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 94 [49152/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 94 [49408/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 94 [49664/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 94 [49920/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 94 [50000/50000]	Loss: 4.6066	LR: 0.200000
epoch 94 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  141018 GB |  141017 GB |
|       from large pool |  400448 KB |    1770 MB |  140891 GB |  140890 GB |
|       from small pool |    3549 KB |       9 MB |     126 GB |     126 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  141018 GB |  141017 GB |
|       from large pool |  400448 KB |    1770 MB |  140891 GB |  140890 GB |
|       from small pool |    3549 KB |       9 MB |     126 GB |     126 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   86766 GB |   86766 GB |
|       from large pool |  244672 KB |  473024 KB |   86621 GB |   86621 GB |
|       from small pool |    4642 KB |    4843 KB |     145 GB |     145 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6018 K  |    6018 K  |
|       from large pool |      36    |      77    |    2910 K  |    2910 K  |
|       from small pool |     186    |     224    |    3108 K  |    3108 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6018 K  |    6018 K  |
|       from large pool |      36    |      77    |    2910 K  |    2910 K  |
|       from small pool |     186    |     224    |    3108 K  |    3108 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    3067 K  |    3067 K  |
|       from large pool |      10    |      11    |    1205 K  |    1205 K  |
|       from small pool |      12    |      23    |    1862 K  |    1862 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 94, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 95 [256/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 95 [512/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 95 [768/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 95 [1024/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 95 [1280/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 95 [1536/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 95 [1792/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 95 [2048/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 95 [2304/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 95 [2560/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 95 [2816/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 95 [3072/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 95 [3328/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 95 [3584/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 95 [3840/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 95 [4096/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 95 [4352/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 95 [4608/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 95 [4864/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 95 [5120/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 95 [5376/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 95 [5632/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 95 [5888/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 95 [6144/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 95 [6400/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 95 [6656/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 95 [6912/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 95 [7168/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 95 [7424/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 95 [7680/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 95 [7936/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 95 [8192/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 95 [8448/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 95 [8704/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 95 [8960/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 95 [9216/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 95 [9472/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 95 [9728/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 95 [9984/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 95 [10240/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 95 [10496/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 95 [10752/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 95 [11008/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 95 [11264/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 95 [11520/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 95 [11776/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 95 [12032/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 95 [12288/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 95 [12544/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 95 [12800/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 95 [13056/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 95 [13312/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 95 [13568/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 95 [13824/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 95 [14080/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 95 [14336/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 95 [14592/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 95 [14848/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 95 [15104/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 95 [15360/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 95 [15616/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 95 [15872/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 95 [16128/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 95 [16384/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 95 [16640/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 95 [16896/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 95 [17152/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 95 [17408/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 95 [17664/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 95 [17920/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 95 [18176/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 95 [18432/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 95 [18688/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 95 [18944/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 95 [19200/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 95 [19456/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 95 [19712/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 95 [19968/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 95 [20224/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 95 [20480/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 95 [20736/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 95 [20992/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 95 [21248/50000]	Loss: 4.6159	LR: 0.200000
Training Epoch: 95 [21504/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 95 [21760/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 95 [22016/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 95 [22272/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 95 [22528/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 95 [22784/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 95 [23040/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 95 [23296/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 95 [23552/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 95 [23808/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 95 [24064/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 95 [24320/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 95 [24576/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 95 [24832/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 95 [25088/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 95 [25344/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 95 [25600/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 95 [25856/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 95 [26112/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 95 [26368/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 95 [26624/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 95 [26880/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 95 [27136/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 95 [27392/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 95 [27648/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 95 [27904/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 95 [28160/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 95 [28416/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 95 [28672/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 95 [28928/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 95 [29184/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 95 [29440/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 95 [29696/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 95 [29952/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 95 [30208/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 95 [30464/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 95 [30720/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 95 [30976/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 95 [31232/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 95 [31488/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 95 [31744/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 95 [32000/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 95 [32256/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 95 [32512/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 95 [32768/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 95 [33024/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 95 [33280/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 95 [33536/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 95 [33792/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 95 [34048/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 95 [34304/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 95 [34560/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 95 [34816/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 95 [35072/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 95 [35328/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 95 [35584/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 95 [35840/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 95 [36096/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 95 [36352/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 95 [36608/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 95 [36864/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 95 [37120/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 95 [37376/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 95 [37632/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 95 [37888/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 95 [38144/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 95 [38400/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 95 [38656/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 95 [38912/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 95 [39168/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 95 [39424/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 95 [39680/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 95 [39936/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 95 [40192/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 95 [40448/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 95 [40704/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 95 [40960/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 95 [41216/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 95 [41472/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 95 [41728/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 95 [41984/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 95 [42240/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 95 [42496/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 95 [42752/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 95 [43008/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 95 [43264/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 95 [43520/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 95 [43776/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 95 [44032/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 95 [44288/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 95 [44544/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 95 [44800/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 95 [45056/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 95 [45312/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 95 [45568/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 95 [45824/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 95 [46080/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 95 [46336/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 95 [46592/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 95 [46848/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 95 [47104/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 95 [47360/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 95 [47616/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 95 [47872/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 95 [48128/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 95 [48384/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 95 [48640/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 95 [48896/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 95 [49152/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 95 [49408/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 95 [49664/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 95 [49920/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 95 [50000/50000]	Loss: 4.6079	LR: 0.200000
epoch 95 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  142518 GB |  142517 GB |
|       from large pool |  400448 KB |    1770 MB |  142389 GB |  142389 GB |
|       from small pool |    3549 KB |       9 MB |     128 GB |     128 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  142518 GB |  142517 GB |
|       from large pool |  400448 KB |    1770 MB |  142389 GB |  142389 GB |
|       from small pool |    3549 KB |       9 MB |     128 GB |     128 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   87689 GB |   87689 GB |
|       from large pool |  244672 KB |  473024 KB |   87543 GB |   87542 GB |
|       from small pool |    2594 KB |    4843 KB |     146 GB |     146 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6082 K  |    6082 K  |
|       from large pool |      36    |      77    |    2941 K  |    2941 K  |
|       from small pool |     186    |     224    |    3141 K  |    3141 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6082 K  |    6082 K  |
|       from large pool |      36    |      77    |    2941 K  |    2941 K  |
|       from small pool |     186    |     224    |    3141 K  |    3141 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3100 K  |    3100 K  |
|       from large pool |      10    |      11    |    1218 K  |    1218 K  |
|       from small pool |      13    |      23    |    1882 K  |    1882 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 95, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 96 [256/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 96 [512/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [768/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 96 [1024/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 96 [1280/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 96 [1536/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 96 [1792/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 96 [2048/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [2304/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 96 [2560/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 96 [2816/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 96 [3072/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 96 [3328/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 96 [3584/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 96 [3840/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 96 [4096/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 96 [4352/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 96 [4608/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 96 [4864/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 96 [5120/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 96 [5376/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 96 [5632/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 96 [5888/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 96 [6144/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 96 [6400/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 96 [6656/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 96 [6912/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 96 [7168/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 96 [7424/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 96 [7680/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 96 [7936/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [8192/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 96 [8448/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 96 [8704/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 96 [8960/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 96 [9216/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 96 [9472/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 96 [9728/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 96 [9984/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 96 [10240/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 96 [10496/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [10752/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 96 [11008/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [11264/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 96 [11520/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 96 [11776/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 96 [12032/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [12288/50000]	Loss: 4.5989	LR: 0.200000
Training Epoch: 96 [12544/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 96 [12800/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 96 [13056/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 96 [13312/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 96 [13568/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 96 [13824/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [14080/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 96 [14336/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 96 [14592/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 96 [14848/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 96 [15104/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 96 [15360/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 96 [15616/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 96 [15872/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 96 [16128/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 96 [16384/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 96 [16640/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 96 [16896/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 96 [17152/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 96 [17408/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 96 [17664/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 96 [17920/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 96 [18176/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 96 [18432/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 96 [18688/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 96 [18944/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 96 [19200/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 96 [19456/50000]	Loss: 4.6160	LR: 0.200000
Training Epoch: 96 [19712/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 96 [19968/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 96 [20224/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 96 [20480/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 96 [20736/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [20992/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 96 [21248/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 96 [21504/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 96 [21760/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 96 [22016/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 96 [22272/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [22528/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 96 [22784/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 96 [23040/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 96 [23296/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 96 [23552/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 96 [23808/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 96 [24064/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 96 [24320/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 96 [24576/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 96 [24832/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 96 [25088/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 96 [25344/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [25600/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 96 [25856/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 96 [26112/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 96 [26368/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 96 [26624/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 96 [26880/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 96 [27136/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 96 [27392/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 96 [27648/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 96 [27904/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [28160/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [28416/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 96 [28672/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 96 [28928/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 96 [29184/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 96 [29440/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 96 [29696/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 96 [29952/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 96 [30208/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 96 [30464/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 96 [30720/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 96 [30976/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [31232/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 96 [31488/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 96 [31744/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 96 [32000/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 96 [32256/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 96 [32512/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 96 [32768/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 96 [33024/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 96 [33280/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 96 [33536/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 96 [33792/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 96 [34048/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 96 [34304/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 96 [34560/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 96 [34816/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 96 [35072/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 96 [35328/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 96 [35584/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [35840/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 96 [36096/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 96 [36352/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 96 [36608/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 96 [36864/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 96 [37120/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 96 [37376/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 96 [37632/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [37888/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 96 [38144/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 96 [38400/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 96 [38656/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 96 [38912/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 96 [39168/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 96 [39424/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 96 [39680/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 96 [39936/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 96 [40192/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 96 [40448/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [40704/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 96 [40960/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [41216/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 96 [41472/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 96 [41728/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 96 [41984/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [42240/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 96 [42496/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 96 [42752/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 96 [43008/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 96 [43264/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 96 [43520/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 96 [43776/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 96 [44032/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 96 [44288/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 96 [44544/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 96 [44800/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 96 [45056/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 96 [45312/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 96 [45568/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 96 [45824/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 96 [46080/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 96 [46336/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 96 [46592/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 96 [46848/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 96 [47104/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 96 [47360/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 96 [47616/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 96 [47872/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 96 [48128/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 96 [48384/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 96 [48640/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 96 [48896/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 96 [49152/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 96 [49408/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 96 [49664/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 96 [49920/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 96 [50000/50000]	Loss: 4.6038	LR: 0.200000
epoch 96 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  144018 GB |  144018 GB |
|       from large pool |  400448 KB |    1770 MB |  143888 GB |  143888 GB |
|       from small pool |    3549 KB |       9 MB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  144018 GB |  144018 GB |
|       from large pool |  400448 KB |    1770 MB |  143888 GB |  143888 GB |
|       from small pool |    3549 KB |       9 MB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   88612 GB |   88612 GB |
|       from large pool |  244672 KB |  473024 KB |   88464 GB |   88464 GB |
|       from small pool |    2594 KB |    4843 KB |     148 GB |     148 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6146 K  |    6146 K  |
|       from large pool |      36    |      77    |    2972 K  |    2972 K  |
|       from small pool |     186    |     224    |    3174 K  |    3174 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6146 K  |    6146 K  |
|       from large pool |      36    |      77    |    2972 K  |    2972 K  |
|       from small pool |     186    |     224    |    3174 K  |    3174 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    3133 K  |    3133 K  |
|       from large pool |      10    |      11    |    1231 K  |    1231 K  |
|       from small pool |      12    |      23    |    1902 K  |    1902 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 96, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 97 [256/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 97 [512/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 97 [768/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 97 [1024/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 97 [1280/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 97 [1536/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 97 [1792/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 97 [2048/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 97 [2304/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 97 [2560/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 97 [2816/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 97 [3072/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 97 [3328/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 97 [3584/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 97 [3840/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 97 [4096/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 97 [4352/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 97 [4608/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 97 [4864/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 97 [5120/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 97 [5376/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 97 [5632/50000]	Loss: 4.5989	LR: 0.200000
Training Epoch: 97 [5888/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 97 [6144/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 97 [6400/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 97 [6656/50000]	Loss: 4.6007	LR: 0.200000
Training Epoch: 97 [6912/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 97 [7168/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 97 [7424/50000]	Loss: 4.6169	LR: 0.200000
Training Epoch: 97 [7680/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 97 [7936/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 97 [8192/50000]	Loss: 4.5991	LR: 0.200000
Training Epoch: 97 [8448/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 97 [8704/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 97 [8960/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 97 [9216/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 97 [9472/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 97 [9728/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 97 [9984/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 97 [10240/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 97 [10496/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 97 [10752/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 97 [11008/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 97 [11264/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 97 [11520/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 97 [11776/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 97 [12032/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 97 [12288/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 97 [12544/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 97 [12800/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 97 [13056/50000]	Loss: 4.6167	LR: 0.200000
Training Epoch: 97 [13312/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 97 [13568/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 97 [13824/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 97 [14080/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 97 [14336/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 97 [14592/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 97 [14848/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 97 [15104/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 97 [15360/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 97 [15616/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 97 [15872/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 97 [16128/50000]	Loss: 4.6174	LR: 0.200000
Training Epoch: 97 [16384/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 97 [16640/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 97 [16896/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 97 [17152/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 97 [17408/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 97 [17664/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 97 [17920/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 97 [18176/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 97 [18432/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 97 [18688/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 97 [18944/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 97 [19200/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 97 [19456/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 97 [19712/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 97 [19968/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 97 [20224/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 97 [20480/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 97 [20736/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 97 [20992/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 97 [21248/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 97 [21504/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 97 [21760/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 97 [22016/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 97 [22272/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 97 [22528/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 97 [22784/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 97 [23040/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 97 [23296/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 97 [23552/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 97 [23808/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 97 [24064/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 97 [24320/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 97 [24576/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 97 [24832/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 97 [25088/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 97 [25344/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 97 [25600/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 97 [25856/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 97 [26112/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 97 [26368/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 97 [26624/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 97 [26880/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 97 [27136/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 97 [27392/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 97 [27648/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 97 [27904/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 97 [28160/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 97 [28416/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 97 [28672/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 97 [28928/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 97 [29184/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 97 [29440/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 97 [29696/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 97 [29952/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 97 [30208/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 97 [30464/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 97 [30720/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 97 [30976/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 97 [31232/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 97 [31488/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 97 [31744/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 97 [32000/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 97 [32256/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 97 [32512/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 97 [32768/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 97 [33024/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 97 [33280/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 97 [33536/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 97 [33792/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 97 [34048/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 97 [34304/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 97 [34560/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 97 [34816/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 97 [35072/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 97 [35328/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 97 [35584/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 97 [35840/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 97 [36096/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 97 [36352/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 97 [36608/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 97 [36864/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 97 [37120/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 97 [37376/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 97 [37632/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 97 [37888/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 97 [38144/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 97 [38400/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 97 [38656/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 97 [38912/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 97 [39168/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 97 [39424/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 97 [39680/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 97 [39936/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 97 [40192/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 97 [40448/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 97 [40704/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 97 [40960/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 97 [41216/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 97 [41472/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 97 [41728/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 97 [41984/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 97 [42240/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 97 [42496/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 97 [42752/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 97 [43008/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 97 [43264/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 97 [43520/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 97 [43776/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 97 [44032/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 97 [44288/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 97 [44544/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 97 [44800/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 97 [45056/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 97 [45312/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 97 [45568/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 97 [45824/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 97 [46080/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 97 [46336/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 97 [46592/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 97 [46848/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 97 [47104/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 97 [47360/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 97 [47616/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 97 [47872/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 97 [48128/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 97 [48384/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 97 [48640/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 97 [48896/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 97 [49152/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 97 [49408/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 97 [49664/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 97 [49920/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 97 [50000/50000]	Loss: 4.6078	LR: 0.200000
epoch 97 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  145518 GB |  145518 GB |
|       from large pool |  400448 KB |    1770 MB |  145387 GB |  145387 GB |
|       from small pool |    3549 KB |       9 MB |     130 GB |     130 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  145518 GB |  145518 GB |
|       from large pool |  400448 KB |    1770 MB |  145387 GB |  145387 GB |
|       from small pool |    3549 KB |       9 MB |     130 GB |     130 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   89535 GB |   89535 GB |
|       from large pool |  244672 KB |  473024 KB |   89386 GB |   89385 GB |
|       from small pool |    2594 KB |    4843 KB |     149 GB |     149 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6210 K  |    6210 K  |
|       from large pool |      36    |      77    |    3003 K  |    3003 K  |
|       from small pool |     186    |     224    |    3207 K  |    3207 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6210 K  |    6210 K  |
|       from large pool |      36    |      77    |    3003 K  |    3003 K  |
|       from small pool |     186    |     224    |    3207 K  |    3207 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3165 K  |    3165 K  |
|       from large pool |      10    |      11    |    1243 K  |    1243 K  |
|       from small pool |      11    |      23    |    1921 K  |    1921 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 97, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 98 [256/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 98 [512/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 98 [768/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 98 [1024/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 98 [1280/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 98 [1536/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 98 [1792/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 98 [2048/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 98 [2304/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 98 [2560/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 98 [2816/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 98 [3072/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 98 [3328/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 98 [3584/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 98 [3840/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 98 [4096/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 98 [4352/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 98 [4608/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 98 [4864/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 98 [5120/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 98 [5376/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 98 [5632/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 98 [5888/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 98 [6144/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 98 [6400/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 98 [6656/50000]	Loss: 4.5989	LR: 0.200000
Training Epoch: 98 [6912/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 98 [7168/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 98 [7424/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 98 [7680/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 98 [7936/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 98 [8192/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 98 [8448/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 98 [8704/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 98 [8960/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 98 [9216/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 98 [9472/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 98 [9728/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 98 [9984/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 98 [10240/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [10496/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 98 [10752/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 98 [11008/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [11264/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 98 [11520/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 98 [11776/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 98 [12032/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 98 [12288/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 98 [12544/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 98 [12800/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 98 [13056/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 98 [13312/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 98 [13568/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 98 [13824/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 98 [14080/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 98 [14336/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 98 [14592/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 98 [14848/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 98 [15104/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 98 [15360/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 98 [15616/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 98 [15872/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 98 [16128/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 98 [16384/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 98 [16640/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 98 [16896/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 98 [17152/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 98 [17408/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 98 [17664/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 98 [17920/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 98 [18176/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 98 [18432/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 98 [18688/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 98 [18944/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 98 [19200/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 98 [19456/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 98 [19712/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 98 [19968/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 98 [20224/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [20480/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 98 [20736/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 98 [20992/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 98 [21248/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 98 [21504/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [21760/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 98 [22016/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 98 [22272/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 98 [22528/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 98 [22784/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 98 [23040/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 98 [23296/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 98 [23552/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 98 [23808/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 98 [24064/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 98 [24320/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 98 [24576/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 98 [24832/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 98 [25088/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 98 [25344/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 98 [25600/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 98 [25856/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 98 [26112/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 98 [26368/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 98 [26624/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 98 [26880/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 98 [27136/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 98 [27392/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 98 [27648/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 98 [27904/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 98 [28160/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 98 [28416/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 98 [28672/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 98 [28928/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 98 [29184/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 98 [29440/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 98 [29696/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 98 [29952/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [30208/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 98 [30464/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 98 [30720/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 98 [30976/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 98 [31232/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 98 [31488/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 98 [31744/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 98 [32000/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 98 [32256/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 98 [32512/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 98 [32768/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 98 [33024/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 98 [33280/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 98 [33536/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 98 [33792/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 98 [34048/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 98 [34304/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 98 [34560/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 98 [34816/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 98 [35072/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 98 [35328/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 98 [35584/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 98 [35840/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 98 [36096/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 98 [36352/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 98 [36608/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [36864/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 98 [37120/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 98 [37376/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 98 [37632/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 98 [37888/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 98 [38144/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 98 [38400/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 98 [38656/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 98 [38912/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 98 [39168/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 98 [39424/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 98 [39680/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 98 [39936/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 98 [40192/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 98 [40448/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 98 [40704/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 98 [40960/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 98 [41216/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 98 [41472/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 98 [41728/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 98 [41984/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 98 [42240/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 98 [42496/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 98 [42752/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 98 [43008/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 98 [43264/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 98 [43520/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 98 [43776/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 98 [44032/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 98 [44288/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 98 [44544/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 98 [44800/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 98 [45056/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 98 [45312/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 98 [45568/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 98 [45824/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 98 [46080/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 98 [46336/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 98 [46592/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 98 [46848/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 98 [47104/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 98 [47360/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 98 [47616/50000]	Loss: 4.6153	LR: 0.200000
Training Epoch: 98 [47872/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 98 [48128/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 98 [48384/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 98 [48640/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 98 [48896/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 98 [49152/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 98 [49408/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 98 [49664/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 98 [49920/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 98 [50000/50000]	Loss: 4.6051	LR: 0.200000
epoch 98 training time consumed: 21.71s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  147018 GB |  147018 GB |
|       from large pool |  400448 KB |    1770 MB |  146886 GB |  146886 GB |
|       from small pool |    3549 KB |       9 MB |     132 GB |     132 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  147018 GB |  147018 GB |
|       from large pool |  400448 KB |    1770 MB |  146886 GB |  146886 GB |
|       from small pool |    3549 KB |       9 MB |     132 GB |     132 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   90458 GB |   90458 GB |
|       from large pool |  244672 KB |  473024 KB |   90307 GB |   90307 GB |
|       from small pool |    2594 KB |    4843 KB |     151 GB |     151 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6274 K  |    6274 K  |
|       from large pool |      36    |      77    |    3034 K  |    3034 K  |
|       from small pool |     186    |     224    |    3240 K  |    3240 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6274 K  |    6274 K  |
|       from large pool |      36    |      77    |    3034 K  |    3034 K  |
|       from small pool |     186    |     224    |    3240 K  |    3240 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3198 K  |    3198 K  |
|       from large pool |      10    |      11    |    1256 K  |    1256 K  |
|       from small pool |      11    |      23    |    1941 K  |    1941 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 98, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 99 [256/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 99 [512/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 99 [768/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 99 [1024/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 99 [1280/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 99 [1536/50000]	Loss: 4.5995	LR: 0.200000
Training Epoch: 99 [1792/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 99 [2048/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 99 [2304/50000]	Loss: 4.6004	LR: 0.200000
Training Epoch: 99 [2560/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 99 [2816/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 99 [3072/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 99 [3328/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 99 [3584/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 99 [3840/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 99 [4096/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 99 [4352/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 99 [4608/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 99 [4864/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 99 [5120/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 99 [5376/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 99 [5632/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 99 [5888/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 99 [6144/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 99 [6400/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 99 [6656/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 99 [6912/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 99 [7168/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 99 [7424/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 99 [7680/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 99 [7936/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 99 [8192/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 99 [8448/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 99 [8704/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 99 [8960/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 99 [9216/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 99 [9472/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 99 [9728/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 99 [9984/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 99 [10240/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 99 [10496/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 99 [10752/50000]	Loss: 4.5965	LR: 0.200000
Training Epoch: 99 [11008/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 99 [11264/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 99 [11520/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 99 [11776/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 99 [12032/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 99 [12288/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 99 [12544/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 99 [12800/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 99 [13056/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 99 [13312/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 99 [13568/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 99 [13824/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 99 [14080/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 99 [14336/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 99 [14592/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 99 [14848/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 99 [15104/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 99 [15360/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 99 [15616/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 99 [15872/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 99 [16128/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 99 [16384/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 99 [16640/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 99 [16896/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 99 [17152/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 99 [17408/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 99 [17664/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 99 [17920/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 99 [18176/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 99 [18432/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 99 [18688/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 99 [18944/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 99 [19200/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 99 [19456/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 99 [19712/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 99 [19968/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 99 [20224/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 99 [20480/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 99 [20736/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 99 [20992/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 99 [21248/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 99 [21504/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 99 [21760/50000]	Loss: 4.6001	LR: 0.200000
Training Epoch: 99 [22016/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 99 [22272/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 99 [22528/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 99 [22784/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 99 [23040/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 99 [23296/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 99 [23552/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 99 [23808/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 99 [24064/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 99 [24320/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 99 [24576/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 99 [24832/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 99 [25088/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 99 [25344/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 99 [25600/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 99 [25856/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 99 [26112/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 99 [26368/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 99 [26624/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 99 [26880/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 99 [27136/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 99 [27392/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 99 [27648/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 99 [27904/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 99 [28160/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 99 [28416/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 99 [28672/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 99 [28928/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 99 [29184/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 99 [29440/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 99 [29696/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 99 [29952/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 99 [30208/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 99 [30464/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 99 [30720/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 99 [30976/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 99 [31232/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 99 [31488/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 99 [31744/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 99 [32000/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 99 [32256/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 99 [32512/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 99 [32768/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 99 [33024/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 99 [33280/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 99 [33536/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 99 [33792/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 99 [34048/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 99 [34304/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 99 [34560/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 99 [34816/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 99 [35072/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 99 [35328/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 99 [35584/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 99 [35840/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 99 [36096/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 99 [36352/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 99 [36608/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 99 [36864/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 99 [37120/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 99 [37376/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 99 [37632/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 99 [37888/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 99 [38144/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 99 [38400/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 99 [38656/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 99 [38912/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 99 [39168/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 99 [39424/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 99 [39680/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 99 [39936/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 99 [40192/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 99 [40448/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 99 [40704/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 99 [40960/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 99 [41216/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 99 [41472/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 99 [41728/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 99 [41984/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 99 [42240/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 99 [42496/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 99 [42752/50000]	Loss: 4.5987	LR: 0.200000
Training Epoch: 99 [43008/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 99 [43264/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 99 [43520/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 99 [43776/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 99 [44032/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 99 [44288/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 99 [44544/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 99 [44800/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 99 [45056/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 99 [45312/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 99 [45568/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 99 [45824/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 99 [46080/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 99 [46336/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 99 [46592/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 99 [46848/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 99 [47104/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 99 [47360/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 99 [47616/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 99 [47872/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 99 [48128/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 99 [48384/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 99 [48640/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 99 [48896/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 99 [49152/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 99 [49408/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 99 [49664/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 99 [49920/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 99 [50000/50000]	Loss: 4.6055	LR: 0.200000
epoch 99 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  148518 GB |  148518 GB |
|       from large pool |  400448 KB |    1770 MB |  148385 GB |  148384 GB |
|       from small pool |    3549 KB |       9 MB |     133 GB |     133 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  148518 GB |  148518 GB |
|       from large pool |  400448 KB |    1770 MB |  148385 GB |  148384 GB |
|       from small pool |    3549 KB |       9 MB |     133 GB |     133 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |   91381 GB |   91381 GB |
|       from large pool |  244672 KB |  473024 KB |   91229 GB |   91228 GB |
|       from small pool |    4642 KB |    4843 KB |     152 GB |     152 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6338 K  |    6338 K  |
|       from large pool |      36    |      77    |    3065 K  |    3064 K  |
|       from small pool |     186    |     224    |    3273 K  |    3273 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6338 K  |    6338 K  |
|       from large pool |      36    |      77    |    3065 K  |    3064 K  |
|       from small pool |     186    |     224    |    3273 K  |    3273 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      33    |    3231 K  |    3231 K  |
|       from large pool |      10    |      11    |    1269 K  |    1269 K  |
|       from small pool |      14    |      23    |    1962 K  |    1962 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 99, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 100 [256/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 100 [512/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 100 [768/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 100 [1024/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 100 [1280/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 100 [1536/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 100 [1792/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 100 [2048/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 100 [2304/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 100 [2560/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 100 [2816/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 100 [3072/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 100 [3328/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 100 [3584/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 100 [3840/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 100 [4096/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 100 [4352/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 100 [4608/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 100 [4864/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 100 [5120/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 100 [5376/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 100 [5632/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 100 [5888/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 100 [6144/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 100 [6400/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 100 [6656/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 100 [6912/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 100 [7168/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 100 [7424/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 100 [7680/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 100 [7936/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 100 [8192/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 100 [8448/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 100 [8704/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 100 [8960/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 100 [9216/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 100 [9472/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 100 [9728/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 100 [9984/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 100 [10240/50000]	Loss: 4.6007	LR: 0.200000
Training Epoch: 100 [10496/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 100 [10752/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 100 [11008/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 100 [11264/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 100 [11520/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 100 [11776/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 100 [12032/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 100 [12288/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 100 [12544/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 100 [12800/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 100 [13056/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 100 [13312/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 100 [13568/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 100 [13824/50000]	Loss: 4.6220	LR: 0.200000
Training Epoch: 100 [14080/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 100 [14336/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 100 [14592/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 100 [14848/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 100 [15104/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 100 [15360/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 100 [15616/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 100 [15872/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 100 [16128/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 100 [16384/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 100 [16640/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 100 [16896/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 100 [17152/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 100 [17408/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 100 [17664/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 100 [17920/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 100 [18176/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 100 [18432/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 100 [18688/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 100 [18944/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 100 [19200/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 100 [19456/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 100 [19712/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 100 [19968/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 100 [20224/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 100 [20480/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 100 [20736/50000]	Loss: 4.6011	LR: 0.200000
Training Epoch: 100 [20992/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 100 [21248/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 100 [21504/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 100 [21760/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 100 [22016/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 100 [22272/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 100 [22528/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 100 [22784/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 100 [23040/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 100 [23296/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 100 [23552/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 100 [23808/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 100 [24064/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 100 [24320/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 100 [24576/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 100 [24832/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 100 [25088/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 100 [25344/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 100 [25600/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 100 [25856/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 100 [26112/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 100 [26368/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 100 [26624/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 100 [26880/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 100 [27136/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 100 [27392/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 100 [27648/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 100 [27904/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 100 [28160/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 100 [28416/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 100 [28672/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 100 [28928/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 100 [29184/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 100 [29440/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 100 [29696/50000]	Loss: 4.6182	LR: 0.200000
Training Epoch: 100 [29952/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 100 [30208/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 100 [30464/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 100 [30720/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 100 [30976/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 100 [31232/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 100 [31488/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 100 [31744/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 100 [32000/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 100 [32256/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 100 [32512/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 100 [32768/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 100 [33024/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 100 [33280/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 100 [33536/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 100 [33792/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 100 [34048/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 100 [34304/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 100 [34560/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 100 [34816/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 100 [35072/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 100 [35328/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 100 [35584/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 100 [35840/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 100 [36096/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 100 [36352/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 100 [36608/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 100 [36864/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 100 [37120/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 100 [37376/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 100 [37632/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 100 [37888/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 100 [38144/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 100 [38400/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 100 [38656/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 100 [38912/50000]	Loss: 4.6002	LR: 0.200000
Training Epoch: 100 [39168/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 100 [39424/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 100 [39680/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 100 [39936/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 100 [40192/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 100 [40448/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 100 [40704/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 100 [40960/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 100 [41216/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 100 [41472/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 100 [41728/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 100 [41984/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 100 [42240/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 100 [42496/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 100 [42752/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 100 [43008/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 100 [43264/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 100 [43520/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 100 [43776/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 100 [44032/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 100 [44288/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 100 [44544/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 100 [44800/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 100 [45056/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 100 [45312/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 100 [45568/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 100 [45824/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 100 [46080/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 100 [46336/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 100 [46592/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 100 [46848/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 100 [47104/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 100 [47360/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 100 [47616/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 100 [47872/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 100 [48128/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 100 [48384/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 100 [48640/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 100 [48896/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 100 [49152/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 100 [49408/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 100 [49664/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 100 [49920/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 100 [50000/50000]	Loss: 4.6113	LR: 0.200000
epoch 100 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  150019 GB |  150018 GB |
|       from large pool |  400448 KB |    1770 MB |  149884 GB |  149883 GB |
|       from small pool |    3549 KB |       9 MB |     134 GB |     134 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  150019 GB |  150018 GB |
|       from large pool |  400448 KB |    1770 MB |  149884 GB |  149883 GB |
|       from small pool |    3549 KB |       9 MB |     134 GB |     134 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   92304 GB |   92304 GB |
|       from large pool |  244672 KB |  473024 KB |   92150 GB |   92150 GB |
|       from small pool |    2594 KB |    4843 KB |     154 GB |     154 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6402 K  |    6402 K  |
|       from large pool |      36    |      77    |    3095 K  |    3095 K  |
|       from small pool |     186    |     224    |    3306 K  |    3306 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6402 K  |    6402 K  |
|       from large pool |      36    |      77    |    3095 K  |    3095 K  |
|       from small pool |     186    |     224    |    3306 K  |    3306 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3264 K  |    3264 K  |
|       from large pool |      10    |      11    |    1282 K  |    1282 K  |
|       from small pool |      13    |      23    |    1981 K  |    1981 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 100, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-100-regular.pth
Training Epoch: 101 [256/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 101 [512/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 101 [768/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 101 [1024/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 101 [1280/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 101 [1536/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 101 [1792/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 101 [2048/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 101 [2304/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 101 [2560/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 101 [2816/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [3072/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 101 [3328/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 101 [3584/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 101 [3840/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 101 [4096/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 101 [4352/50000]	Loss: 4.5993	LR: 0.200000
Training Epoch: 101 [4608/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [4864/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [5120/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 101 [5376/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 101 [5632/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 101 [5888/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 101 [6144/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 101 [6400/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 101 [6656/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 101 [6912/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 101 [7168/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 101 [7424/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 101 [7680/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 101 [7936/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 101 [8192/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 101 [8448/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 101 [8704/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 101 [8960/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [9216/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 101 [9472/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 101 [9728/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 101 [9984/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 101 [10240/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [10496/50000]	Loss: 4.6179	LR: 0.200000
Training Epoch: 101 [10752/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 101 [11008/50000]	Loss: 4.5998	LR: 0.200000
Training Epoch: 101 [11264/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 101 [11520/50000]	Loss: 4.6160	LR: 0.200000
Training Epoch: 101 [11776/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 101 [12032/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 101 [12288/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 101 [12544/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 101 [12800/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 101 [13056/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [13312/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 101 [13568/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [13824/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 101 [14080/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 101 [14336/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 101 [14592/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 101 [14848/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 101 [15104/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 101 [15360/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 101 [15616/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 101 [15872/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 101 [16128/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 101 [16384/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 101 [16640/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 101 [16896/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 101 [17152/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 101 [17408/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 101 [17664/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 101 [17920/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 101 [18176/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 101 [18432/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 101 [18688/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 101 [18944/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 101 [19200/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [19456/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 101 [19712/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 101 [19968/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 101 [20224/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 101 [20480/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 101 [20736/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 101 [20992/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 101 [21248/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 101 [21504/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 101 [21760/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 101 [22016/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 101 [22272/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 101 [22528/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 101 [22784/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 101 [23040/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 101 [23296/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 101 [23552/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 101 [23808/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 101 [24064/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 101 [24320/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 101 [24576/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 101 [24832/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 101 [25088/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 101 [25344/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 101 [25600/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 101 [25856/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 101 [26112/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 101 [26368/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 101 [26624/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 101 [26880/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 101 [27136/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 101 [27392/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 101 [27648/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 101 [27904/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 101 [28160/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 101 [28416/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 101 [28672/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 101 [28928/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 101 [29184/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 101 [29440/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 101 [29696/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 101 [29952/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 101 [30208/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 101 [30464/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 101 [30720/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 101 [30976/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 101 [31232/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 101 [31488/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 101 [31744/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 101 [32000/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 101 [32256/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 101 [32512/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 101 [32768/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 101 [33024/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 101 [33280/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 101 [33536/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 101 [33792/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 101 [34048/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 101 [34304/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 101 [34560/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 101 [34816/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 101 [35072/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 101 [35328/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 101 [35584/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 101 [35840/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 101 [36096/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [36352/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 101 [36608/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [36864/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 101 [37120/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 101 [37376/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 101 [37632/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 101 [37888/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 101 [38144/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 101 [38400/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 101 [38656/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 101 [38912/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 101 [39168/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 101 [39424/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 101 [39680/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 101 [39936/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [40192/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 101 [40448/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 101 [40704/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 101 [40960/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [41216/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 101 [41472/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 101 [41728/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 101 [41984/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 101 [42240/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 101 [42496/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 101 [42752/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 101 [43008/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 101 [43264/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 101 [43520/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 101 [43776/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 101 [44032/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 101 [44288/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 101 [44544/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 101 [44800/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 101 [45056/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 101 [45312/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 101 [45568/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 101 [45824/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 101 [46080/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 101 [46336/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 101 [46592/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 101 [46848/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 101 [47104/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 101 [47360/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 101 [47616/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 101 [47872/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 101 [48128/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 101 [48384/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 101 [48640/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 101 [48896/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 101 [49152/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 101 [49408/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 101 [49664/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 101 [49920/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 101 [50000/50000]	Loss: 4.6054	LR: 0.200000
epoch 101 training time consumed: 21.71s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  151519 GB |  151518 GB |
|       from large pool |  400448 KB |    1770 MB |  151382 GB |  151382 GB |
|       from small pool |    3549 KB |       9 MB |     136 GB |     136 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  151519 GB |  151518 GB |
|       from large pool |  400448 KB |    1770 MB |  151382 GB |  151382 GB |
|       from small pool |    3549 KB |       9 MB |     136 GB |     136 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   93227 GB |   93227 GB |
|       from large pool |  244672 KB |  473024 KB |   93072 GB |   93071 GB |
|       from small pool |    2594 KB |    4843 KB |     155 GB |     155 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6466 K  |    6466 K  |
|       from large pool |      36    |      77    |    3126 K  |    3126 K  |
|       from small pool |     186    |     224    |    3339 K  |    3339 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6466 K  |    6466 K  |
|       from large pool |      36    |      77    |    3126 K  |    3126 K  |
|       from small pool |     186    |     224    |    3339 K  |    3339 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    3296 K  |    3296 K  |
|       from large pool |      10    |      11    |    1295 K  |    1295 K  |
|       from small pool |      10    |      23    |    2001 K  |    2001 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 101, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 102 [256/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 102 [512/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 102 [768/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 102 [1024/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 102 [1280/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 102 [1536/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 102 [1792/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 102 [2048/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 102 [2304/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 102 [2560/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 102 [2816/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 102 [3072/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 102 [3328/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 102 [3584/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 102 [3840/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 102 [4096/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 102 [4352/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 102 [4608/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 102 [4864/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 102 [5120/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 102 [5376/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 102 [5632/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 102 [5888/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 102 [6144/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 102 [6400/50000]	Loss: 4.5998	LR: 0.200000
Training Epoch: 102 [6656/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 102 [6912/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 102 [7168/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 102 [7424/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 102 [7680/50000]	Loss: 4.5984	LR: 0.200000
Training Epoch: 102 [7936/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 102 [8192/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 102 [8448/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 102 [8704/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 102 [8960/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 102 [9216/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 102 [9472/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 102 [9728/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 102 [9984/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 102 [10240/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 102 [10496/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 102 [10752/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 102 [11008/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 102 [11264/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 102 [11520/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 102 [11776/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 102 [12032/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 102 [12288/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 102 [12544/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 102 [12800/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 102 [13056/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 102 [13312/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 102 [13568/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 102 [13824/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 102 [14080/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 102 [14336/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 102 [14592/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 102 [14848/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 102 [15104/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 102 [15360/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 102 [15616/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 102 [15872/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 102 [16128/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 102 [16384/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 102 [16640/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 102 [16896/50000]	Loss: 4.5981	LR: 0.200000
Training Epoch: 102 [17152/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 102 [17408/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 102 [17664/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 102 [17920/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 102 [18176/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 102 [18432/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 102 [18688/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 102 [18944/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 102 [19200/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 102 [19456/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 102 [19712/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 102 [19968/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 102 [20224/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 102 [20480/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 102 [20736/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 102 [20992/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 102 [21248/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 102 [21504/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 102 [21760/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 102 [22016/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 102 [22272/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 102 [22528/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 102 [22784/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 102 [23040/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 102 [23296/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 102 [23552/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 102 [23808/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 102 [24064/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 102 [24320/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 102 [24576/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 102 [24832/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 102 [25088/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 102 [25344/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 102 [25600/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 102 [25856/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 102 [26112/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 102 [26368/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 102 [26624/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 102 [26880/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 102 [27136/50000]	Loss: 4.6011	LR: 0.200000
Training Epoch: 102 [27392/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 102 [27648/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 102 [27904/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 102 [28160/50000]	Loss: 4.6161	LR: 0.200000
Training Epoch: 102 [28416/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 102 [28672/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 102 [28928/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 102 [29184/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 102 [29440/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 102 [29696/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 102 [29952/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 102 [30208/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 102 [30464/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 102 [30720/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 102 [30976/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 102 [31232/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 102 [31488/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 102 [31744/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 102 [32000/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 102 [32256/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 102 [32512/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 102 [32768/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 102 [33024/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 102 [33280/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 102 [33536/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 102 [33792/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 102 [34048/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 102 [34304/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 102 [34560/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 102 [34816/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 102 [35072/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 102 [35328/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 102 [35584/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 102 [35840/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 102 [36096/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 102 [36352/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 102 [36608/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 102 [36864/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 102 [37120/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 102 [37376/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 102 [37632/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 102 [37888/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 102 [38144/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 102 [38400/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 102 [38656/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 102 [38912/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 102 [39168/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 102 [39424/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 102 [39680/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 102 [39936/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 102 [40192/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 102 [40448/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 102 [40704/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 102 [40960/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 102 [41216/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 102 [41472/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 102 [41728/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 102 [41984/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 102 [42240/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 102 [42496/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 102 [42752/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 102 [43008/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 102 [43264/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 102 [43520/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 102 [43776/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 102 [44032/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 102 [44288/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 102 [44544/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 102 [44800/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 102 [45056/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 102 [45312/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 102 [45568/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 102 [45824/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 102 [46080/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 102 [46336/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 102 [46592/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 102 [46848/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 102 [47104/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 102 [47360/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 102 [47616/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 102 [47872/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 102 [48128/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 102 [48384/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 102 [48640/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 102 [48896/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 102 [49152/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 102 [49408/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 102 [49664/50000]	Loss: 4.6153	LR: 0.200000
Training Epoch: 102 [49920/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 102 [50000/50000]	Loss: 4.6024	LR: 0.200000
epoch 102 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  153019 GB |  153019 GB |
|       from large pool |  400448 KB |    1770 MB |  152881 GB |  152881 GB |
|       from small pool |    3549 KB |       9 MB |     137 GB |     137 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  153019 GB |  153019 GB |
|       from large pool |  400448 KB |    1770 MB |  152881 GB |  152881 GB |
|       from small pool |    3549 KB |       9 MB |     137 GB |     137 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   94150 GB |   94150 GB |
|       from large pool |  244672 KB |  473024 KB |   93993 GB |   93993 GB |
|       from small pool |    2594 KB |    4843 KB |     157 GB |     157 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6530 K  |    6530 K  |
|       from large pool |      36    |      77    |    3157 K  |    3157 K  |
|       from small pool |     186    |     224    |    3373 K  |    3372 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6530 K  |    6530 K  |
|       from large pool |      36    |      77    |    3157 K  |    3157 K  |
|       from small pool |     186    |     224    |    3373 K  |    3372 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3328 K  |    3328 K  |
|       from large pool |      10    |      11    |    1307 K  |    1307 K  |
|       from small pool |      11    |      23    |    2020 K  |    2020 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 102, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 103 [256/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 103 [512/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 103 [768/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 103 [1024/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 103 [1280/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 103 [1536/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 103 [1792/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 103 [2048/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 103 [2304/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 103 [2560/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 103 [2816/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 103 [3072/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 103 [3328/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 103 [3584/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 103 [3840/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 103 [4096/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 103 [4352/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 103 [4608/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 103 [4864/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 103 [5120/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 103 [5376/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 103 [5632/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 103 [5888/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 103 [6144/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 103 [6400/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 103 [6656/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 103 [6912/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 103 [7168/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 103 [7424/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 103 [7680/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 103 [7936/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 103 [8192/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 103 [8448/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 103 [8704/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 103 [8960/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 103 [9216/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 103 [9472/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 103 [9728/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 103 [9984/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 103 [10240/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 103 [10496/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 103 [10752/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 103 [11008/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 103 [11264/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 103 [11520/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 103 [11776/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 103 [12032/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 103 [12288/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 103 [12544/50000]	Loss: 4.6007	LR: 0.200000
Training Epoch: 103 [12800/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 103 [13056/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 103 [13312/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 103 [13568/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 103 [13824/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 103 [14080/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 103 [14336/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 103 [14592/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 103 [14848/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 103 [15104/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 103 [15360/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 103 [15616/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 103 [15872/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 103 [16128/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 103 [16384/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 103 [16640/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 103 [16896/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 103 [17152/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 103 [17408/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 103 [17664/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 103 [17920/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 103 [18176/50000]	Loss: 4.5996	LR: 0.200000
Training Epoch: 103 [18432/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 103 [18688/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 103 [18944/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 103 [19200/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 103 [19456/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 103 [19712/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 103 [19968/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 103 [20224/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 103 [20480/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 103 [20736/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 103 [20992/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 103 [21248/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 103 [21504/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 103 [21760/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 103 [22016/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 103 [22272/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 103 [22528/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 103 [22784/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 103 [23040/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 103 [23296/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 103 [23552/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 103 [23808/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 103 [24064/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 103 [24320/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 103 [24576/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 103 [24832/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 103 [25088/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 103 [25344/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 103 [25600/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 103 [25856/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 103 [26112/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 103 [26368/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 103 [26624/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 103 [26880/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 103 [27136/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 103 [27392/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 103 [27648/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 103 [27904/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 103 [28160/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 103 [28416/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 103 [28672/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 103 [28928/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 103 [29184/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 103 [29440/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 103 [29696/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 103 [29952/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 103 [30208/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 103 [30464/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 103 [30720/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 103 [30976/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 103 [31232/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 103 [31488/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 103 [31744/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 103 [32000/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 103 [32256/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 103 [32512/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 103 [32768/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 103 [33024/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 103 [33280/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 103 [33536/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 103 [33792/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 103 [34048/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 103 [34304/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 103 [34560/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 103 [34816/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 103 [35072/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 103 [35328/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 103 [35584/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 103 [35840/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 103 [36096/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 103 [36352/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 103 [36608/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 103 [36864/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 103 [37120/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 103 [37376/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 103 [37632/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 103 [37888/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 103 [38144/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 103 [38400/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 103 [38656/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 103 [38912/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 103 [39168/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 103 [39424/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 103 [39680/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 103 [39936/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 103 [40192/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 103 [40448/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 103 [40704/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 103 [40960/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 103 [41216/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 103 [41472/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 103 [41728/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 103 [41984/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 103 [42240/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 103 [42496/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 103 [42752/50000]	Loss: 4.6161	LR: 0.200000
Training Epoch: 103 [43008/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 103 [43264/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 103 [43520/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 103 [43776/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 103 [44032/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 103 [44288/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 103 [44544/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 103 [44800/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 103 [45056/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 103 [45312/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 103 [45568/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 103 [45824/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 103 [46080/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 103 [46336/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 103 [46592/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 103 [46848/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 103 [47104/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 103 [47360/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 103 [47616/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 103 [47872/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 103 [48128/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 103 [48384/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 103 [48640/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 103 [48896/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 103 [49152/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 103 [49408/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 103 [49664/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 103 [49920/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 103 [50000/50000]	Loss: 4.6102	LR: 0.200000
epoch 103 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  154519 GB |  154519 GB |
|       from large pool |  400448 KB |    1770 MB |  154380 GB |  154380 GB |
|       from small pool |    3549 KB |       9 MB |     139 GB |     139 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  154519 GB |  154519 GB |
|       from large pool |  400448 KB |    1770 MB |  154380 GB |  154380 GB |
|       from small pool |    3549 KB |       9 MB |     139 GB |     139 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   95074 GB |   95073 GB |
|       from large pool |  244672 KB |  473024 KB |   94915 GB |   94914 GB |
|       from small pool |    2594 KB |    4843 KB |     158 GB |     158 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6594 K  |    6594 K  |
|       from large pool |      36    |      77    |    3188 K  |    3188 K  |
|       from small pool |     186    |     224    |    3406 K  |    3405 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6594 K  |    6594 K  |
|       from large pool |      36    |      77    |    3188 K  |    3188 K  |
|       from small pool |     186    |     224    |    3406 K  |    3405 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    3360 K  |    3360 K  |
|       from large pool |      10    |      11    |    1320 K  |    1320 K  |
|       from small pool |      10    |      23    |    2039 K  |    2039 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 103, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 104 [256/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 104 [512/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 104 [768/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 104 [1024/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 104 [1280/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 104 [1536/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 104 [1792/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 104 [2048/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 104 [2304/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 104 [2560/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 104 [2816/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 104 [3072/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 104 [3328/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 104 [3584/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 104 [3840/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 104 [4096/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 104 [4352/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 104 [4608/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 104 [4864/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 104 [5120/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 104 [5376/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [5632/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 104 [5888/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 104 [6144/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 104 [6400/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 104 [6656/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 104 [6912/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 104 [7168/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 104 [7424/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 104 [7680/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 104 [7936/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 104 [8192/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 104 [8448/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 104 [8704/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 104 [8960/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 104 [9216/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 104 [9472/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 104 [9728/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 104 [9984/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 104 [10240/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 104 [10496/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 104 [10752/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 104 [11008/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 104 [11264/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 104 [11520/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 104 [11776/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 104 [12032/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 104 [12288/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 104 [12544/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 104 [12800/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 104 [13056/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 104 [13312/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 104 [13568/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 104 [13824/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 104 [14080/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 104 [14336/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 104 [14592/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 104 [14848/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 104 [15104/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 104 [15360/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 104 [15616/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 104 [15872/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 104 [16128/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 104 [16384/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 104 [16640/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 104 [16896/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 104 [17152/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 104 [17408/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 104 [17664/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 104 [17920/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [18176/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 104 [18432/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 104 [18688/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 104 [18944/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 104 [19200/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 104 [19456/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [19712/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 104 [19968/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 104 [20224/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 104 [20480/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 104 [20736/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 104 [20992/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 104 [21248/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 104 [21504/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 104 [21760/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 104 [22016/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 104 [22272/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 104 [22528/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 104 [22784/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 104 [23040/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 104 [23296/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 104 [23552/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 104 [23808/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 104 [24064/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 104 [24320/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 104 [24576/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 104 [24832/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 104 [25088/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 104 [25344/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 104 [25600/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 104 [25856/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 104 [26112/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 104 [26368/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 104 [26624/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 104 [26880/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 104 [27136/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 104 [27392/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 104 [27648/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 104 [27904/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 104 [28160/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 104 [28416/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 104 [28672/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 104 [28928/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 104 [29184/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 104 [29440/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 104 [29696/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 104 [29952/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 104 [30208/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 104 [30464/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 104 [30720/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 104 [30976/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 104 [31232/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [31488/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 104 [31744/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 104 [32000/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 104 [32256/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 104 [32512/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 104 [32768/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 104 [33024/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 104 [33280/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 104 [33536/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 104 [33792/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 104 [34048/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 104 [34304/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 104 [34560/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 104 [34816/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 104 [35072/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [35328/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 104 [35584/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 104 [35840/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 104 [36096/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 104 [36352/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 104 [36608/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 104 [36864/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 104 [37120/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 104 [37376/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 104 [37632/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 104 [37888/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 104 [38144/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 104 [38400/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 104 [38656/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 104 [38912/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 104 [39168/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 104 [39424/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 104 [39680/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 104 [39936/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 104 [40192/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 104 [40448/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 104 [40704/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 104 [40960/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 104 [41216/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 104 [41472/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 104 [41728/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 104 [41984/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 104 [42240/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 104 [42496/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 104 [42752/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 104 [43008/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 104 [43264/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 104 [43520/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 104 [43776/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 104 [44032/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 104 [44288/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 104 [44544/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 104 [44800/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 104 [45056/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 104 [45312/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 104 [45568/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 104 [45824/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 104 [46080/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 104 [46336/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [46592/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 104 [46848/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 104 [47104/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 104 [47360/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 104 [47616/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 104 [47872/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 104 [48128/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 104 [48384/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 104 [48640/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 104 [48896/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 104 [49152/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 104 [49408/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 104 [49664/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 104 [49920/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 104 [50000/50000]	Loss: 4.6057	LR: 0.200000
epoch 104 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  156019 GB |  156019 GB |
|       from large pool |  400448 KB |    1770 MB |  155879 GB |  155879 GB |
|       from small pool |    3549 KB |       9 MB |     140 GB |     140 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  156019 GB |  156019 GB |
|       from large pool |  400448 KB |    1770 MB |  155879 GB |  155879 GB |
|       from small pool |    3549 KB |       9 MB |     140 GB |     140 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   95997 GB |   95996 GB |
|       from large pool |  244672 KB |  473024 KB |   95836 GB |   95836 GB |
|       from small pool |    2594 KB |    4843 KB |     160 GB |     160 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6658 K  |    6658 K  |
|       from large pool |      36    |      77    |    3219 K  |    3219 K  |
|       from small pool |     186    |     224    |    3439 K  |    3438 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6658 K  |    6658 K  |
|       from large pool |      36    |      77    |    3219 K  |    3219 K  |
|       from small pool |     186    |     224    |    3439 K  |    3438 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3393 K  |    3393 K  |
|       from large pool |      10    |      11    |    1333 K  |    1333 K  |
|       from small pool |      13    |      23    |    2060 K  |    2060 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 104, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 105 [256/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 105 [512/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 105 [768/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 105 [1024/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 105 [1280/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 105 [1536/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 105 [1792/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 105 [2048/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 105 [2304/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 105 [2560/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 105 [2816/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 105 [3072/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 105 [3328/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 105 [3584/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 105 [3840/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 105 [4096/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 105 [4352/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 105 [4608/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 105 [4864/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 105 [5120/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 105 [5376/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 105 [5632/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 105 [5888/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 105 [6144/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 105 [6400/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 105 [6656/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 105 [6912/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 105 [7168/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 105 [7424/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 105 [7680/50000]	Loss: 4.5981	LR: 0.200000
Training Epoch: 105 [7936/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 105 [8192/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 105 [8448/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 105 [8704/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 105 [8960/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [9216/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 105 [9472/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 105 [9728/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 105 [9984/50000]	Loss: 4.6010	LR: 0.200000
Training Epoch: 105 [10240/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 105 [10496/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 105 [10752/50000]	Loss: 4.6001	LR: 0.200000
Training Epoch: 105 [11008/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 105 [11264/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 105 [11520/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [11776/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 105 [12032/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 105 [12288/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 105 [12544/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 105 [12800/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [13056/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 105 [13312/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 105 [13568/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 105 [13824/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 105 [14080/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 105 [14336/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 105 [14592/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 105 [14848/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 105 [15104/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 105 [15360/50000]	Loss: 4.6007	LR: 0.200000
Training Epoch: 105 [15616/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 105 [15872/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 105 [16128/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 105 [16384/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 105 [16640/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 105 [16896/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 105 [17152/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 105 [17408/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 105 [17664/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 105 [17920/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 105 [18176/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 105 [18432/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 105 [18688/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [18944/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 105 [19200/50000]	Loss: 4.6182	LR: 0.200000
Training Epoch: 105 [19456/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 105 [19712/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 105 [19968/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 105 [20224/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 105 [20480/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 105 [20736/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 105 [20992/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 105 [21248/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 105 [21504/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 105 [21760/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 105 [22016/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 105 [22272/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 105 [22528/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 105 [22784/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 105 [23040/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 105 [23296/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 105 [23552/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 105 [23808/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 105 [24064/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 105 [24320/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 105 [24576/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 105 [24832/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 105 [25088/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 105 [25344/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 105 [25600/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 105 [25856/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 105 [26112/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 105 [26368/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 105 [26624/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 105 [26880/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 105 [27136/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 105 [27392/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 105 [27648/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 105 [27904/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 105 [28160/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 105 [28416/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 105 [28672/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 105 [28928/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 105 [29184/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 105 [29440/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 105 [29696/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 105 [29952/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 105 [30208/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 105 [30464/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 105 [30720/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 105 [30976/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 105 [31232/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 105 [31488/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 105 [31744/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 105 [32000/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 105 [32256/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 105 [32512/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 105 [32768/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 105 [33024/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [33280/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 105 [33536/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 105 [33792/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 105 [34048/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [34304/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 105 [34560/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 105 [34816/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 105 [35072/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 105 [35328/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 105 [35584/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 105 [35840/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 105 [36096/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 105 [36352/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 105 [36608/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 105 [36864/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 105 [37120/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 105 [37376/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 105 [37632/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 105 [37888/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 105 [38144/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 105 [38400/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 105 [38656/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 105 [38912/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 105 [39168/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 105 [39424/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 105 [39680/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 105 [39936/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 105 [40192/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 105 [40448/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 105 [40704/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 105 [40960/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 105 [41216/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 105 [41472/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 105 [41728/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 105 [41984/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 105 [42240/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 105 [42496/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 105 [42752/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 105 [43008/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 105 [43264/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 105 [43520/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 105 [43776/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 105 [44032/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 105 [44288/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 105 [44544/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 105 [44800/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 105 [45056/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 105 [45312/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 105 [45568/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 105 [45824/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 105 [46080/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 105 [46336/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 105 [46592/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 105 [46848/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 105 [47104/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 105 [47360/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 105 [47616/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 105 [47872/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 105 [48128/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 105 [48384/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 105 [48640/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 105 [48896/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 105 [49152/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 105 [49408/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 105 [49664/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 105 [49920/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 105 [50000/50000]	Loss: 4.6083	LR: 0.200000
epoch 105 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  157520 GB |  157519 GB |
|       from large pool |  400448 KB |    1770 MB |  157378 GB |  157377 GB |
|       from small pool |    3549 KB |       9 MB |     141 GB |     141 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  157520 GB |  157519 GB |
|       from large pool |  400448 KB |    1770 MB |  157378 GB |  157377 GB |
|       from small pool |    3549 KB |       9 MB |     141 GB |     141 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   96920 GB |   96919 GB |
|       from large pool |  244672 KB |  473024 KB |   96758 GB |   96757 GB |
|       from small pool |    2594 KB |    4843 KB |     162 GB |     162 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6722 K  |    6722 K  |
|       from large pool |      36    |      77    |    3250 K  |    3250 K  |
|       from small pool |     186    |     224    |    3472 K  |    3472 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6722 K  |    6722 K  |
|       from large pool |      36    |      77    |    3250 K  |    3250 K  |
|       from small pool |     186    |     224    |    3472 K  |    3472 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    3425 K  |    3425 K  |
|       from large pool |      10    |      11    |    1346 K  |    1346 K  |
|       from small pool |      10    |      23    |    2079 K  |    2079 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 105, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 106 [256/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 106 [512/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 106 [768/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 106 [1024/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 106 [1280/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 106 [1536/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 106 [1792/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 106 [2048/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 106 [2304/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 106 [2560/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 106 [2816/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 106 [3072/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 106 [3328/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 106 [3584/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 106 [3840/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 106 [4096/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 106 [4352/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 106 [4608/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 106 [4864/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 106 [5120/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 106 [5376/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 106 [5632/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 106 [5888/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 106 [6144/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 106 [6400/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [6656/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 106 [6912/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 106 [7168/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 106 [7424/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 106 [7680/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 106 [7936/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 106 [8192/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 106 [8448/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 106 [8704/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 106 [8960/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 106 [9216/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 106 [9472/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 106 [9728/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 106 [9984/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 106 [10240/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 106 [10496/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 106 [10752/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 106 [11008/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 106 [11264/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 106 [11520/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 106 [11776/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [12032/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 106 [12288/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 106 [12544/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 106 [12800/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 106 [13056/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 106 [13312/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 106 [13568/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 106 [13824/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 106 [14080/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 106 [14336/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 106 [14592/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 106 [14848/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 106 [15104/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 106 [15360/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 106 [15616/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 106 [15872/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 106 [16128/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 106 [16384/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 106 [16640/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 106 [16896/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [17152/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 106 [17408/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 106 [17664/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [17920/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 106 [18176/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 106 [18432/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 106 [18688/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 106 [18944/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 106 [19200/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 106 [19456/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 106 [19712/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 106 [19968/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [20224/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 106 [20480/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 106 [20736/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 106 [20992/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 106 [21248/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 106 [21504/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 106 [21760/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 106 [22016/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 106 [22272/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 106 [22528/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 106 [22784/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 106 [23040/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 106 [23296/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [23552/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 106 [23808/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 106 [24064/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 106 [24320/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 106 [24576/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 106 [24832/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 106 [25088/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 106 [25344/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 106 [25600/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 106 [25856/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 106 [26112/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 106 [26368/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 106 [26624/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 106 [26880/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [27136/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 106 [27392/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 106 [27648/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 106 [27904/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 106 [28160/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 106 [28416/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 106 [28672/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 106 [28928/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 106 [29184/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 106 [29440/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 106 [29696/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 106 [29952/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 106 [30208/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 106 [30464/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 106 [30720/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 106 [30976/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 106 [31232/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 106 [31488/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 106 [31744/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 106 [32000/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [32256/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 106 [32512/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 106 [32768/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 106 [33024/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 106 [33280/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [33536/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 106 [33792/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 106 [34048/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 106 [34304/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 106 [34560/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 106 [34816/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 106 [35072/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 106 [35328/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 106 [35584/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 106 [35840/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 106 [36096/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 106 [36352/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 106 [36608/50000]	Loss: 4.6155	LR: 0.200000
Training Epoch: 106 [36864/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 106 [37120/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 106 [37376/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 106 [37632/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 106 [37888/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 106 [38144/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 106 [38400/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 106 [38656/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 106 [38912/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 106 [39168/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 106 [39424/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 106 [39680/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 106 [39936/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 106 [40192/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 106 [40448/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 106 [40704/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 106 [40960/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 106 [41216/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 106 [41472/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 106 [41728/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 106 [41984/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 106 [42240/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 106 [42496/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 106 [42752/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 106 [43008/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 106 [43264/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 106 [43520/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 106 [43776/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 106 [44032/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 106 [44288/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 106 [44544/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 106 [44800/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 106 [45056/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 106 [45312/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 106 [45568/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 106 [45824/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 106 [46080/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 106 [46336/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 106 [46592/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 106 [46848/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 106 [47104/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 106 [47360/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 106 [47616/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 106 [47872/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 106 [48128/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 106 [48384/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 106 [48640/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 106 [48896/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 106 [49152/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 106 [49408/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 106 [49664/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 106 [49920/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 106 [50000/50000]	Loss: 4.6013	LR: 0.200000
epoch 106 training time consumed: 21.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  159020 GB |  159019 GB |
|       from large pool |  400448 KB |    1770 MB |  158877 GB |  158876 GB |
|       from small pool |    3549 KB |       9 MB |     143 GB |     143 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  159020 GB |  159019 GB |
|       from large pool |  400448 KB |    1770 MB |  158877 GB |  158876 GB |
|       from small pool |    3549 KB |       9 MB |     143 GB |     143 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   97843 GB |   97842 GB |
|       from large pool |  244672 KB |  473024 KB |   97679 GB |   97679 GB |
|       from small pool |    2594 KB |    4843 KB |     163 GB |     163 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6786 K  |    6786 K  |
|       from large pool |      36    |      77    |    3281 K  |    3281 K  |
|       from small pool |     186    |     224    |    3505 K  |    3505 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6786 K  |    6786 K  |
|       from large pool |      36    |      77    |    3281 K  |    3281 K  |
|       from small pool |     186    |     224    |    3505 K  |    3505 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3458 K  |    3458 K  |
|       from large pool |      10    |      11    |    1359 K  |    1359 K  |
|       from small pool |      13    |      23    |    2099 K  |    2099 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 106, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 107 [256/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 107 [512/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 107 [768/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 107 [1024/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 107 [1280/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 107 [1536/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 107 [1792/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 107 [2048/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 107 [2304/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 107 [2560/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 107 [2816/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 107 [3072/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 107 [3328/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 107 [3584/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 107 [3840/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 107 [4096/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 107 [4352/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 107 [4608/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 107 [4864/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 107 [5120/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 107 [5376/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 107 [5632/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 107 [5888/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 107 [6144/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 107 [6400/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 107 [6656/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 107 [6912/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 107 [7168/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 107 [7424/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 107 [7680/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 107 [7936/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 107 [8192/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 107 [8448/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 107 [8704/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 107 [8960/50000]	Loss: 4.6174	LR: 0.200000
Training Epoch: 107 [9216/50000]	Loss: 4.6169	LR: 0.200000
Training Epoch: 107 [9472/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 107 [9728/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 107 [9984/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 107 [10240/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 107 [10496/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 107 [10752/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 107 [11008/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 107 [11264/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 107 [11520/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 107 [11776/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 107 [12032/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 107 [12288/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 107 [12544/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 107 [12800/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 107 [13056/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 107 [13312/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 107 [13568/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 107 [13824/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 107 [14080/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 107 [14336/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 107 [14592/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 107 [14848/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 107 [15104/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 107 [15360/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 107 [15616/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 107 [15872/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 107 [16128/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 107 [16384/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [16640/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 107 [16896/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 107 [17152/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 107 [17408/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 107 [17664/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 107 [17920/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 107 [18176/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 107 [18432/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 107 [18688/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 107 [18944/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 107 [19200/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 107 [19456/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 107 [19712/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 107 [19968/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 107 [20224/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 107 [20480/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 107 [20736/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 107 [20992/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 107 [21248/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [21504/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 107 [21760/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 107 [22016/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 107 [22272/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 107 [22528/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 107 [22784/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 107 [23040/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 107 [23296/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 107 [23552/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 107 [23808/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 107 [24064/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 107 [24320/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 107 [24576/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 107 [24832/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 107 [25088/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 107 [25344/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 107 [25600/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 107 [25856/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 107 [26112/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 107 [26368/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 107 [26624/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 107 [26880/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 107 [27136/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 107 [27392/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 107 [27648/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 107 [27904/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 107 [28160/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 107 [28416/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [28672/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 107 [28928/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 107 [29184/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 107 [29440/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 107 [29696/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 107 [29952/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 107 [30208/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 107 [30464/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 107 [30720/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 107 [30976/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 107 [31232/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 107 [31488/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 107 [31744/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 107 [32000/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 107 [32256/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 107 [32512/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 107 [32768/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 107 [33024/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 107 [33280/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 107 [33536/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 107 [33792/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 107 [34048/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 107 [34304/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 107 [34560/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 107 [34816/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 107 [35072/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 107 [35328/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 107 [35584/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 107 [35840/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 107 [36096/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 107 [36352/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 107 [36608/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 107 [36864/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 107 [37120/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 107 [37376/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 107 [37632/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 107 [37888/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 107 [38144/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 107 [38400/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 107 [38656/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 107 [38912/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 107 [39168/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 107 [39424/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 107 [39680/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 107 [39936/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 107 [40192/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [40448/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 107 [40704/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 107 [40960/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 107 [41216/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 107 [41472/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 107 [41728/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 107 [41984/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 107 [42240/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 107 [42496/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 107 [42752/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [43008/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 107 [43264/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 107 [43520/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 107 [43776/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [44032/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 107 [44288/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 107 [44544/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 107 [44800/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 107 [45056/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 107 [45312/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 107 [45568/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 107 [45824/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 107 [46080/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 107 [46336/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 107 [46592/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [46848/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 107 [47104/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 107 [47360/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 107 [47616/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 107 [47872/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 107 [48128/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 107 [48384/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 107 [48640/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 107 [48896/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 107 [49152/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 107 [49408/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 107 [49664/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 107 [49920/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 107 [50000/50000]	Loss: 4.6116	LR: 0.200000
epoch 107 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  160520 GB |  160520 GB |
|       from large pool |  400448 KB |    1770 MB |  160375 GB |  160375 GB |
|       from small pool |    3549 KB |       9 MB |     144 GB |     144 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  160520 GB |  160520 GB |
|       from large pool |  400448 KB |    1770 MB |  160375 GB |  160375 GB |
|       from small pool |    3549 KB |       9 MB |     144 GB |     144 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   98766 GB |   98765 GB |
|       from large pool |  244672 KB |  473024 KB |   98600 GB |   98600 GB |
|       from small pool |    2594 KB |    4843 KB |     165 GB |     165 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6851 K  |    6850 K  |
|       from large pool |      36    |      77    |    3312 K  |    3312 K  |
|       from small pool |     186    |     224    |    3538 K  |    3538 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6851 K  |    6850 K  |
|       from large pool |      36    |      77    |    3312 K  |    3312 K  |
|       from small pool |     186    |     224    |    3538 K  |    3538 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    3491 K  |    3491 K  |
|       from large pool |      10    |      11    |    1372 K  |    1372 K  |
|       from small pool |      12    |      23    |    2119 K  |    2119 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 107, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 108 [256/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 108 [512/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 108 [768/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 108 [1024/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 108 [1280/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 108 [1536/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 108 [1792/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 108 [2048/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 108 [2304/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 108 [2560/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 108 [2816/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 108 [3072/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 108 [3328/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 108 [3584/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 108 [3840/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 108 [4096/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 108 [4352/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 108 [4608/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 108 [4864/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 108 [5120/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 108 [5376/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 108 [5632/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 108 [5888/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 108 [6144/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 108 [6400/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 108 [6656/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 108 [6912/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 108 [7168/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 108 [7424/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 108 [7680/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 108 [7936/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 108 [8192/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 108 [8448/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 108 [8704/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 108 [8960/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 108 [9216/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 108 [9472/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 108 [9728/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 108 [9984/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 108 [10240/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 108 [10496/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 108 [10752/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 108 [11008/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 108 [11264/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 108 [11520/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 108 [11776/50000]	Loss: 4.6155	LR: 0.200000
Training Epoch: 108 [12032/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 108 [12288/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 108 [12544/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 108 [12800/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 108 [13056/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 108 [13312/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 108 [13568/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 108 [13824/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 108 [14080/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 108 [14336/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 108 [14592/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 108 [14848/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 108 [15104/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 108 [15360/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 108 [15616/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 108 [15872/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 108 [16128/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 108 [16384/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 108 [16640/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 108 [16896/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 108 [17152/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 108 [17408/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 108 [17664/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 108 [17920/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 108 [18176/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 108 [18432/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 108 [18688/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 108 [18944/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 108 [19200/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 108 [19456/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 108 [19712/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 108 [19968/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 108 [20224/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 108 [20480/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 108 [20736/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 108 [20992/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 108 [21248/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 108 [21504/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 108 [21760/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 108 [22016/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 108 [22272/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 108 [22528/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 108 [22784/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 108 [23040/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 108 [23296/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 108 [23552/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 108 [23808/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 108 [24064/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 108 [24320/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 108 [24576/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 108 [24832/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 108 [25088/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 108 [25344/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 108 [25600/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 108 [25856/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 108 [26112/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 108 [26368/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 108 [26624/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 108 [26880/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 108 [27136/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 108 [27392/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 108 [27648/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 108 [27904/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 108 [28160/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 108 [28416/50000]	Loss: 4.6172	LR: 0.200000
Training Epoch: 108 [28672/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 108 [28928/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 108 [29184/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 108 [29440/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 108 [29696/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 108 [29952/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 108 [30208/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 108 [30464/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 108 [30720/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 108 [30976/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 108 [31232/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 108 [31488/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 108 [31744/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 108 [32000/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 108 [32256/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 108 [32512/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 108 [32768/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 108 [33024/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 108 [33280/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 108 [33536/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 108 [33792/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 108 [34048/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 108 [34304/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 108 [34560/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 108 [34816/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 108 [35072/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 108 [35328/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 108 [35584/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 108 [35840/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 108 [36096/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 108 [36352/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 108 [36608/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 108 [36864/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 108 [37120/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 108 [37376/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 108 [37632/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 108 [37888/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 108 [38144/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 108 [38400/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 108 [38656/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 108 [38912/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 108 [39168/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 108 [39424/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 108 [39680/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 108 [39936/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 108 [40192/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 108 [40448/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 108 [40704/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 108 [40960/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 108 [41216/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 108 [41472/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 108 [41728/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 108 [41984/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 108 [42240/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 108 [42496/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 108 [42752/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 108 [43008/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 108 [43264/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 108 [43520/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 108 [43776/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 108 [44032/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 108 [44288/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 108 [44544/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 108 [44800/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 108 [45056/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 108 [45312/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 108 [45568/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 108 [45824/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 108 [46080/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 108 [46336/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 108 [46592/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 108 [46848/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 108 [47104/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 108 [47360/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 108 [47616/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 108 [47872/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 108 [48128/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 108 [48384/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 108 [48640/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 108 [48896/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 108 [49152/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 108 [49408/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 108 [49664/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 108 [49920/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 108 [50000/50000]	Loss: 4.6145	LR: 0.200000
epoch 108 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  162020 GB |  162020 GB |
|       from large pool |  400448 KB |    1770 MB |  161874 GB |  161874 GB |
|       from small pool |    3549 KB |       9 MB |     145 GB |     145 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  162020 GB |  162020 GB |
|       from large pool |  400448 KB |    1770 MB |  161874 GB |  161874 GB |
|       from small pool |    3549 KB |       9 MB |     145 GB |     145 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |   99689 GB |   99688 GB |
|       from large pool |  244672 KB |  473024 KB |   99522 GB |   99522 GB |
|       from small pool |    2594 KB |    4843 KB |     166 GB |     166 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6915 K  |    6914 K  |
|       from large pool |      36    |      77    |    3343 K  |    3343 K  |
|       from small pool |     186    |     224    |    3571 K  |    3571 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6915 K  |    6914 K  |
|       from large pool |      36    |      77    |    3343 K  |    3343 K  |
|       from small pool |     186    |     224    |    3571 K  |    3571 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3524 K  |    3523 K  |
|       from large pool |      10    |      11    |    1384 K  |    1384 K  |
|       from small pool |      11    |      23    |    2139 K  |    2139 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 108, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 109 [256/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 109 [512/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 109 [768/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 109 [1024/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 109 [1280/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 109 [1536/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 109 [1792/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 109 [2048/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 109 [2304/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 109 [2560/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [2816/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 109 [3072/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 109 [3328/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 109 [3584/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 109 [3840/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 109 [4096/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 109 [4352/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 109 [4608/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 109 [4864/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 109 [5120/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 109 [5376/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 109 [5632/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 109 [5888/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 109 [6144/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 109 [6400/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 109 [6656/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 109 [6912/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 109 [7168/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 109 [7424/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 109 [7680/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 109 [7936/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 109 [8192/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 109 [8448/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [8704/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 109 [8960/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 109 [9216/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 109 [9472/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 109 [9728/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 109 [9984/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 109 [10240/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 109 [10496/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 109 [10752/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 109 [11008/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 109 [11264/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 109 [11520/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 109 [11776/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 109 [12032/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 109 [12288/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 109 [12544/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 109 [12800/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 109 [13056/50000]	Loss: 4.6154	LR: 0.200000
Training Epoch: 109 [13312/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 109 [13568/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 109 [13824/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 109 [14080/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 109 [14336/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 109 [14592/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 109 [14848/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 109 [15104/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 109 [15360/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 109 [15616/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 109 [15872/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 109 [16128/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 109 [16384/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 109 [16640/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 109 [16896/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 109 [17152/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 109 [17408/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 109 [17664/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 109 [17920/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 109 [18176/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 109 [18432/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 109 [18688/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 109 [18944/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 109 [19200/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [19456/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 109 [19712/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 109 [19968/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 109 [20224/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 109 [20480/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 109 [20736/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 109 [20992/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 109 [21248/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 109 [21504/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 109 [21760/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 109 [22016/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 109 [22272/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 109 [22528/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 109 [22784/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 109 [23040/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 109 [23296/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 109 [23552/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 109 [23808/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 109 [24064/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 109 [24320/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 109 [24576/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 109 [24832/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 109 [25088/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 109 [25344/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 109 [25600/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 109 [25856/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 109 [26112/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 109 [26368/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 109 [26624/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 109 [26880/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 109 [27136/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 109 [27392/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 109 [27648/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 109 [27904/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 109 [28160/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 109 [28416/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 109 [28672/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 109 [28928/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 109 [29184/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 109 [29440/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 109 [29696/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 109 [29952/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 109 [30208/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 109 [30464/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 109 [30720/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 109 [30976/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 109 [31232/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [31488/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 109 [31744/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 109 [32000/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 109 [32256/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 109 [32512/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 109 [32768/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 109 [33024/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 109 [33280/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 109 [33536/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 109 [33792/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 109 [34048/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 109 [34304/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 109 [34560/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 109 [34816/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 109 [35072/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 109 [35328/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 109 [35584/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 109 [35840/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 109 [36096/50000]	Loss: 4.6016	LR: 0.200000
Training Epoch: 109 [36352/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 109 [36608/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 109 [36864/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 109 [37120/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 109 [37376/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 109 [37632/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 109 [37888/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 109 [38144/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 109 [38400/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 109 [38656/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 109 [38912/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 109 [39168/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 109 [39424/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 109 [39680/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 109 [39936/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 109 [40192/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 109 [40448/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 109 [40704/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 109 [40960/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 109 [41216/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 109 [41472/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 109 [41728/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 109 [41984/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 109 [42240/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [42496/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 109 [42752/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [43008/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 109 [43264/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 109 [43520/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 109 [43776/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 109 [44032/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 109 [44288/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 109 [44544/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 109 [44800/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 109 [45056/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [45312/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 109 [45568/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 109 [45824/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 109 [46080/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 109 [46336/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 109 [46592/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 109 [46848/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 109 [47104/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 109 [47360/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 109 [47616/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 109 [47872/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 109 [48128/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 109 [48384/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 109 [48640/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 109 [48896/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 109 [49152/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 109 [49408/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 109 [49664/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 109 [49920/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 109 [50000/50000]	Loss: 4.6096	LR: 0.200000
epoch 109 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  163520 GB |  163520 GB |
|       from large pool |  400448 KB |    1770 MB |  163373 GB |  163373 GB |
|       from small pool |    3549 KB |       9 MB |     147 GB |     147 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  163520 GB |  163520 GB |
|       from large pool |  400448 KB |    1770 MB |  163373 GB |  163373 GB |
|       from small pool |    3549 KB |       9 MB |     147 GB |     147 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  100612 GB |  100611 GB |
|       from large pool |  244672 KB |  473024 KB |  100443 GB |  100443 GB |
|       from small pool |    2594 KB |    4843 KB |     168 GB |     168 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    6979 K  |    6978 K  |
|       from large pool |      36    |      77    |    3374 K  |    3374 K  |
|       from small pool |     186    |     224    |    3604 K  |    3604 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    6979 K  |    6978 K  |
|       from large pool |      36    |      77    |    3374 K  |    3374 K  |
|       from small pool |     186    |     224    |    3604 K  |    3604 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3556 K  |    3556 K  |
|       from large pool |      10    |      11    |    1397 K  |    1397 K  |
|       from small pool |      11    |      23    |    2159 K  |    2159 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 109, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 110 [256/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 110 [512/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 110 [768/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 110 [1024/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 110 [1280/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 110 [1536/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 110 [1792/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 110 [2048/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 110 [2304/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 110 [2560/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 110 [2816/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 110 [3072/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 110 [3328/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 110 [3584/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 110 [3840/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 110 [4096/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [4352/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 110 [4608/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 110 [4864/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [5120/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 110 [5376/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 110 [5632/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 110 [5888/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 110 [6144/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 110 [6400/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 110 [6656/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 110 [6912/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 110 [7168/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 110 [7424/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 110 [7680/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 110 [7936/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 110 [8192/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 110 [8448/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 110 [8704/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 110 [8960/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 110 [9216/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 110 [9472/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 110 [9728/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 110 [9984/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 110 [10240/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 110 [10496/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 110 [10752/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 110 [11008/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 110 [11264/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 110 [11520/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 110 [11776/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 110 [12032/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 110 [12288/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 110 [12544/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 110 [12800/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 110 [13056/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 110 [13312/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 110 [13568/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 110 [13824/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 110 [14080/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 110 [14336/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 110 [14592/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 110 [14848/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 110 [15104/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 110 [15360/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 110 [15616/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 110 [15872/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 110 [16128/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 110 [16384/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 110 [16640/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 110 [16896/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 110 [17152/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 110 [17408/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 110 [17664/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 110 [17920/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 110 [18176/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 110 [18432/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 110 [18688/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 110 [18944/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 110 [19200/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 110 [19456/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 110 [19712/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 110 [19968/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 110 [20224/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 110 [20480/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 110 [20736/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 110 [20992/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 110 [21248/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 110 [21504/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 110 [21760/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 110 [22016/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 110 [22272/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 110 [22528/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 110 [22784/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 110 [23040/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 110 [23296/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 110 [23552/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 110 [23808/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 110 [24064/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 110 [24320/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 110 [24576/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 110 [24832/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 110 [25088/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 110 [25344/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 110 [25600/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 110 [25856/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 110 [26112/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 110 [26368/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 110 [26624/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 110 [26880/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 110 [27136/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 110 [27392/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 110 [27648/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 110 [27904/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 110 [28160/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 110 [28416/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 110 [28672/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 110 [28928/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 110 [29184/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 110 [29440/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 110 [29696/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 110 [29952/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 110 [30208/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 110 [30464/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 110 [30720/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 110 [30976/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 110 [31232/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 110 [31488/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 110 [31744/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 110 [32000/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 110 [32256/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 110 [32512/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 110 [32768/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 110 [33024/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 110 [33280/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 110 [33536/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 110 [33792/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 110 [34048/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 110 [34304/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 110 [34560/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 110 [34816/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 110 [35072/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 110 [35328/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 110 [35584/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [35840/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 110 [36096/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 110 [36352/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 110 [36608/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 110 [36864/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 110 [37120/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 110 [37376/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 110 [37632/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 110 [37888/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 110 [38144/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 110 [38400/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 110 [38656/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 110 [38912/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 110 [39168/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 110 [39424/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 110 [39680/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 110 [39936/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 110 [40192/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 110 [40448/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 110 [40704/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 110 [40960/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 110 [41216/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 110 [41472/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 110 [41728/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 110 [41984/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 110 [42240/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 110 [42496/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 110 [42752/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 110 [43008/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 110 [43264/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 110 [43520/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [43776/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 110 [44032/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 110 [44288/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 110 [44544/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 110 [44800/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [45056/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 110 [45312/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [45568/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 110 [45824/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 110 [46080/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 110 [46336/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 110 [46592/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 110 [46848/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 110 [47104/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 110 [47360/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 110 [47616/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 110 [47872/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 110 [48128/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 110 [48384/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 110 [48640/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 110 [48896/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 110 [49152/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 110 [49408/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 110 [49664/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 110 [49920/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 110 [50000/50000]	Loss: 4.6106	LR: 0.200000
epoch 110 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  165020 GB |  165020 GB |
|       from large pool |  400448 KB |    1770 MB |  164872 GB |  164872 GB |
|       from small pool |    3549 KB |       9 MB |     148 GB |     148 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  165020 GB |  165020 GB |
|       from large pool |  400448 KB |    1770 MB |  164872 GB |  164872 GB |
|       from small pool |    3549 KB |       9 MB |     148 GB |     148 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  101535 GB |  101534 GB |
|       from large pool |  244672 KB |  473024 KB |  101365 GB |  101365 GB |
|       from small pool |    2594 KB |    4843 KB |     169 GB |     169 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7043 K  |    7042 K  |
|       from large pool |      36    |      77    |    3405 K  |    3405 K  |
|       from small pool |     186    |     224    |    3637 K  |    3637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7043 K  |    7042 K  |
|       from large pool |      36    |      77    |    3405 K  |    3405 K  |
|       from small pool |     186    |     224    |    3637 K  |    3637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    3589 K  |    3589 K  |
|       from large pool |      10    |      11    |    1410 K  |    1410 K  |
|       from small pool |      12    |      23    |    2179 K  |    2179 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 110, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.48s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-110-regular.pth
Training Epoch: 111 [256/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 111 [512/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 111 [768/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 111 [1024/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 111 [1280/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 111 [1536/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 111 [1792/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 111 [2048/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 111 [2304/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 111 [2560/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 111 [2816/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 111 [3072/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 111 [3328/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 111 [3584/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 111 [3840/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 111 [4096/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 111 [4352/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 111 [4608/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 111 [4864/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 111 [5120/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 111 [5376/50000]	Loss: 4.5993	LR: 0.200000
Training Epoch: 111 [5632/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 111 [5888/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 111 [6144/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 111 [6400/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 111 [6656/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 111 [6912/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 111 [7168/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 111 [7424/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 111 [7680/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 111 [7936/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 111 [8192/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 111 [8448/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 111 [8704/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 111 [8960/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 111 [9216/50000]	Loss: 4.5993	LR: 0.200000
Training Epoch: 111 [9472/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 111 [9728/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 111 [9984/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 111 [10240/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 111 [10496/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 111 [10752/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 111 [11008/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 111 [11264/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 111 [11520/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 111 [11776/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 111 [12032/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 111 [12288/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 111 [12544/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 111 [12800/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 111 [13056/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 111 [13312/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 111 [13568/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 111 [13824/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 111 [14080/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 111 [14336/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 111 [14592/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 111 [14848/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 111 [15104/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 111 [15360/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 111 [15616/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 111 [15872/50000]	Loss: 4.6005	LR: 0.200000
Training Epoch: 111 [16128/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 111 [16384/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 111 [16640/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 111 [16896/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 111 [17152/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 111 [17408/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 111 [17664/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 111 [17920/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 111 [18176/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 111 [18432/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 111 [18688/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 111 [18944/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 111 [19200/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 111 [19456/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 111 [19712/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 111 [19968/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [20224/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 111 [20480/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 111 [20736/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 111 [20992/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [21248/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 111 [21504/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 111 [21760/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 111 [22016/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 111 [22272/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 111 [22528/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 111 [22784/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 111 [23040/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 111 [23296/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 111 [23552/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 111 [23808/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 111 [24064/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 111 [24320/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 111 [24576/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 111 [24832/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 111 [25088/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 111 [25344/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 111 [25600/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 111 [25856/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [26112/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 111 [26368/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 111 [26624/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 111 [26880/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 111 [27136/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 111 [27392/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 111 [27648/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 111 [27904/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 111 [28160/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 111 [28416/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [28672/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [28928/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 111 [29184/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 111 [29440/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 111 [29696/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 111 [29952/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 111 [30208/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 111 [30464/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 111 [30720/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 111 [30976/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 111 [31232/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 111 [31488/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 111 [31744/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 111 [32000/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 111 [32256/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 111 [32512/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 111 [32768/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 111 [33024/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 111 [33280/50000]	Loss: 4.6153	LR: 0.200000
Training Epoch: 111 [33536/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 111 [33792/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 111 [34048/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 111 [34304/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 111 [34560/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 111 [34816/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 111 [35072/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 111 [35328/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 111 [35584/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 111 [35840/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 111 [36096/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 111 [36352/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 111 [36608/50000]	Loss: 4.6144	LR: 0.200000
Training Epoch: 111 [36864/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 111 [37120/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 111 [37376/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 111 [37632/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 111 [37888/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [38144/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 111 [38400/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 111 [38656/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 111 [38912/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 111 [39168/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 111 [39424/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 111 [39680/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 111 [39936/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 111 [40192/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 111 [40448/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 111 [40704/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 111 [40960/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 111 [41216/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 111 [41472/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 111 [41728/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 111 [41984/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 111 [42240/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 111 [42496/50000]	Loss: 4.6156	LR: 0.200000
Training Epoch: 111 [42752/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 111 [43008/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 111 [43264/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 111 [43520/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 111 [43776/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 111 [44032/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 111 [44288/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 111 [44544/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 111 [44800/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 111 [45056/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 111 [45312/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 111 [45568/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 111 [45824/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 111 [46080/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 111 [46336/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 111 [46592/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 111 [46848/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 111 [47104/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 111 [47360/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 111 [47616/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 111 [47872/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 111 [48128/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 111 [48384/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 111 [48640/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 111 [48896/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 111 [49152/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 111 [49408/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 111 [49664/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 111 [49920/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 111 [50000/50000]	Loss: 4.6201	LR: 0.200000
epoch 111 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  166521 GB |  166520 GB |
|       from large pool |  400448 KB |    1770 MB |  166371 GB |  166370 GB |
|       from small pool |    3549 KB |       9 MB |     149 GB |     149 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  166521 GB |  166520 GB |
|       from large pool |  400448 KB |    1770 MB |  166371 GB |  166370 GB |
|       from small pool |    3549 KB |       9 MB |     149 GB |     149 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  102458 GB |  102457 GB |
|       from large pool |  244672 KB |  473024 KB |  102286 GB |  102286 GB |
|       from small pool |    2594 KB |    4843 KB |     171 GB |     171 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7107 K  |    7106 K  |
|       from large pool |      36    |      77    |    3436 K  |    3436 K  |
|       from small pool |     186    |     224    |    3670 K  |    3670 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7107 K  |    7106 K  |
|       from large pool |      36    |      77    |    3436 K  |    3436 K  |
|       from small pool |     186    |     224    |    3670 K  |    3670 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      26    |      33    |    3622 K  |    3622 K  |
|       from large pool |      10    |      11    |    1423 K  |    1423 K  |
|       from small pool |      16    |      23    |    2199 K  |    2199 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 111, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 112 [256/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 112 [512/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 112 [768/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 112 [1024/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 112 [1280/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 112 [1536/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 112 [1792/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 112 [2048/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 112 [2304/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 112 [2560/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 112 [2816/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 112 [3072/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 112 [3328/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 112 [3584/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 112 [3840/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 112 [4096/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 112 [4352/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 112 [4608/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 112 [4864/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 112 [5120/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 112 [5376/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 112 [5632/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 112 [5888/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 112 [6144/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 112 [6400/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 112 [6656/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 112 [6912/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 112 [7168/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 112 [7424/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 112 [7680/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 112 [7936/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 112 [8192/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 112 [8448/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 112 [8704/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 112 [8960/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 112 [9216/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 112 [9472/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 112 [9728/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 112 [9984/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 112 [10240/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 112 [10496/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 112 [10752/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 112 [11008/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 112 [11264/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 112 [11520/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 112 [11776/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 112 [12032/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 112 [12288/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 112 [12544/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 112 [12800/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 112 [13056/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 112 [13312/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 112 [13568/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 112 [13824/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 112 [14080/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 112 [14336/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 112 [14592/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 112 [14848/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 112 [15104/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 112 [15360/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 112 [15616/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 112 [15872/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 112 [16128/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 112 [16384/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 112 [16640/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 112 [16896/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 112 [17152/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 112 [17408/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 112 [17664/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 112 [17920/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 112 [18176/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 112 [18432/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 112 [18688/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 112 [18944/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 112 [19200/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 112 [19456/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 112 [19712/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 112 [19968/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 112 [20224/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 112 [20480/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 112 [20736/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 112 [20992/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 112 [21248/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 112 [21504/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 112 [21760/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 112 [22016/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 112 [22272/50000]	Loss: 4.6008	LR: 0.200000
Training Epoch: 112 [22528/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 112 [22784/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 112 [23040/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 112 [23296/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 112 [23552/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 112 [23808/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 112 [24064/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 112 [24320/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 112 [24576/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 112 [24832/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 112 [25088/50000]	Loss: 4.6147	LR: 0.200000
Training Epoch: 112 [25344/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 112 [25600/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 112 [25856/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 112 [26112/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 112 [26368/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 112 [26624/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 112 [26880/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 112 [27136/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 112 [27392/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 112 [27648/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 112 [27904/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 112 [28160/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 112 [28416/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 112 [28672/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 112 [28928/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 112 [29184/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 112 [29440/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 112 [29696/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 112 [29952/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 112 [30208/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 112 [30464/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 112 [30720/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 112 [30976/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 112 [31232/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 112 [31488/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 112 [31744/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 112 [32000/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 112 [32256/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 112 [32512/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 112 [32768/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 112 [33024/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 112 [33280/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 112 [33536/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 112 [33792/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 112 [34048/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 112 [34304/50000]	Loss: 4.6162	LR: 0.200000
Training Epoch: 112 [34560/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 112 [34816/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 112 [35072/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 112 [35328/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 112 [35584/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 112 [35840/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 112 [36096/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 112 [36352/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 112 [36608/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 112 [36864/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 112 [37120/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 112 [37376/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 112 [37632/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 112 [37888/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 112 [38144/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 112 [38400/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 112 [38656/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 112 [38912/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 112 [39168/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 112 [39424/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 112 [39680/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 112 [39936/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 112 [40192/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 112 [40448/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 112 [40704/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 112 [40960/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 112 [41216/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 112 [41472/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 112 [41728/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 112 [41984/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 112 [42240/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 112 [42496/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 112 [42752/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 112 [43008/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 112 [43264/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 112 [43520/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 112 [43776/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 112 [44032/50000]	Loss: 4.6159	LR: 0.200000
Training Epoch: 112 [44288/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 112 [44544/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 112 [44800/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 112 [45056/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 112 [45312/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 112 [45568/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 112 [45824/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 112 [46080/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 112 [46336/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 112 [46592/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 112 [46848/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 112 [47104/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 112 [47360/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 112 [47616/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 112 [47872/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 112 [48128/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 112 [48384/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 112 [48640/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 112 [48896/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 112 [49152/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 112 [49408/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 112 [49664/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 112 [49920/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 112 [50000/50000]	Loss: 4.6197	LR: 0.200000
epoch 112 training time consumed: 21.72s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  168021 GB |  168020 GB |
|       from large pool |  400448 KB |    1770 MB |  167870 GB |  167869 GB |
|       from small pool |    3549 KB |       9 MB |     151 GB |     151 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  168021 GB |  168020 GB |
|       from large pool |  400448 KB |    1770 MB |  167870 GB |  167869 GB |
|       from small pool |    3549 KB |       9 MB |     151 GB |     151 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  103381 GB |  103381 GB |
|       from large pool |  244672 KB |  473024 KB |  103208 GB |  103208 GB |
|       from small pool |    2594 KB |    4843 KB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7171 K  |    7170 K  |
|       from large pool |      36    |      77    |    3467 K  |    3467 K  |
|       from small pool |     186    |     224    |    3703 K  |    3703 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7171 K  |    7170 K  |
|       from large pool |      36    |      77    |    3467 K  |    3467 K  |
|       from small pool |     186    |     224    |    3703 K  |    3703 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3655 K  |    3655 K  |
|       from large pool |      10    |      11    |    1436 K  |    1436 K  |
|       from small pool |      11    |      23    |    2219 K  |    2219 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 112, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 113 [256/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 113 [512/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 113 [768/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 113 [1024/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 113 [1280/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 113 [1536/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 113 [1792/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 113 [2048/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 113 [2304/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 113 [2560/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 113 [2816/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 113 [3072/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 113 [3328/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 113 [3584/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 113 [3840/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 113 [4096/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 113 [4352/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 113 [4608/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 113 [4864/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 113 [5120/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 113 [5376/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 113 [5632/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 113 [5888/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 113 [6144/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [6400/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 113 [6656/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 113 [6912/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 113 [7168/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 113 [7424/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 113 [7680/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 113 [7936/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 113 [8192/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 113 [8448/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 113 [8704/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 113 [8960/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 113 [9216/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 113 [9472/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 113 [9728/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 113 [9984/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 113 [10240/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [10496/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 113 [10752/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 113 [11008/50000]	Loss: 4.6009	LR: 0.200000
Training Epoch: 113 [11264/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 113 [11520/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 113 [11776/50000]	Loss: 4.5985	LR: 0.200000
Training Epoch: 113 [12032/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 113 [12288/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 113 [12544/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 113 [12800/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 113 [13056/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 113 [13312/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 113 [13568/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 113 [13824/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 113 [14080/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 113 [14336/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 113 [14592/50000]	Loss: 4.6151	LR: 0.200000
Training Epoch: 113 [14848/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 113 [15104/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 113 [15360/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 113 [15616/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 113 [15872/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 113 [16128/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 113 [16384/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 113 [16640/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 113 [16896/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 113 [17152/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 113 [17408/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 113 [17664/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 113 [17920/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 113 [18176/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [18432/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 113 [18688/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 113 [18944/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 113 [19200/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 113 [19456/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 113 [19712/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 113 [19968/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 113 [20224/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 113 [20480/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 113 [20736/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 113 [20992/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 113 [21248/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 113 [21504/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 113 [21760/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 113 [22016/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 113 [22272/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 113 [22528/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 113 [22784/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 113 [23040/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 113 [23296/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 113 [23552/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 113 [23808/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 113 [24064/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 113 [24320/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 113 [24576/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 113 [24832/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 113 [25088/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 113 [25344/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 113 [25600/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 113 [25856/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 113 [26112/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 113 [26368/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 113 [26624/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 113 [26880/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 113 [27136/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 113 [27392/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 113 [27648/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 113 [27904/50000]	Loss: 4.6146	LR: 0.200000
Training Epoch: 113 [28160/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 113 [28416/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 113 [28672/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 113 [28928/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 113 [29184/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 113 [29440/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 113 [29696/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 113 [29952/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 113 [30208/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 113 [30464/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 113 [30720/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 113 [30976/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 113 [31232/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 113 [31488/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 113 [31744/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 113 [32000/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 113 [32256/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 113 [32512/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 113 [32768/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 113 [33024/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 113 [33280/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 113 [33536/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 113 [33792/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 113 [34048/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 113 [34304/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 113 [34560/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 113 [34816/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [35072/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 113 [35328/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 113 [35584/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 113 [35840/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 113 [36096/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 113 [36352/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 113 [36608/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 113 [36864/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 113 [37120/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 113 [37376/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 113 [37632/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 113 [37888/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 113 [38144/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 113 [38400/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [38656/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 113 [38912/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 113 [39168/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 113 [39424/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 113 [39680/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 113 [39936/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 113 [40192/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 113 [40448/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 113 [40704/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 113 [40960/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 113 [41216/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 113 [41472/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [41728/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 113 [41984/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 113 [42240/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 113 [42496/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 113 [42752/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 113 [43008/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 113 [43264/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 113 [43520/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 113 [43776/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 113 [44032/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 113 [44288/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 113 [44544/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 113 [44800/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 113 [45056/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 113 [45312/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 113 [45568/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 113 [45824/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 113 [46080/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 113 [46336/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 113 [46592/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 113 [46848/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 113 [47104/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 113 [47360/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 113 [47616/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 113 [47872/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 113 [48128/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 113 [48384/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 113 [48640/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 113 [48896/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 113 [49152/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 113 [49408/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 113 [49664/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 113 [49920/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 113 [50000/50000]	Loss: 4.6141	LR: 0.200000
epoch 113 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  169521 GB |  169521 GB |
|       from large pool |  400448 KB |    1770 MB |  169368 GB |  169368 GB |
|       from small pool |    3549 KB |       9 MB |     152 GB |     152 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  169521 GB |  169521 GB |
|       from large pool |  400448 KB |    1770 MB |  169368 GB |  169368 GB |
|       from small pool |    3549 KB |       9 MB |     152 GB |     152 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  249314 KB |  477764 KB |  104304 GB |  104304 GB |
|       from large pool |  244672 KB |  473024 KB |  104129 GB |  104129 GB |
|       from small pool |    4642 KB |    4843 KB |     174 GB |     174 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7235 K  |    7234 K  |
|       from large pool |      36    |      77    |    3498 K  |    3498 K  |
|       from small pool |     186    |     224    |    3736 K  |    3736 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7235 K  |    7234 K  |
|       from large pool |      36    |      77    |    3498 K  |    3498 K  |
|       from small pool |     186    |     224    |    3736 K  |    3736 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      33    |    3688 K  |    3688 K  |
|       from large pool |      10    |      11    |    1449 K  |    1449 K  |
|       from small pool |      14    |      23    |    2239 K  |    2239 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 113, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 114 [256/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 114 [512/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 114 [768/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 114 [1024/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 114 [1280/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 114 [1536/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 114 [1792/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 114 [2048/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 114 [2304/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 114 [2560/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 114 [2816/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 114 [3072/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 114 [3328/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 114 [3584/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 114 [3840/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 114 [4096/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 114 [4352/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 114 [4608/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 114 [4864/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 114 [5120/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 114 [5376/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 114 [5632/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 114 [5888/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 114 [6144/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 114 [6400/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 114 [6656/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 114 [6912/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 114 [7168/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 114 [7424/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 114 [7680/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 114 [7936/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 114 [8192/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 114 [8448/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 114 [8704/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 114 [8960/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 114 [9216/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 114 [9472/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 114 [9728/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 114 [9984/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 114 [10240/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 114 [10496/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 114 [10752/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 114 [11008/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 114 [11264/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 114 [11520/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 114 [11776/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 114 [12032/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 114 [12288/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 114 [12544/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 114 [12800/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 114 [13056/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 114 [13312/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 114 [13568/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 114 [13824/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 114 [14080/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 114 [14336/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 114 [14592/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 114 [14848/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 114 [15104/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 114 [15360/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 114 [15616/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 114 [15872/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 114 [16128/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 114 [16384/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 114 [16640/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 114 [16896/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 114 [17152/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 114 [17408/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 114 [17664/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 114 [17920/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 114 [18176/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 114 [18432/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 114 [18688/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 114 [18944/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 114 [19200/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 114 [19456/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 114 [19712/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 114 [19968/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 114 [20224/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 114 [20480/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 114 [20736/50000]	Loss: 4.6164	LR: 0.200000
Training Epoch: 114 [20992/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 114 [21248/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 114 [21504/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 114 [21760/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 114 [22016/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 114 [22272/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 114 [22528/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 114 [22784/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 114 [23040/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 114 [23296/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 114 [23552/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 114 [23808/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 114 [24064/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 114 [24320/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 114 [24576/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 114 [24832/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 114 [25088/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 114 [25344/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 114 [25600/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 114 [25856/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 114 [26112/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 114 [26368/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 114 [26624/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 114 [26880/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 114 [27136/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 114 [27392/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 114 [27648/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 114 [27904/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 114 [28160/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 114 [28416/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 114 [28672/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 114 [28928/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 114 [29184/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 114 [29440/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 114 [29696/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 114 [29952/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 114 [30208/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 114 [30464/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 114 [30720/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 114 [30976/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 114 [31232/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 114 [31488/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 114 [31744/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 114 [32000/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 114 [32256/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 114 [32512/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 114 [32768/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 114 [33024/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 114 [33280/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 114 [33536/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 114 [33792/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 114 [34048/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 114 [34304/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 114 [34560/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 114 [34816/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 114 [35072/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 114 [35328/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 114 [35584/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 114 [35840/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 114 [36096/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 114 [36352/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 114 [36608/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 114 [36864/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 114 [37120/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 114 [37376/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 114 [37632/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 114 [37888/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 114 [38144/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 114 [38400/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 114 [38656/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 114 [38912/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 114 [39168/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 114 [39424/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 114 [39680/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 114 [39936/50000]	Loss: 4.6025	LR: 0.200000
Training Epoch: 114 [40192/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 114 [40448/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 114 [40704/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 114 [40960/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 114 [41216/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 114 [41472/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 114 [41728/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 114 [41984/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 114 [42240/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 114 [42496/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 114 [42752/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 114 [43008/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 114 [43264/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 114 [43520/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 114 [43776/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 114 [44032/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 114 [44288/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 114 [44544/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 114 [44800/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 114 [45056/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 114 [45312/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 114 [45568/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 114 [45824/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 114 [46080/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 114 [46336/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 114 [46592/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 114 [46848/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 114 [47104/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 114 [47360/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 114 [47616/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 114 [47872/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 114 [48128/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 114 [48384/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 114 [48640/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 114 [48896/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 114 [49152/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 114 [49408/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 114 [49664/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 114 [49920/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 114 [50000/50000]	Loss: 4.6105	LR: 0.200000
epoch 114 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  171021 GB |  171021 GB |
|       from large pool |  400448 KB |    1770 MB |  170867 GB |  170867 GB |
|       from small pool |    3549 KB |       9 MB |     153 GB |     153 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  171021 GB |  171021 GB |
|       from large pool |  400448 KB |    1770 MB |  170867 GB |  170867 GB |
|       from small pool |    3549 KB |       9 MB |     153 GB |     153 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  105227 GB |  105227 GB |
|       from large pool |  244672 KB |  473024 KB |  105051 GB |  105051 GB |
|       from small pool |    2594 KB |    4843 KB |     175 GB |     175 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7299 K  |    7298 K  |
|       from large pool |      36    |      77    |    3529 K  |    3529 K  |
|       from small pool |     186    |     224    |    3769 K  |    3769 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7299 K  |    7298 K  |
|       from large pool |      36    |      77    |    3529 K  |    3529 K  |
|       from small pool |     186    |     224    |    3769 K  |    3769 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    3720 K  |    3720 K  |
|       from large pool |      10    |      11    |    1461 K  |    1461 K  |
|       from small pool |      10    |      23    |    2259 K  |    2259 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 114, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 115 [256/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 115 [512/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 115 [768/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 115 [1024/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 115 [1280/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 115 [1536/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 115 [1792/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 115 [2048/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 115 [2304/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 115 [2560/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 115 [2816/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 115 [3072/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 115 [3328/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 115 [3584/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 115 [3840/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 115 [4096/50000]	Loss: 4.6013	LR: 0.200000
Training Epoch: 115 [4352/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 115 [4608/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 115 [4864/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 115 [5120/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 115 [5376/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 115 [5632/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 115 [5888/50000]	Loss: 4.6153	LR: 0.200000
Training Epoch: 115 [6144/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 115 [6400/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 115 [6656/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 115 [6912/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 115 [7168/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 115 [7424/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 115 [7680/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 115 [7936/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 115 [8192/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 115 [8448/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 115 [8704/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 115 [8960/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 115 [9216/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 115 [9472/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 115 [9728/50000]	Loss: 4.6157	LR: 0.200000
Training Epoch: 115 [9984/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 115 [10240/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 115 [10496/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 115 [10752/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 115 [11008/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 115 [11264/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 115 [11520/50000]	Loss: 4.6139	LR: 0.200000
Training Epoch: 115 [11776/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 115 [12032/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 115 [12288/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 115 [12544/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 115 [12800/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 115 [13056/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 115 [13312/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 115 [13568/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 115 [13824/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 115 [14080/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 115 [14336/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 115 [14592/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 115 [14848/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 115 [15104/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 115 [15360/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 115 [15616/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 115 [15872/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 115 [16128/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 115 [16384/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 115 [16640/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 115 [16896/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 115 [17152/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 115 [17408/50000]	Loss: 4.6137	LR: 0.200000
Training Epoch: 115 [17664/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 115 [17920/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 115 [18176/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 115 [18432/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 115 [18688/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 115 [18944/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 115 [19200/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 115 [19456/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 115 [19712/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 115 [19968/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 115 [20224/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 115 [20480/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 115 [20736/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 115 [20992/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 115 [21248/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 115 [21504/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 115 [21760/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 115 [22016/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 115 [22272/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 115 [22528/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 115 [22784/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 115 [23040/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 115 [23296/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 115 [23552/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 115 [23808/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 115 [24064/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 115 [24320/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 115 [24576/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 115 [24832/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 115 [25088/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 115 [25344/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 115 [25600/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 115 [25856/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 115 [26112/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 115 [26368/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 115 [26624/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 115 [26880/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 115 [27136/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 115 [27392/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 115 [27648/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 115 [27904/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 115 [28160/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 115 [28416/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 115 [28672/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 115 [28928/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 115 [29184/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 115 [29440/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 115 [29696/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 115 [29952/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 115 [30208/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 115 [30464/50000]	Loss: 4.6026	LR: 0.200000
Training Epoch: 115 [30720/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 115 [30976/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 115 [31232/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 115 [31488/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 115 [31744/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 115 [32000/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 115 [32256/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 115 [32512/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 115 [32768/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 115 [33024/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 115 [33280/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 115 [33536/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 115 [33792/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 115 [34048/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 115 [34304/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 115 [34560/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 115 [34816/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 115 [35072/50000]	Loss: 4.6109	LR: 0.200000
Training Epoch: 115 [35328/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 115 [35584/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 115 [35840/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 115 [36096/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 115 [36352/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 115 [36608/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 115 [36864/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 115 [37120/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 115 [37376/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 115 [37632/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 115 [37888/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 115 [38144/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 115 [38400/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 115 [38656/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 115 [38912/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 115 [39168/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 115 [39424/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 115 [39680/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 115 [39936/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 115 [40192/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 115 [40448/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 115 [40704/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 115 [40960/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 115 [41216/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 115 [41472/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 115 [41728/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 115 [41984/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 115 [42240/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 115 [42496/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 115 [42752/50000]	Loss: 4.6115	LR: 0.200000
Training Epoch: 115 [43008/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 115 [43264/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 115 [43520/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 115 [43776/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 115 [44032/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 115 [44288/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 115 [44544/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 115 [44800/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 115 [45056/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 115 [45312/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 115 [45568/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 115 [45824/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 115 [46080/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 115 [46336/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 115 [46592/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 115 [46848/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 115 [47104/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 115 [47360/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 115 [47616/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 115 [47872/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 115 [48128/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 115 [48384/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 115 [48640/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 115 [48896/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 115 [49152/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 115 [49408/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 115 [49664/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 115 [49920/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 115 [50000/50000]	Loss: 4.6076	LR: 0.200000
epoch 115 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  172521 GB |  172521 GB |
|       from large pool |  400448 KB |    1770 MB |  172366 GB |  172366 GB |
|       from small pool |    3549 KB |       9 MB |     155 GB |     155 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  172521 GB |  172521 GB |
|       from large pool |  400448 KB |    1770 MB |  172366 GB |  172366 GB |
|       from small pool |    3549 KB |       9 MB |     155 GB |     155 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  106150 GB |  106150 GB |
|       from large pool |  244672 KB |  473024 KB |  105972 GB |  105972 GB |
|       from small pool |    2594 KB |    4843 KB |     177 GB |     177 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7363 K  |    7362 K  |
|       from large pool |      36    |      77    |    3560 K  |    3560 K  |
|       from small pool |     186    |     224    |    3802 K  |    3802 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7363 K  |    7362 K  |
|       from large pool |      36    |      77    |    3560 K  |    3560 K  |
|       from small pool |     186    |     224    |    3802 K  |    3802 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    3752 K  |    3752 K  |
|       from large pool |      10    |      11    |    1474 K  |    1474 K  |
|       from small pool |       9    |      23    |    2278 K  |    2278 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 115, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 116 [256/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 116 [512/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 116 [768/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 116 [1024/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 116 [1280/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 116 [1536/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 116 [1792/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 116 [2048/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 116 [2304/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 116 [2560/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 116 [2816/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 116 [3072/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 116 [3328/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 116 [3584/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 116 [3840/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 116 [4096/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 116 [4352/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 116 [4608/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 116 [4864/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 116 [5120/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 116 [5376/50000]	Loss: 4.6046	LR: 0.200000
Training Epoch: 116 [5632/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 116 [5888/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 116 [6144/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 116 [6400/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 116 [6656/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 116 [6912/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 116 [7168/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 116 [7424/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 116 [7680/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 116 [7936/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 116 [8192/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 116 [8448/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 116 [8704/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 116 [8960/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 116 [9216/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 116 [9472/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 116 [9728/50000]	Loss: 4.6140	LR: 0.200000
Training Epoch: 116 [9984/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 116 [10240/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 116 [10496/50000]	Loss: 4.6173	LR: 0.200000
Training Epoch: 116 [10752/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 116 [11008/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 116 [11264/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 116 [11520/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 116 [11776/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 116 [12032/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 116 [12288/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 116 [12544/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 116 [12800/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 116 [13056/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 116 [13312/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 116 [13568/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 116 [13824/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 116 [14080/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 116 [14336/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 116 [14592/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 116 [14848/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 116 [15104/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 116 [15360/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 116 [15616/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 116 [15872/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 116 [16128/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 116 [16384/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 116 [16640/50000]	Loss: 4.6171	LR: 0.200000
Training Epoch: 116 [16896/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 116 [17152/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 116 [17408/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 116 [17664/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 116 [17920/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 116 [18176/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 116 [18432/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 116 [18688/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 116 [18944/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 116 [19200/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 116 [19456/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 116 [19712/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 116 [19968/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 116 [20224/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 116 [20480/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 116 [20736/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 116 [20992/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 116 [21248/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 116 [21504/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 116 [21760/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 116 [22016/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 116 [22272/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 116 [22528/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 116 [22784/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 116 [23040/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 116 [23296/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 116 [23552/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 116 [23808/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 116 [24064/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 116 [24320/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 116 [24576/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 116 [24832/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 116 [25088/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 116 [25344/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 116 [25600/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 116 [25856/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 116 [26112/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 116 [26368/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 116 [26624/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 116 [26880/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 116 [27136/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 116 [27392/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 116 [27648/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 116 [27904/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 116 [28160/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 116 [28416/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 116 [28672/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 116 [28928/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 116 [29184/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 116 [29440/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 116 [29696/50000]	Loss: 4.5984	LR: 0.200000
Training Epoch: 116 [29952/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 116 [30208/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 116 [30464/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 116 [30720/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 116 [30976/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 116 [31232/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 116 [31488/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 116 [31744/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 116 [32000/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 116 [32256/50000]	Loss: 4.6142	LR: 0.200000
Training Epoch: 116 [32512/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 116 [32768/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 116 [33024/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 116 [33280/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 116 [33536/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 116 [33792/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 116 [34048/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 116 [34304/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 116 [34560/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 116 [34816/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 116 [35072/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 116 [35328/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 116 [35584/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 116 [35840/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 116 [36096/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 116 [36352/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 116 [36608/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 116 [36864/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 116 [37120/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 116 [37376/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 116 [37632/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 116 [37888/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 116 [38144/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 116 [38400/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 116 [38656/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 116 [38912/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 116 [39168/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 116 [39424/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 116 [39680/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 116 [39936/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 116 [40192/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 116 [40448/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 116 [40704/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 116 [40960/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 116 [41216/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 116 [41472/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 116 [41728/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 116 [41984/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 116 [42240/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 116 [42496/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 116 [42752/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 116 [43008/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 116 [43264/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 116 [43520/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 116 [43776/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 116 [44032/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 116 [44288/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 116 [44544/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 116 [44800/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 116 [45056/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 116 [45312/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 116 [45568/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 116 [45824/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 116 [46080/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 116 [46336/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 116 [46592/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 116 [46848/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 116 [47104/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 116 [47360/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 116 [47616/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 116 [47872/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 116 [48128/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 116 [48384/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 116 [48640/50000]	Loss: 4.6012	LR: 0.200000
Training Epoch: 116 [48896/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 116 [49152/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 116 [49408/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 116 [49664/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 116 [49920/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 116 [50000/50000]	Loss: 4.6041	LR: 0.200000
epoch 116 training time consumed: 21.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  174022 GB |  174021 GB |
|       from large pool |  400448 KB |    1770 MB |  173865 GB |  173865 GB |
|       from small pool |    3549 KB |       9 MB |     156 GB |     156 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  174022 GB |  174021 GB |
|       from large pool |  400448 KB |    1770 MB |  173865 GB |  173865 GB |
|       from small pool |    3549 KB |       9 MB |     156 GB |     156 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  107073 GB |  107073 GB |
|       from large pool |  244672 KB |  473024 KB |  106894 GB |  106894 GB |
|       from small pool |    2594 KB |    4843 KB |     179 GB |     179 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7427 K  |    7427 K  |
|       from large pool |      36    |      77    |    3591 K  |    3591 K  |
|       from small pool |     186    |     224    |    3835 K  |    3835 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7427 K  |    7427 K  |
|       from large pool |      36    |      77    |    3591 K  |    3591 K  |
|       from small pool |     186    |     224    |    3835 K  |    3835 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3785 K  |    3785 K  |
|       from large pool |      10    |      11    |    1487 K  |    1487 K  |
|       from small pool |      13    |      23    |    2298 K  |    2298 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 116, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 117 [256/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 117 [512/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 117 [768/50000]	Loss: 4.6018	LR: 0.200000
Training Epoch: 117 [1024/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 117 [1280/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 117 [1536/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 117 [1792/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 117 [2048/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 117 [2304/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 117 [2560/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 117 [2816/50000]	Loss: 4.5972	LR: 0.200000
Training Epoch: 117 [3072/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 117 [3328/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 117 [3584/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 117 [3840/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 117 [4096/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 117 [4352/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 117 [4608/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 117 [4864/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 117 [5120/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 117 [5376/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 117 [5632/50000]	Loss: 4.6188	LR: 0.200000
Training Epoch: 117 [5888/50000]	Loss: 4.5994	LR: 0.200000
Training Epoch: 117 [6144/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 117 [6400/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 117 [6656/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 117 [6912/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 117 [7168/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 117 [7424/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 117 [7680/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 117 [7936/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 117 [8192/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 117 [8448/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 117 [8704/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 117 [8960/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 117 [9216/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 117 [9472/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 117 [9728/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 117 [9984/50000]	Loss: 4.6163	LR: 0.200000
Training Epoch: 117 [10240/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 117 [10496/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 117 [10752/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 117 [11008/50000]	Loss: 4.6152	LR: 0.200000
Training Epoch: 117 [11264/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 117 [11520/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 117 [11776/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 117 [12032/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 117 [12288/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 117 [12544/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 117 [12800/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 117 [13056/50000]	Loss: 4.6135	LR: 0.200000
Training Epoch: 117 [13312/50000]	Loss: 4.6162	LR: 0.200000
Training Epoch: 117 [13568/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 117 [13824/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 117 [14080/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 117 [14336/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 117 [14592/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 117 [14848/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 117 [15104/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 117 [15360/50000]	Loss: 4.6032	LR: 0.200000
Training Epoch: 117 [15616/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 117 [15872/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 117 [16128/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 117 [16384/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 117 [16640/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 117 [16896/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 117 [17152/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 117 [17408/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 117 [17664/50000]	Loss: 4.6148	LR: 0.200000
Training Epoch: 117 [17920/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 117 [18176/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 117 [18432/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 117 [18688/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 117 [18944/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 117 [19200/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 117 [19456/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 117 [19712/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 117 [19968/50000]	Loss: 4.5985	LR: 0.200000
Training Epoch: 117 [20224/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 117 [20480/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 117 [20736/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 117 [20992/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 117 [21248/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 117 [21504/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 117 [21760/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 117 [22016/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 117 [22272/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 117 [22528/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 117 [22784/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 117 [23040/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 117 [23296/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 117 [23552/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 117 [23808/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 117 [24064/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 117 [24320/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 117 [24576/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 117 [24832/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 117 [25088/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 117 [25344/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 117 [25600/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 117 [25856/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 117 [26112/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 117 [26368/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 117 [26624/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 117 [26880/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 117 [27136/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 117 [27392/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 117 [27648/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 117 [27904/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 117 [28160/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 117 [28416/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 117 [28672/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 117 [28928/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 117 [29184/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 117 [29440/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 117 [29696/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 117 [29952/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 117 [30208/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 117 [30464/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 117 [30720/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 117 [30976/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 117 [31232/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 117 [31488/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 117 [31744/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 117 [32000/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 117 [32256/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 117 [32512/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 117 [32768/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 117 [33024/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 117 [33280/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 117 [33536/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 117 [33792/50000]	Loss: 4.6039	LR: 0.200000
Training Epoch: 117 [34048/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 117 [34304/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 117 [34560/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 117 [34816/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 117 [35072/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 117 [35328/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 117 [35584/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 117 [35840/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 117 [36096/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 117 [36352/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 117 [36608/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 117 [36864/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 117 [37120/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 117 [37376/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 117 [37632/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 117 [37888/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 117 [38144/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 117 [38400/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 117 [38656/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 117 [38912/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 117 [39168/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 117 [39424/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 117 [39680/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 117 [39936/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 117 [40192/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 117 [40448/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 117 [40704/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 117 [40960/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 117 [41216/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 117 [41472/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 117 [41728/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 117 [41984/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 117 [42240/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 117 [42496/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 117 [42752/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 117 [43008/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 117 [43264/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 117 [43520/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 117 [43776/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 117 [44032/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 117 [44288/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 117 [44544/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 117 [44800/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 117 [45056/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 117 [45312/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 117 [45568/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 117 [45824/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 117 [46080/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 117 [46336/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 117 [46592/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 117 [46848/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 117 [47104/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 117 [47360/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 117 [47616/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 117 [47872/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 117 [48128/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 117 [48384/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 117 [48640/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 117 [48896/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 117 [49152/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 117 [49408/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 117 [49664/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 117 [49920/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 117 [50000/50000]	Loss: 4.6030	LR: 0.200000
epoch 117 training time consumed: 21.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  175522 GB |  175521 GB |
|       from large pool |  400448 KB |    1770 MB |  175364 GB |  175363 GB |
|       from small pool |    3549 KB |       9 MB |     157 GB |     157 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  175522 GB |  175521 GB |
|       from large pool |  400448 KB |    1770 MB |  175364 GB |  175363 GB |
|       from small pool |    3549 KB |       9 MB |     157 GB |     157 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  107996 GB |  107996 GB |
|       from large pool |  244672 KB |  473024 KB |  107815 GB |  107815 GB |
|       from small pool |    2594 KB |    4843 KB |     180 GB |     180 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7491 K  |    7491 K  |
|       from large pool |      36    |      77    |    3622 K  |    3622 K  |
|       from small pool |     186    |     224    |    3868 K  |    3868 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7491 K  |    7491 K  |
|       from large pool |      36    |      77    |    3622 K  |    3622 K  |
|       from small pool |     186    |     224    |    3868 K  |    3868 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3817 K  |    3817 K  |
|       from large pool |      10    |      11    |    1500 K  |    1500 K  |
|       from small pool |      11    |      23    |    2317 K  |    2317 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 117, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 118 [256/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 118 [512/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 118 [768/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 118 [1024/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 118 [1280/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 118 [1536/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 118 [1792/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 118 [2048/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 118 [2304/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 118 [2560/50000]	Loss: 4.6002	LR: 0.200000
Training Epoch: 118 [2816/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 118 [3072/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 118 [3328/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 118 [3584/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 118 [3840/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 118 [4096/50000]	Loss: 4.6189	LR: 0.200000
Training Epoch: 118 [4352/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 118 [4608/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 118 [4864/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 118 [5120/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 118 [5376/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 118 [5632/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 118 [5888/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 118 [6144/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 118 [6400/50000]	Loss: 4.6022	LR: 0.200000
Training Epoch: 118 [6656/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 118 [6912/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 118 [7168/50000]	Loss: 4.6023	LR: 0.200000
Training Epoch: 118 [7424/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 118 [7680/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 118 [7936/50000]	Loss: 4.6003	LR: 0.200000
Training Epoch: 118 [8192/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 118 [8448/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 118 [8704/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 118 [8960/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 118 [9216/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 118 [9472/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 118 [9728/50000]	Loss: 4.6169	LR: 0.200000
Training Epoch: 118 [9984/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 118 [10240/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 118 [10496/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 118 [10752/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 118 [11008/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 118 [11264/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 118 [11520/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 118 [11776/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 118 [12032/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 118 [12288/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 118 [12544/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 118 [12800/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 118 [13056/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 118 [13312/50000]	Loss: 4.6007	LR: 0.200000
Training Epoch: 118 [13568/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 118 [13824/50000]	Loss: 4.6106	LR: 0.200000
Training Epoch: 118 [14080/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 118 [14336/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 118 [14592/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 118 [14848/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 118 [15104/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 118 [15360/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 118 [15616/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 118 [15872/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 118 [16128/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 118 [16384/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 118 [16640/50000]	Loss: 4.6074	LR: 0.200000
Training Epoch: 118 [16896/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 118 [17152/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 118 [17408/50000]	Loss: 4.6094	LR: 0.200000
Training Epoch: 118 [17664/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 118 [17920/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 118 [18176/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 118 [18432/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 118 [18688/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 118 [18944/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 118 [19200/50000]	Loss: 4.6145	LR: 0.200000
Training Epoch: 118 [19456/50000]	Loss: 4.6123	LR: 0.200000
Training Epoch: 118 [19712/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 118 [19968/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 118 [20224/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 118 [20480/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 118 [20736/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 118 [20992/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 118 [21248/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 118 [21504/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 118 [21760/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 118 [22016/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 118 [22272/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 118 [22528/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 118 [22784/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 118 [23040/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 118 [23296/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 118 [23552/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 118 [23808/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 118 [24064/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 118 [24320/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 118 [24576/50000]	Loss: 4.6030	LR: 0.200000
Training Epoch: 118 [24832/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 118 [25088/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 118 [25344/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 118 [25600/50000]	Loss: 4.6033	LR: 0.200000
Training Epoch: 118 [25856/50000]	Loss: 4.6096	LR: 0.200000
Training Epoch: 118 [26112/50000]	Loss: 4.6050	LR: 0.200000
Training Epoch: 118 [26368/50000]	Loss: 4.6065	LR: 0.200000
Training Epoch: 118 [26624/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 118 [26880/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 118 [27136/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 118 [27392/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 118 [27648/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 118 [27904/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 118 [28160/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 118 [28416/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 118 [28672/50000]	Loss: 4.6067	LR: 0.200000
Training Epoch: 118 [28928/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 118 [29184/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 118 [29440/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 118 [29696/50000]	Loss: 4.6021	LR: 0.200000
Training Epoch: 118 [29952/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 118 [30208/50000]	Loss: 4.6131	LR: 0.200000
Training Epoch: 118 [30464/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 118 [30720/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 118 [30976/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 118 [31232/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 118 [31488/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 118 [31744/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 118 [32000/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 118 [32256/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 118 [32512/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 118 [32768/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 118 [33024/50000]	Loss: 4.6158	LR: 0.200000
Training Epoch: 118 [33280/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 118 [33536/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 118 [33792/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 118 [34048/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 118 [34304/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 118 [34560/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 118 [34816/50000]	Loss: 4.6150	LR: 0.200000
Training Epoch: 118 [35072/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 118 [35328/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 118 [35584/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 118 [35840/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 118 [36096/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 118 [36352/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 118 [36608/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 118 [36864/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 118 [37120/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 118 [37376/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 118 [37632/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 118 [37888/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 118 [38144/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 118 [38400/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 118 [38656/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 118 [38912/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 118 [39168/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 118 [39424/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 118 [39680/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 118 [39936/50000]	Loss: 4.6124	LR: 0.200000
Training Epoch: 118 [40192/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 118 [40448/50000]	Loss: 4.6164	LR: 0.200000
Training Epoch: 118 [40704/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 118 [40960/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 118 [41216/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 118 [41472/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 118 [41728/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 118 [41984/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 118 [42240/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 118 [42496/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 118 [42752/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 118 [43008/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 118 [43264/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 118 [43520/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 118 [43776/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 118 [44032/50000]	Loss: 4.6122	LR: 0.200000
Training Epoch: 118 [44288/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 118 [44544/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 118 [44800/50000]	Loss: 4.6119	LR: 0.200000
Training Epoch: 118 [45056/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 118 [45312/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 118 [45568/50000]	Loss: 4.6121	LR: 0.200000
Training Epoch: 118 [45824/50000]	Loss: 4.6141	LR: 0.200000
Training Epoch: 118 [46080/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 118 [46336/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 118 [46592/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 118 [46848/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 118 [47104/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 118 [47360/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 118 [47616/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 118 [47872/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 118 [48128/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 118 [48384/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 118 [48640/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 118 [48896/50000]	Loss: 4.6130	LR: 0.200000
Training Epoch: 118 [49152/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 118 [49408/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 118 [49664/50000]	Loss: 4.6125	LR: 0.200000
Training Epoch: 118 [49920/50000]	Loss: 4.6128	LR: 0.200000
Training Epoch: 118 [50000/50000]	Loss: 4.6053	LR: 0.200000
epoch 118 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  177022 GB |  177021 GB |
|       from large pool |  400448 KB |    1770 MB |  176863 GB |  176862 GB |
|       from small pool |    3549 KB |       9 MB |     159 GB |     159 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  177022 GB |  177021 GB |
|       from large pool |  400448 KB |    1770 MB |  176863 GB |  176862 GB |
|       from small pool |    3549 KB |       9 MB |     159 GB |     159 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  108919 GB |  108919 GB |
|       from large pool |  244672 KB |  473024 KB |  108737 GB |  108737 GB |
|       from small pool |    2594 KB |    4843 KB |     182 GB |     182 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7555 K  |    7555 K  |
|       from large pool |      36    |      77    |    3653 K  |    3653 K  |
|       from small pool |     186    |     224    |    3902 K  |    3901 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7555 K  |    7555 K  |
|       from large pool |      36    |      77    |    3653 K  |    3653 K  |
|       from small pool |     186    |     224    |    3902 K  |    3901 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    3851 K  |    3851 K  |
|       from large pool |      10    |      11    |    1513 K  |    1513 K  |
|       from small pool |      13    |      23    |    2338 K  |    2337 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 118, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 119 [256/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 119 [512/50000]	Loss: 4.6040	LR: 0.200000
Training Epoch: 119 [768/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 119 [1024/50000]	Loss: 4.6038	LR: 0.200000
Training Epoch: 119 [1280/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 119 [1536/50000]	Loss: 4.6062	LR: 0.200000
Training Epoch: 119 [1792/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 119 [2048/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 119 [2304/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 119 [2560/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 119 [2816/50000]	Loss: 4.6061	LR: 0.200000
Training Epoch: 119 [3072/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 119 [3328/50000]	Loss: 4.6052	LR: 0.200000
Training Epoch: 119 [3584/50000]	Loss: 4.6035	LR: 0.200000
Training Epoch: 119 [3840/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 119 [4096/50000]	Loss: 4.6028	LR: 0.200000
Training Epoch: 119 [4352/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 119 [4608/50000]	Loss: 4.6045	LR: 0.200000
Training Epoch: 119 [4864/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 119 [5120/50000]	Loss: 4.6006	LR: 0.200000
Training Epoch: 119 [5376/50000]	Loss: 4.6019	LR: 0.200000
Training Epoch: 119 [5632/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 119 [5888/50000]	Loss: 4.6098	LR: 0.200000
Training Epoch: 119 [6144/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 119 [6400/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 119 [6656/50000]	Loss: 4.6104	LR: 0.200000
Training Epoch: 119 [6912/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 119 [7168/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 119 [7424/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 119 [7680/50000]	Loss: 4.6076	LR: 0.200000
Training Epoch: 119 [7936/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 119 [8192/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 119 [8448/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 119 [8704/50000]	Loss: 4.6101	LR: 0.200000
Training Epoch: 119 [8960/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 119 [9216/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 119 [9472/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 119 [9728/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 119 [9984/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 119 [10240/50000]	Loss: 4.6136	LR: 0.200000
Training Epoch: 119 [10496/50000]	Loss: 4.6064	LR: 0.200000
Training Epoch: 119 [10752/50000]	Loss: 4.6102	LR: 0.200000
Training Epoch: 119 [11008/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 119 [11264/50000]	Loss: 4.6017	LR: 0.200000
Training Epoch: 119 [11520/50000]	Loss: 4.6117	LR: 0.200000
Training Epoch: 119 [11776/50000]	Loss: 4.6132	LR: 0.200000
Training Epoch: 119 [12032/50000]	Loss: 4.6154	LR: 0.200000
Training Epoch: 119 [12288/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 119 [12544/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 119 [12800/50000]	Loss: 4.6165	LR: 0.200000
Training Epoch: 119 [13056/50000]	Loss: 4.6036	LR: 0.200000
Training Epoch: 119 [13312/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 119 [13568/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 119 [13824/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 119 [14080/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 119 [14336/50000]	Loss: 4.6043	LR: 0.200000
Training Epoch: 119 [14592/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 119 [14848/50000]	Loss: 4.6073	LR: 0.200000
Training Epoch: 119 [15104/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 119 [15360/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 119 [15616/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 119 [15872/50000]	Loss: 4.6120	LR: 0.200000
Training Epoch: 119 [16128/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 119 [16384/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 119 [16640/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 119 [16896/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 119 [17152/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 119 [17408/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 119 [17664/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 119 [17920/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 119 [18176/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 119 [18432/50000]	Loss: 4.6047	LR: 0.200000
Training Epoch: 119 [18688/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 119 [18944/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 119 [19200/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 119 [19456/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 119 [19712/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 119 [19968/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 119 [20224/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 119 [20480/50000]	Loss: 4.6051	LR: 0.200000
Training Epoch: 119 [20736/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 119 [20992/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 119 [21248/50000]	Loss: 4.6108	LR: 0.200000
Training Epoch: 119 [21504/50000]	Loss: 4.6024	LR: 0.200000
Training Epoch: 119 [21760/50000]	Loss: 4.6134	LR: 0.200000
Training Epoch: 119 [22016/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 119 [22272/50000]	Loss: 4.6126	LR: 0.200000
Training Epoch: 119 [22528/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 119 [22784/50000]	Loss: 4.6066	LR: 0.200000
Training Epoch: 119 [23040/50000]	Loss: 4.6015	LR: 0.200000
Training Epoch: 119 [23296/50000]	Loss: 4.6143	LR: 0.200000
Training Epoch: 119 [23552/50000]	Loss: 4.6075	LR: 0.200000
Training Epoch: 119 [23808/50000]	Loss: 4.6105	LR: 0.200000
Training Epoch: 119 [24064/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 119 [24320/50000]	Loss: 4.6014	LR: 0.200000
Training Epoch: 119 [24576/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 119 [24832/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 119 [25088/50000]	Loss: 4.6149	LR: 0.200000
Training Epoch: 119 [25344/50000]	Loss: 4.6084	LR: 0.200000
Training Epoch: 119 [25600/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 119 [25856/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 119 [26112/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 119 [26368/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 119 [26624/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 119 [26880/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 119 [27136/50000]	Loss: 4.6020	LR: 0.200000
Training Epoch: 119 [27392/50000]	Loss: 4.6027	LR: 0.200000
Training Epoch: 119 [27648/50000]	Loss: 4.6041	LR: 0.200000
Training Epoch: 119 [27904/50000]	Loss: 4.6088	LR: 0.200000
Training Epoch: 119 [28160/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 119 [28416/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 119 [28672/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 119 [28928/50000]	Loss: 4.6093	LR: 0.200000
Training Epoch: 119 [29184/50000]	Loss: 4.6071	LR: 0.200000
Training Epoch: 119 [29440/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 119 [29696/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 119 [29952/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 119 [30208/50000]	Loss: 4.6133	LR: 0.200000
Training Epoch: 119 [30464/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 119 [30720/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 119 [30976/50000]	Loss: 4.6068	LR: 0.200000
Training Epoch: 119 [31232/50000]	Loss: 4.6111	LR: 0.200000
Training Epoch: 119 [31488/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 119 [31744/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 119 [32000/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 119 [32256/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 119 [32512/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 119 [32768/50000]	Loss: 4.6034	LR: 0.200000
Training Epoch: 119 [33024/50000]	Loss: 4.6091	LR: 0.200000
Training Epoch: 119 [33280/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 119 [33536/50000]	Loss: 4.6083	LR: 0.200000
Training Epoch: 119 [33792/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 119 [34048/50000]	Loss: 4.6029	LR: 0.200000
Training Epoch: 119 [34304/50000]	Loss: 4.6099	LR: 0.200000
Training Epoch: 119 [34560/50000]	Loss: 4.6129	LR: 0.200000
Training Epoch: 119 [34816/50000]	Loss: 4.6127	LR: 0.200000
Training Epoch: 119 [35072/50000]	Loss: 4.6072	LR: 0.200000
Training Epoch: 119 [35328/50000]	Loss: 4.6138	LR: 0.200000
Training Epoch: 119 [35584/50000]	Loss: 4.6080	LR: 0.200000
Training Epoch: 119 [35840/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 119 [36096/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 119 [36352/50000]	Loss: 4.6087	LR: 0.200000
Training Epoch: 119 [36608/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 119 [36864/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 119 [37120/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 119 [37376/50000]	Loss: 4.6058	LR: 0.200000
Training Epoch: 119 [37632/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 119 [37888/50000]	Loss: 4.6100	LR: 0.200000
Training Epoch: 119 [38144/50000]	Loss: 4.6053	LR: 0.200000
Training Epoch: 119 [38400/50000]	Loss: 4.6063	LR: 0.200000
Training Epoch: 119 [38656/50000]	Loss: 4.6059	LR: 0.200000
Training Epoch: 119 [38912/50000]	Loss: 4.6116	LR: 0.200000
Training Epoch: 119 [39168/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 119 [39424/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 119 [39680/50000]	Loss: 4.6103	LR: 0.200000
Training Epoch: 119 [39936/50000]	Loss: 4.6042	LR: 0.200000
Training Epoch: 119 [40192/50000]	Loss: 4.6044	LR: 0.200000
Training Epoch: 119 [40448/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 119 [40704/50000]	Loss: 4.6110	LR: 0.200000
Training Epoch: 119 [40960/50000]	Loss: 4.6070	LR: 0.200000
Training Epoch: 119 [41216/50000]	Loss: 4.6107	LR: 0.200000
Training Epoch: 119 [41472/50000]	Loss: 4.6079	LR: 0.200000
Training Epoch: 119 [41728/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 119 [41984/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 119 [42240/50000]	Loss: 4.6060	LR: 0.200000
Training Epoch: 119 [42496/50000]	Loss: 4.6097	LR: 0.200000
Training Epoch: 119 [42752/50000]	Loss: 4.6056	LR: 0.200000
Training Epoch: 119 [43008/50000]	Loss: 4.6048	LR: 0.200000
Training Epoch: 119 [43264/50000]	Loss: 4.6095	LR: 0.200000
Training Epoch: 119 [43520/50000]	Loss: 4.6031	LR: 0.200000
Training Epoch: 119 [43776/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 119 [44032/50000]	Loss: 4.6081	LR: 0.200000
Training Epoch: 119 [44288/50000]	Loss: 4.6086	LR: 0.200000
Training Epoch: 119 [44544/50000]	Loss: 4.6113	LR: 0.200000
Training Epoch: 119 [44800/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 119 [45056/50000]	Loss: 4.6049	LR: 0.200000
Training Epoch: 119 [45312/50000]	Loss: 4.6054	LR: 0.200000
Training Epoch: 119 [45568/50000]	Loss: 4.6037	LR: 0.200000
Training Epoch: 119 [45824/50000]	Loss: 4.6112	LR: 0.200000
Training Epoch: 119 [46080/50000]	Loss: 4.6055	LR: 0.200000
Training Epoch: 119 [46336/50000]	Loss: 4.6069	LR: 0.200000
Training Epoch: 119 [46592/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 119 [46848/50000]	Loss: 4.6092	LR: 0.200000
Training Epoch: 119 [47104/50000]	Loss: 4.6082	LR: 0.200000
Training Epoch: 119 [47360/50000]	Loss: 4.6085	LR: 0.200000
Training Epoch: 119 [47616/50000]	Loss: 4.6089	LR: 0.200000
Training Epoch: 119 [47872/50000]	Loss: 4.6000	LR: 0.200000
Training Epoch: 119 [48128/50000]	Loss: 4.6114	LR: 0.200000
Training Epoch: 119 [48384/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 119 [48640/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 119 [48896/50000]	Loss: 4.6057	LR: 0.200000
Training Epoch: 119 [49152/50000]	Loss: 4.6078	LR: 0.200000
Training Epoch: 119 [49408/50000]	Loss: 4.6077	LR: 0.200000
Training Epoch: 119 [49664/50000]	Loss: 4.6090	LR: 0.200000
Training Epoch: 119 [49920/50000]	Loss: 4.6118	LR: 0.200000
Training Epoch: 119 [50000/50000]	Loss: 4.6089	LR: 0.200000
epoch 119 training time consumed: 21.71s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  178522 GB |  178522 GB |
|       from large pool |  400448 KB |    1770 MB |  178361 GB |  178361 GB |
|       from small pool |    3549 KB |       9 MB |     160 GB |     160 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  178522 GB |  178522 GB |
|       from large pool |  400448 KB |    1770 MB |  178361 GB |  178361 GB |
|       from small pool |    3549 KB |       9 MB |     160 GB |     160 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  109842 GB |  109842 GB |
|       from large pool |  244672 KB |  473024 KB |  109658 GB |  109658 GB |
|       from small pool |    2594 KB |    4843 KB |     183 GB |     183 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7619 K  |    7619 K  |
|       from large pool |      36    |      77    |    3684 K  |    3684 K  |
|       from small pool |     186    |     224    |    3935 K  |    3934 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7619 K  |    7619 K  |
|       from large pool |      36    |      77    |    3684 K  |    3684 K  |
|       from small pool |     186    |     224    |    3935 K  |    3934 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    3883 K  |    3883 K  |
|       from large pool |      10    |      11    |    1525 K  |    1525 K  |
|       from small pool |      11    |      23    |    2357 K  |    2357 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 119, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 120 [256/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 120 [512/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 120 [768/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 120 [1024/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 120 [1280/50000]	Loss: 4.6027	LR: 0.040000
Training Epoch: 120 [1536/50000]	Loss: 4.6117	LR: 0.040000
Training Epoch: 120 [1792/50000]	Loss: 4.6102	LR: 0.040000
Training Epoch: 120 [2048/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [2304/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 120 [2560/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 120 [2816/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [3072/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 120 [3328/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 120 [3584/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 120 [3840/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 120 [4096/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [4352/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 120 [4608/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [4864/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 120 [5120/50000]	Loss: 4.6010	LR: 0.040000
Training Epoch: 120 [5376/50000]	Loss: 4.6096	LR: 0.040000
Training Epoch: 120 [5632/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 120 [5888/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 120 [6144/50000]	Loss: 4.6112	LR: 0.040000
Training Epoch: 120 [6400/50000]	Loss: 4.6024	LR: 0.040000
Training Epoch: 120 [6656/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 120 [6912/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 120 [7168/50000]	Loss: 4.6104	LR: 0.040000
Training Epoch: 120 [7424/50000]	Loss: 4.6019	LR: 0.040000
Training Epoch: 120 [7680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 120 [7936/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 120 [8192/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 120 [8448/50000]	Loss: 4.6018	LR: 0.040000
Training Epoch: 120 [8704/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [8960/50000]	Loss: 4.6097	LR: 0.040000
Training Epoch: 120 [9216/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 120 [9472/50000]	Loss: 4.6089	LR: 0.040000
Training Epoch: 120 [9728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 120 [9984/50000]	Loss: 4.6030	LR: 0.040000
Training Epoch: 120 [10240/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [10496/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 120 [10752/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 120 [11008/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [11264/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 120 [11520/50000]	Loss: 4.6089	LR: 0.040000
Training Epoch: 120 [11776/50000]	Loss: 4.6021	LR: 0.040000
Training Epoch: 120 [12032/50000]	Loss: 4.6087	LR: 0.040000
Training Epoch: 120 [12288/50000]	Loss: 4.6011	LR: 0.040000
Training Epoch: 120 [12544/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 120 [12800/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 120 [13056/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 120 [13312/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 120 [13568/50000]	Loss: 4.6025	LR: 0.040000
Training Epoch: 120 [13824/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [14080/50000]	Loss: 4.6016	LR: 0.040000
Training Epoch: 120 [14336/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 120 [14592/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 120 [14848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 120 [15104/50000]	Loss: 4.6009	LR: 0.040000
Training Epoch: 120 [15360/50000]	Loss: 4.6004	LR: 0.040000
Training Epoch: 120 [15616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 120 [15872/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 120 [16128/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 120 [16384/50000]	Loss: 4.6090	LR: 0.040000
Training Epoch: 120 [16640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 120 [16896/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 120 [17152/50000]	Loss: 4.5983	LR: 0.040000
Training Epoch: 120 [17408/50000]	Loss: 4.6096	LR: 0.040000
Training Epoch: 120 [17664/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [17920/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 120 [18176/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 120 [18432/50000]	Loss: 4.6029	LR: 0.040000
Training Epoch: 120 [18688/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 120 [18944/50000]	Loss: 4.6030	LR: 0.040000
Training Epoch: 120 [19200/50000]	Loss: 4.6094	LR: 0.040000
Training Epoch: 120 [19456/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 120 [19712/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 120 [19968/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 120 [20224/50000]	Loss: 4.6035	LR: 0.040000
Training Epoch: 120 [20480/50000]	Loss: 4.6017	LR: 0.040000
Training Epoch: 120 [20736/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 120 [20992/50000]	Loss: 4.6123	LR: 0.040000
Training Epoch: 120 [21248/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 120 [21504/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 120 [21760/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 120 [22016/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 120 [22272/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 120 [22528/50000]	Loss: 4.6116	LR: 0.040000
Training Epoch: 120 [22784/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 120 [23040/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 120 [23296/50000]	Loss: 4.6087	LR: 0.040000
Training Epoch: 120 [23552/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 120 [23808/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 120 [24064/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 120 [24320/50000]	Loss: 4.6114	LR: 0.040000
Training Epoch: 120 [24576/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 120 [24832/50000]	Loss: 4.6094	LR: 0.040000
Training Epoch: 120 [25088/50000]	Loss: 4.6089	LR: 0.040000
Training Epoch: 120 [25344/50000]	Loss: 4.6092	LR: 0.040000
Training Epoch: 120 [25600/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 120 [25856/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 120 [26112/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 120 [26368/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 120 [26624/50000]	Loss: 4.6094	LR: 0.040000
Training Epoch: 120 [26880/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 120 [27136/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 120 [27392/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 120 [27648/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 120 [27904/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 120 [28160/50000]	Loss: 4.6091	LR: 0.040000
Training Epoch: 120 [28416/50000]	Loss: 4.6104	LR: 0.040000
Training Epoch: 120 [28672/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 120 [28928/50000]	Loss: 4.6089	LR: 0.040000
Training Epoch: 120 [29184/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 120 [29440/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 120 [29696/50000]	Loss: 4.6114	LR: 0.040000
Training Epoch: 120 [29952/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 120 [30208/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 120 [30464/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [30720/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 120 [30976/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 120 [31232/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 120 [31488/50000]	Loss: 4.6105	LR: 0.040000
Training Epoch: 120 [31744/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 120 [32000/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 120 [32256/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 120 [32512/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 120 [32768/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 120 [33024/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 120 [33280/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 120 [33536/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 120 [33792/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 120 [34048/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 120 [34304/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 120 [34560/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 120 [34816/50000]	Loss: 4.6031	LR: 0.040000
Training Epoch: 120 [35072/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 120 [35328/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 120 [35584/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 120 [35840/50000]	Loss: 4.6034	LR: 0.040000
Training Epoch: 120 [36096/50000]	Loss: 4.6105	LR: 0.040000
Training Epoch: 120 [36352/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 120 [36608/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 120 [36864/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 120 [37120/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 120 [37376/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 120 [37632/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 120 [37888/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 120 [38144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [38400/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 120 [38656/50000]	Loss: 4.6121	LR: 0.040000
Training Epoch: 120 [38912/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 120 [39168/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 120 [39424/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 120 [39680/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 120 [39936/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 120 [40192/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [40448/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 120 [40704/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 120 [40960/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 120 [41216/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 120 [41472/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 120 [41728/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 120 [41984/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 120 [42240/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 120 [42496/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 120 [42752/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 120 [43008/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 120 [43264/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 120 [43520/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 120 [43776/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 120 [44032/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 120 [44288/50000]	Loss: 4.6098	LR: 0.040000
Training Epoch: 120 [44544/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 120 [44800/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 120 [45056/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 120 [45312/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 120 [45568/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 120 [45824/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 120 [46080/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 120 [46336/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 120 [46592/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 120 [46848/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 120 [47104/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 120 [47360/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 120 [47616/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 120 [47872/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 120 [48128/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 120 [48384/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 120 [48640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 120 [48896/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 120 [49152/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 120 [49408/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 120 [49664/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 120 [49920/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 120 [50000/50000]	Loss: 4.6025	LR: 0.040000
epoch 120 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  180022 GB |  180022 GB |
|       from large pool |  400448 KB |    1770 MB |  179860 GB |  179860 GB |
|       from small pool |    3549 KB |       9 MB |     161 GB |     161 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  180022 GB |  180022 GB |
|       from large pool |  400448 KB |    1770 MB |  179860 GB |  179860 GB |
|       from small pool |    3549 KB |       9 MB |     161 GB |     161 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  110765 GB |  110765 GB |
|       from large pool |  244672 KB |  473024 KB |  110580 GB |  110580 GB |
|       from small pool |    2594 KB |    4843 KB |     185 GB |     185 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7683 K  |    7683 K  |
|       from large pool |      36    |      77    |    3715 K  |    3715 K  |
|       from small pool |     186    |     224    |    3968 K  |    3967 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7683 K  |    7683 K  |
|       from large pool |      36    |      77    |    3715 K  |    3715 K  |
|       from small pool |     186    |     224    |    3968 K  |    3967 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    3916 K  |    3916 K  |
|       from large pool |      10    |      11    |    1538 K  |    1538 K  |
|       from small pool |      10    |      23    |    2377 K  |    2377 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 120, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-120-regular.pth
Training Epoch: 121 [256/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [512/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [1024/50000]	Loss: 4.6031	LR: 0.040000
Training Epoch: 121 [1280/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 121 [1536/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 121 [1792/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 121 [2048/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 121 [2304/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 121 [2560/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 121 [2816/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [3072/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 121 [3328/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 121 [3584/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [3840/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 121 [4096/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 121 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [4608/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 121 [4864/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [5120/50000]	Loss: 4.6031	LR: 0.040000
Training Epoch: 121 [5376/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [5632/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [5888/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 121 [6144/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 121 [6400/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [6656/50000]	Loss: 4.6029	LR: 0.040000
Training Epoch: 121 [6912/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 121 [7168/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 121 [7424/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 121 [7680/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 121 [7936/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [8192/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 121 [8448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [8704/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 121 [8960/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [9216/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [9472/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 121 [9728/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [9984/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 121 [10240/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 121 [10496/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 121 [10752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [11008/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 121 [11264/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 121 [11520/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [11776/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 121 [12032/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [12288/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [12544/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [12800/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [13056/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 121 [13312/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 121 [13568/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 121 [13824/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [14080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 121 [14336/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [14592/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 121 [14848/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 121 [15104/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 121 [15360/50000]	Loss: 4.6030	LR: 0.040000
Training Epoch: 121 [15616/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [15872/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [16128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [16384/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [16640/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [16896/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 121 [17152/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [17408/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 121 [17664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 121 [17920/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [18176/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 121 [18432/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 121 [18688/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 121 [18944/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 121 [19200/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 121 [19456/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [19712/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [19968/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [20224/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 121 [20480/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [20736/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 121 [20992/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [21248/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 121 [21504/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 121 [21760/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [22016/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [22272/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [22528/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [22784/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [23040/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 121 [23296/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 121 [23552/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [23808/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [24064/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [24320/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [24576/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [24832/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 121 [25088/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 121 [25344/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [25600/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [25856/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [26112/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 121 [26368/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 121 [26624/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [26880/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 121 [27136/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 121 [27392/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 121 [27648/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [27904/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 121 [28160/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [28416/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 121 [28672/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 121 [28928/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [29184/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [29440/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [29696/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 121 [29952/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [30208/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 121 [30464/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 121 [30720/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 121 [30976/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [31232/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 121 [31488/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 121 [31744/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 121 [32000/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [32256/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [32512/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [32768/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 121 [33024/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [33280/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [33536/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 121 [33792/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 121 [34048/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 121 [34304/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 121 [34560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 121 [34816/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [35072/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 121 [35328/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [35584/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [35840/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [36096/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [36352/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [36608/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [36864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [37120/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [37376/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 121 [37632/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [37888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [38144/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [38400/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 121 [38656/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 121 [38912/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [39168/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [39424/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 121 [39680/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 121 [39936/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 121 [40192/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [40448/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [40704/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 121 [40960/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [41216/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 121 [41472/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 121 [41728/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [41984/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [42240/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 121 [42496/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [42752/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 121 [43008/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 121 [43264/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 121 [43520/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 121 [43776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [44032/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 121 [44288/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 121 [44544/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 121 [44800/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 121 [45056/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 121 [45312/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [45568/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 121 [45824/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [46080/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 121 [46336/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 121 [46592/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 121 [46848/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 121 [47104/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [47360/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 121 [47616/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [47872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 121 [48128/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 121 [48384/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 121 [48640/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 121 [48896/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 121 [49152/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 121 [49408/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 121 [49664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 121 [49920/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 121 [50000/50000]	Loss: 4.6061	LR: 0.040000
epoch 121 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  181522 GB |  181522 GB |
|       from large pool |  400448 KB |    1770 MB |  181359 GB |  181359 GB |
|       from small pool |    3549 KB |       9 MB |     163 GB |     163 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  181522 GB |  181522 GB |
|       from large pool |  400448 KB |    1770 MB |  181359 GB |  181359 GB |
|       from small pool |    3549 KB |       9 MB |     163 GB |     163 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  111688 GB |  111688 GB |
|       from large pool |  244672 KB |  473024 KB |  111501 GB |  111501 GB |
|       from small pool |    2594 KB |    4843 KB |     186 GB |     186 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7747 K  |    7747 K  |
|       from large pool |      36    |      77    |    3746 K  |    3746 K  |
|       from small pool |     186    |     224    |    4001 K  |    4001 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7747 K  |    7747 K  |
|       from large pool |      36    |      77    |    3746 K  |    3746 K  |
|       from small pool |     186    |     224    |    4001 K  |    4001 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    3948 K  |    3948 K  |
|       from large pool |      10    |      11    |    1551 K  |    1551 K  |
|       from small pool |      10    |      23    |    2396 K  |    2396 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 121, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-121-best.pth
Training Epoch: 122 [256/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [512/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 122 [768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 122 [1024/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [1280/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 122 [1536/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [1792/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [2048/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 122 [2560/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 122 [2816/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 122 [3072/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 122 [3328/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [3584/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 122 [3840/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 122 [4096/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [4352/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [4608/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [4864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [5120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [5376/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 122 [5632/50000]	Loss: 4.6035	LR: 0.040000
Training Epoch: 122 [5888/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [6144/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 122 [6400/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 122 [6656/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [6912/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 122 [7168/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 122 [7424/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [7680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [7936/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 122 [8192/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 122 [8448/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [8704/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [8960/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [9216/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 122 [9472/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 122 [9728/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 122 [9984/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [10240/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [10496/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 122 [10752/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 122 [11008/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 122 [11264/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [11520/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 122 [11776/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [12032/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 122 [12288/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 122 [12544/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [12800/50000]	Loss: 4.6030	LR: 0.040000
Training Epoch: 122 [13056/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 122 [13312/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [13568/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 122 [13824/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [14080/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 122 [14336/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 122 [14592/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [14848/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 122 [15104/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 122 [15360/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [15616/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 122 [15872/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [16128/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [16384/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 122 [16640/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [16896/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [17152/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [17408/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 122 [17664/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [17920/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [18176/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [18432/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 122 [18688/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 122 [18944/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 122 [19200/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 122 [19456/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 122 [19712/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [19968/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [20224/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [20480/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 122 [20736/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 122 [20992/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 122 [21248/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [21504/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [21760/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 122 [22016/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [22272/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [22528/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [22784/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 122 [23040/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 122 [23296/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 122 [23552/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [23808/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [24064/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 122 [24320/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 122 [24576/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [24832/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [25088/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [25344/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [25600/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 122 [25856/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [26112/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 122 [26368/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 122 [26624/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 122 [26880/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [27136/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [27392/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 122 [27648/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [27904/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 122 [28160/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 122 [28416/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [28672/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [28928/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [29184/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 122 [29440/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [29696/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 122 [29952/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 122 [30208/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [30464/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [30720/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 122 [30976/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 122 [31232/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 122 [31488/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 122 [31744/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [32000/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 122 [32256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [32512/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [32768/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [33024/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [33280/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [33536/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 122 [33792/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 122 [34048/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 122 [34304/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [34560/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 122 [34816/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [35072/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 122 [35328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 122 [35584/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 122 [35840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [36096/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 122 [36352/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 122 [36608/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [36864/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [37120/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [37376/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 122 [37632/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 122 [37888/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 122 [38144/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 122 [38400/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 122 [38656/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 122 [38912/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 122 [39168/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 122 [39424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [39680/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 122 [39936/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [40192/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [40448/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 122 [40704/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 122 [40960/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [41216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [41472/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [41728/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [41984/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [42240/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 122 [42496/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [42752/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 122 [43008/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 122 [43264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [43520/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [43776/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [44032/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [44288/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 122 [44544/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 122 [44800/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [45056/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 122 [45312/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 122 [45568/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [45824/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 122 [46080/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [46336/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [46592/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 122 [46848/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [47104/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 122 [47360/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 122 [47616/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [47872/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 122 [48128/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 122 [48384/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 122 [48640/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 122 [48896/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 122 [49152/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 122 [49408/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 122 [49664/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 122 [49920/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 122 [50000/50000]	Loss: 4.6077	LR: 0.040000
epoch 122 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403997 KB |    1773 MB |  183023 GB |  183022 GB |
|       from large pool |  400448 KB |    1770 MB |  182858 GB |  182858 GB |
|       from small pool |    3549 KB |       9 MB |     164 GB |     164 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403997 KB |    1773 MB |  183023 GB |  183022 GB |
|       from large pool |  400448 KB |    1770 MB |  182858 GB |  182858 GB |
|       from small pool |    3549 KB |       9 MB |     164 GB |     164 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  112611 GB |  112611 GB |
|       from large pool |  244672 KB |  473024 KB |  112423 GB |  112423 GB |
|       from small pool |    2594 KB |    4843 KB |     188 GB |     188 GB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     292    |    7811 K  |    7811 K  |
|       from large pool |      36    |      77    |    3777 K  |    3777 K  |
|       from small pool |     186    |     224    |    4034 K  |    4034 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     292    |    7811 K  |    7811 K  |
|       from large pool |      36    |      77    |    3777 K  |    3777 K  |
|       from small pool |     186    |     224    |    4034 K  |    4034 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    3981 K  |    3981 K  |
|       from large pool |      10    |      11    |    1564 K  |    1564 K  |
|       from small pool |      12    |      23    |    2417 K  |    2417 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 122, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 123 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 123 [512/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [768/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [1024/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 123 [1280/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 123 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [1792/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 123 [2048/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 123 [2304/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [2560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [2816/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [3072/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [3328/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [3584/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [3840/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [4096/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [4352/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [4608/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [4864/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [5120/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [5376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [5632/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 123 [5888/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [6144/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [6400/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [6656/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 123 [6912/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [7168/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [7424/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [7680/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [7936/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [8192/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 123 [8448/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [8704/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [8960/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [9216/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [9472/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [9728/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [9984/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [10240/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 123 [10496/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [10752/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [11008/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [11264/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 123 [11520/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 123 [11776/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [12032/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [12288/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [12544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [12800/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [13056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [13312/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [13568/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [13824/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 123 [14080/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [14336/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 123 [14592/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [14848/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [15104/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [15360/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [15616/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [15872/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [16128/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [16384/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 123 [16640/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [16896/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 123 [17152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [17408/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 123 [17664/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 123 [17920/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [18176/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [18432/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 123 [18688/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 123 [18944/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 123 [19200/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 123 [19456/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [19712/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 123 [19968/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [20224/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [20480/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [20736/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [20992/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [21248/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 123 [21504/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [21760/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [22016/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [22272/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [22528/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [22784/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 123 [23040/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [23296/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [23552/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [23808/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [24064/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [24320/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 123 [24576/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 123 [24832/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [25088/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [25344/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [25600/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 123 [25856/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [26112/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [26368/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [26624/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [26880/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [27136/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [27392/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [27648/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 123 [27904/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [28160/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 123 [28416/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [28672/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [28928/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [29184/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [29440/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [29696/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 123 [29952/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 123 [30208/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 123 [30464/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 123 [30720/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [30976/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 123 [31232/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [31488/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [31744/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 123 [32000/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 123 [32256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 123 [32512/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [32768/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 123 [33024/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [33280/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 123 [33536/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [33792/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [34048/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 123 [34304/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 123 [34560/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [34816/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [35072/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 123 [35328/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [35584/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 123 [35840/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 123 [36096/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 123 [36352/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [36608/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [36864/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 123 [37120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 123 [37376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [37632/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 123 [37888/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 123 [38144/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 123 [38400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [38656/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [38912/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 123 [39168/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 123 [39424/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 123 [39680/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 123 [39936/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [40192/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 123 [40448/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 123 [40704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [40960/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [41216/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [41472/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 123 [41728/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 123 [41984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [42240/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [42496/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [42752/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [43008/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [43264/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [43520/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 123 [43776/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [44032/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [44288/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [44544/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [44800/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 123 [45056/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [45312/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [45568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 123 [45824/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [46080/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [46336/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 123 [46592/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [46848/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 123 [47104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [47360/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 123 [47616/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [47872/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 123 [48128/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [48384/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 123 [48640/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 123 [48896/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 123 [49152/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 123 [49408/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 123 [49664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 123 [49920/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 123 [50000/50000]	Loss: 4.6063	LR: 0.040000
epoch 123 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  184523 GB |  184522 GB |
|       from large pool |  400448 KB |    1770 MB |  184357 GB |  184356 GB |
|       from small pool |    3550 KB |       9 MB |     166 GB |     166 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  184523 GB |  184522 GB |
|       from large pool |  400448 KB |    1770 MB |  184357 GB |  184356 GB |
|       from small pool |    3550 KB |       9 MB |     166 GB |     166 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  113534 GB |  113534 GB |
|       from large pool |  244672 KB |  473024 KB |  113344 GB |  113344 GB |
|       from small pool |    2594 KB |    4843 KB |     189 GB |     189 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    7875 K  |    7875 K  |
|       from large pool |      36    |      77    |    3808 K  |    3807 K  |
|       from small pool |     187    |     225    |    4067 K  |    4067 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    7875 K  |    7875 K  |
|       from large pool |      36    |      77    |    3808 K  |    3807 K  |
|       from small pool |     187    |     225    |    4067 K  |    4067 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    4013 K  |    4013 K  |
|       from large pool |      10    |      11    |    1577 K  |    1577 K  |
|       from small pool |      12    |      23    |    2435 K  |    2435 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 123, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 124 [256/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 124 [512/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [768/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [1024/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 124 [1280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 124 [1536/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [1792/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [2048/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [2304/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [2560/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [2816/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 124 [3072/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 124 [3328/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [3584/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 124 [3840/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 124 [4096/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 124 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [4608/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 124 [4864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [5120/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [5376/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 124 [5888/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 124 [6144/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [6400/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 124 [6656/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [6912/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [7168/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 124 [7424/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 124 [7680/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [7936/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 124 [8192/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [8448/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 124 [8704/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 124 [8960/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 124 [9216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 124 [9472/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [9728/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [9984/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [10240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [10496/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [10752/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [11008/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [11264/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 124 [11520/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [11776/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [12032/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [12288/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [12544/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [12800/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [13056/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [13312/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 124 [13568/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [13824/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [14080/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 124 [14336/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 124 [14592/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [14848/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 124 [15104/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 124 [15360/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [15616/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [15872/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [16128/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [16384/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 124 [16640/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [16896/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 124 [17152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [17408/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [17664/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [17920/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 124 [18176/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 124 [18432/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 124 [18688/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 124 [18944/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 124 [19200/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 124 [19456/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 124 [19712/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 124 [19968/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [20224/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [20480/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 124 [20736/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 124 [20992/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [21248/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 124 [21504/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [21760/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [22016/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [22272/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 124 [22528/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [22784/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 124 [23040/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 124 [23296/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 124 [23552/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [23808/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 124 [24064/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [24320/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 124 [24576/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 124 [24832/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 124 [25088/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 124 [25344/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [25600/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 124 [25856/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [26112/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 124 [26368/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [26624/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [26880/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 124 [27136/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [27392/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 124 [27648/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 124 [27904/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 124 [28160/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 124 [28416/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 124 [28672/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 124 [28928/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [29184/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [29440/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [29696/50000]	Loss: 4.6091	LR: 0.040000
Training Epoch: 124 [29952/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 124 [30208/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [30464/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [30720/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 124 [30976/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 124 [31232/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 124 [31488/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [31744/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [32000/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [32256/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [32512/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [32768/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 124 [33024/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [33280/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [33536/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 124 [33792/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [34048/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 124 [34304/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 124 [34560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [34816/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [35072/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 124 [35328/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [35584/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [35840/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 124 [36096/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 124 [36352/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 124 [36608/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 124 [36864/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [37120/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [37376/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [37632/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 124 [37888/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 124 [38144/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [38400/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 124 [38656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [38912/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 124 [39168/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 124 [39424/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [39680/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [39936/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 124 [40192/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 124 [40448/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 124 [40704/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 124 [40960/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 124 [41216/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 124 [41472/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 124 [41728/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 124 [41984/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [42240/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 124 [42496/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 124 [42752/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 124 [43008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [43264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 124 [43520/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 124 [43776/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [44032/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 124 [44288/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 124 [44544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 124 [44800/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [45056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [45312/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [45568/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 124 [45824/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 124 [46080/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [46336/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [46592/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 124 [46848/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 124 [47104/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 124 [47360/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 124 [47616/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 124 [47872/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [48128/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 124 [48384/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 124 [48640/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 124 [48896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 124 [49152/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 124 [49408/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 124 [49664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [49920/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 124 [50000/50000]	Loss: 4.6057	LR: 0.040000
epoch 124 training time consumed: 21.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  186023 GB |  186023 GB |
|       from large pool |  400448 KB |    1770 MB |  185856 GB |  185855 GB |
|       from small pool |    3550 KB |       9 MB |     167 GB |     167 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  186023 GB |  186023 GB |
|       from large pool |  400448 KB |    1770 MB |  185856 GB |  185855 GB |
|       from small pool |    3550 KB |       9 MB |     167 GB |     167 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  114457 GB |  114457 GB |
|       from large pool |  244672 KB |  473024 KB |  114266 GB |  114265 GB |
|       from small pool |    2594 KB |    4843 KB |     191 GB |     191 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    7939 K  |    7939 K  |
|       from large pool |      36    |      77    |    3838 K  |    3838 K  |
|       from small pool |     187    |     225    |    4100 K  |    4100 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    7939 K  |    7939 K  |
|       from large pool |      36    |      77    |    3838 K  |    3838 K  |
|       from small pool |     187    |     225    |    4100 K  |    4100 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4044 K  |    4044 K  |
|       from large pool |      10    |      11    |    1590 K  |    1590 K  |
|       from small pool |      11    |      23    |    2454 K  |    2454 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 124, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 125 [256/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [512/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 125 [768/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [1024/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 125 [1280/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 125 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [2048/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [2304/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [2560/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [2816/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [3072/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 125 [3328/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [3584/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [3840/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 125 [4096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 125 [4352/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [4608/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [4864/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [5120/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [5376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [5632/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 125 [5888/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 125 [6144/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 125 [6400/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [6656/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [6912/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [7168/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [7424/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [7680/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 125 [7936/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [8192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [8448/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 125 [8704/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [8960/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [9216/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [9472/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [9728/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [9984/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [10240/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [10496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [10752/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [11008/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [11264/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [11520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [11776/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [12032/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [12288/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [12544/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [12800/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [13056/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [13312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 125 [13568/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 125 [13824/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [14080/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [14336/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [14592/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [14848/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [15104/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 125 [15360/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [15616/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 125 [15872/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [16128/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [16384/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 125 [16640/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [16896/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [17152/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [17408/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [17664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [17920/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [18176/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [18432/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [18688/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [18944/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [19200/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [19456/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 125 [19712/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 125 [19968/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 125 [20224/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [20480/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 125 [20736/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [20992/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [21248/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 125 [21504/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [21760/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [22016/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [22272/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [22528/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [22784/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [23040/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [23296/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 125 [23552/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 125 [23808/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 125 [24064/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 125 [24320/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [24576/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 125 [24832/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [25088/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 125 [25344/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [25600/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [25856/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 125 [26112/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 125 [26368/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [26624/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [26880/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [27136/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 125 [27392/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [27648/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [27904/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [28160/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [28416/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 125 [28672/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 125 [28928/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 125 [29184/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [29440/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 125 [29696/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [29952/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 125 [30208/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 125 [30464/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 125 [30720/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [30976/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [31232/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [31488/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 125 [31744/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 125 [32000/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [32256/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 125 [32512/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [32768/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 125 [33024/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 125 [33280/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [33536/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 125 [33792/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [34048/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 125 [34304/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 125 [34560/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 125 [34816/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [35072/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [35328/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [35584/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [35840/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 125 [36096/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [36352/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [36608/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [36864/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [37120/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [37376/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [37632/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [37888/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 125 [38144/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [38400/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [38656/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [38912/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 125 [39168/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [39424/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 125 [39680/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [39936/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [40192/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 125 [40448/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 125 [40704/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 125 [40960/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [41216/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 125 [41472/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 125 [41728/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [41984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [42240/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [42496/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 125 [42752/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 125 [43008/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [43264/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 125 [43520/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [43776/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 125 [44032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [44288/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [44544/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [44800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [45056/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 125 [45312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [45568/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 125 [45824/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 125 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [46336/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [46592/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 125 [46848/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 125 [47104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 125 [47360/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [47616/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 125 [47872/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [48128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 125 [48384/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 125 [48640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 125 [48896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [49152/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 125 [49408/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [49664/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 125 [49920/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 125 [50000/50000]	Loss: 4.6062	LR: 0.040000
epoch 125 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  187523 GB |  187523 GB |
|       from large pool |  400448 KB |    1770 MB |  187354 GB |  187354 GB |
|       from small pool |    3550 KB |       9 MB |     168 GB |     168 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  187523 GB |  187523 GB |
|       from large pool |  400448 KB |    1770 MB |  187354 GB |  187354 GB |
|       from small pool |    3550 KB |       9 MB |     168 GB |     168 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  115380 GB |  115380 GB |
|       from large pool |  244672 KB |  473024 KB |  115187 GB |  115187 GB |
|       from small pool |    2594 KB |    4843 KB |     192 GB |     192 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8003 K  |    8003 K  |
|       from large pool |      36    |      77    |    3869 K  |    3869 K  |
|       from small pool |     187    |     225    |    4133 K  |    4133 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8003 K  |    8003 K  |
|       from large pool |      36    |      77    |    3869 K  |    3869 K  |
|       from small pool |     187    |     225    |    4133 K  |    4133 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4076 K  |    4076 K  |
|       from large pool |      10    |      11    |    1602 K  |    1602 K  |
|       from small pool |      11    |      23    |    2473 K  |    2473 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 125, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 126 [256/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 126 [512/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 126 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 126 [1024/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [1280/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [1536/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [1792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [2048/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 126 [2304/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [2560/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 126 [2816/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [3072/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 126 [3328/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [3584/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 126 [3840/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [4096/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 126 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 126 [4608/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 126 [4864/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 126 [5120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [5376/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 126 [5632/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [5888/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 126 [6400/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [6656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [6912/50000]	Loss: 4.6035	LR: 0.040000
Training Epoch: 126 [7168/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [7424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [7680/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 126 [7936/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [8192/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [8448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [8704/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [8960/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [9216/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [9472/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 126 [9728/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [9984/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [10240/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [10496/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [10752/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [11008/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 126 [11264/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [11520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [11776/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 126 [12032/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 126 [12288/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [12544/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [12800/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 126 [13056/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [13312/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [13568/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 126 [13824/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 126 [14080/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [14336/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 126 [14592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [14848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [15104/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [15360/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 126 [15616/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [15872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [16128/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [16384/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [16640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [16896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [17152/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 126 [17408/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [17664/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 126 [17920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [18176/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 126 [18432/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [18688/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [18944/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 126 [19200/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [19456/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [19712/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [19968/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [20224/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [20480/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [20736/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [20992/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [21248/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [21504/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 126 [21760/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [22016/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [22272/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [22528/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 126 [22784/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 126 [23040/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 126 [23296/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [23552/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [23808/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [24064/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 126 [24320/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 126 [24576/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 126 [24832/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [25088/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 126 [25344/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 126 [25600/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 126 [25856/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 126 [26112/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 126 [26368/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [26624/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [26880/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [27136/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 126 [27392/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 126 [27648/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [27904/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 126 [28160/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [28416/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [28672/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 126 [28928/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [29184/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 126 [29440/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [29696/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [29952/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 126 [30208/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [30464/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [30720/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 126 [30976/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [31232/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 126 [31488/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [31744/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [32000/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [32256/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [32512/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [32768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [33024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [33280/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [33536/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [33792/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 126 [34048/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 126 [34304/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [34560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [34816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [35072/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [35328/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [35584/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [35840/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [36096/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [36352/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [36608/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [36864/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [37120/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [37376/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [37632/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 126 [37888/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 126 [38144/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [38400/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 126 [38656/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [38912/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 126 [39168/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [39424/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [39680/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 126 [39936/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [40192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [40448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [40704/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 126 [40960/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 126 [41216/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [41472/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [41728/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 126 [41984/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 126 [42240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [42496/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 126 [42752/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [43008/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 126 [43264/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 126 [43520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [43776/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 126 [44032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [44288/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [44544/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [44800/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [45056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [45312/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 126 [45568/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [45824/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [46336/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 126 [46592/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [46848/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 126 [47104/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [47360/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 126 [47616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 126 [47872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 126 [48128/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 126 [48384/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 126 [48640/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [48896/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [49152/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 126 [49408/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 126 [49664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 126 [49920/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 126 [50000/50000]	Loss: 4.6064	LR: 0.040000
epoch 126 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  189023 GB |  189023 GB |
|       from large pool |  400448 KB |    1770 MB |  188853 GB |  188853 GB |
|       from small pool |    3550 KB |       9 MB |     170 GB |     170 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  189023 GB |  189023 GB |
|       from large pool |  400448 KB |    1770 MB |  188853 GB |  188853 GB |
|       from small pool |    3550 KB |       9 MB |     170 GB |     170 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  116303 GB |  116303 GB |
|       from large pool |  244672 KB |  473024 KB |  116109 GB |  116108 GB |
|       from small pool |    2594 KB |    4843 KB |     194 GB |     194 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8067 K  |    8067 K  |
|       from large pool |      36    |      77    |    3900 K  |    3900 K  |
|       from small pool |     187    |     225    |    4166 K  |    4166 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8067 K  |    8067 K  |
|       from large pool |      36    |      77    |    3900 K  |    3900 K  |
|       from small pool |     187    |     225    |    4166 K  |    4166 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    4107 K  |    4107 K  |
|       from large pool |      10    |      11    |    1615 K  |    1615 K  |
|       from small pool |      12    |      23    |    2491 K  |    2491 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 126, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 127 [256/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [512/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [1280/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [1536/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 127 [2048/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [2304/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [2560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [2816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [3072/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [3328/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 127 [3584/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 127 [3840/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 127 [4096/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [4352/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [4608/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [4864/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [5120/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 127 [5376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [5632/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [5888/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 127 [6144/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 127 [6400/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [6656/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [6912/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [7168/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 127 [7424/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [7680/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [7936/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 127 [8192/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [8448/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [8704/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [8960/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 127 [9216/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 127 [9472/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [9728/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [9984/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 127 [10240/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [10496/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 127 [10752/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [11008/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [11264/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 127 [11520/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [11776/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [12032/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [12288/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [12544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [12800/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 127 [13056/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 127 [13312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 127 [13568/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [13824/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [14080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 127 [14336/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 127 [14592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [14848/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 127 [15104/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [15360/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 127 [15616/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 127 [15872/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [16128/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [16384/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 127 [16640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [16896/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [17152/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [17408/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [17664/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 127 [17920/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [18176/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [18432/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [18688/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [18944/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 127 [19200/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [19456/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [19712/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [19968/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [20224/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [20480/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 127 [20736/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [20992/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 127 [21248/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 127 [21504/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [21760/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [22016/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [22272/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 127 [22528/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 127 [22784/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [23040/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 127 [23296/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 127 [23552/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [23808/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [24064/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 127 [24320/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [24576/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 127 [24832/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 127 [25088/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [25344/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [25600/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 127 [25856/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 127 [26112/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 127 [26368/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [26624/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 127 [26880/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 127 [27136/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 127 [27392/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [27648/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 127 [27904/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 127 [28160/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [28416/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [28672/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [28928/50000]	Loss: 4.6034	LR: 0.040000
Training Epoch: 127 [29184/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [29440/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 127 [29696/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 127 [29952/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 127 [30208/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 127 [30464/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 127 [30720/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [30976/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [31232/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [31488/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [31744/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 127 [32000/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [32256/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 127 [32512/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 127 [32768/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 127 [33024/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 127 [33280/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [33536/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 127 [33792/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 127 [34048/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 127 [34304/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 127 [34560/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [34816/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [35072/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 127 [35328/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 127 [35584/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [35840/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [36096/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 127 [36352/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [36608/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [36864/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 127 [37120/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [37376/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 127 [37632/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [37888/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 127 [38144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [38400/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 127 [38656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [38912/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 127 [39168/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [39424/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [39680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 127 [39936/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [40192/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 127 [40448/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [40704/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 127 [40960/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 127 [41216/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [41472/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 127 [41728/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 127 [41984/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 127 [42240/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 127 [42496/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 127 [42752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 127 [43008/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 127 [43264/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [43520/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 127 [43776/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [44032/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 127 [44288/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [44544/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [44800/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [45056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 127 [45312/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [45568/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 127 [45824/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 127 [46080/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 127 [46336/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [46592/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 127 [46848/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [47104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 127 [47360/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 127 [47616/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 127 [47872/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 127 [48128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 127 [48384/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [48640/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 127 [48896/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 127 [49152/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 127 [49408/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 127 [49664/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 127 [49920/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 127 [50000/50000]	Loss: 4.6062	LR: 0.040000
epoch 127 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  190524 GB |  190523 GB |
|       from large pool |  400448 KB |    1770 MB |  190352 GB |  190352 GB |
|       from small pool |    3550 KB |       9 MB |     171 GB |     171 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  190524 GB |  190523 GB |
|       from large pool |  400448 KB |    1770 MB |  190352 GB |  190352 GB |
|       from small pool |    3550 KB |       9 MB |     171 GB |     171 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  117226 GB |  117226 GB |
|       from large pool |  244672 KB |  473024 KB |  117030 GB |  117030 GB |
|       from small pool |    2594 KB |    4843 KB |     196 GB |     195 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8131 K  |    8131 K  |
|       from large pool |      36    |      77    |    3931 K  |    3931 K  |
|       from small pool |     187    |     225    |    4199 K  |    4199 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8131 K  |    8131 K  |
|       from large pool |      36    |      77    |    3931 K  |    3931 K  |
|       from small pool |     187    |     225    |    4199 K  |    4199 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    4138 K  |    4138 K  |
|       from large pool |      10    |      11    |    1628 K  |    1628 K  |
|       from small pool |      12    |      23    |    2509 K  |    2509 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 127, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 128 [256/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [512/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [768/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 128 [1024/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [1536/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 128 [1792/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [2048/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [2304/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [2560/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [2816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 128 [3072/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [3328/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [3584/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [3840/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [4096/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [4352/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 128 [4608/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 128 [4864/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [5120/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 128 [5376/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [5632/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [5888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 128 [6144/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [6400/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [6656/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 128 [6912/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [7168/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 128 [7424/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 128 [7680/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [7936/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [8192/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [8448/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 128 [8704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [8960/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 128 [9216/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [9472/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 128 [9728/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [9984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [10240/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 128 [10496/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 128 [10752/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [11008/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 128 [11264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 128 [11520/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 128 [11776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [12032/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [12288/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [12544/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [12800/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 128 [13056/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 128 [13312/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [13568/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [13824/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [14080/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [14336/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [14592/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 128 [14848/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 128 [15104/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [15360/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [15616/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 128 [15872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [16128/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 128 [16384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 128 [16640/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 128 [16896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [17152/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [17408/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 128 [17664/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [17920/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [18176/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 128 [18432/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 128 [18688/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 128 [18944/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 128 [19200/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [19456/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 128 [19712/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 128 [19968/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [20224/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 128 [20480/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [20736/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [20992/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [21248/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 128 [21504/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 128 [21760/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [22016/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 128 [22272/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 128 [22528/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [22784/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [23040/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [23296/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [23552/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [23808/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [24064/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [24320/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 128 [24576/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [24832/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [25088/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [25344/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [25600/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [25856/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 128 [26112/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [26368/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 128 [26624/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 128 [26880/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 128 [27136/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [27392/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [27648/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [27904/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [28160/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 128 [28416/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [28672/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 128 [28928/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [29184/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 128 [29440/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 128 [29696/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [29952/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 128 [30208/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [30464/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [30720/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 128 [30976/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [31232/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 128 [31488/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 128 [31744/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [32000/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [32256/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 128 [32512/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [32768/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [33024/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [33280/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 128 [33536/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 128 [33792/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [34048/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 128 [34304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [34560/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [34816/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [35072/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [35328/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 128 [35584/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 128 [35840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [36096/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 128 [36352/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 128 [36608/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 128 [36864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 128 [37120/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [37376/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 128 [37632/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 128 [37888/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 128 [38144/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 128 [38400/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [38656/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [38912/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [39168/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 128 [39424/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [39680/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 128 [39936/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 128 [40192/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 128 [40448/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 128 [40704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [40960/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 128 [41216/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 128 [41472/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 128 [41728/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 128 [41984/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 128 [42240/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 128 [42496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 128 [42752/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 128 [43008/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 128 [43264/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 128 [43520/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [43776/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 128 [44032/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 128 [44288/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 128 [44544/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 128 [44800/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 128 [45056/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 128 [45312/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 128 [45568/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 128 [45824/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [46080/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 128 [46336/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [46592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 128 [46848/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 128 [47104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 128 [47360/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 128 [47616/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [47872/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [48128/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [48384/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 128 [48640/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [48896/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 128 [49152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 128 [49408/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 128 [49664/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 128 [49920/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 128 [50000/50000]	Loss: 4.6065	LR: 0.040000
epoch 128 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  192024 GB |  192023 GB |
|       from large pool |  400448 KB |    1770 MB |  191851 GB |  191851 GB |
|       from small pool |    3550 KB |       9 MB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  192024 GB |  192023 GB |
|       from large pool |  400448 KB |    1770 MB |  191851 GB |  191851 GB |
|       from small pool |    3550 KB |       9 MB |     172 GB |     172 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  118149 GB |  118149 GB |
|       from large pool |  244672 KB |  473024 KB |  117952 GB |  117951 GB |
|       from small pool |    2594 KB |    4843 KB |     197 GB |     197 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8195 K  |    8195 K  |
|       from large pool |      36    |      77    |    3962 K  |    3962 K  |
|       from small pool |     187    |     225    |    4232 K  |    4232 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8195 K  |    8195 K  |
|       from large pool |      36    |      77    |    3962 K  |    3962 K  |
|       from small pool |     187    |     225    |    4232 K  |    4232 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4171 K  |    4171 K  |
|       from large pool |      10    |      11    |    1641 K  |    1641 K  |
|       from small pool |      11    |      23    |    2529 K  |    2529 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 128, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 129 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [768/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [1024/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [1280/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [1536/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 129 [1792/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [2048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [2304/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [2560/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [2816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [3072/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [3328/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 129 [3584/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 129 [3840/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [4096/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [4352/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [4608/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 129 [4864/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [5120/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 129 [5376/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [5632/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [5888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [6144/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [6400/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [6656/50000]	Loss: 4.6034	LR: 0.040000
Training Epoch: 129 [6912/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [7168/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 129 [7424/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 129 [7680/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 129 [7936/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 129 [8192/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [8448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [8704/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 129 [8960/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [9216/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [9472/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [9728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [9984/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [10240/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [10496/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [10752/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 129 [11008/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [11264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [11520/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [11776/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [12032/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [12288/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [12544/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 129 [12800/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [13056/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [13312/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 129 [13568/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [13824/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [14080/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 129 [14336/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [14592/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 129 [14848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [15104/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [15360/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 129 [15616/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [15872/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 129 [16128/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [16384/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [16640/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [16896/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [17152/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [17408/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [17664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [17920/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 129 [18176/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 129 [18432/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [18688/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [18944/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [19200/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [19456/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 129 [19712/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [19968/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 129 [20224/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 129 [20480/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 129 [20736/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [20992/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [21248/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [21504/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [21760/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [22016/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [22272/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [22528/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [22784/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [23040/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [23296/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 129 [23552/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [23808/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 129 [24064/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [24320/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 129 [24576/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [24832/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 129 [25088/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [25344/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [25600/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [25856/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [26112/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [26368/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 129 [26624/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [26880/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [27136/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [27392/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [27648/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [27904/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [28160/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 129 [28416/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [28672/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [28928/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 129 [29184/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 129 [29440/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [29696/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [29952/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [30208/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 129 [30464/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [30720/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [30976/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [31232/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [31488/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 129 [31744/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [32000/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [32256/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [32512/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 129 [32768/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 129 [33024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [33280/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [33536/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [33792/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 129 [34048/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 129 [34304/50000]	Loss: 4.6034	LR: 0.040000
Training Epoch: 129 [34560/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [34816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [35072/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [35328/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [35584/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [35840/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [36096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 129 [36352/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 129 [36608/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [36864/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [37120/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [37376/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 129 [37632/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 129 [37888/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 129 [38144/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [38400/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 129 [38656/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [38912/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [39168/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [39424/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 129 [39680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [39936/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [40192/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [40448/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [40704/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [40960/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 129 [41216/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 129 [41472/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [41728/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [41984/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 129 [42240/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [42496/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [42752/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [43008/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [43264/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 129 [43520/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [43776/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 129 [44032/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 129 [44288/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [44544/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 129 [44800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 129 [45056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 129 [45312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [45568/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [45824/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [46336/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 129 [46592/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 129 [46848/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 129 [47104/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [47360/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [47616/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 129 [47872/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 129 [48128/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 129 [48384/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 129 [48640/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [48896/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 129 [49152/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 129 [49408/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 129 [49664/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 129 [49920/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 129 [50000/50000]	Loss: 4.6066	LR: 0.040000
epoch 129 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  193524 GB |  193523 GB |
|       from large pool |  400448 KB |    1770 MB |  193350 GB |  193349 GB |
|       from small pool |    3550 KB |       9 MB |     174 GB |     174 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  193524 GB |  193523 GB |
|       from large pool |  400448 KB |    1770 MB |  193350 GB |  193349 GB |
|       from small pool |    3550 KB |       9 MB |     174 GB |     174 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  119072 GB |  119072 GB |
|       from large pool |  244672 KB |  473024 KB |  118873 GB |  118873 GB |
|       from small pool |    2594 KB |    4843 KB |     199 GB |     199 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8259 K  |    8259 K  |
|       from large pool |      36    |      77    |    3993 K  |    3993 K  |
|       from small pool |     187    |     225    |    4265 K  |    4265 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8259 K  |    8259 K  |
|       from large pool |      36    |      77    |    3993 K  |    3993 K  |
|       from small pool |     187    |     225    |    4265 K  |    4265 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4203 K  |    4203 K  |
|       from large pool |      10    |      11    |    1654 K  |    1654 K  |
|       from small pool |      11    |      23    |    2549 K  |    2549 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 129, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 130 [256/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [512/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [768/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [1024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 130 [1280/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [1536/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 130 [1792/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [2048/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 130 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [2560/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [2816/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [3072/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [3328/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [3584/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [3840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [4096/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [4352/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [4608/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 130 [4864/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 130 [5120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 130 [5376/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [5632/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [5888/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 130 [6144/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [6400/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 130 [6656/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [6912/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [7168/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 130 [7424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [7680/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [7936/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [8192/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [8448/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [8704/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 130 [8960/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [9216/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [9472/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 130 [9728/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [9984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [10240/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [10496/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [10752/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [11008/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [11264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [11520/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [11776/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 130 [12032/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [12288/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 130 [12544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 130 [12800/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 130 [13056/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [13312/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [13568/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 130 [13824/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [14080/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [14336/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 130 [14592/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 130 [14848/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [15104/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [15360/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [15616/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 130 [15872/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [16128/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [16384/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [16640/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [16896/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 130 [17152/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 130 [17408/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [17664/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 130 [17920/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [18176/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [18432/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [18688/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 130 [18944/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [19200/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [19456/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [19712/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 130 [19968/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [20224/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [20480/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [20736/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [20992/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [21248/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 130 [21504/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 130 [21760/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [22016/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [22272/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 130 [22528/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 130 [22784/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [23040/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [23296/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [23552/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [23808/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 130 [24064/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [24320/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [24576/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [24832/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 130 [25088/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 130 [25344/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 130 [25600/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [25856/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [26112/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [26368/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 130 [26624/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 130 [26880/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [27136/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [27392/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [27648/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 130 [27904/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 130 [28160/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [28416/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [28672/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [28928/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [29184/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 130 [29440/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 130 [29696/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [29952/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [30208/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 130 [30464/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 130 [30720/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [30976/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [31232/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 130 [31488/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 130 [31744/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [32000/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 130 [32256/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [32512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [32768/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 130 [33024/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [33280/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 130 [33536/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [33792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [34048/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 130 [34304/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [34560/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 130 [34816/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [35072/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [35328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [35584/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 130 [35840/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [36096/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [36352/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 130 [36608/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [36864/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [37120/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [37376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [37632/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [37888/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [38144/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [38400/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [38656/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 130 [38912/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [39168/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 130 [39424/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [39680/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [39936/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 130 [40192/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [40448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [40704/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 130 [40960/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [41216/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 130 [41472/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [41728/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 130 [41984/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [42240/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [42496/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [42752/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [43008/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [43264/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 130 [43520/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 130 [43776/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [44032/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [44288/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 130 [44544/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 130 [44800/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [45056/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [45312/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [45568/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [45824/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 130 [46080/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 130 [46336/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 130 [46592/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 130 [46848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 130 [47104/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 130 [47360/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [47616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 130 [47872/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 130 [48128/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [48384/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 130 [48640/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 130 [49152/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 130 [49408/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 130 [49664/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 130 [49920/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 130 [50000/50000]	Loss: 4.6072	LR: 0.040000
epoch 130 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  195024 GB |  195024 GB |
|       from large pool |  400448 KB |    1770 MB |  194849 GB |  194848 GB |
|       from small pool |    3550 KB |       9 MB |     175 GB |     175 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  195024 GB |  195024 GB |
|       from large pool |  400448 KB |    1770 MB |  194849 GB |  194848 GB |
|       from small pool |    3550 KB |       9 MB |     175 GB |     175 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  119995 GB |  119995 GB |
|       from large pool |  244672 KB |  473024 KB |  119795 GB |  119794 GB |
|       from small pool |    2594 KB |    4843 KB |     200 GB |     200 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8323 K  |    8323 K  |
|       from large pool |      36    |      77    |    4024 K  |    4024 K  |
|       from small pool |     187    |     225    |    4298 K  |    4298 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8323 K  |    8323 K  |
|       from large pool |      36    |      77    |    4024 K  |    4024 K  |
|       from small pool |     187    |     225    |    4298 K  |    4298 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    4234 K  |    4234 K  |
|       from large pool |      10    |      11    |    1667 K  |    1667 K  |
|       from small pool |      12    |      23    |    2567 K  |    2567 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 130, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-130-regular.pth
Training Epoch: 131 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [512/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [768/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [1024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [1280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [1536/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 131 [1792/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [2048/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [2304/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [2560/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [2816/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [3072/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [3328/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [3584/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 131 [3840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 131 [4096/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [4352/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [4608/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [4864/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 131 [5120/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 131 [5376/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [5632/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 131 [5888/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [6144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [6400/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [6656/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 131 [6912/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [7168/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 131 [7424/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [7680/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 131 [7936/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [8192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [8448/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 131 [8704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [8960/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [9216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 131 [9472/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 131 [9728/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [9984/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [10240/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [10496/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [10752/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [11008/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [11264/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [11520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [11776/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 131 [12032/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [12288/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [12544/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [12800/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [13056/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [13312/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [13568/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 131 [13824/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 131 [14080/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [14336/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 131 [14592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [14848/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [15104/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [15360/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 131 [15616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [15872/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [16128/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [16384/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [16640/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [16896/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 131 [17152/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [17408/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [17664/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 131 [17920/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [18176/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [18432/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 131 [18688/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 131 [18944/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [19200/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 131 [19456/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [19712/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [19968/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [20224/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [20480/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 131 [20736/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [20992/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [21248/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [21504/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 131 [21760/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 131 [22016/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [22272/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 131 [22528/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [22784/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 131 [23040/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [23296/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [23552/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [23808/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [24064/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [24320/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [24576/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [24832/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 131 [25088/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [25344/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [25600/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 131 [25856/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [26112/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [26368/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [26624/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 131 [26880/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [27136/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 131 [27392/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [27648/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 131 [27904/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [28160/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [28416/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 131 [28672/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [28928/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [29184/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [29440/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [29696/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [29952/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 131 [30208/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [30464/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 131 [30720/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 131 [30976/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 131 [31232/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [31488/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [31744/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 131 [32000/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 131 [32256/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [32512/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [32768/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 131 [33024/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 131 [33280/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [33536/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [33792/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [34048/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 131 [34304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 131 [34560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 131 [34816/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 131 [35072/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 131 [35328/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [35584/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 131 [35840/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [36096/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [36352/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [36608/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [36864/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [37120/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 131 [37376/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 131 [37632/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [37888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [38144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [38400/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [38656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [38912/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [39168/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [39424/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 131 [39680/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 131 [39936/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [40192/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 131 [40448/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 131 [40704/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [40960/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 131 [41216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [41472/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [41728/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [41984/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 131 [42240/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 131 [42496/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [42752/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [43008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 131 [43264/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [43520/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 131 [43776/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 131 [44032/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [44288/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 131 [44544/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [44800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [45056/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 131 [45312/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 131 [45568/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 131 [45824/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [46080/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [46336/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [46592/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [46848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [47104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 131 [47360/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 131 [47616/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 131 [47872/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 131 [48128/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 131 [48384/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 131 [48640/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 131 [48896/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 131 [49152/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 131 [49408/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [49664/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 131 [49920/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 131 [50000/50000]	Loss: 4.6068	LR: 0.040000
epoch 131 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  196524 GB |  196524 GB |
|       from large pool |  400448 KB |    1770 MB |  196347 GB |  196347 GB |
|       from small pool |    3550 KB |       9 MB |     176 GB |     176 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  196524 GB |  196524 GB |
|       from large pool |  400448 KB |    1770 MB |  196347 GB |  196347 GB |
|       from small pool |    3550 KB |       9 MB |     176 GB |     176 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  120918 GB |  120918 GB |
|       from large pool |  244672 KB |  473024 KB |  120716 GB |  120716 GB |
|       from small pool |    2594 KB |    4843 KB |     202 GB |     202 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8387 K  |    8387 K  |
|       from large pool |      36    |      77    |    4055 K  |    4055 K  |
|       from small pool |     187    |     225    |    4331 K  |    4331 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8387 K  |    8387 K  |
|       from large pool |      36    |      77    |    4055 K  |    4055 K  |
|       from small pool |     187    |     225    |    4331 K  |    4331 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    4266 K  |    4266 K  |
|       from large pool |      10    |      11    |    1679 K  |    1679 K  |
|       from small pool |      13    |      23    |    2586 K  |    2586 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 131, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 132 [256/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 132 [512/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 132 [768/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 132 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [1280/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 132 [1536/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [1792/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [2048/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 132 [2304/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 132 [2560/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [2816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [3072/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 132 [3328/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [3584/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 132 [3840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 132 [4096/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 132 [4352/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 132 [4608/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [4864/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 132 [5120/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 132 [5376/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 132 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [5888/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 132 [6144/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [6400/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 132 [6656/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [6912/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 132 [7168/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [7424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [7680/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 132 [7936/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [8192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [8448/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 132 [8704/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 132 [8960/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [9216/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [9472/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [9728/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 132 [9984/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 132 [10240/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 132 [10496/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [10752/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 132 [11008/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [11264/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 132 [11520/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [11776/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [12032/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 132 [12288/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [12544/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [12800/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [13056/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [13312/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 132 [13568/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 132 [13824/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 132 [14080/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 132 [14336/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [14592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 132 [14848/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 132 [15104/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 132 [15360/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 132 [15616/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [15872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [16128/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 132 [16384/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 132 [16640/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 132 [16896/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 132 [17152/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 132 [17408/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [17664/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 132 [17920/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [18176/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [18432/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 132 [18688/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 132 [18944/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [19200/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [19456/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 132 [19712/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [19968/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [20224/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 132 [20480/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 132 [20736/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [20992/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 132 [21248/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [21504/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 132 [21760/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [22016/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 132 [22272/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 132 [22528/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [22784/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 132 [23040/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 132 [23296/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 132 [23552/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 132 [23808/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [24064/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [24320/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [24576/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 132 [24832/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 132 [25088/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 132 [25344/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 132 [25600/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [25856/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [26112/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 132 [26368/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 132 [26624/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 132 [26880/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [27136/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [27392/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 132 [27648/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 132 [27904/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 132 [28160/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [28416/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [28672/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [28928/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [29184/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [29440/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [29696/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [29952/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 132 [30208/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 132 [30464/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [30720/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [30976/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 132 [31232/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 132 [31488/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [31744/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 132 [32000/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 132 [32256/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 132 [32512/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [32768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [33024/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 132 [33280/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 132 [33536/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [33792/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [34048/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [34304/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [34560/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [34816/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 132 [35072/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 132 [35328/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [35584/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 132 [35840/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [36096/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 132 [36352/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [36608/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 132 [36864/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [37120/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [37376/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 132 [37632/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [37888/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [38144/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 132 [38400/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 132 [38656/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 132 [38912/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 132 [39168/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 132 [39424/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 132 [39680/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 132 [39936/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 132 [40192/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [40448/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 132 [40704/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 132 [40960/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 132 [41216/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 132 [41472/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 132 [41728/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [41984/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 132 [42240/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 132 [42496/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 132 [42752/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 132 [43008/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 132 [43264/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 132 [43520/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 132 [43776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [44032/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 132 [44288/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 132 [44544/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [44800/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 132 [45056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [45312/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 132 [45568/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 132 [45824/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 132 [46080/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [46336/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 132 [46592/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 132 [46848/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 132 [47104/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 132 [47360/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 132 [47616/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [47872/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [48128/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 132 [48384/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 132 [48640/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 132 [48896/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [49152/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 132 [49408/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 132 [49664/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 132 [49920/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 132 [50000/50000]	Loss: 4.6067	LR: 0.040000
epoch 132 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  198024 GB |  198024 GB |
|       from large pool |  400448 KB |    1770 MB |  197846 GB |  197846 GB |
|       from small pool |    3550 KB |       9 MB |     178 GB |     178 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  198024 GB |  198024 GB |
|       from large pool |  400448 KB |    1770 MB |  197846 GB |  197846 GB |
|       from small pool |    3550 KB |       9 MB |     178 GB |     178 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  121841 GB |  121841 GB |
|       from large pool |  244672 KB |  473024 KB |  121638 GB |  121637 GB |
|       from small pool |    2594 KB |    4843 KB |     203 GB |     203 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8451 K  |    8451 K  |
|       from large pool |      36    |      77    |    4086 K  |    4086 K  |
|       from small pool |     187    |     225    |    4364 K  |    4364 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8451 K  |    8451 K  |
|       from large pool |      36    |      77    |    4086 K  |    4086 K  |
|       from small pool |     187    |     225    |    4364 K  |    4364 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    4297 K  |    4297 K  |
|       from large pool |      10    |      11    |    1692 K  |    1692 K  |
|       from small pool |       9    |      23    |    2604 K  |    2604 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 132, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 133 [256/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [512/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [768/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [1024/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [1280/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [1536/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [1792/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [2048/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [2560/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 133 [2816/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [3072/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [3328/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [3584/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 133 [3840/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [4096/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 133 [4352/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [4608/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [4864/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 133 [5120/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 133 [5376/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 133 [5632/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [5888/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 133 [6400/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 133 [6656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [6912/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [7168/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 133 [7424/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 133 [7680/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [7936/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 133 [8192/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [8448/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [8704/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [8960/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [9216/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 133 [9472/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [9728/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [9984/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [10240/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [10496/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [10752/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 133 [11008/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [11264/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [11520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [11776/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [12032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [12288/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [12544/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [12800/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [13056/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 133 [13312/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [13568/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [13824/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [14080/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [14336/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [14592/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 133 [14848/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [15104/50000]	Loss: 4.6091	LR: 0.040000
Training Epoch: 133 [15360/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 133 [15616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 133 [15872/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [16128/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 133 [16384/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [16640/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [16896/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [17152/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 133 [17408/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [17664/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 133 [17920/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [18176/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 133 [18432/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 133 [18688/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 133 [18944/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 133 [19200/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [19456/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 133 [19712/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 133 [19968/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [20224/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [20480/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 133 [20736/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 133 [20992/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 133 [21248/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [21504/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 133 [21760/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [22016/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [22272/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [22528/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 133 [22784/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [23040/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 133 [23296/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [23552/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 133 [23808/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [24064/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [24320/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [24576/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [24832/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [25088/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [25344/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 133 [25600/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [25856/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [26112/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 133 [26368/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [26624/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [26880/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [27136/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 133 [27392/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [27648/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [27904/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 133 [28160/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [28416/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 133 [28672/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [28928/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [29184/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [29440/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [29696/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [29952/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 133 [30208/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [30464/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [30720/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 133 [30976/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 133 [31232/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 133 [31488/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [31744/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [32000/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [32256/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 133 [32512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [32768/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [33024/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [33280/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 133 [33536/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 133 [33792/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 133 [34048/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 133 [34304/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [34560/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 133 [34816/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 133 [35072/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [35328/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [35584/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [35840/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [36096/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [36352/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [36608/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [36864/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [37120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [37376/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 133 [37632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [37888/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [38144/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 133 [38400/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 133 [38656/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 133 [38912/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [39168/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [39424/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [39680/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [39936/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 133 [40192/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [40448/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 133 [40704/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [40960/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [41216/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [41472/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 133 [41728/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [41984/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 133 [42240/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 133 [42496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [42752/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [43008/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [43264/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [43520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [43776/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [44032/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [44288/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 133 [44544/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [44800/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 133 [45056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 133 [45312/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 133 [45568/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 133 [45824/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 133 [46080/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 133 [46336/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 133 [46592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 133 [46848/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [47104/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [47360/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 133 [47616/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [47872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 133 [48128/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [48384/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [48640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 133 [48896/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 133 [49152/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 133 [49408/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 133 [49664/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 133 [49920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 133 [50000/50000]	Loss: 4.6056	LR: 0.040000
epoch 133 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  199525 GB |  199524 GB |
|       from large pool |  400448 KB |    1770 MB |  199345 GB |  199345 GB |
|       from small pool |    3550 KB |       9 MB |     179 GB |     179 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  199525 GB |  199524 GB |
|       from large pool |  400448 KB |    1770 MB |  199345 GB |  199345 GB |
|       from small pool |    3550 KB |       9 MB |     179 GB |     179 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  122764 GB |  122764 GB |
|       from large pool |  244672 KB |  473024 KB |  122559 GB |  122559 GB |
|       from small pool |    2594 KB |    4843 KB |     205 GB |     205 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8515 K  |    8515 K  |
|       from large pool |      36    |      77    |    4117 K  |    4117 K  |
|       from small pool |     187    |     225    |    4398 K  |    4397 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8515 K  |    8515 K  |
|       from large pool |      36    |      77    |    4117 K  |    4117 K  |
|       from small pool |     187    |     225    |    4398 K  |    4397 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    4327 K  |    4327 K  |
|       from large pool |      10    |      11    |    1705 K  |    1705 K  |
|       from small pool |       9    |      23    |    2622 K  |    2622 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 133, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.41s

Training Epoch: 134 [256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 134 [512/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [1024/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 134 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [1536/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 134 [1792/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 134 [2048/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 134 [2304/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 134 [2560/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 134 [2816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 134 [3072/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [3328/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [3584/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 134 [3840/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [4096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [4352/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [4608/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 134 [4864/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [5120/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [5376/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [5888/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 134 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [6400/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [6656/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [6912/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [7168/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [7424/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [7680/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [7936/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 134 [8192/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 134 [8448/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [8704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [8960/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [9216/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [9472/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 134 [9728/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [9984/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [10240/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [10496/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [10752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 134 [11008/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 134 [11264/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [11520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [11776/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 134 [12032/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 134 [12288/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [12544/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 134 [12800/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [13056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 134 [13312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [13568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [13824/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 134 [14080/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [14336/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [14592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 134 [14848/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [15104/50000]	Loss: 4.6029	LR: 0.040000
Training Epoch: 134 [15360/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [15616/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [15872/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [16128/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [16384/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [16640/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 134 [16896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [17152/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [17408/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 134 [17664/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 134 [17920/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [18176/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [18432/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [18688/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [18944/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [19200/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 134 [19456/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 134 [19712/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 134 [19968/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 134 [20224/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [20480/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [20736/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [20992/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [21248/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 134 [21504/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 134 [21760/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [22016/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 134 [22272/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [22528/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [22784/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [23040/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 134 [23296/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [23552/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [23808/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [24064/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 134 [24320/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 134 [24576/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [24832/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 134 [25088/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [25344/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 134 [25600/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [25856/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [26112/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 134 [26368/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [26624/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 134 [26880/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [27136/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [27392/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [27648/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [27904/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 134 [28160/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [28416/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 134 [28672/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [28928/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [29184/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 134 [29440/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [29696/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [29952/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 134 [30208/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [30464/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [30720/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 134 [30976/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [31232/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [31488/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 134 [31744/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 134 [32000/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 134 [32256/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 134 [32512/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [32768/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [33024/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [33280/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 134 [33536/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 134 [33792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 134 [34048/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 134 [34304/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [34560/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [34816/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 134 [35072/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 134 [35328/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [35584/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 134 [35840/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 134 [36096/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 134 [36352/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [36608/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [36864/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [37120/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [37376/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [37632/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [37888/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 134 [38144/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 134 [38400/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [38656/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [38912/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [39168/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [39424/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [39680/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 134 [39936/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [40192/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 134 [40448/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [40704/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 134 [40960/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 134 [41216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [41472/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [41728/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 134 [41984/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 134 [42240/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [42496/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [42752/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 134 [43008/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 134 [43264/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [43520/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [43776/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 134 [44032/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [44288/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [44544/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [44800/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [45056/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [45312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 134 [45568/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 134 [45824/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 134 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 134 [46336/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 134 [46592/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 134 [46848/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 134 [47104/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 134 [47360/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 134 [47616/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 134 [47872/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 134 [48128/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [48384/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 134 [48640/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 134 [48896/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [49152/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 134 [49408/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 134 [49664/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 134 [49920/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 134 [50000/50000]	Loss: 4.6091	LR: 0.040000
epoch 134 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  201025 GB |  201024 GB |
|       from large pool |  400448 KB |    1770 MB |  200844 GB |  200844 GB |
|       from small pool |    3550 KB |       9 MB |     180 GB |     180 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  201025 GB |  201024 GB |
|       from large pool |  400448 KB |    1770 MB |  200844 GB |  200844 GB |
|       from small pool |    3550 KB |       9 MB |     180 GB |     180 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  123687 GB |  123687 GB |
|       from large pool |  244672 KB |  473024 KB |  123481 GB |  123480 GB |
|       from small pool |    2594 KB |    4843 KB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8579 K  |    8579 K  |
|       from large pool |      36    |      77    |    4148 K  |    4148 K  |
|       from small pool |     187    |     225    |    4431 K  |    4430 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8579 K  |    8579 K  |
|       from large pool |      36    |      77    |    4148 K  |    4148 K  |
|       from small pool |     187    |     225    |    4431 K  |    4430 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    4358 K  |    4358 K  |
|       from large pool |      10    |      11    |    1718 K  |    1718 K  |
|       from small pool |       9    |      23    |    2640 K  |    2640 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 134, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 135 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 135 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [1280/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 135 [1536/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [1792/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [2048/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 135 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [2560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [2816/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 135 [3072/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [3328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [3584/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [3840/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 135 [4096/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 135 [4352/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [4608/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [4864/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 135 [5120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [5376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 135 [5632/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [5888/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [6144/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [6400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [6656/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [6912/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [7168/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 135 [7424/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [7680/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 135 [7936/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [8192/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [8448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [8704/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [8960/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [9216/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [9472/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 135 [9728/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 135 [9984/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [10240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 135 [10496/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 135 [10752/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [11008/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 135 [11264/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [11520/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 135 [11776/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [12032/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [12288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [12544/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [12800/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [13056/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 135 [13312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 135 [13568/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 135 [13824/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [14080/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [14336/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [14592/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [14848/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 135 [15104/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [15360/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 135 [15616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [15872/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 135 [16128/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 135 [16384/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 135 [16640/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 135 [16896/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [17152/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 135 [17408/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 135 [17664/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [17920/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [18176/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [18432/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 135 [18688/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 135 [18944/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 135 [19200/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [19456/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [19712/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 135 [19968/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 135 [20224/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [20480/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [20736/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 135 [20992/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 135 [21248/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [21504/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [21760/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 135 [22016/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [22272/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [22528/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [22784/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 135 [23040/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 135 [23296/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [23552/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [23808/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [24064/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [24320/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [24576/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 135 [24832/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [25088/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [25344/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 135 [25600/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [25856/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 135 [26112/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 135 [26368/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [26624/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [26880/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 135 [27136/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [27392/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 135 [27648/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [27904/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 135 [28160/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 135 [28416/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 135 [28672/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 135 [28928/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [29184/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [29440/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [29696/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 135 [29952/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 135 [30208/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [30464/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 135 [30720/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [30976/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [31232/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 135 [31488/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 135 [31744/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [32000/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [32256/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [32512/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [32768/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 135 [33024/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [33280/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 135 [33536/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 135 [33792/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 135 [34048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [34304/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 135 [34560/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [34816/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 135 [35072/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 135 [35328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 135 [35584/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [35840/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 135 [36096/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 135 [36352/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 135 [36608/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 135 [36864/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [37120/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [37376/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 135 [37632/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [37888/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [38144/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 135 [38400/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [38656/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [38912/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 135 [39168/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [39424/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 135 [39680/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 135 [39936/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [40192/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [40448/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [40704/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [40960/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 135 [41216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [41472/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [41728/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 135 [41984/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [42240/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 135 [42496/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [42752/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [43008/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 135 [43264/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 135 [43520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [43776/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [44032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 135 [44288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 135 [44544/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [44800/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [45056/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 135 [45312/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [45568/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [45824/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 135 [46080/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 135 [46336/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 135 [46592/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 135 [46848/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 135 [47104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [47360/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 135 [47616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 135 [47872/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 135 [48128/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 135 [48384/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 135 [48640/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 135 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [49152/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 135 [49408/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 135 [49664/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 135 [49920/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 135 [50000/50000]	Loss: 4.6060	LR: 0.040000
epoch 135 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  202525 GB |  202525 GB |
|       from large pool |  400448 KB |    1770 MB |  202343 GB |  202342 GB |
|       from small pool |    3550 KB |       9 MB |     182 GB |     182 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  202525 GB |  202525 GB |
|       from large pool |  400448 KB |    1770 MB |  202343 GB |  202342 GB |
|       from small pool |    3550 KB |       9 MB |     182 GB |     182 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  124610 GB |  124610 GB |
|       from large pool |  244672 KB |  473024 KB |  124402 GB |  124402 GB |
|       from small pool |    2594 KB |    4843 KB |     208 GB |     208 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8643 K  |    8643 K  |
|       from large pool |      36    |      77    |    4179 K  |    4179 K  |
|       from small pool |     187    |     225    |    4464 K  |    4463 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8643 K  |    8643 K  |
|       from large pool |      36    |      77    |    4179 K  |    4179 K  |
|       from small pool |     187    |     225    |    4464 K  |    4463 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4391 K  |    4391 K  |
|       from large pool |      10    |      11    |    1731 K  |    1731 K  |
|       from small pool |      11    |      23    |    2660 K  |    2660 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 135, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 136 [256/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 136 [512/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [768/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [1024/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [1536/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [1792/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [2048/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 136 [2304/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [2560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [2816/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [3072/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [3328/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 136 [3584/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [3840/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 136 [4096/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [4352/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [4608/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [4864/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [5120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [5376/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 136 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [5888/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [6144/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 136 [6400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [6656/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 136 [6912/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 136 [7168/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [7424/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [7680/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [7936/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 136 [8192/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [8448/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 136 [8704/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 136 [8960/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 136 [9216/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [9472/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 136 [9728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [9984/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [10240/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [10496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [10752/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [11008/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 136 [11264/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 136 [11520/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 136 [11776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 136 [12032/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 136 [12288/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 136 [12544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [12800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [13056/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [13312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [13568/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [13824/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 136 [14080/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [14336/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 136 [14592/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [14848/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [15104/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 136 [15360/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [15616/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 136 [15872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [16128/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [16384/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 136 [16640/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 136 [16896/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 136 [17152/50000]	Loss: 4.6031	LR: 0.040000
Training Epoch: 136 [17408/50000]	Loss: 4.6031	LR: 0.040000
Training Epoch: 136 [17664/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 136 [17920/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 136 [18176/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 136 [18432/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [18688/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [18944/50000]	Loss: 4.6030	LR: 0.040000
Training Epoch: 136 [19200/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [19456/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 136 [19712/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 136 [19968/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 136 [20224/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [20480/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [20736/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [20992/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [21248/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [21504/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 136 [21760/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [22016/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [22272/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 136 [22528/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [22784/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 136 [23040/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [23296/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [23552/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 136 [23808/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 136 [24064/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 136 [24320/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 136 [24576/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [24832/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [25088/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 136 [25344/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [25600/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [25856/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 136 [26112/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [26368/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [26624/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [26880/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 136 [27136/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 136 [27392/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 136 [27648/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 136 [27904/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 136 [28160/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [28416/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 136 [28672/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [28928/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [29184/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [29440/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [29696/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [29952/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [30208/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 136 [30464/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [30720/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [30976/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [31232/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [31488/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 136 [31744/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [32000/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [32256/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [32512/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [32768/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 136 [33024/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [33280/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [33536/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 136 [33792/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 136 [34048/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 136 [34304/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 136 [34560/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 136 [34816/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 136 [35072/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 136 [35328/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [35584/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [35840/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [36096/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 136 [36352/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 136 [36608/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [36864/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 136 [37120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 136 [37376/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [37632/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [37888/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [38144/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 136 [38400/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [38656/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 136 [38912/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 136 [39168/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [39424/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [39680/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 136 [39936/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [40192/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 136 [40448/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 136 [40704/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [40960/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [41216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [41472/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 136 [41728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [41984/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 136 [42240/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 136 [42496/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 136 [42752/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 136 [43008/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 136 [43264/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [43520/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [43776/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [44032/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [44288/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [44544/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 136 [44800/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [45056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 136 [45312/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [45568/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 136 [45824/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 136 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [46336/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [46592/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 136 [46848/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [47104/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [47360/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 136 [47616/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [47872/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 136 [48128/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 136 [48384/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 136 [48640/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 136 [48896/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [49152/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 136 [49408/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 136 [49664/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 136 [49920/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 136 [50000/50000]	Loss: 4.6062	LR: 0.040000
epoch 136 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  204025 GB |  204025 GB |
|       from large pool |  400448 KB |    1770 MB |  203842 GB |  203841 GB |
|       from small pool |    3550 KB |       9 MB |     183 GB |     183 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  204025 GB |  204025 GB |
|       from large pool |  400448 KB |    1770 MB |  203842 GB |  203841 GB |
|       from small pool |    3550 KB |       9 MB |     183 GB |     183 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  125533 GB |  125533 GB |
|       from large pool |  244672 KB |  473024 KB |  125324 GB |  125323 GB |
|       from small pool |    2594 KB |    4843 KB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8707 K  |    8707 K  |
|       from large pool |      36    |      77    |    4210 K  |    4210 K  |
|       from small pool |     187    |     225    |    4497 K  |    4497 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8707 K  |    8707 K  |
|       from large pool |      36    |      77    |    4210 K  |    4210 K  |
|       from small pool |     187    |     225    |    4497 K  |    4497 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    4423 K  |    4423 K  |
|       from large pool |      10    |      11    |    1743 K  |    1743 K  |
|       from small pool |      10    |      23    |    2679 K  |    2679 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 136, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 137 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [512/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [1024/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 137 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [1536/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [1792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [2048/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [2304/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 137 [2560/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 137 [2816/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 137 [3072/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [3328/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [3584/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 137 [3840/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [4096/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 137 [4352/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [4608/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 137 [4864/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [5120/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 137 [5376/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [5632/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 137 [5888/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [6144/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [6400/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [6656/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [6912/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 137 [7168/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [7424/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 137 [7680/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 137 [7936/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 137 [8192/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 137 [8448/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [8704/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 137 [8960/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [9216/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 137 [9472/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 137 [9728/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [9984/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 137 [10240/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 137 [10496/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [10752/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 137 [11008/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 137 [11264/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [11520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [11776/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 137 [12032/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [12288/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [12544/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 137 [12800/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 137 [13056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [13312/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 137 [13568/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [13824/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [14080/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 137 [14336/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 137 [14592/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [14848/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 137 [15104/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [15360/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [15616/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [15872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [16128/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [16384/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 137 [16640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [16896/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 137 [17152/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [17408/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [17664/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 137 [17920/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [18176/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 137 [18432/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [18688/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 137 [18944/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 137 [19200/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 137 [19456/50000]	Loss: 4.6029	LR: 0.040000
Training Epoch: 137 [19712/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 137 [19968/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [20224/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 137 [20480/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [20736/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 137 [20992/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [21248/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 137 [21504/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [21760/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 137 [22016/50000]	Loss: 4.6026	LR: 0.040000
Training Epoch: 137 [22272/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 137 [22528/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 137 [22784/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [23040/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 137 [23296/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 137 [23552/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [23808/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 137 [24064/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [24320/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [24576/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 137 [24832/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [25088/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [25344/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [25600/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [25856/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [26112/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [26368/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [26624/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 137 [26880/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 137 [27136/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 137 [27392/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [27648/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 137 [27904/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 137 [28160/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 137 [28416/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [28672/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 137 [28928/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [29184/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [29440/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [29696/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [29952/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 137 [30208/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [30464/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 137 [30720/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [30976/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [31232/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [31488/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 137 [31744/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [32000/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [32256/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [32512/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 137 [32768/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 137 [33024/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [33280/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [33536/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [33792/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [34048/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 137 [34304/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 137 [34560/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [34816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [35072/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [35328/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [35584/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [35840/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [36096/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 137 [36352/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [36608/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 137 [36864/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 137 [37120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [37376/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 137 [37632/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [37888/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [38144/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [38400/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 137 [38656/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [38912/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [39168/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [39424/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [39680/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [39936/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [40192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 137 [40448/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 137 [40704/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [40960/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [41216/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 137 [41472/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [41728/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [41984/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [42240/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [42496/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [42752/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 137 [43008/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [43264/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [43520/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 137 [43776/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 137 [44032/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 137 [44288/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [44544/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [44800/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [45056/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [45312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 137 [45568/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 137 [45824/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 137 [46080/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [46336/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 137 [46592/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [46848/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 137 [47104/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 137 [47360/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [47616/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [47872/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 137 [48128/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 137 [48384/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 137 [48640/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 137 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [49152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [49408/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 137 [49664/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 137 [49920/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 137 [50000/50000]	Loss: 4.6067	LR: 0.040000
epoch 137 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  205525 GB |  205525 GB |
|       from large pool |  400448 KB |    1770 MB |  205340 GB |  205340 GB |
|       from small pool |    3550 KB |       9 MB |     184 GB |     184 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  205525 GB |  205525 GB |
|       from large pool |  400448 KB |    1770 MB |  205340 GB |  205340 GB |
|       from small pool |    3550 KB |       9 MB |     184 GB |     184 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  126456 GB |  126456 GB |
|       from large pool |  244672 KB |  473024 KB |  126245 GB |  126245 GB |
|       from small pool |    2594 KB |    4843 KB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8771 K  |    8771 K  |
|       from large pool |      36    |      77    |    4241 K  |    4241 K  |
|       from small pool |     187    |     225    |    4530 K  |    4530 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8771 K  |    8771 K  |
|       from large pool |      36    |      77    |    4241 K  |    4241 K  |
|       from small pool |     187    |     225    |    4530 K  |    4530 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    4454 K  |    4454 K  |
|       from large pool |      10    |      11    |    1756 K  |    1756 K  |
|       from small pool |      10    |      23    |    2697 K  |    2697 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 137, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 138 [256/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [512/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [1024/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 138 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 138 [1536/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [1792/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [2048/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [2304/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [2560/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 138 [2816/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 138 [3072/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 138 [3328/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [3584/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [3840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [4096/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [4352/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 138 [4608/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [4864/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 138 [5120/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [5376/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [5632/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [5888/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [6144/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [6400/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [6656/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [6912/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [7168/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [7424/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [7680/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [7936/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [8192/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [8448/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 138 [8704/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [8960/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 138 [9216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [9472/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [9728/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 138 [9984/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [10240/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 138 [10496/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [10752/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 138 [11008/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [11264/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 138 [11520/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [11776/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [12032/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [12288/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [12544/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [12800/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 138 [13056/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [13312/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 138 [13568/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [13824/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [14080/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 138 [14336/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [14592/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [14848/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [15104/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [15360/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 138 [15616/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 138 [15872/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 138 [16128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [16384/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [16640/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 138 [16896/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 138 [17152/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 138 [17408/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [17664/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [17920/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [18176/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 138 [18432/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [18688/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 138 [18944/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 138 [19200/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [19456/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [19712/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [19968/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 138 [20224/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [20480/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 138 [20736/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [20992/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [21248/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 138 [21504/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 138 [21760/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [22016/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 138 [22272/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 138 [22528/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [22784/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [23040/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [23296/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [23552/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 138 [23808/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [24064/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [24320/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 138 [24576/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [24832/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 138 [25088/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [25344/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [25600/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [25856/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [26112/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 138 [26368/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 138 [26624/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [26880/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [27136/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [27392/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 138 [27648/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [27904/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 138 [28160/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [28416/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 138 [28672/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 138 [28928/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 138 [29184/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [29440/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [29696/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 138 [29952/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [30208/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [30464/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [30720/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 138 [30976/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 138 [31232/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [31488/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [31744/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [32000/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [32256/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 138 [32512/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 138 [32768/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 138 [33024/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 138 [33280/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [33536/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 138 [33792/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [34048/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 138 [34304/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [34560/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 138 [34816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [35072/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [35328/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [35584/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [35840/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [36096/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [36352/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [36608/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [36864/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [37120/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [37376/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [37632/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 138 [37888/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [38144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [38400/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [38656/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 138 [38912/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [39168/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 138 [39424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [39680/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [39936/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [40192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 138 [40448/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [40704/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [40960/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 138 [41216/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 138 [41472/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 138 [41728/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 138 [41984/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [42240/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [42496/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [42752/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [43008/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 138 [43264/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 138 [43520/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [43776/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 138 [44032/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [44288/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 138 [44544/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [44800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [45056/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [45312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 138 [45568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 138 [45824/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [46080/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [46336/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [46592/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 138 [46848/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 138 [47104/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 138 [47360/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [47616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 138 [47872/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 138 [48128/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 138 [48384/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 138 [48640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [48896/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 138 [49152/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 138 [49408/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 138 [49664/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 138 [49920/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 138 [50000/50000]	Loss: 4.6065	LR: 0.040000
epoch 138 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  207025 GB |  207025 GB |
|       from large pool |  400448 KB |    1770 MB |  206839 GB |  206839 GB |
|       from small pool |    3550 KB |       9 MB |     186 GB |     186 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  207025 GB |  207025 GB |
|       from large pool |  400448 KB |    1770 MB |  206839 GB |  206839 GB |
|       from small pool |    3550 KB |       9 MB |     186 GB |     186 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  127380 GB |  127379 GB |
|       from large pool |  244672 KB |  473024 KB |  127167 GB |  127166 GB |
|       from small pool |    2594 KB |    4843 KB |     212 GB |     212 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8835 K  |    8835 K  |
|       from large pool |      36    |      77    |    4272 K  |    4272 K  |
|       from small pool |     187    |     225    |    4563 K  |    4563 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8835 K  |    8835 K  |
|       from large pool |      36    |      77    |    4272 K  |    4272 K  |
|       from small pool |     187    |     225    |    4563 K  |    4563 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    4487 K  |    4487 K  |
|       from large pool |      10    |      11    |    1769 K  |    1769 K  |
|       from small pool |      13    |      23    |    2717 K  |    2717 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 138, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 139 [256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [512/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [768/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [1280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 139 [1536/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [2048/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [2304/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 139 [2560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [2816/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [3072/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 139 [3328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [3584/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [3840/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 139 [4096/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 139 [4352/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [4608/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [4864/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 139 [5120/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [5376/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [5632/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [5888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [6144/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 139 [6400/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [6656/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [6912/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 139 [7168/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [7424/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 139 [7680/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 139 [7936/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [8192/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [8448/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [8704/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [8960/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 139 [9216/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [9472/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [9728/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [9984/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [10240/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [10496/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [10752/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [11008/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [11264/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 139 [11520/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [11776/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 139 [12032/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [12288/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 139 [12544/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 139 [12800/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 139 [13056/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 139 [13312/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [13568/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [13824/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [14080/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 139 [14336/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 139 [14592/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [14848/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 139 [15104/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [15360/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 139 [15616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [15872/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [16128/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [16384/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 139 [16640/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 139 [16896/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 139 [17152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [17408/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [17664/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 139 [17920/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [18176/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [18432/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 139 [18688/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [18944/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [19200/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [19456/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 139 [19712/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 139 [19968/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [20224/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [20480/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [20736/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 139 [20992/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 139 [21248/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [21504/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [21760/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 139 [22016/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [22272/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [22528/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [22784/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 139 [23040/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [23296/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 139 [23552/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [23808/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [24064/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [24320/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [24576/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [24832/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [25088/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [25344/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [25600/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [25856/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 139 [26112/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [26368/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [26624/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [26880/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [27136/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [27392/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [27648/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 139 [27904/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 139 [28160/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 139 [28416/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 139 [28672/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [28928/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [29184/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 139 [29440/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [29696/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [29952/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [30208/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [30464/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 139 [30720/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [30976/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [31232/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 139 [31488/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 139 [31744/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [32000/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [32256/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [32512/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [32768/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 139 [33024/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [33280/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [33536/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [33792/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [34048/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [34304/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [34560/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [34816/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [35072/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 139 [35328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [35584/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 139 [35840/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 139 [36096/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 139 [36352/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [36608/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [36864/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [37120/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [37376/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [37632/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [37888/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [38144/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [38400/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [38656/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 139 [38912/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 139 [39168/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [39424/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [39680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [39936/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 139 [40192/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [40448/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 139 [40704/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [40960/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [41216/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 139 [41472/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [41728/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [41984/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 139 [42240/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [42496/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 139 [42752/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 139 [43008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [43264/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [43520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [43776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [44032/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [44288/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 139 [44544/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [44800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [45056/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 139 [45312/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 139 [45568/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [45824/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [46080/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 139 [46336/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 139 [46592/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 139 [46848/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 139 [47104/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 139 [47360/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [47616/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [47872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 139 [48128/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [48384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 139 [48640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 139 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 139 [49152/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 139 [49408/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 139 [49664/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 139 [49920/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 139 [50000/50000]	Loss: 4.6070	LR: 0.040000
epoch 139 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  208526 GB |  208525 GB |
|       from large pool |  400448 KB |    1770 MB |  208338 GB |  208338 GB |
|       from small pool |    3550 KB |       9 MB |     187 GB |     187 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  208526 GB |  208525 GB |
|       from large pool |  400448 KB |    1770 MB |  208338 GB |  208338 GB |
|       from small pool |    3550 KB |       9 MB |     187 GB |     187 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  128303 GB |  128302 GB |
|       from large pool |  244672 KB |  473024 KB |  128088 GB |  128088 GB |
|       from small pool |    2594 KB |    4843 KB |     214 GB |     214 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8899 K  |    8899 K  |
|       from large pool |      36    |      77    |    4303 K  |    4303 K  |
|       from small pool |     187    |     225    |    4596 K  |    4596 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8899 K  |    8899 K  |
|       from large pool |      36    |      77    |    4303 K  |    4303 K  |
|       from small pool |     187    |     225    |    4596 K  |    4596 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4519 K  |    4519 K  |
|       from large pool |      10    |      11    |    1782 K  |    1782 K  |
|       from small pool |      11    |      23    |    2737 K  |    2737 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 139, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 140 [256/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [512/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 140 [768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 140 [1024/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [1280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [1792/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 140 [2048/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 140 [2304/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [2560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [2816/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [3072/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 140 [3328/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [3584/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 140 [3840/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [4096/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [4352/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [4608/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [4864/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 140 [5120/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [5376/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 140 [5632/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [5888/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 140 [6144/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [6400/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 140 [6656/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [6912/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [7168/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 140 [7424/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 140 [7680/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 140 [7936/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [8192/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 140 [8448/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [8704/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 140 [8960/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 140 [9216/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 140 [9472/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [9728/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 140 [9984/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [10240/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [10496/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [10752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [11008/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [11264/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [11520/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [11776/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 140 [12032/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [12288/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 140 [12544/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 140 [12800/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 140 [13056/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 140 [13312/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [13568/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 140 [13824/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [14080/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 140 [14336/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [14592/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 140 [14848/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [15104/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 140 [15360/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 140 [15616/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [15872/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 140 [16128/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 140 [16384/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [16640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [16896/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [17152/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 140 [17408/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 140 [17664/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [17920/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [18176/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [18432/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 140 [18688/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [18944/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 140 [19200/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [19456/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [19712/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 140 [19968/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [20224/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 140 [20480/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 140 [20736/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 140 [20992/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [21248/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 140 [21504/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [21760/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 140 [22016/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [22272/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 140 [22528/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [22784/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 140 [23040/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 140 [23296/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [23552/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [23808/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 140 [24064/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [24320/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 140 [24576/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 140 [24832/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [25088/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [25344/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [25600/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [25856/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 140 [26112/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 140 [26368/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [26624/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 140 [26880/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [27136/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [27392/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 140 [27648/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [27904/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 140 [28160/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 140 [28416/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 140 [28672/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 140 [28928/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [29184/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [29440/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [29696/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 140 [29952/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 140 [30208/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [30464/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [30720/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [30976/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 140 [31232/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 140 [31488/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [31744/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 140 [32000/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 140 [32256/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 140 [32512/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 140 [32768/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [33024/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 140 [33280/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 140 [33536/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [33792/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [34048/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [34304/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [34560/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 140 [34816/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [35072/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [35328/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [35584/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 140 [35840/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [36096/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 140 [36352/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 140 [36608/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 140 [36864/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 140 [37120/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [37376/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [37632/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [37888/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [38144/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 140 [38400/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 140 [38656/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 140 [38912/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [39168/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 140 [39424/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [39680/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [39936/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [40192/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [40448/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 140 [40704/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [40960/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [41216/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 140 [41472/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 140 [41728/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 140 [41984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [42240/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [42496/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 140 [42752/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 140 [43008/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 140 [43264/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 140 [43520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [43776/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [44032/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 140 [44288/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 140 [44544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 140 [44800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [45056/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 140 [45312/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 140 [45568/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 140 [45824/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [46080/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [46336/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [46592/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 140 [46848/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 140 [47104/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [47360/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 140 [47616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 140 [47872/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 140 [48128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [48384/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [48640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 140 [48896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 140 [49152/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [49408/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 140 [49664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 140 [49920/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 140 [50000/50000]	Loss: 4.6060	LR: 0.040000
epoch 140 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  210026 GB |  210025 GB |
|       from large pool |  400448 KB |    1770 MB |  209837 GB |  209837 GB |
|       from small pool |    3550 KB |       9 MB |     188 GB |     188 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  210026 GB |  210025 GB |
|       from large pool |  400448 KB |    1770 MB |  209837 GB |  209837 GB |
|       from small pool |    3550 KB |       9 MB |     188 GB |     188 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  129226 GB |  129225 GB |
|       from large pool |  244672 KB |  473024 KB |  129009 GB |  129009 GB |
|       from small pool |    2594 KB |    4843 KB |     216 GB |     216 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    8963 K  |    8963 K  |
|       from large pool |      36    |      77    |    4334 K  |    4334 K  |
|       from small pool |     187    |     225    |    4629 K  |    4629 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    8963 K  |    8963 K  |
|       from large pool |      36    |      77    |    4334 K  |    4334 K  |
|       from small pool |     187    |     225    |    4629 K  |    4629 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    4552 K  |    4552 K  |
|       from large pool |      10    |      11    |    1795 K  |    1795 K  |
|       from small pool |      13    |      23    |    2756 K  |    2756 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 140, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-140-regular.pth
Training Epoch: 141 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 141 [512/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [768/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [1024/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [1280/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [1536/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 141 [2048/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 141 [2304/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [2560/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [2816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 141 [3072/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 141 [3328/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [3584/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [3840/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 141 [4096/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 141 [4352/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [4608/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 141 [4864/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [5120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [5376/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [5632/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 141 [5888/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 141 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [6400/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 141 [6656/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 141 [6912/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [7168/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [7424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [7680/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [7936/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 141 [8192/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 141 [8448/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 141 [8704/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 141 [8960/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [9216/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 141 [9472/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [9728/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [9984/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [10240/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 141 [10496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [10752/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 141 [11008/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 141 [11264/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 141 [11520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [11776/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 141 [12032/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 141 [12288/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 141 [12544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [12800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 141 [13056/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [13312/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 141 [13568/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 141 [13824/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 141 [14080/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [14336/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [14592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [14848/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [15104/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [15360/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [15616/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [15872/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 141 [16128/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 141 [16384/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 141 [16640/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 141 [16896/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [17152/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 141 [17408/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 141 [17664/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 141 [17920/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 141 [18176/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [18432/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 141 [18688/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 141 [18944/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [19200/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 141 [19456/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 141 [19712/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [19968/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [20224/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 141 [20480/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 141 [20736/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 141 [20992/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [21248/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 141 [21504/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [21760/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 141 [22016/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [22272/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 141 [22528/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [22784/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 141 [23040/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 141 [23296/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [23552/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [23808/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 141 [24064/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [24320/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 141 [24576/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 141 [24832/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 141 [25088/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [25344/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [25600/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [25856/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 141 [26112/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 141 [26368/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 141 [26624/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [26880/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 141 [27136/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [27392/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 141 [27648/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [27904/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [28160/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [28416/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [28672/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 141 [28928/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [29184/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 141 [29440/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 141 [29696/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [29952/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 141 [30208/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [30464/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [30720/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 141 [30976/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 141 [31232/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 141 [31488/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 141 [31744/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 141 [32000/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [32256/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 141 [32512/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [32768/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [33024/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [33280/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 141 [33536/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [33792/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [34048/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 141 [34304/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [34560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [34816/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [35072/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 141 [35328/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 141 [35584/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [35840/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 141 [36096/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 141 [36352/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [36608/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 141 [36864/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [37120/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [37376/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [37632/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [37888/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [38144/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [38400/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 141 [38656/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 141 [38912/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [39168/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 141 [39424/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 141 [39680/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [39936/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 141 [40192/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [40448/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 141 [40704/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 141 [40960/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [41216/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 141 [41472/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 141 [41728/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 141 [41984/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [42240/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 141 [42496/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [42752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 141 [43008/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 141 [43264/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 141 [43520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [43776/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 141 [44032/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 141 [44288/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 141 [44544/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 141 [44800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 141 [45056/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 141 [45312/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [45568/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 141 [45824/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [46080/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 141 [46336/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 141 [46592/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 141 [46848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 141 [47104/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 141 [47360/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [47616/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 141 [47872/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [48128/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 141 [48384/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 141 [48640/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 141 [48896/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 141 [49152/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 141 [49408/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [49664/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 141 [49920/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 141 [50000/50000]	Loss: 4.6064	LR: 0.040000
epoch 141 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  211526 GB |  211526 GB |
|       from large pool |  400448 KB |    1770 MB |  211336 GB |  211335 GB |
|       from small pool |    3550 KB |       9 MB |     190 GB |     190 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  211526 GB |  211526 GB |
|       from large pool |  400448 KB |    1770 MB |  211336 GB |  211335 GB |
|       from small pool |    3550 KB |       9 MB |     190 GB |     190 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  130149 GB |  130148 GB |
|       from large pool |  244672 KB |  473024 KB |  129931 GB |  129931 GB |
|       from small pool |    2594 KB |    4843 KB |     217 GB |     217 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9027 K  |    9027 K  |
|       from large pool |      36    |      77    |    4365 K  |    4365 K  |
|       from small pool |     187    |     225    |    4662 K  |    4662 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9027 K  |    9027 K  |
|       from large pool |      36    |      77    |    4365 K  |    4365 K  |
|       from small pool |     187    |     225    |    4662 K  |    4662 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    4584 K  |    4583 K  |
|       from large pool |      10    |      11    |    1808 K  |    1808 K  |
|       from small pool |      12    |      23    |    2775 K  |    2775 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 141, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 142 [256/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [512/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [1024/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [1280/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 142 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [1792/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [2048/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 142 [2304/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 142 [2560/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [2816/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [3072/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 142 [3328/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 142 [3584/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [3840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [4096/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [4352/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [4608/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [4864/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [5120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 142 [5376/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 142 [5632/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 142 [5888/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [6144/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 142 [6400/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [6656/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [6912/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 142 [7168/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [7424/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [7680/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 142 [7936/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [8192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [8448/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [8704/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [8960/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [9216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [9472/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 142 [9728/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [9984/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 142 [10240/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [10496/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [10752/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [11008/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 142 [11264/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [11520/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [11776/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [12032/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 142 [12288/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [12544/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 142 [12800/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 142 [13056/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [13312/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [13568/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 142 [13824/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [14080/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [14336/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [14592/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [14848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [15104/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [15360/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [15616/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 142 [15872/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [16128/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 142 [16384/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 142 [16640/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [16896/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 142 [17152/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 142 [17408/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 142 [17664/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [17920/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 142 [18176/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 142 [18432/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [18688/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [18944/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 142 [19200/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [19456/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 142 [19712/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [19968/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 142 [20224/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [20480/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [20736/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [20992/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [21248/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 142 [21504/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 142 [21760/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [22016/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 142 [22272/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 142 [22528/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 142 [22784/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [23040/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [23296/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [23552/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [23808/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 142 [24064/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 142 [24320/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [24576/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [24832/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [25088/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [25344/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [25600/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [25856/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [26112/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 142 [26368/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [26624/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [26880/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 142 [27136/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [27392/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 142 [27648/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 142 [27904/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 142 [28160/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 142 [28416/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 142 [28672/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 142 [28928/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 142 [29184/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [29440/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [29696/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [29952/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [30208/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [30464/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 142 [30720/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [30976/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 142 [31232/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 142 [31488/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 142 [31744/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [32000/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 142 [32256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 142 [32512/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [32768/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 142 [33024/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [33280/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [33536/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 142 [33792/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [34048/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [34304/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [34560/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 142 [34816/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 142 [35072/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [35328/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 142 [35584/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [35840/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [36096/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 142 [36352/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [36608/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [36864/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [37120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [37376/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 142 [37632/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [37888/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 142 [38144/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 142 [38400/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [38656/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [38912/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 142 [39168/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 142 [39424/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [39680/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 142 [39936/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 142 [40192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [40448/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 142 [40704/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [40960/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 142 [41216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [41472/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 142 [41728/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 142 [41984/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [42240/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [42496/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [42752/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [43008/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [43264/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [43520/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [43776/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [44032/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 142 [44288/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 142 [44544/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 142 [44800/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 142 [45056/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [45312/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [45568/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 142 [45824/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [46080/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 142 [46336/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [46592/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 142 [46848/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 142 [47104/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [47360/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 142 [47616/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 142 [47872/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 142 [48128/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 142 [48384/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [48640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 142 [48896/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 142 [49152/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 142 [49408/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 142 [49664/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 142 [49920/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 142 [50000/50000]	Loss: 4.6057	LR: 0.040000
epoch 142 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  213026 GB |  213026 GB |
|       from large pool |  400448 KB |    1770 MB |  212835 GB |  212834 GB |
|       from small pool |    3550 KB |       9 MB |     191 GB |     191 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  213026 GB |  213026 GB |
|       from large pool |  400448 KB |    1770 MB |  212835 GB |  212834 GB |
|       from small pool |    3550 KB |       9 MB |     191 GB |     191 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  131072 GB |  131071 GB |
|       from large pool |  244672 KB |  473024 KB |  130852 GB |  130852 GB |
|       from small pool |    2594 KB |    4843 KB |     219 GB |     219 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9091 K  |    9091 K  |
|       from large pool |      36    |      77    |    4396 K  |    4396 K  |
|       from small pool |     187    |     225    |    4695 K  |    4695 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9091 K  |    9091 K  |
|       from large pool |      36    |      77    |    4396 K  |    4396 K  |
|       from small pool |     187    |     225    |    4695 K  |    4695 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    4616 K  |    4616 K  |
|       from large pool |      10    |      11    |    1820 K  |    1820 K  |
|       from small pool |      10    |      23    |    2795 K  |    2795 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 142, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.48s

Training Epoch: 143 [256/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [512/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [1536/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [1792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [2048/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 143 [2560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [2816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [3072/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [3328/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [3584/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [3840/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [4096/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [4352/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [4608/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 143 [4864/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [5120/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [5376/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [5632/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [5888/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 143 [6144/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [6400/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [6656/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [6912/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [7168/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [7424/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [7680/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 143 [7936/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [8192/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 143 [8448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [8704/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [8960/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [9216/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [9472/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 143 [9728/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [9984/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [10240/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [10496/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [10752/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [11008/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 143 [11264/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 143 [11520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [11776/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [12032/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [12288/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [12544/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 143 [12800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [13056/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 143 [13312/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 143 [13568/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [13824/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [14080/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [14336/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 143 [14592/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [14848/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 143 [15104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 143 [15360/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [15616/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 143 [15872/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 143 [16128/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [16384/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 143 [16640/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 143 [16896/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 143 [17152/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 143 [17408/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [17664/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 143 [17920/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [18176/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 143 [18432/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [18688/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [18944/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [19200/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 143 [19456/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 143 [19712/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 143 [19968/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [20224/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 143 [20480/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 143 [20736/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 143 [20992/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 143 [21248/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [21504/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [21760/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 143 [22016/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [22272/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [22528/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 143 [22784/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [23040/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 143 [23296/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 143 [23552/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [23808/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 143 [24064/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 143 [24320/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [24576/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [24832/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 143 [25088/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [25344/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [25600/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [25856/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 143 [26112/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [26368/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 143 [26624/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [26880/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 143 [27136/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [27392/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [27648/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 143 [27904/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [28160/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 143 [28416/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 143 [28672/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 143 [28928/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [29184/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 143 [29440/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 143 [29696/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 143 [29952/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [30208/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [30464/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 143 [30720/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 143 [30976/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [31232/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 143 [31488/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 143 [31744/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 143 [32000/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 143 [32256/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 143 [32512/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 143 [32768/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [33024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [33280/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 143 [33536/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [33792/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [34048/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 143 [34304/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 143 [34560/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 143 [34816/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [35072/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 143 [35328/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [35584/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [35840/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 143 [36096/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [36352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [36608/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 143 [36864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 143 [37120/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [37376/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 143 [37632/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 143 [37888/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 143 [38144/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [38400/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [38656/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [38912/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 143 [39168/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 143 [39424/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 143 [39680/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 143 [39936/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 143 [40192/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 143 [40448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 143 [40704/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [40960/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [41216/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [41472/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 143 [41728/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [41984/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [42240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [42496/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [42752/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [43008/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 143 [43264/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 143 [43520/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 143 [43776/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 143 [44032/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 143 [44288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 143 [44544/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 143 [44800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [45056/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 143 [45312/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 143 [45568/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 143 [45824/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 143 [46080/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 143 [46336/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [46592/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 143 [46848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 143 [47104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 143 [47360/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [47616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 143 [47872/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 143 [48128/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [48384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 143 [48640/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 143 [48896/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 143 [49152/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 143 [49408/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 143 [49664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 143 [49920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 143 [50000/50000]	Loss: 4.6069	LR: 0.040000
epoch 143 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  214526 GB |  214526 GB |
|       from large pool |  400448 KB |    1770 MB |  214333 GB |  214333 GB |
|       from small pool |    3550 KB |       9 MB |     193 GB |     193 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  214526 GB |  214526 GB |
|       from large pool |  400448 KB |    1770 MB |  214333 GB |  214333 GB |
|       from small pool |    3550 KB |       9 MB |     193 GB |     193 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  131995 GB |  131994 GB |
|       from large pool |  244672 KB |  473024 KB |  131774 GB |  131774 GB |
|       from small pool |    2594 KB |    4843 KB |     220 GB |     220 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9155 K  |    9155 K  |
|       from large pool |      36    |      77    |    4427 K  |    4427 K  |
|       from small pool |     187    |     225    |    4728 K  |    4728 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9155 K  |    9155 K  |
|       from large pool |      36    |      77    |    4427 K  |    4427 K  |
|       from small pool |     187    |     225    |    4728 K  |    4728 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4649 K  |    4649 K  |
|       from large pool |      10    |      11    |    1833 K  |    1833 K  |
|       from small pool |      11    |      23    |    2815 K  |    2815 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 143, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 144 [256/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [512/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [768/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [1280/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [1536/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [1792/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 144 [2048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [2304/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [2560/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 144 [2816/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [3072/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 144 [3328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [3584/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 144 [3840/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [4096/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [4352/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 144 [4608/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [4864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [5120/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [5376/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 144 [5632/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 144 [5888/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 144 [6144/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 144 [6400/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 144 [6656/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [6912/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [7168/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [7424/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [7680/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 144 [7936/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 144 [8192/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 144 [8448/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [8704/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [8960/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [9216/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 144 [9472/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [9728/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 144 [9984/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 144 [10240/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [10496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 144 [10752/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [11008/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 144 [11264/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [11520/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [11776/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [12032/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [12288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [12544/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 144 [12800/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [13056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 144 [13312/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 144 [13568/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 144 [13824/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 144 [14080/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [14336/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 144 [14592/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 144 [14848/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [15104/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [15360/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [15616/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [15872/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 144 [16128/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 144 [16384/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [16640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [16896/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [17152/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [17408/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [17664/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 144 [17920/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [18176/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [18432/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [18688/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [18944/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [19200/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [19456/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [19712/50000]	Loss: 4.6034	LR: 0.040000
Training Epoch: 144 [19968/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [20224/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 144 [20480/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [20736/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 144 [20992/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 144 [21248/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [21504/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [21760/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 144 [22016/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 144 [22272/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [22528/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [22784/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 144 [23040/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 144 [23296/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [23552/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [23808/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 144 [24064/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 144 [24320/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [24576/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [24832/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 144 [25088/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 144 [25344/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 144 [25600/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [25856/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [26112/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 144 [26368/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [26624/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [26880/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 144 [27136/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 144 [27392/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [27648/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [27904/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [28160/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [28416/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 144 [28672/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [28928/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [29184/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [29440/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [29696/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [29952/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 144 [30208/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [30464/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [30720/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [30976/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 144 [31232/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 144 [31488/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [31744/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [32000/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 144 [32256/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 144 [32512/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [32768/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [33024/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 144 [33280/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 144 [33536/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 144 [33792/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [34048/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 144 [34304/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [34560/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 144 [34816/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 144 [35072/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [35328/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [35584/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [35840/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 144 [36096/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [36352/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 144 [36608/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 144 [36864/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [37120/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [37376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [37632/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 144 [37888/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [38144/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 144 [38400/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 144 [38656/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [38912/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 144 [39168/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 144 [39424/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [39680/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 144 [39936/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [40192/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 144 [40448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [40704/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [40960/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [41216/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 144 [41472/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 144 [41728/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 144 [41984/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 144 [42240/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [42496/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [42752/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [43008/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [43264/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [43520/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [43776/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 144 [44032/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [44288/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 144 [44544/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 144 [44800/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [45056/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 144 [45312/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 144 [45568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [45824/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [46080/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 144 [46336/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 144 [46592/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 144 [46848/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [47104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [47360/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [47616/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 144 [47872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 144 [48128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [48384/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 144 [48640/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 144 [49152/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 144 [49408/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 144 [49664/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 144 [49920/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 144 [50000/50000]	Loss: 4.6072	LR: 0.040000
epoch 144 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  216027 GB |  216026 GB |
|       from large pool |  400448 KB |    1770 MB |  215832 GB |  215832 GB |
|       from small pool |    3550 KB |       9 MB |     194 GB |     194 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  216027 GB |  216026 GB |
|       from large pool |  400448 KB |    1770 MB |  215832 GB |  215832 GB |
|       from small pool |    3550 KB |       9 MB |     194 GB |     194 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  132918 GB |  132917 GB |
|       from large pool |  244672 KB |  473024 KB |  132695 GB |  132695 GB |
|       from small pool |    2594 KB |    4843 KB |     222 GB |     222 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9219 K  |    9219 K  |
|       from large pool |      36    |      77    |    4458 K  |    4458 K  |
|       from small pool |     187    |     225    |    4761 K  |    4761 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9219 K  |    9219 K  |
|       from large pool |      36    |      77    |    4458 K  |    4458 K  |
|       from small pool |     187    |     225    |    4761 K  |    4761 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    4680 K  |    4680 K  |
|       from large pool |      10    |      11    |    1846 K  |    1846 K  |
|       from small pool |       9    |      23    |    2833 K  |    2833 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 144, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 145 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [512/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [768/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 145 [1024/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 145 [1280/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [1536/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [1792/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [2048/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [2304/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 145 [2560/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 145 [2816/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [3072/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [3328/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 145 [3584/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [3840/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 145 [4096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 145 [4352/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 145 [4608/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 145 [4864/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 145 [5120/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [5376/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 145 [5632/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 145 [5888/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [6144/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 145 [6400/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [6656/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 145 [6912/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [7168/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 145 [7424/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [7680/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [7936/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 145 [8192/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [8448/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 145 [8704/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [8960/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 145 [9216/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 145 [9472/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [9728/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [9984/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 145 [10240/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 145 [10496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 145 [10752/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 145 [11008/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [11264/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 145 [11520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [11776/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [12032/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 145 [12288/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [12544/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [12800/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [13056/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [13312/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [13568/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 145 [13824/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 145 [14080/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [14336/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 145 [14592/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 145 [14848/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [15104/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [15360/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 145 [15616/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 145 [15872/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [16128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [16384/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [16640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [16896/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 145 [17152/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [17408/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [17664/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [17920/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 145 [18176/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 145 [18432/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 145 [18688/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [18944/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 145 [19200/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [19456/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [19712/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 145 [19968/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [20224/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 145 [20480/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 145 [20736/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [20992/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [21248/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [21504/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 145 [21760/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 145 [22016/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [22272/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [22528/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [22784/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [23040/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 145 [23296/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [23552/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 145 [23808/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [24064/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [24320/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 145 [24576/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [24832/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 145 [25088/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 145 [25344/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 145 [25600/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [25856/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [26112/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [26368/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [26624/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [26880/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 145 [27136/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [27392/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [27648/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [27904/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [28160/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 145 [28416/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [28672/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [28928/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [29184/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 145 [29440/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 145 [29696/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [29952/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [30208/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [30464/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 145 [30720/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 145 [30976/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [31232/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [31488/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 145 [31744/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [32000/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [32256/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [32512/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [32768/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [33024/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [33280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 145 [33536/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [33792/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [34048/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [34304/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 145 [34560/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [34816/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 145 [35072/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 145 [35328/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [35584/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [35840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 145 [36096/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [36352/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [36608/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [36864/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 145 [37120/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 145 [37376/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [37632/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 145 [37888/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [38144/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 145 [38400/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [38656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [38912/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [39168/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 145 [39424/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [39680/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [39936/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [40192/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 145 [40448/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [40704/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [40960/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [41216/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [41472/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [41728/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [41984/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [42240/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 145 [42496/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [42752/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [43008/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 145 [43264/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 145 [43520/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 145 [43776/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [44032/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [44288/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [44544/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [44800/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [45056/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 145 [45312/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [45568/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 145 [45824/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [46336/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [46592/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 145 [46848/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 145 [47104/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [47360/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 145 [47616/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [47872/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 145 [48128/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 145 [48384/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 145 [48640/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 145 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 145 [49152/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 145 [49408/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 145 [49664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 145 [49920/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 145 [50000/50000]	Loss: 4.6060	LR: 0.040000
epoch 145 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  217527 GB |  217526 GB |
|       from large pool |  400448 KB |    1770 MB |  217331 GB |  217331 GB |
|       from small pool |    3550 KB |       9 MB |     195 GB |     195 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  217527 GB |  217526 GB |
|       from large pool |  400448 KB |    1770 MB |  217331 GB |  217331 GB |
|       from small pool |    3550 KB |       9 MB |     195 GB |     195 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  133841 GB |  133840 GB |
|       from large pool |  244672 KB |  473024 KB |  133617 GB |  133617 GB |
|       from small pool |    2594 KB |    4843 KB |     223 GB |     223 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9283 K  |    9283 K  |
|       from large pool |      36    |      77    |    4489 K  |    4489 K  |
|       from small pool |     187    |     225    |    4794 K  |    4794 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9283 K  |    9283 K  |
|       from large pool |      36    |      77    |    4489 K  |    4489 K  |
|       from small pool |     187    |     225    |    4794 K  |    4794 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    4711 K  |    4711 K  |
|       from large pool |      10    |      11    |    1859 K  |    1859 K  |
|       from small pool |       9    |      23    |    2851 K  |    2851 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 145, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 146 [256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 146 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [1024/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [1280/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 146 [1536/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [1792/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [2048/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 146 [2304/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [2560/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [2816/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [3072/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 146 [3328/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 146 [3584/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 146 [3840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [4096/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 146 [4352/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [4608/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [4864/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [5120/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [5376/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [5632/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 146 [5888/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 146 [6144/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 146 [6400/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [6656/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [6912/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [7168/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [7424/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [7680/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 146 [7936/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [8192/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [8448/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 146 [8704/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [8960/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [9216/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [9472/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 146 [9728/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 146 [9984/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [10240/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [10496/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 146 [10752/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [11008/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [11264/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [11520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [11776/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 146 [12032/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [12288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [12544/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [12800/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 146 [13056/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [13312/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 146 [13568/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [13824/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 146 [14080/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [14336/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 146 [14592/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 146 [14848/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 146 [15104/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [15360/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 146 [15616/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 146 [15872/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 146 [16128/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 146 [16384/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 146 [16640/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [16896/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 146 [17152/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 146 [17408/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [17664/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [17920/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [18176/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [18432/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 146 [18688/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 146 [18944/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 146 [19200/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 146 [19456/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [19712/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [19968/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [20224/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 146 [20480/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 146 [20736/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [20992/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 146 [21248/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 146 [21504/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 146 [21760/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [22016/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [22272/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 146 [22528/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [22784/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [23040/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [23296/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [23552/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [23808/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [24064/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 146 [24320/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 146 [24576/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [24832/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 146 [25088/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [25344/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 146 [25600/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [25856/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [26112/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [26368/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [26624/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 146 [26880/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 146 [27136/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 146 [27392/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [27648/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 146 [27904/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [28160/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [28416/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 146 [28672/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [28928/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 146 [29184/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 146 [29440/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 146 [29696/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [29952/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [30208/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [30464/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [30720/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [30976/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 146 [31232/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 146 [31488/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [31744/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [32000/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [32256/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [32512/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 146 [32768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 146 [33024/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [33280/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [33536/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [33792/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 146 [34048/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 146 [34304/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [34560/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [34816/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 146 [35072/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [35328/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [35584/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [35840/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 146 [36096/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 146 [36352/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [36608/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [36864/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 146 [37120/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [37376/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [37632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [37888/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 146 [38144/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 146 [38400/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 146 [38656/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 146 [38912/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [39168/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [39424/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 146 [39680/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [39936/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 146 [40192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [40448/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [40704/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 146 [40960/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 146 [41216/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 146 [41472/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [41728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 146 [41984/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 146 [42240/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [42496/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [42752/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 146 [43008/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 146 [43264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [43520/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [43776/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [44032/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 146 [44288/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [44544/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [44800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [45056/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [45312/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [45568/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [45824/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 146 [46080/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 146 [46336/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 146 [46592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 146 [46848/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [47104/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 146 [47360/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [47616/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [47872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 146 [48128/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 146 [48384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 146 [48640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 146 [48896/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 146 [49152/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 146 [49408/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 146 [49664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [49920/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 146 [50000/50000]	Loss: 4.6061	LR: 0.040000
epoch 146 training time consumed: 21.73s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  219027 GB |  219027 GB |
|       from large pool |  400448 KB |    1770 MB |  218830 GB |  218829 GB |
|       from small pool |    3550 KB |       9 MB |     197 GB |     197 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  219027 GB |  219027 GB |
|       from large pool |  400448 KB |    1770 MB |  218830 GB |  218829 GB |
|       from small pool |    3550 KB |       9 MB |     197 GB |     197 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  134764 GB |  134763 GB |
|       from large pool |  244672 KB |  473024 KB |  134538 GB |  134538 GB |
|       from small pool |    2594 KB |    4843 KB |     225 GB |     225 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9347 K  |    9347 K  |
|       from large pool |      36    |      77    |    4520 K  |    4520 K  |
|       from small pool |     187    |     225    |    4827 K  |    4827 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9347 K  |    9347 K  |
|       from large pool |      36    |      77    |    4520 K  |    4520 K  |
|       from small pool |     187    |     225    |    4827 K  |    4827 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4743 K  |    4743 K  |
|       from large pool |      10    |      11    |    1872 K  |    1872 K  |
|       from small pool |      11    |      23    |    2871 K  |    2871 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 146, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.48s

Training Epoch: 147 [256/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [512/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 147 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [1024/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [1280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [1536/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [1792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [2048/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 147 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [2560/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [2816/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 147 [3072/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [3328/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 147 [3584/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 147 [3840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [4096/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [4352/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [4608/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 147 [4864/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 147 [5120/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 147 [5376/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [5632/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [5888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [6144/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 147 [6400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 147 [6656/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [6912/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [7168/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 147 [7424/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 147 [7680/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [7936/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [8192/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 147 [8448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [8704/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 147 [8960/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [9216/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [9472/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [9728/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [9984/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [10240/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [10496/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [10752/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 147 [11008/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [11264/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 147 [11520/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 147 [11776/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [12032/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 147 [12288/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [12544/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 147 [12800/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [13056/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 147 [13312/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [13568/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [13824/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [14080/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 147 [14336/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [14592/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [14848/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 147 [15104/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [15360/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [15616/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 147 [15872/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 147 [16128/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [16384/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 147 [16640/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 147 [16896/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [17152/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [17408/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [17664/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [17920/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [18176/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [18432/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [18688/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [18944/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [19200/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [19456/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [19712/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [19968/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 147 [20224/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [20480/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [20736/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [20992/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 147 [21248/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [21504/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [21760/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 147 [22016/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [22272/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [22528/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [22784/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [23040/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [23296/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [23552/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [23808/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 147 [24064/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [24320/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [24576/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 147 [24832/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [25088/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 147 [25344/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 147 [25600/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [25856/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [26112/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [26368/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [26624/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 147 [26880/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 147 [27136/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 147 [27392/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [27648/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 147 [27904/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 147 [28160/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [28416/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [28672/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 147 [28928/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 147 [29184/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [29440/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [29696/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 147 [29952/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [30208/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 147 [30464/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [30720/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [30976/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [31232/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [31488/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 147 [31744/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 147 [32000/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [32256/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 147 [32512/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [32768/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 147 [33024/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [33280/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 147 [33536/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 147 [33792/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [34048/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 147 [34304/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 147 [34560/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [34816/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 147 [35072/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [35328/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 147 [35584/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [35840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [36096/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 147 [36352/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 147 [36608/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [36864/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [37120/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 147 [37376/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [37632/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [37888/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [38144/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [38400/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [38656/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [38912/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [39168/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [39424/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [39680/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 147 [39936/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [40192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 147 [40448/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 147 [40704/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 147 [40960/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [41216/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [41472/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [41728/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [41984/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [42240/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [42496/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [42752/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 147 [43008/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [43264/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 147 [43520/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 147 [43776/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 147 [44032/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [44288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 147 [44544/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [44800/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 147 [45056/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 147 [45312/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [45568/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 147 [45824/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 147 [46080/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 147 [46336/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 147 [46592/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 147 [46848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [47104/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [47360/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 147 [47616/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 147 [47872/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 147 [48128/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [48384/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [48640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 147 [48896/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [49152/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 147 [49408/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [49664/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 147 [49920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 147 [50000/50000]	Loss: 4.6064	LR: 0.040000
epoch 147 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  220527 GB |  220527 GB |
|       from large pool |  400448 KB |    1770 MB |  220329 GB |  220328 GB |
|       from small pool |    3550 KB |       9 MB |     198 GB |     198 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  220527 GB |  220527 GB |
|       from large pool |  400448 KB |    1770 MB |  220329 GB |  220328 GB |
|       from small pool |    3550 KB |       9 MB |     198 GB |     198 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  135687 GB |  135687 GB |
|       from large pool |  244672 KB |  473024 KB |  135460 GB |  135460 GB |
|       from small pool |    2594 KB |    4843 KB |     226 GB |     226 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9412 K  |    9411 K  |
|       from large pool |      36    |      77    |    4551 K  |    4551 K  |
|       from small pool |     187    |     225    |    4860 K  |    4860 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9412 K  |    9411 K  |
|       from large pool |      36    |      77    |    4551 K  |    4551 K  |
|       from small pool |     187    |     225    |    4860 K  |    4860 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    4775 K  |    4775 K  |
|       from large pool |      10    |      11    |    1885 K  |    1885 K  |
|       from small pool |      10    |      23    |    2890 K  |    2890 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 147, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 148 [256/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [512/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [1024/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [1280/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [1792/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 148 [2048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 148 [2304/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [2560/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [2816/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 148 [3072/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [3328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [3584/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [3840/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 148 [4096/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [4352/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 148 [4608/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [4864/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 148 [5120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [5376/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 148 [5632/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [5888/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 148 [6400/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 148 [6656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [6912/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [7168/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 148 [7424/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [7680/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [7936/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 148 [8192/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 148 [8448/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [8704/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 148 [8960/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [9216/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [9472/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 148 [9728/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [9984/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [10240/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [10496/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [10752/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [11008/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 148 [11264/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 148 [11520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [11776/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [12032/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [12288/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 148 [12544/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [12800/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 148 [13056/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [13312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [13568/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 148 [13824/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 148 [14080/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [14336/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 148 [14592/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [14848/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 148 [15104/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [15360/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [15616/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 148 [15872/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 148 [16128/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [16384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 148 [16640/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [16896/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 148 [17152/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 148 [17408/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [17664/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [17920/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [18176/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [18432/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [18688/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 148 [18944/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 148 [19200/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 148 [19456/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [19712/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [19968/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [20224/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 148 [20480/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [20736/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [20992/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 148 [21248/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [21504/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [21760/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [22016/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [22272/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 148 [22528/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [22784/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 148 [23040/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [23296/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [23552/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 148 [23808/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [24064/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [24320/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 148 [24576/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [24832/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 148 [25088/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [25344/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 148 [25600/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [25856/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [26112/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 148 [26368/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 148 [26624/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 148 [26880/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [27136/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [27392/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [27648/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 148 [27904/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 148 [28160/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 148 [28416/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [28672/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 148 [28928/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 148 [29184/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 148 [29440/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 148 [29696/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 148 [29952/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [30208/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 148 [30464/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 148 [30720/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [30976/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 148 [31232/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 148 [31488/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [31744/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 148 [32000/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [32256/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [32512/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [32768/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 148 [33024/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [33280/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 148 [33536/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [33792/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [34048/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [34304/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [34560/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [34816/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [35072/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 148 [35328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [35584/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 148 [35840/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [36096/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 148 [36352/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [36608/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 148 [36864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [37120/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [37376/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 148 [37632/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [37888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 148 [38144/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [38400/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 148 [38656/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [38912/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 148 [39168/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 148 [39424/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [39680/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [39936/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 148 [40192/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 148 [40448/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 148 [40704/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [40960/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 148 [41216/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [41472/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [41728/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 148 [41984/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [42240/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [42496/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 148 [42752/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 148 [43008/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [43264/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [43520/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 148 [43776/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 148 [44032/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [44288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 148 [44544/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 148 [44800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 148 [45056/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 148 [45312/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 148 [45568/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [45824/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [46080/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 148 [46336/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 148 [46592/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 148 [46848/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 148 [47104/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [47360/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [47616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [47872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 148 [48128/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 148 [48384/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 148 [48640/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [48896/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 148 [49152/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 148 [49408/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 148 [49664/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 148 [49920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 148 [50000/50000]	Loss: 4.6056	LR: 0.040000
epoch 148 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  222027 GB |  222027 GB |
|       from large pool |  400448 KB |    1770 MB |  221828 GB |  221827 GB |
|       from small pool |    3550 KB |       9 MB |     199 GB |     199 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  222027 GB |  222027 GB |
|       from large pool |  400448 KB |    1770 MB |  221828 GB |  221827 GB |
|       from small pool |    3550 KB |       9 MB |     199 GB |     199 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  136610 GB |  136610 GB |
|       from large pool |  244672 KB |  473024 KB |  136381 GB |  136381 GB |
|       from small pool |    2594 KB |    4843 KB |     228 GB |     228 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9476 K  |    9475 K  |
|       from large pool |      36    |      77    |    4582 K  |    4581 K  |
|       from small pool |     187    |     225    |    4894 K  |    4893 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9476 K  |    9475 K  |
|       from large pool |      36    |      77    |    4582 K  |    4581 K  |
|       from small pool |     187    |     225    |    4894 K  |    4893 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    4807 K  |    4807 K  |
|       from large pool |      10    |      11    |    1897 K  |    1897 K  |
|       from small pool |      10    |      23    |    2909 K  |    2909 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 148, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 149 [256/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 149 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [768/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [1024/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [1280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [1536/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 149 [2048/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 149 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [2560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [2816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [3072/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 149 [3328/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [3584/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 149 [3840/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [4096/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [4352/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [4608/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 149 [4864/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [5120/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [5376/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 149 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [5888/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [6144/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 149 [6400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [6656/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 149 [6912/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 149 [7168/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [7424/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [7680/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [7936/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 149 [8192/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 149 [8448/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [8704/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [8960/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [9216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [9472/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 149 [9728/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 149 [9984/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [10240/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [10496/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 149 [10752/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [11008/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [11264/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [11520/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 149 [11776/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 149 [12032/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 149 [12288/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 149 [12544/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 149 [12800/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [13056/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 149 [13312/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [13568/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [13824/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [14080/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [14336/50000]	Loss: 4.6085	LR: 0.040000
Training Epoch: 149 [14592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [14848/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [15104/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [15360/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [15616/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [15872/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [16128/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [16384/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [16640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [16896/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 149 [17152/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 149 [17408/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 149 [17664/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [17920/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 149 [18176/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 149 [18432/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 149 [18688/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [18944/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [19200/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [19456/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [19712/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [19968/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 149 [20224/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [20480/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 149 [20736/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [20992/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 149 [21248/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [21504/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 149 [21760/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [22016/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 149 [22272/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [22528/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [22784/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 149 [23040/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [23296/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 149 [23552/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [23808/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [24064/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 149 [24320/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [24576/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [24832/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 149 [25088/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 149 [25344/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [25600/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [25856/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [26112/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 149 [26368/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [26624/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 149 [26880/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [27136/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [27392/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [27648/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 149 [27904/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [28160/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [28416/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 149 [28672/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 149 [28928/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [29184/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [29440/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 149 [29696/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [29952/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 149 [30208/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [30464/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 149 [30720/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [30976/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [31232/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [31488/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 149 [31744/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [32000/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 149 [32256/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 149 [32512/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [32768/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 149 [33024/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [33280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [33536/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [33792/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [34048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [34304/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 149 [34560/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 149 [34816/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 149 [35072/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [35328/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 149 [35584/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 149 [35840/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [36096/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [36352/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 149 [36608/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 149 [36864/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [37120/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 149 [37376/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [37632/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [37888/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [38144/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 149 [38400/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [38656/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [38912/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [39168/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [39424/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [39680/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [39936/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 149 [40192/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [40448/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 149 [40704/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 149 [40960/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [41216/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [41472/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [41728/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [41984/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 149 [42240/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 149 [42496/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [42752/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [43008/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [43264/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 149 [43520/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 149 [43776/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 149 [44032/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 149 [44288/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [44544/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 149 [44800/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [45056/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 149 [45312/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 149 [45568/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [45824/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [46080/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [46336/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [46592/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 149 [46848/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [47104/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 149 [47360/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 149 [47616/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 149 [47872/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 149 [48128/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 149 [48384/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 149 [48640/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [48896/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 149 [49152/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 149 [49408/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 149 [49664/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 149 [49920/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 149 [50000/50000]	Loss: 4.6063	LR: 0.040000
epoch 149 training time consumed: 21.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  223527 GB |  223527 GB |
|       from large pool |  400448 KB |    1770 MB |  223326 GB |  223326 GB |
|       from small pool |    3550 KB |       9 MB |     201 GB |     201 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  223527 GB |  223527 GB |
|       from large pool |  400448 KB |    1770 MB |  223326 GB |  223326 GB |
|       from small pool |    3550 KB |       9 MB |     201 GB |     201 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  137533 GB |  137533 GB |
|       from large pool |  244672 KB |  473024 KB |  137303 GB |  137303 GB |
|       from small pool |    2594 KB |    4843 KB |     229 GB |     229 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9540 K  |    9539 K  |
|       from large pool |      36    |      77    |    4612 K  |    4612 K  |
|       from small pool |     187    |     225    |    4927 K  |    4926 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9540 K  |    9539 K  |
|       from large pool |      36    |      77    |    4612 K  |    4612 K  |
|       from small pool |     187    |     225    |    4927 K  |    4926 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    4839 K  |    4839 K  |
|       from large pool |      10    |      11    |    1910 K  |    1910 K  |
|       from small pool |      13    |      23    |    2928 K  |    2928 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 149, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 150 [256/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [512/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [768/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 150 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 150 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [1536/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 150 [1792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [2048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [2304/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 150 [2560/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [2816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [3072/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [3328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [3584/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [3840/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [4096/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 150 [4352/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [4608/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 150 [4864/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 150 [5120/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [5376/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [5632/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [5888/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [6144/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 150 [6400/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 150 [6656/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 150 [6912/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [7168/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 150 [7424/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 150 [7680/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [7936/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 150 [8192/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 150 [8448/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 150 [8704/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 150 [8960/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 150 [9216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [9472/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [9728/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [9984/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 150 [10240/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [10496/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 150 [10752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [11008/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [11264/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [11520/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [11776/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 150 [12032/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [12288/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [12544/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [12800/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [13056/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [13312/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [13568/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 150 [13824/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [14080/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [14336/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [14592/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 150 [14848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 150 [15104/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [15360/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [15616/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [15872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [16128/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [16384/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [16640/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [16896/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [17152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [17408/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [17664/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 150 [17920/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [18176/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 150 [18432/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [18688/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [18944/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [19200/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [19456/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [19712/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [19968/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [20224/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [20480/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [20736/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 150 [20992/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [21248/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 150 [21504/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 150 [21760/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [22016/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [22272/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [22528/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [22784/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [23040/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 150 [23296/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [23552/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [23808/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [24064/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [24320/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 150 [24576/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 150 [24832/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [25088/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [25344/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 150 [25600/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [25856/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [26112/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 150 [26368/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 150 [26624/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [26880/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [27136/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 150 [27392/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [27648/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [27904/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [28160/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [28416/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 150 [28672/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [28928/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [29184/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 150 [29440/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [29696/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [29952/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [30208/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [30464/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [30720/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [30976/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 150 [31232/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [31488/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 150 [31744/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [32000/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 150 [32256/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [32512/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [32768/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [33024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 150 [33280/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 150 [33536/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [33792/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 150 [34048/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [34304/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [34560/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [34816/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [35072/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [35328/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [35584/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [35840/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 150 [36096/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [36352/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 150 [36608/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [36864/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 150 [37120/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 150 [37376/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 150 [37632/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [37888/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [38144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [38400/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 150 [38656/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [38912/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [39168/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 150 [39424/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [39680/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 150 [39936/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 150 [40192/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [40448/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [40704/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [40960/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [41216/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [41472/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [41728/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 150 [41984/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [42240/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 150 [42496/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [42752/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 150 [43008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 150 [43264/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 150 [43520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [43776/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 150 [44032/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 150 [44288/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [44544/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [44800/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 150 [45056/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [45312/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 150 [45568/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [45824/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 150 [46080/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [46336/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 150 [46592/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 150 [46848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [47104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [47360/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [47616/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 150 [47872/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 150 [48128/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 150 [48384/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 150 [48640/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [48896/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 150 [49152/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 150 [49408/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 150 [49664/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 150 [49920/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 150 [50000/50000]	Loss: 4.6052	LR: 0.040000
epoch 150 training time consumed: 21.73s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  225028 GB |  225027 GB |
|       from large pool |  400448 KB |    1770 MB |  224825 GB |  224825 GB |
|       from small pool |    3550 KB |       9 MB |     202 GB |     202 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  225028 GB |  225027 GB |
|       from large pool |  400448 KB |    1770 MB |  224825 GB |  224825 GB |
|       from small pool |    3550 KB |       9 MB |     202 GB |     202 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  138456 GB |  138456 GB |
|       from large pool |  244672 KB |  473024 KB |  138224 GB |  138224 GB |
|       from small pool |    2594 KB |    4843 KB |     231 GB |     231 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9604 K  |    9603 K  |
|       from large pool |      36    |      77    |    4643 K  |    4643 K  |
|       from small pool |     187    |     225    |    4960 K  |    4959 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9604 K  |    9603 K  |
|       from large pool |      36    |      77    |    4643 K  |    4643 K  |
|       from small pool |     187    |     225    |    4960 K  |    4959 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    4872 K  |    4871 K  |
|       from large pool |      10    |      11    |    1923 K  |    1923 K  |
|       from small pool |      11    |      23    |    2948 K  |    2948 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 150, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-150-regular.pth
Training Epoch: 151 [256/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 151 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [768/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 151 [1024/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 151 [1280/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 151 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 151 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 151 [2048/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [2304/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [2560/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 151 [2816/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [3072/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 151 [3328/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [3584/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 151 [3840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [4096/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 151 [4352/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 151 [4608/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 151 [4864/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 151 [5120/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 151 [5376/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 151 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [5888/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 151 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 151 [6400/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 151 [6656/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [6912/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 151 [7168/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [7424/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 151 [7680/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 151 [7936/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [8192/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [8448/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [8704/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [8960/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [9216/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 151 [9472/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 151 [9728/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 151 [9984/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [10240/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [10496/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [10752/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 151 [11008/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [11264/50000]	Loss: 4.6034	LR: 0.040000
Training Epoch: 151 [11520/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 151 [11776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [12032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [12288/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 151 [12544/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 151 [12800/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [13056/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 151 [13312/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [13568/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [13824/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [14080/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [14336/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [14592/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 151 [14848/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [15104/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 151 [15360/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [15616/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [15872/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [16128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [16384/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 151 [16640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [16896/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [17152/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [17408/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [17664/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 151 [17920/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [18176/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [18432/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 151 [18688/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 151 [18944/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [19200/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 151 [19456/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 151 [19712/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 151 [19968/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 151 [20224/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [20480/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 151 [20736/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 151 [20992/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [21248/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 151 [21504/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [21760/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 151 [22016/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [22272/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 151 [22528/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [22784/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [23040/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [23296/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 151 [23552/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [23808/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [24064/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [24320/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [24576/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 151 [24832/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 151 [25088/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [25344/50000]	Loss: 4.6084	LR: 0.040000
Training Epoch: 151 [25600/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 151 [25856/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 151 [26112/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [26368/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 151 [26624/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 151 [26880/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 151 [27136/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [27392/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [27648/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 151 [27904/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 151 [28160/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [28416/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [28672/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [28928/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [29184/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [29440/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [29696/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [29952/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [30208/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [30464/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 151 [30720/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [30976/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 151 [31232/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [31488/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 151 [31744/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [32000/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [32256/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [32512/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 151 [32768/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [33024/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 151 [33280/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [33536/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 151 [33792/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [34048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [34304/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 151 [34560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [34816/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [35072/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [35328/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [35584/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 151 [35840/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 151 [36096/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [36352/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [36608/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 151 [36864/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 151 [37120/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 151 [37376/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 151 [37632/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [37888/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [38144/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 151 [38400/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [38656/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [38912/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 151 [39168/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [39424/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 151 [39680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [39936/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [40192/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [40448/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [40704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [40960/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 151 [41216/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [41472/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 151 [41728/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 151 [41984/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 151 [42240/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [42496/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [42752/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 151 [43008/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [43264/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [43520/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 151 [43776/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [44032/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 151 [44288/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 151 [44544/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 151 [44800/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 151 [45056/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [45312/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 151 [45568/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [45824/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 151 [46080/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 151 [46336/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [46592/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 151 [46848/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 151 [47104/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [47360/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [47616/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 151 [47872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [48128/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 151 [48384/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [48640/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [48896/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 151 [49152/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 151 [49408/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 151 [49664/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 151 [49920/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 151 [50000/50000]	Loss: 4.6059	LR: 0.040000
epoch 151 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  226528 GB |  226527 GB |
|       from large pool |  400448 KB |    1770 MB |  226324 GB |  226324 GB |
|       from small pool |    3550 KB |       9 MB |     203 GB |     203 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  226528 GB |  226527 GB |
|       from large pool |  400448 KB |    1770 MB |  226324 GB |  226324 GB |
|       from small pool |    3550 KB |       9 MB |     203 GB |     203 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  139379 GB |  139379 GB |
|       from large pool |  244672 KB |  473024 KB |  139146 GB |  139146 GB |
|       from small pool |    2594 KB |    4843 KB |     233 GB |     233 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9668 K  |    9667 K  |
|       from large pool |      36    |      77    |    4674 K  |    4674 K  |
|       from small pool |     187    |     225    |    4993 K  |    4993 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9668 K  |    9667 K  |
|       from large pool |      36    |      77    |    4674 K  |    4674 K  |
|       from small pool |     187    |     225    |    4993 K  |    4993 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    4904 K  |    4904 K  |
|       from large pool |      10    |      11    |    1936 K  |    1936 K  |
|       from small pool |      13    |      23    |    2968 K  |    2968 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 151, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 152 [256/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [768/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [1024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [1536/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [1792/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [2048/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [2304/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [2560/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [2816/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [3072/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [3328/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [3584/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [3840/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [4096/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [4608/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [4864/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [5120/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 152 [5376/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [5632/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [5888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [6144/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 152 [6400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [6656/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 152 [6912/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 152 [7168/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [7424/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [7680/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [7936/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 152 [8192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [8448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [8704/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [8960/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [9216/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [9472/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [9728/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [9984/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 152 [10240/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 152 [10496/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 152 [10752/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [11008/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [11264/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [11520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [11776/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [12032/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 152 [12288/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [12544/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [12800/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [13056/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [13312/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [13568/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [13824/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 152 [14080/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 152 [14336/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 152 [14592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [14848/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [15104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [15360/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [15616/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 152 [15872/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 152 [16128/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [16384/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 152 [16640/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [16896/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [17152/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [17408/50000]	Loss: 4.6092	LR: 0.040000
Training Epoch: 152 [17664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [17920/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [18176/50000]	Loss: 4.6087	LR: 0.040000
Training Epoch: 152 [18432/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 152 [18688/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [18944/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [19200/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [19456/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [19712/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 152 [19968/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 152 [20224/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [20480/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [20736/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 152 [20992/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [21248/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 152 [21504/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [21760/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [22016/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [22272/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [22528/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [22784/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 152 [23040/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 152 [23296/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [23552/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [23808/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [24064/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [24320/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 152 [24576/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 152 [24832/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [25088/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [25344/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [25600/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [25856/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [26112/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [26368/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [26624/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 152 [26880/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [27136/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [27392/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 152 [27648/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 152 [27904/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [28160/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [28416/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [28672/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 152 [28928/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [29184/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 152 [29440/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [29696/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 152 [29952/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 152 [30208/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [30464/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 152 [30720/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [30976/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [31232/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [31488/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [31744/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [32000/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [32256/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [32512/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [32768/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 152 [33024/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 152 [33280/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [33536/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 152 [33792/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 152 [34048/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [34304/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [34560/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [34816/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [35072/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [35328/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 152 [35584/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [35840/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [36096/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [36352/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [36608/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [36864/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [37120/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [37376/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [37632/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [37888/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [38144/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 152 [38400/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [38656/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 152 [38912/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 152 [39168/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [39424/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 152 [39680/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 152 [39936/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 152 [40192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [40448/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [40704/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [40960/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [41216/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [41472/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [41728/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [41984/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 152 [42240/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 152 [42496/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [42752/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [43008/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 152 [43264/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 152 [43520/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [43776/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 152 [44032/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [44288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [44544/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [44800/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [45056/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 152 [45312/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [45568/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [45824/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [46080/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 152 [46336/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [46592/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 152 [46848/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 152 [47104/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 152 [47360/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 152 [47616/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [47872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 152 [48128/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 152 [48384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [48640/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 152 [48896/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 152 [49152/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 152 [49408/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 152 [49664/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 152 [49920/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 152 [50000/50000]	Loss: 4.6072	LR: 0.040000
epoch 152 training time consumed: 21.75s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  228028 GB |  228028 GB |
|       from large pool |  400448 KB |    1770 MB |  227823 GB |  227822 GB |
|       from small pool |    3550 KB |       9 MB |     205 GB |     205 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  228028 GB |  228028 GB |
|       from large pool |  400448 KB |    1770 MB |  227823 GB |  227822 GB |
|       from small pool |    3550 KB |       9 MB |     205 GB |     205 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  140302 GB |  140302 GB |
|       from large pool |  244672 KB |  473024 KB |  140067 GB |  140067 GB |
|       from small pool |    2594 KB |    4843 KB |     234 GB |     234 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9732 K  |    9731 K  |
|       from large pool |      36    |      77    |    4705 K  |    4705 K  |
|       from small pool |     187    |     225    |    5026 K  |    5026 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9732 K  |    9731 K  |
|       from large pool |      36    |      77    |    4705 K  |    4705 K  |
|       from small pool |     187    |     225    |    5026 K  |    5026 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    4936 K  |    4936 K  |
|       from large pool |      10    |      11    |    1949 K  |    1949 K  |
|       from small pool |      12    |      23    |    2987 K  |    2987 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 152, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 153 [256/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 153 [512/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [1024/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 153 [1280/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [1536/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [1792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [2048/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 153 [2304/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [2560/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 153 [2816/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 153 [3072/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [3328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 153 [3584/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [3840/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [4096/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [4352/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [4608/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [4864/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [5120/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 153 [5376/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [5632/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [5888/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 153 [6144/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [6400/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [6656/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [6912/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [7168/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [7424/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [7680/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 153 [7936/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [8192/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 153 [8448/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 153 [8704/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [8960/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 153 [9216/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [9472/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [9728/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [9984/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [10240/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 153 [10496/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 153 [10752/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 153 [11008/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [11264/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 153 [11520/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [11776/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 153 [12032/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [12288/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [12544/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 153 [12800/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [13056/50000]	Loss: 4.6082	LR: 0.040000
Training Epoch: 153 [13312/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 153 [13568/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 153 [13824/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 153 [14080/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [14336/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [14592/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 153 [14848/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [15104/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 153 [15360/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [15616/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 153 [15872/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 153 [16128/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [16384/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [16640/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [16896/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 153 [17152/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [17408/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [17664/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [17920/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [18176/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 153 [18432/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 153 [18688/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [18944/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [19200/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 153 [19456/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 153 [19712/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 153 [19968/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 153 [20224/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [20480/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [20736/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [20992/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [21248/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 153 [21504/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [21760/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 153 [22016/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [22272/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [22528/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [22784/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [23040/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 153 [23296/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 153 [23552/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 153 [23808/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 153 [24064/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 153 [24320/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 153 [24576/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [24832/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 153 [25088/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 153 [25344/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 153 [25600/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [25856/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 153 [26112/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 153 [26368/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [26624/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 153 [26880/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 153 [27136/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 153 [27392/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [27648/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [27904/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [28160/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [28416/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 153 [28672/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [28928/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [29184/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 153 [29440/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [29696/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 153 [29952/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [30208/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [30464/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 153 [30720/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 153 [30976/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [31232/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 153 [31488/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 153 [31744/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [32000/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 153 [32256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [32512/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [32768/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 153 [33024/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 153 [33280/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 153 [33536/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 153 [33792/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [34048/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 153 [34304/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 153 [34560/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [34816/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [35072/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 153 [35328/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 153 [35584/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [35840/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [36096/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [36352/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 153 [36608/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [36864/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 153 [37120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [37376/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 153 [37632/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 153 [37888/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 153 [38144/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 153 [38400/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [38656/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [38912/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 153 [39168/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 153 [39424/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [39680/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [39936/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 153 [40192/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 153 [40448/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [40704/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [40960/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [41216/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [41472/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 153 [41728/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [41984/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 153 [42240/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [42496/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [42752/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 153 [43008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [43264/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [43520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [43776/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [44032/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 153 [44288/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 153 [44544/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 153 [44800/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [45056/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 153 [45312/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 153 [45568/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [45824/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 153 [46080/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [46336/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [46592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 153 [46848/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 153 [47104/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [47360/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 153 [47616/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [47872/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 153 [48128/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [48384/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 153 [48640/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 153 [48896/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 153 [49152/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 153 [49408/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 153 [49664/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 153 [49920/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 153 [50000/50000]	Loss: 4.6067	LR: 0.040000
epoch 153 training time consumed: 21.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  229528 GB |  229528 GB |
|       from large pool |  400448 KB |    1770 MB |  229322 GB |  229321 GB |
|       from small pool |    3550 KB |       9 MB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  229528 GB |  229528 GB |
|       from large pool |  400448 KB |    1770 MB |  229322 GB |  229321 GB |
|       from small pool |    3550 KB |       9 MB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  141225 GB |  141225 GB |
|       from large pool |  244672 KB |  473024 KB |  140989 GB |  140989 GB |
|       from small pool |    2594 KB |    4843 KB |     236 GB |     236 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9796 K  |    9795 K  |
|       from large pool |      36    |      77    |    4736 K  |    4736 K  |
|       from small pool |     187    |     225    |    5059 K  |    5059 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9796 K  |    9795 K  |
|       from large pool |      36    |      77    |    4736 K  |    4736 K  |
|       from small pool |     187    |     225    |    5059 K  |    5059 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    4968 K  |    4968 K  |
|       from large pool |      10    |      11    |    1961 K  |    1961 K  |
|       from small pool |      10    |      23    |    3006 K  |    3006 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 153, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 154 [256/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 154 [512/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [768/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 154 [1024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [1280/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [1536/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 154 [1792/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [2048/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 154 [2304/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [2560/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 154 [2816/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [3072/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [3328/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 154 [3584/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [3840/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [4096/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [4352/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [4608/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [4864/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [5120/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [5376/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [5632/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 154 [5888/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 154 [6144/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [6400/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [6656/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 154 [6912/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [7168/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [7424/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 154 [7680/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [7936/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [8192/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [8448/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [8704/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [8960/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [9216/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [9472/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 154 [9728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 154 [9984/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [10240/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [10496/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [10752/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 154 [11008/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [11264/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [11520/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [11776/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 154 [12032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [12288/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 154 [12544/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [12800/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 154 [13056/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [13312/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 154 [13568/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [13824/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 154 [14080/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [14336/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [14592/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [14848/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 154 [15104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [15360/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [15616/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [15872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [16128/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [16384/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 154 [16640/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 154 [16896/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [17152/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 154 [17408/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 154 [17664/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 154 [17920/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [18176/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [18432/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 154 [18688/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [18944/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [19200/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [19456/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 154 [19712/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 154 [19968/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 154 [20224/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [20480/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [20736/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [20992/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [21248/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [21504/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 154 [21760/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 154 [22016/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [22272/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 154 [22528/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 154 [22784/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [23040/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [23296/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 154 [23552/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [23808/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [24064/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [24320/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [24576/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 154 [24832/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 154 [25088/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 154 [25344/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 154 [25600/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [25856/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [26112/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [26368/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 154 [26624/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 154 [26880/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 154 [27136/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 154 [27392/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 154 [27648/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [27904/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 154 [28160/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [28416/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [28672/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 154 [28928/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 154 [29184/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 154 [29440/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [29696/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [29952/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 154 [30208/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [30464/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [30720/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 154 [30976/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [31232/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [31488/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [31744/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [32000/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 154 [32256/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [32512/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [32768/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 154 [33024/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 154 [33280/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 154 [33536/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [33792/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 154 [34048/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 154 [34304/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 154 [34560/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 154 [34816/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 154 [35072/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 154 [35328/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 154 [35584/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 154 [35840/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [36096/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [36352/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 154 [36608/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 154 [36864/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [37120/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [37376/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [37632/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [37888/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [38144/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [38400/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [38656/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [38912/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [39168/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 154 [39424/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 154 [39680/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 154 [39936/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [40192/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [40448/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 154 [40704/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [40960/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 154 [41216/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 154 [41472/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 154 [41728/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [41984/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [42240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [42496/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [42752/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [43008/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [43264/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 154 [43520/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 154 [43776/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 154 [44032/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [44288/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [44544/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [44800/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 154 [45056/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [45312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 154 [45568/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [45824/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 154 [46080/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 154 [46336/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 154 [46592/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [46848/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 154 [47104/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 154 [47360/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [47616/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 154 [47872/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 154 [48128/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 154 [48384/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 154 [48640/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 154 [48896/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 154 [49152/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 154 [49408/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 154 [49664/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 154 [49920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 154 [50000/50000]	Loss: 4.6066	LR: 0.040000
epoch 154 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  231028 GB |  231028 GB |
|       from large pool |  400448 KB |    1770 MB |  230821 GB |  230820 GB |
|       from small pool |    3550 KB |       9 MB |     207 GB |     207 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  231028 GB |  231028 GB |
|       from large pool |  400448 KB |    1770 MB |  230821 GB |  230820 GB |
|       from small pool |    3550 KB |       9 MB |     207 GB |     207 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  142148 GB |  142148 GB |
|       from large pool |  244672 KB |  473024 KB |  141910 GB |  141910 GB |
|       from small pool |    2594 KB |    4843 KB |     237 GB |     237 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9860 K  |    9859 K  |
|       from large pool |      36    |      77    |    4767 K  |    4767 K  |
|       from small pool |     187    |     225    |    5092 K  |    5092 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9860 K  |    9859 K  |
|       from large pool |      36    |      77    |    4767 K  |    4767 K  |
|       from small pool |     187    |     225    |    5092 K  |    5092 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5001 K  |    5001 K  |
|       from large pool |      10    |      11    |    1974 K  |    1974 K  |
|       from small pool |      11    |      23    |    3027 K  |    3027 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 154, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 155 [256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 155 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [768/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 155 [1024/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 155 [1280/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [1536/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [1792/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 155 [2048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [2304/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [2560/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [2816/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [3072/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [3328/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [3584/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 155 [3840/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 155 [4096/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 155 [4352/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 155 [4608/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [4864/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 155 [5120/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [5376/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [5632/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [5888/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [6144/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 155 [6400/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 155 [6656/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 155 [6912/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 155 [7168/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [7424/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [7680/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 155 [7936/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [8192/50000]	Loss: 4.6031	LR: 0.040000
Training Epoch: 155 [8448/50000]	Loss: 4.6035	LR: 0.040000
Training Epoch: 155 [8704/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 155 [8960/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 155 [9216/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [9472/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [9728/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [9984/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 155 [10240/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 155 [10496/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [10752/50000]	Loss: 4.6080	LR: 0.040000
Training Epoch: 155 [11008/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [11264/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [11520/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 155 [11776/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 155 [12032/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 155 [12288/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [12544/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 155 [12800/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [13056/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 155 [13312/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 155 [13568/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [13824/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 155 [14080/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [14336/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 155 [14592/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 155 [14848/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [15104/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [15360/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [15616/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 155 [15872/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 155 [16128/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [16384/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [16640/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 155 [16896/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 155 [17152/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [17408/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 155 [17664/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 155 [17920/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 155 [18176/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 155 [18432/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [18688/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 155 [18944/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [19200/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [19456/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 155 [19712/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [19968/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [20224/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [20480/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 155 [20736/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [20992/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [21248/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [21504/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [21760/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 155 [22016/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [22272/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [22528/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 155 [22784/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [23040/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [23296/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 155 [23552/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [23808/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 155 [24064/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 155 [24320/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 155 [24576/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 155 [24832/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 155 [25088/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [25344/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 155 [25600/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [25856/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 155 [26112/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [26368/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 155 [26624/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [26880/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 155 [27136/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 155 [27392/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [27648/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [27904/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 155 [28160/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 155 [28416/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [28672/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [28928/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 155 [29184/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [29440/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 155 [29696/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [29952/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [30208/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [30464/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [30720/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [30976/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [31232/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [31488/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [31744/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [32000/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 155 [32256/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [32512/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 155 [32768/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 155 [33024/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [33280/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 155 [33536/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [33792/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [34048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [34304/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 155 [34560/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 155 [34816/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 155 [35072/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [35328/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [35584/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [35840/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [36096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 155 [36352/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [36608/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [36864/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [37120/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [37376/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 155 [37632/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [37888/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 155 [38144/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [38400/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 155 [38656/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [38912/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [39168/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 155 [39424/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 155 [39680/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [39936/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 155 [40192/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [40448/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [40704/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [40960/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [41216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [41472/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [41728/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [41984/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 155 [42240/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 155 [42496/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 155 [42752/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 155 [43008/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 155 [43264/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 155 [43520/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 155 [43776/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 155 [44032/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [44288/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [44544/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 155 [44800/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [45056/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 155 [45312/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 155 [45568/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 155 [45824/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [46080/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [46336/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 155 [46592/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 155 [46848/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [47104/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [47360/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [47616/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 155 [47872/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 155 [48128/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [48384/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 155 [48640/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 155 [48896/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [49152/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 155 [49408/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 155 [49664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 155 [49920/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 155 [50000/50000]	Loss: 4.6065	LR: 0.040000
epoch 155 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  232529 GB |  232528 GB |
|       from large pool |  400448 KB |    1770 MB |  232319 GB |  232319 GB |
|       from small pool |    3550 KB |       9 MB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  232529 GB |  232528 GB |
|       from large pool |  400448 KB |    1770 MB |  232319 GB |  232319 GB |
|       from small pool |    3550 KB |       9 MB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  143071 GB |  143071 GB |
|       from large pool |  244672 KB |  473024 KB |  142832 GB |  142832 GB |
|       from small pool |    2594 KB |    4843 KB |     239 GB |     239 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9924 K  |    9923 K  |
|       from large pool |      36    |      77    |    4798 K  |    4798 K  |
|       from small pool |     187    |     225    |    5125 K  |    5125 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9924 K  |    9923 K  |
|       from large pool |      36    |      77    |    4798 K  |    4798 K  |
|       from small pool |     187    |     225    |    5125 K  |    5125 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    5032 K  |    5032 K  |
|       from large pool |      10    |      11    |    1987 K  |    1987 K  |
|       from small pool |       9    |      23    |    3045 K  |    3045 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 155, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 156 [256/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [512/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 156 [768/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [1024/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 156 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 156 [1536/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 156 [1792/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 156 [2048/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 156 [2304/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 156 [2560/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [2816/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [3072/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [3328/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 156 [3584/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 156 [3840/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [4096/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [4608/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 156 [4864/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 156 [5120/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 156 [5376/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 156 [5632/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 156 [5888/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [6144/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [6400/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 156 [6656/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 156 [6912/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 156 [7168/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 156 [7424/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 156 [7680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 156 [7936/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [8192/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 156 [8448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 156 [8704/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 156 [8960/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 156 [9216/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [9472/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 156 [9728/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 156 [9984/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 156 [10240/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 156 [10496/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [10752/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 156 [11008/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [11264/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 156 [11520/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [11776/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [12032/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 156 [12288/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 156 [12544/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 156 [12800/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [13056/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [13312/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 156 [13568/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 156 [13824/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 156 [14080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [14336/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 156 [14592/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 156 [14848/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [15104/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [15360/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 156 [15616/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 156 [15872/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 156 [16128/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [16384/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [16640/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 156 [16896/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 156 [17152/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 156 [17408/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [17664/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [17920/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [18176/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 156 [18432/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 156 [18688/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [18944/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [19200/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [19456/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [19712/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 156 [19968/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [20224/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 156 [20480/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 156 [20736/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 156 [20992/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [21248/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 156 [21504/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 156 [21760/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 156 [22016/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 156 [22272/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [22528/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 156 [22784/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 156 [23040/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 156 [23296/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 156 [23552/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [23808/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [24064/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 156 [24320/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [24576/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 156 [24832/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [25088/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [25344/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 156 [25600/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 156 [25856/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [26112/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 156 [26368/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 156 [26624/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 156 [26880/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 156 [27136/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [27392/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [27648/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 156 [27904/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 156 [28160/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [28416/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 156 [28672/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 156 [28928/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 156 [29184/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 156 [29440/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 156 [29696/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 156 [29952/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [30208/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [30464/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [30720/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 156 [30976/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 156 [31232/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [31488/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [31744/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 156 [32000/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [32256/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [32512/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 156 [32768/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 156 [33024/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 156 [33280/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 156 [33536/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 156 [33792/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 156 [34048/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [34304/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [34560/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 156 [34816/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 156 [35072/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 156 [35328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [35584/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 156 [35840/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [36096/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [36352/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [36608/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [36864/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 156 [37120/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [37376/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [37632/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [37888/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 156 [38144/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 156 [38400/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 156 [38656/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 156 [38912/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 156 [39168/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 156 [39424/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 156 [39680/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 156 [39936/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [40192/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [40448/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 156 [40704/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [40960/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 156 [41216/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 156 [41472/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [41728/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 156 [41984/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [42240/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 156 [42496/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [42752/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 156 [43008/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 156 [43264/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 156 [43520/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [43776/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [44032/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [44288/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [44544/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 156 [44800/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 156 [45056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 156 [45312/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [45568/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 156 [45824/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 156 [46080/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 156 [46336/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 156 [46592/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 156 [46848/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 156 [47104/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 156 [47360/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 156 [47616/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 156 [47872/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 156 [48128/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 156 [48384/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 156 [48640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 156 [48896/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 156 [49152/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 156 [49408/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 156 [49664/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 156 [49920/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 156 [50000/50000]	Loss: 4.6059	LR: 0.040000
epoch 156 training time consumed: 21.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  234029 GB |  234028 GB |
|       from large pool |  400448 KB |    1770 MB |  233818 GB |  233818 GB |
|       from small pool |    3550 KB |       9 MB |     210 GB |     210 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  234029 GB |  234028 GB |
|       from large pool |  400448 KB |    1770 MB |  233818 GB |  233818 GB |
|       from small pool |    3550 KB |       9 MB |     210 GB |     210 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  143994 GB |  143994 GB |
|       from large pool |  244672 KB |  473024 KB |  143753 GB |  143753 GB |
|       from small pool |    2594 KB |    4843 KB |     240 GB |     240 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |    9988 K  |    9988 K  |
|       from large pool |      36    |      77    |    4829 K  |    4829 K  |
|       from small pool |     187    |     225    |    5158 K  |    5158 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |    9988 K  |    9988 K  |
|       from large pool |      36    |      77    |    4829 K  |    4829 K  |
|       from small pool |     187    |     225    |    5158 K  |    5158 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    5063 K  |    5063 K  |
|       from large pool |      10    |      11    |    2000 K  |    2000 K  |
|       from small pool |       9    |      23    |    3063 K  |    3063 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 156, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 157 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 157 [512/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 157 [768/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [1024/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [1280/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 157 [1536/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 157 [1792/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [2048/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [2304/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [2560/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [2816/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [3072/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 157 [3328/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [3584/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 157 [3840/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 157 [4096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [4608/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 157 [4864/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [5120/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [5376/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 157 [5632/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 157 [5888/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 157 [6144/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [6400/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 157 [6656/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [6912/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [7168/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 157 [7424/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [7680/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [7936/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 157 [8192/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 157 [8448/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [8704/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 157 [8960/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [9216/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 157 [9472/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 157 [9728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 157 [9984/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 157 [10240/50000]	Loss: 4.6030	LR: 0.040000
Training Epoch: 157 [10496/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 157 [10752/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 157 [11008/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [11264/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 157 [11520/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 157 [11776/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [12032/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [12288/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [12544/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 157 [12800/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 157 [13056/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 157 [13312/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [13568/50000]	Loss: 4.6037	LR: 0.040000
Training Epoch: 157 [13824/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [14080/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [14336/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [14592/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [14848/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [15104/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 157 [15360/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 157 [15616/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 157 [15872/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 157 [16128/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [16384/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [16640/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 157 [16896/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 157 [17152/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [17408/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [17664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [17920/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 157 [18176/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [18432/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [18688/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 157 [18944/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 157 [19200/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [19456/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [19712/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [19968/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 157 [20224/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 157 [20480/50000]	Loss: 4.6027	LR: 0.040000
Training Epoch: 157 [20736/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [20992/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 157 [21248/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 157 [21504/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [21760/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [22016/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 157 [22272/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [22528/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 157 [22784/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [23040/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 157 [23296/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 157 [23552/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [23808/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [24064/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 157 [24320/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 157 [24576/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 157 [24832/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 157 [25088/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 157 [25344/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [25600/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 157 [25856/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 157 [26112/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [26368/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [26624/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 157 [26880/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [27136/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 157 [27392/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [27648/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 157 [27904/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [28160/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 157 [28416/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [28672/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 157 [28928/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [29184/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 157 [29440/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 157 [29696/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 157 [29952/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [30208/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 157 [30464/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [30720/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [30976/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [31232/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [31488/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 157 [31744/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [32000/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 157 [32256/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [32512/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 157 [32768/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [33024/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [33280/50000]	Loss: 4.6079	LR: 0.040000
Training Epoch: 157 [33536/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 157 [33792/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [34048/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 157 [34304/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 157 [34560/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [34816/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [35072/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 157 [35328/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 157 [35584/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [35840/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 157 [36096/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [36352/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 157 [36608/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [36864/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 157 [37120/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 157 [37376/50000]	Loss: 4.6078	LR: 0.040000
Training Epoch: 157 [37632/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [37888/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 157 [38144/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 157 [38400/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 157 [38656/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [38912/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 157 [39168/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [39424/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 157 [39680/50000]	Loss: 4.6081	LR: 0.040000
Training Epoch: 157 [39936/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 157 [40192/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 157 [40448/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 157 [40704/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 157 [40960/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 157 [41216/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [41472/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 157 [41728/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 157 [41984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [42240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [42496/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [42752/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 157 [43008/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 157 [43264/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [43520/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 157 [43776/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 157 [44032/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [44288/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 157 [44544/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 157 [44800/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [45056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 157 [45312/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 157 [45568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 157 [45824/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 157 [46080/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 157 [46336/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 157 [46592/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [46848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 157 [47104/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 157 [47360/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 157 [47616/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 157 [47872/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [48128/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [48384/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 157 [48640/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 157 [48896/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [49152/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [49408/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 157 [49664/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 157 [49920/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 157 [50000/50000]	Loss: 4.6063	LR: 0.040000
epoch 157 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  235529 GB |  235529 GB |
|       from large pool |  400448 KB |    1770 MB |  235317 GB |  235317 GB |
|       from small pool |    3550 KB |       9 MB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  235529 GB |  235529 GB |
|       from large pool |  400448 KB |    1770 MB |  235317 GB |  235317 GB |
|       from small pool |    3550 KB |       9 MB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  144917 GB |  144917 GB |
|       from large pool |  244672 KB |  473024 KB |  144675 GB |  144675 GB |
|       from small pool |    2594 KB |    4843 KB |     242 GB |     242 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10052 K  |   10052 K  |
|       from large pool |      36    |      77    |    4860 K  |    4860 K  |
|       from small pool |     187    |     225    |    5191 K  |    5191 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10052 K  |   10052 K  |
|       from large pool |      36    |      77    |    4860 K  |    4860 K  |
|       from small pool |     187    |     225    |    5191 K  |    5191 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5096 K  |    5096 K  |
|       from large pool |      10    |      11    |    2013 K  |    2013 K  |
|       from small pool |      11    |      23    |    3082 K  |    3082 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 157, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 158 [256/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [512/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [1024/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [1280/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 158 [1536/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [1792/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [2048/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [2304/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [2560/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 158 [2816/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 158 [3072/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [3328/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [3584/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [3840/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [4096/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 158 [4352/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 158 [4608/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [4864/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 158 [5120/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 158 [5376/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [5632/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 158 [5888/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [6144/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 158 [6400/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [6656/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [6912/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [7168/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 158 [7424/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [7680/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [7936/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [8192/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 158 [8448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [8704/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [8960/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 158 [9216/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 158 [9472/50000]	Loss: 4.6035	LR: 0.040000
Training Epoch: 158 [9728/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [9984/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 158 [10240/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 158 [10496/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [10752/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [11008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 158 [11264/50000]	Loss: 4.6036	LR: 0.040000
Training Epoch: 158 [11520/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [11776/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [12032/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 158 [12288/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 158 [12544/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 158 [12800/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [13056/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [13312/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 158 [13568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [13824/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [14080/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 158 [14336/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 158 [14592/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 158 [14848/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 158 [15104/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 158 [15360/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 158 [15616/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 158 [15872/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [16128/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [16384/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 158 [16640/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 158 [16896/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 158 [17152/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 158 [17408/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [17664/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [17920/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [18176/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [18432/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [18688/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [18944/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [19200/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 158 [19456/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [19712/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 158 [19968/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 158 [20224/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 158 [20480/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [20736/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [20992/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 158 [21248/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [21504/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [21760/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 158 [22016/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 158 [22272/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [22528/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 158 [22784/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 158 [23040/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [23296/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 158 [23552/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [23808/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 158 [24064/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [24320/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 158 [24576/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [24832/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 158 [25088/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [25344/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 158 [25600/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [25856/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 158 [26112/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [26368/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [26624/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 158 [26880/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [27136/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [27392/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 158 [27648/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 158 [27904/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [28160/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 158 [28416/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [28672/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 158 [28928/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [29184/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 158 [29440/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [29696/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [29952/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 158 [30208/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [30464/50000]	Loss: 4.6043	LR: 0.040000
Training Epoch: 158 [30720/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [30976/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [31232/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 158 [31488/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [31744/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 158 [32000/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [32256/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [32512/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 158 [32768/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 158 [33024/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [33280/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [33536/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [33792/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 158 [34048/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [34304/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 158 [34560/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [34816/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 158 [35072/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [35328/50000]	Loss: 4.6076	LR: 0.040000
Training Epoch: 158 [35584/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 158 [35840/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [36096/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 158 [36352/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 158 [36608/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [36864/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [37120/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [37376/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [37632/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 158 [37888/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [38144/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [38400/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 158 [38656/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 158 [38912/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [39168/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [39424/50000]	Loss: 4.6077	LR: 0.040000
Training Epoch: 158 [39680/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [39936/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [40192/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 158 [40448/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 158 [40704/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [40960/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [41216/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [41472/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 158 [41728/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 158 [41984/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 158 [42240/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [42496/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [42752/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 158 [43008/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 158 [43264/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 158 [43520/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 158 [43776/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [44032/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [44288/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 158 [44544/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 158 [44800/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [45056/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 158 [45312/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [45568/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 158 [45824/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 158 [46080/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 158 [46336/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [46592/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 158 [46848/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 158 [47104/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [47360/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 158 [47616/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [47872/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [48128/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [48384/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 158 [48640/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 158 [48896/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [49152/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 158 [49408/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 158 [49664/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 158 [49920/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 158 [50000/50000]	Loss: 4.6057	LR: 0.040000
epoch 158 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  237029 GB |  237029 GB |
|       from large pool |  400448 KB |    1770 MB |  236816 GB |  236815 GB |
|       from small pool |    3550 KB |       9 MB |     213 GB |     213 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  237029 GB |  237029 GB |
|       from large pool |  400448 KB |    1770 MB |  236816 GB |  236815 GB |
|       from small pool |    3550 KB |       9 MB |     213 GB |     213 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  145840 GB |  145840 GB |
|       from large pool |  244672 KB |  473024 KB |  145596 GB |  145596 GB |
|       from small pool |    2594 KB |    4843 KB |     243 GB |     243 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10116 K  |   10116 K  |
|       from large pool |      36    |      77    |    4891 K  |    4891 K  |
|       from small pool |     187    |     225    |    5224 K  |    5224 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10116 K  |   10116 K  |
|       from large pool |      36    |      77    |    4891 K  |    4891 K  |
|       from small pool |     187    |     225    |    5224 K  |    5224 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5128 K  |    5128 K  |
|       from large pool |      10    |      11    |    2026 K  |    2026 K  |
|       from small pool |      10    |      23    |    3102 K  |    3102 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 158, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 159 [256/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [512/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 159 [768/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [1024/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [1280/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 159 [1536/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [1792/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [2048/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [2304/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [2560/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 159 [2816/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 159 [3072/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [3328/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [3584/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 159 [3840/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [4096/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [4352/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [4608/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 159 [4864/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [5120/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [5376/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [5632/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [5888/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [6144/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 159 [6400/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [6656/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 159 [6912/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 159 [7168/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [7424/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [7680/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [7936/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [8192/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [8448/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [8704/50000]	Loss: 4.6032	LR: 0.040000
Training Epoch: 159 [8960/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [9216/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [9472/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [9728/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [9984/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 159 [10240/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [10496/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 159 [10752/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [11008/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [11264/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [11520/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 159 [11776/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 159 [12032/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [12288/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 159 [12544/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [12800/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [13056/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 159 [13312/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 159 [13568/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [13824/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [14080/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [14336/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [14592/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [14848/50000]	Loss: 4.6038	LR: 0.040000
Training Epoch: 159 [15104/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 159 [15360/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [15616/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [15872/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [16128/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 159 [16384/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 159 [16640/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [16896/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [17152/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [17408/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [17664/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [17920/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [18176/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 159 [18432/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [18688/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [18944/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 159 [19200/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [19456/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [19712/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [19968/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 159 [20224/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 159 [20480/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 159 [20736/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [20992/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [21248/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 159 [21504/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [21760/50000]	Loss: 4.6039	LR: 0.040000
Training Epoch: 159 [22016/50000]	Loss: 4.6083	LR: 0.040000
Training Epoch: 159 [22272/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [22528/50000]	Loss: 4.6057	LR: 0.040000
Training Epoch: 159 [22784/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [23040/50000]	Loss: 4.6042	LR: 0.040000
Training Epoch: 159 [23296/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 159 [23552/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 159 [23808/50000]	Loss: 4.6047	LR: 0.040000
Training Epoch: 159 [24064/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 159 [24320/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [24576/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [24832/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [25088/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 159 [25344/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [25600/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 159 [25856/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [26112/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [26368/50000]	Loss: 4.6074	LR: 0.040000
Training Epoch: 159 [26624/50000]	Loss: 4.6073	LR: 0.040000
Training Epoch: 159 [26880/50000]	Loss: 4.6040	LR: 0.040000
Training Epoch: 159 [27136/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [27392/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [27648/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [27904/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 159 [28160/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [28416/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [28672/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [28928/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [29184/50000]	Loss: 4.6070	LR: 0.040000
Training Epoch: 159 [29440/50000]	Loss: 4.6046	LR: 0.040000
Training Epoch: 159 [29696/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [29952/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 159 [30208/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [30464/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 159 [30720/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [30976/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 159 [31232/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [31488/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [31744/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [32000/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 159 [32256/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [32512/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [32768/50000]	Loss: 4.6049	LR: 0.040000
Training Epoch: 159 [33024/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [33280/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [33536/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [33792/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [34048/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [34304/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [34560/50000]	Loss: 4.6044	LR: 0.040000
Training Epoch: 159 [34816/50000]	Loss: 4.6072	LR: 0.040000
Training Epoch: 159 [35072/50000]	Loss: 4.6041	LR: 0.040000
Training Epoch: 159 [35328/50000]	Loss: 4.6045	LR: 0.040000
Training Epoch: 159 [35584/50000]	Loss: 4.6075	LR: 0.040000
Training Epoch: 159 [35840/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 159 [36096/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 159 [36352/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 159 [36608/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [36864/50000]	Loss: 4.6050	LR: 0.040000
Training Epoch: 159 [37120/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [37376/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [37632/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [37888/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 159 [38144/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 159 [38400/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [38656/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 159 [38912/50000]	Loss: 4.6086	LR: 0.040000
Training Epoch: 159 [39168/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [39424/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [39680/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [39936/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 159 [40192/50000]	Loss: 4.6069	LR: 0.040000
Training Epoch: 159 [40448/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [40704/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [40960/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [41216/50000]	Loss: 4.6056	LR: 0.040000
Training Epoch: 159 [41472/50000]	Loss: 4.6071	LR: 0.040000
Training Epoch: 159 [41728/50000]	Loss: 4.6067	LR: 0.040000
Training Epoch: 159 [41984/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [42240/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [42496/50000]	Loss: 4.6059	LR: 0.040000
Training Epoch: 159 [42752/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [43008/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [43264/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [43520/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 159 [43776/50000]	Loss: 4.6068	LR: 0.040000
Training Epoch: 159 [44032/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [44288/50000]	Loss: 4.6051	LR: 0.040000
Training Epoch: 159 [44544/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [44800/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [45056/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [45312/50000]	Loss: 4.6060	LR: 0.040000
Training Epoch: 159 [45568/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [45824/50000]	Loss: 4.6048	LR: 0.040000
Training Epoch: 159 [46080/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [46336/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [46592/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [46848/50000]	Loss: 4.6053	LR: 0.040000
Training Epoch: 159 [47104/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [47360/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [47616/50000]	Loss: 4.6058	LR: 0.040000
Training Epoch: 159 [47872/50000]	Loss: 4.6052	LR: 0.040000
Training Epoch: 159 [48128/50000]	Loss: 4.6065	LR: 0.040000
Training Epoch: 159 [48384/50000]	Loss: 4.6064	LR: 0.040000
Training Epoch: 159 [48640/50000]	Loss: 4.6055	LR: 0.040000
Training Epoch: 159 [48896/50000]	Loss: 4.6061	LR: 0.040000
Training Epoch: 159 [49152/50000]	Loss: 4.6054	LR: 0.040000
Training Epoch: 159 [49408/50000]	Loss: 4.6066	LR: 0.040000
Training Epoch: 159 [49664/50000]	Loss: 4.6062	LR: 0.040000
Training Epoch: 159 [49920/50000]	Loss: 4.6063	LR: 0.040000
Training Epoch: 159 [50000/50000]	Loss: 4.6055	LR: 0.040000
epoch 159 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  238529 GB |  238529 GB |
|       from large pool |  400448 KB |    1770 MB |  238315 GB |  238314 GB |
|       from small pool |    3550 KB |       9 MB |     214 GB |     214 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  238529 GB |  238529 GB |
|       from large pool |  400448 KB |    1770 MB |  238315 GB |  238314 GB |
|       from small pool |    3550 KB |       9 MB |     214 GB |     214 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  146763 GB |  146763 GB |
|       from large pool |  244672 KB |  473024 KB |  146518 GB |  146517 GB |
|       from small pool |    2594 KB |    4843 KB |     245 GB |     245 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10180 K  |   10180 K  |
|       from large pool |      36    |      77    |    4922 K  |    4922 K  |
|       from small pool |     187    |     225    |    5257 K  |    5257 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10180 K  |   10180 K  |
|       from large pool |      36    |      77    |    4922 K  |    4922 K  |
|       from small pool |     187    |     225    |    5257 K  |    5257 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5159 K  |    5159 K  |
|       from large pool |      10    |      11    |    2038 K  |    2038 K  |
|       from small pool |      10    |      23    |    3120 K  |    3120 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 159, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 160 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [512/50000]	Loss: 4.6044	LR: 0.008000
Training Epoch: 160 [768/50000]	Loss: 4.6044	LR: 0.008000
Training Epoch: 160 [1024/50000]	Loss: 4.6038	LR: 0.008000
Training Epoch: 160 [1280/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [2048/50000]	Loss: 4.6045	LR: 0.008000
Training Epoch: 160 [2304/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [2560/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [2816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [3072/50000]	Loss: 4.6044	LR: 0.008000
Training Epoch: 160 [3328/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [3584/50000]	Loss: 4.6045	LR: 0.008000
Training Epoch: 160 [3840/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [4096/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [4352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [4608/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 160 [4864/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [5120/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 160 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [5888/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 160 [6144/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [6656/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [6912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [7424/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [7680/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [8448/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [8704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [8960/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [9472/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 160 [9728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [10496/50000]	Loss: 4.6044	LR: 0.008000
Training Epoch: 160 [10752/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 160 [11008/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [11264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [11520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [11776/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [12032/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [12288/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [12544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [12800/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 160 [13056/50000]	Loss: 4.6062	LR: 0.008000
Training Epoch: 160 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [13568/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [13824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [14080/50000]	Loss: 4.6036	LR: 0.008000
Training Epoch: 160 [14336/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [14592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [15104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [15360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [15616/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [15872/50000]	Loss: 4.6062	LR: 0.008000
Training Epoch: 160 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [16640/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [16896/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [17152/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [17408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [17664/50000]	Loss: 4.6045	LR: 0.008000
Training Epoch: 160 [17920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [18176/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 160 [18432/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [18688/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [18944/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [19200/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 160 [19456/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [20224/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [20480/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [20736/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [20992/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [21248/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 160 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [21760/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [22016/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [22272/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [22528/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [22784/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [23552/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [23808/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 160 [24064/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [24320/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [24576/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [24832/50000]	Loss: 4.6063	LR: 0.008000
Training Epoch: 160 [25088/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [25344/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [25600/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 160 [25856/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [26112/50000]	Loss: 4.6062	LR: 0.008000
Training Epoch: 160 [26368/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [26880/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [27392/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [27904/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [28160/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [28416/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [28928/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [29184/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [29440/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [29696/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [29952/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [30208/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [30464/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [30720/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [30976/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [31488/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [31744/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [32000/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [32256/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 160 [32512/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [33280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [33536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [33792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [34304/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [34560/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [35072/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [35328/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 160 [35584/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [35840/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [36096/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [36608/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [37376/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [37632/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [37888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [38144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 160 [38400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [38912/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [39424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [39936/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [40192/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [40448/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [41216/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [41472/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [41984/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [42240/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 160 [42496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [42752/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [43008/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [43264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [43520/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 160 [43776/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 160 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [44544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [44800/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [45056/50000]	Loss: 4.6064	LR: 0.008000
Training Epoch: 160 [45312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [46592/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [46848/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 160 [47104/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 160 [47360/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 160 [47616/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 160 [47872/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 160 [48128/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 160 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [48640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 160 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 160 [49152/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 160 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 160 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 160 [50000/50000]	Loss: 4.6063	LR: 0.008000
epoch 160 training time consumed: 21.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  240029 GB |  240029 GB |
|       from large pool |  400448 KB |    1770 MB |  239814 GB |  239813 GB |
|       from small pool |    3550 KB |       9 MB |     215 GB |     215 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  240029 GB |  240029 GB |
|       from large pool |  400448 KB |    1770 MB |  239814 GB |  239813 GB |
|       from small pool |    3550 KB |       9 MB |     215 GB |     215 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  147686 GB |  147686 GB |
|       from large pool |  244672 KB |  473024 KB |  147439 GB |  147439 GB |
|       from small pool |    2594 KB |    4843 KB |     246 GB |     246 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10244 K  |   10244 K  |
|       from large pool |      36    |      77    |    4953 K  |    4953 K  |
|       from small pool |     187    |     225    |    5290 K  |    5290 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10244 K  |   10244 K  |
|       from large pool |      36    |      77    |    4953 K  |    4953 K  |
|       from small pool |     187    |     225    |    5290 K  |    5290 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    5191 K  |    5191 K  |
|       from large pool |      10    |      11    |    2051 K  |    2051 K  |
|       from small pool |      13    |      23    |    3140 K  |    3140 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 160, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-160-regular.pth
Training Epoch: 161 [256/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [768/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [1280/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 161 [1536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [1792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [2048/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 161 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [3072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [3328/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 161 [3584/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [3840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [4096/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [4352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [4608/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 161 [4864/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [5120/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [5376/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [5888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [6144/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 161 [6400/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [6656/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [7168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [7680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [7936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [8192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [8960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [9216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [9472/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 161 [9728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [9984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [10240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [10496/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [10752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [11008/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [11264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [11520/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [11776/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [12032/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 161 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [12544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [12800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [13056/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 161 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [13568/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 161 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [14080/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [14336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [14592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [15360/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [15872/50000]	Loss: 4.6041	LR: 0.008000
Training Epoch: 161 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [16896/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [17152/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [17408/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [17920/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [18176/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [18432/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [18688/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [18944/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [19456/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [19712/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [19968/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [20224/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [20480/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [20736/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [21248/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [21504/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [22016/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [22272/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [22528/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [22784/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [23040/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [23552/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [23808/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [24064/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [24320/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [24576/50000]	Loss: 4.6062	LR: 0.008000
Training Epoch: 161 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [25600/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [25856/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [26112/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [26368/50000]	Loss: 4.6045	LR: 0.008000
Training Epoch: 161 [26624/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [26880/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [27136/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [27392/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [28160/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [28416/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [28672/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [29440/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [29696/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [29952/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [30208/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [30464/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [30720/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 161 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [31232/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 161 [31488/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [31744/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [32256/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [32512/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [33536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [34048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [34304/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [34816/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [35072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [35328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [35584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [36096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [36352/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [36608/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [36864/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 161 [37120/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 161 [37376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [37888/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [38144/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [38912/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [39168/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 161 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [39680/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [39936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [40960/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [41216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [41728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [42752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [43008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [43520/50000]	Loss: 4.6063	LR: 0.008000
Training Epoch: 161 [43776/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 161 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [44288/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [44544/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [45056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [45312/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [45568/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 161 [45824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [46080/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 161 [46336/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 161 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [46848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 161 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 161 [48384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 161 [48896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 161 [49408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 161 [49664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [49920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 161 [50000/50000]	Loss: 4.6060	LR: 0.008000
epoch 161 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  241530 GB |  241529 GB |
|       from large pool |  400448 KB |    1770 MB |  241312 GB |  241312 GB |
|       from small pool |    3550 KB |       9 MB |     217 GB |     217 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  241530 GB |  241529 GB |
|       from large pool |  400448 KB |    1770 MB |  241312 GB |  241312 GB |
|       from small pool |    3550 KB |       9 MB |     217 GB |     217 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  148609 GB |  148609 GB |
|       from large pool |  244672 KB |  473024 KB |  148361 GB |  148360 GB |
|       from small pool |    2594 KB |    4843 KB |     248 GB |     248 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10308 K  |   10308 K  |
|       from large pool |      36    |      77    |    4984 K  |    4984 K  |
|       from small pool |     187    |     225    |    5323 K  |    5323 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10308 K  |   10308 K  |
|       from large pool |      36    |      77    |    4984 K  |    4984 K  |
|       from small pool |     187    |     225    |    5323 K  |    5323 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5224 K  |    5224 K  |
|       from large pool |      10    |      11    |    2064 K  |    2064 K  |
|       from small pool |      11    |      23    |    3159 K  |    3159 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 161, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 162 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [1024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [1280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [1536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [1792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [2816/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 162 [3072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [3328/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 162 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [4096/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 162 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [5120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [5632/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 162 [5888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [6144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [6400/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [7168/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 162 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [8192/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [8448/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [8704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [8960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [9216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [9472/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [9728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [9984/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [10752/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [11008/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 162 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [11776/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [12032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [12288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [13056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [14336/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [14592/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [14848/50000]	Loss: 4.6045	LR: 0.008000
Training Epoch: 162 [15104/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [15360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [15872/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 162 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [16384/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [16640/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 162 [16896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [17152/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 162 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [18176/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [18432/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [18688/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [18944/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [19200/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [20736/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [22016/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 162 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [22528/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [23040/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [23296/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 162 [23552/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [23808/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 162 [24064/50000]	Loss: 4.6062	LR: 0.008000
Training Epoch: 162 [24320/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 162 [24576/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [24832/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [25344/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [25600/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [25856/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [26112/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 162 [26368/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 162 [26624/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [26880/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [27136/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 162 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [28160/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [28416/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [28672/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [29440/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [29952/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [30720/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [30976/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 162 [31232/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 162 [31488/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [31744/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [32000/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [32256/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 162 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [32768/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [33024/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [33280/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 162 [33536/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [33792/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [34048/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 162 [34304/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [34560/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 162 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [35072/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [35328/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [35584/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [35840/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [36608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [36864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [37632/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [37888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [38144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [38400/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [38656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [39168/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [39680/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [40192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [40448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [40960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [41472/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 162 [41728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [41984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [42240/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [42496/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 162 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [43264/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 162 [43520/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 162 [43776/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [44288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [44544/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [44800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [45056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [45312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [45568/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 162 [46080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [47360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 162 [47616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [47872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [48128/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [48384/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 162 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 162 [48896/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 162 [49152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 162 [49408/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [49664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 162 [49920/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 162 [50000/50000]	Loss: 4.6062	LR: 0.008000
epoch 162 training time consumed: 21.72s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  243030 GB |  243029 GB |
|       from large pool |  400448 KB |    1770 MB |  242811 GB |  242811 GB |
|       from small pool |    3550 KB |       9 MB |     218 GB |     218 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  243030 GB |  243029 GB |
|       from large pool |  400448 KB |    1770 MB |  242811 GB |  242811 GB |
|       from small pool |    3550 KB |       9 MB |     218 GB |     218 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  149532 GB |  149532 GB |
|       from large pool |  244672 KB |  473024 KB |  149282 GB |  149282 GB |
|       from small pool |    2594 KB |    4843 KB |     250 GB |     250 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10372 K  |   10372 K  |
|       from large pool |      36    |      77    |    5015 K  |    5015 K  |
|       from small pool |     187    |     225    |    5356 K  |    5356 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10372 K  |   10372 K  |
|       from large pool |      36    |      77    |    5015 K  |    5015 K  |
|       from small pool |     187    |     225    |    5356 K  |    5356 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    5256 K  |    5256 K  |
|       from large pool |      10    |      11    |    2077 K  |    2077 K  |
|       from small pool |      13    |      23    |    3179 K  |    3179 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 162, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 163 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [512/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 163 [768/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 163 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [1536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [2048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [2304/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [3072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [3584/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 163 [3840/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [4096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [4352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [4864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [5888/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 163 [6144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [6656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [7168/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [7424/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [8192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [8704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [8960/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [9216/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [9472/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [9728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [9984/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 163 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [11008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [11520/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 163 [11776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [12032/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 163 [12288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [12544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [12800/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [13056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [13568/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [13824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [14080/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [14336/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [14592/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [15360/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [15872/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 163 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [16384/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [16640/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [17408/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [17920/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [18176/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [18432/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [18944/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [19200/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [19712/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 163 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [20224/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [20480/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [20736/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 163 [20992/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 163 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [21760/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [22016/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [22272/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [22528/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [23040/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [23296/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [23552/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [24064/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [24320/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [24576/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [25088/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [25344/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [25600/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 163 [25856/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [26368/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [26624/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [26880/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [27392/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [27648/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [27904/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [28160/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [28416/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [28672/50000]	Loss: 4.6063	LR: 0.008000
Training Epoch: 163 [28928/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [29696/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [29952/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [30208/50000]	Loss: 4.6045	LR: 0.008000
Training Epoch: 163 [30464/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [30720/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [30976/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [31232/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [31488/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [31744/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [32256/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [33024/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [33280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [34048/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [34304/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [34816/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [35072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [35584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [36352/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [36864/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [37120/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [37376/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [37632/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [37888/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 163 [38144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [38656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [38912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [39424/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 163 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [39936/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [40192/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 163 [40448/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [40704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [41216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [41472/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [41728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [41984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [42240/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [42752/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [43264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [44288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [44800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [45056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 163 [45312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [45568/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [46080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 163 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [47104/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 163 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [48128/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 163 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [48896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 163 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 163 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 163 [50000/50000]	Loss: 4.6056	LR: 0.008000
epoch 163 training time consumed: 21.78s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  244530 GB |  244530 GB |
|       from large pool |  400448 KB |    1770 MB |  244310 GB |  244310 GB |
|       from small pool |    3550 KB |       9 MB |     220 GB |     220 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  244530 GB |  244530 GB |
|       from large pool |  400448 KB |    1770 MB |  244310 GB |  244310 GB |
|       from small pool |    3550 KB |       9 MB |     220 GB |     220 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  150455 GB |  150455 GB |
|       from large pool |  244672 KB |  473024 KB |  150204 GB |  150203 GB |
|       from small pool |    2594 KB |    4843 KB |     251 GB |     251 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10436 K  |   10436 K  |
|       from large pool |      36    |      77    |    5046 K  |    5046 K  |
|       from small pool |     187    |     225    |    5390 K  |    5389 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10436 K  |   10436 K  |
|       from large pool |      36    |      77    |    5046 K  |    5046 K  |
|       from small pool |     187    |     225    |    5390 K  |    5389 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    5288 K  |    5288 K  |
|       from large pool |      10    |      11    |    2090 K  |    2090 K  |
|       from small pool |      12    |      23    |    3198 K  |    3198 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 163, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 164 [256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [512/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [1280/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [1536/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [2560/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [2816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [3328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [3584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [3840/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [4096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [4352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [4864/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [5120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [5376/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [5632/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [5888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [6144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [6400/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [7936/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [8192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [8704/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [9216/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [9728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [9984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [10496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [10752/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 164 [11008/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [11264/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 164 [11520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [11776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [12032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [12544/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [13824/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [14080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [14336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [14848/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [15104/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [15360/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [15616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [15872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [16896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [18432/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [18944/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 164 [19200/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [19456/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 164 [19712/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [20224/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [20480/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [21248/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [21504/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 164 [21760/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [22272/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [22528/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [23296/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 164 [23552/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [23808/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [24064/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [24320/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [24576/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [25088/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [25344/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [25600/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 164 [25856/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [26368/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [26624/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [26880/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [27648/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [28416/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 164 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [28928/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [29440/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [29952/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [30208/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [30720/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [31232/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [31488/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 164 [31744/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [32000/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [32256/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 164 [32512/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 164 [32768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [33024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [33280/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [33536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [33792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [34304/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [34560/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [34816/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 164 [35072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [35840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [36608/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [36864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [37120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [37376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [37632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [38144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [38400/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [38656/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [38912/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [40448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [40704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [41216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [41472/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 164 [41728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [42240/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [43008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 164 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [44288/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [44800/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [45312/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 164 [45568/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 164 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [46080/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 164 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [46592/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [48128/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 164 [48896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 164 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [49408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 164 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 164 [49920/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 164 [50000/50000]	Loss: 4.6052	LR: 0.008000
epoch 164 training time consumed: 21.73s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  246030 GB |  246030 GB |
|       from large pool |  400448 KB |    1770 MB |  245809 GB |  245808 GB |
|       from small pool |    3550 KB |       9 MB |     221 GB |     221 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  246030 GB |  246030 GB |
|       from large pool |  400448 KB |    1770 MB |  245809 GB |  245808 GB |
|       from small pool |    3550 KB |       9 MB |     221 GB |     221 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  151378 GB |  151378 GB |
|       from large pool |  244672 KB |  473024 KB |  151125 GB |  151125 GB |
|       from small pool |    2594 KB |    4843 KB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10500 K  |   10500 K  |
|       from large pool |      36    |      77    |    5077 K  |    5077 K  |
|       from small pool |     187    |     225    |    5423 K  |    5422 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10500 K  |   10500 K  |
|       from large pool |      36    |      77    |    5077 K  |    5077 K  |
|       from small pool |     187    |     225    |    5423 K  |    5422 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5321 K  |    5321 K  |
|       from large pool |      10    |      11    |    2103 K  |    2102 K  |
|       from small pool |      10    |      23    |    3218 K  |    3218 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 164, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 165 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [1280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [1536/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [2048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [2816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [3328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [3584/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [3840/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [4096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [4352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [6400/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [6656/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [7424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [7936/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [8192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [8960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [9216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [9472/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 165 [9728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [10496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [11008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [11264/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [11520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [11776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [12032/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [13312/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 165 [13568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [14080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [14848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [15104/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [15360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [16896/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [17152/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [18176/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [18432/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [18688/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [19456/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [19712/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [20224/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [20480/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [21248/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 165 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [22272/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [22528/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [23040/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [23296/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 165 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [23808/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [24064/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [24320/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [24576/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [25088/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [25600/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [25856/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [26112/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [26368/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [26624/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [26880/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [27392/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 165 [27648/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [28416/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 165 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [29184/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [29440/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [29952/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [30976/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 165 [31232/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [31488/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [32000/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 165 [32256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [32768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 165 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [33536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [33792/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 165 [34048/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [34560/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 165 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [35328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [35840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [36352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [36608/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 165 [36864/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [37376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [37632/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [37888/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [38912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [39680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [39936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [40192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [40960/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [41216/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 165 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [41984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [42496/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [42752/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [43520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [45056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [45312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [45568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 165 [45824/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [46336/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 165 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [47104/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [48128/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [48384/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 165 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 165 [48896/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 165 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 165 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 165 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [49920/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 165 [50000/50000]	Loss: 4.6058	LR: 0.008000
epoch 165 training time consumed: 21.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  247530 GB |  247530 GB |
|       from large pool |  400448 KB |    1770 MB |  247308 GB |  247307 GB |
|       from small pool |    3550 KB |       9 MB |     222 GB |     222 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  247530 GB |  247530 GB |
|       from large pool |  400448 KB |    1770 MB |  247308 GB |  247307 GB |
|       from small pool |    3550 KB |       9 MB |     222 GB |     222 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  152301 GB |  152301 GB |
|       from large pool |  244672 KB |  473024 KB |  152047 GB |  152046 GB |
|       from small pool |    2594 KB |    4843 KB |     254 GB |     254 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10564 K  |   10564 K  |
|       from large pool |      36    |      77    |    5108 K  |    5108 K  |
|       from small pool |     187    |     225    |    5456 K  |    5455 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10564 K  |   10564 K  |
|       from large pool |      36    |      77    |    5108 K  |    5108 K  |
|       from small pool |     187    |     225    |    5456 K  |    5455 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5354 K  |    5354 K  |
|       from large pool |      10    |      11    |    2115 K  |    2115 K  |
|       from small pool |      11    |      23    |    3238 K  |    3238 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 165, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 166 [256/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 166 [512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [1536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [2048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [2304/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [3584/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [4608/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [4864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [5632/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [5888/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [7936/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [8960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [9472/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [10496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [11008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [11264/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [11520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [11776/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [12544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [13568/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 166 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [14080/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [14848/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 166 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [16128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [17408/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [18176/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [18944/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [19456/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [19712/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [19968/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [20480/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [20736/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [20992/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [21248/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [22016/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [23040/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [23552/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [24320/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [24832/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [25088/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [25344/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 166 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [26368/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 166 [26624/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [26880/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [27136/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [27392/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [27648/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [27904/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [28160/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [28416/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [28672/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [28928/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [29696/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [29952/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [30208/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [30720/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [32512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [32768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [33024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [33280/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [33536/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [34304/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 166 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [34816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [35328/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [35584/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [36096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [37376/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 166 [37632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [37888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [38400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [38656/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [38912/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [39168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [39936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [40448/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [40704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [40960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [41216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [41728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [42240/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [42496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [42752/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [43776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [44032/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [44800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [45056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [45312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 166 [45568/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 166 [45824/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [46080/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 166 [46336/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [46592/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 166 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [47360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 166 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [48640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 166 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 166 [49664/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 166 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 166 [50000/50000]	Loss: 4.6058	LR: 0.008000
epoch 166 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  249031 GB |  249030 GB |
|       from large pool |  400448 KB |    1770 MB |  248807 GB |  248806 GB |
|       from small pool |    3550 KB |       9 MB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  249031 GB |  249030 GB |
|       from large pool |  400448 KB |    1770 MB |  248807 GB |  248806 GB |
|       from small pool |    3550 KB |       9 MB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  153224 GB |  153224 GB |
|       from large pool |  244672 KB |  473024 KB |  152968 GB |  152968 GB |
|       from small pool |    2594 KB |    4843 KB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10628 K  |   10628 K  |
|       from large pool |      36    |      77    |    5139 K  |    5139 K  |
|       from small pool |     187    |     225    |    5489 K  |    5489 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10628 K  |   10628 K  |
|       from large pool |      36    |      77    |    5139 K  |    5139 K  |
|       from small pool |     187    |     225    |    5489 K  |    5489 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    5385 K  |    5385 K  |
|       from large pool |      10    |      11    |    2128 K  |    2128 K  |
|       from small pool |       9    |      23    |    3256 K  |    3256 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 166, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 167 [256/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [1280/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [1536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [3072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [3328/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [3840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [5120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [7168/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [8448/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [8704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [8960/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 167 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [9472/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [10496/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [10752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [11264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [11520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [11776/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [12032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [12288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [12800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [13056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [13312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [14080/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [14336/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [15360/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [16128/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [16640/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 167 [16896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [18176/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [18688/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [18944/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [19200/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [19456/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [20480/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [20736/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [20992/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [21760/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [22016/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [22528/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [23552/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [24064/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [24320/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [24576/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [25344/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [25600/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [25856/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [26368/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [26880/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [27904/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [28416/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [28928/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [29184/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [29440/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [30464/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [30720/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [30976/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [31488/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 167 [31744/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [32512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [32768/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [33536/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [34048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [34304/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [34560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [34816/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 167 [35072/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 167 [35328/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [36352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 167 [37120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [37376/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [37632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [38144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [38400/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [38656/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [39168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [39424/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [40192/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 167 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [40704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 167 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [41216/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 167 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [41984/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [42496/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 167 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [43008/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 167 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [44800/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 167 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [45568/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [45824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [47104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [48896/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 167 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 167 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 167 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 167 [50000/50000]	Loss: 4.6056	LR: 0.008000
epoch 167 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  250531 GB |  250530 GB |
|       from large pool |  400448 KB |    1770 MB |  250305 GB |  250305 GB |
|       from small pool |    3550 KB |       9 MB |     225 GB |     225 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  250531 GB |  250530 GB |
|       from large pool |  400448 KB |    1770 MB |  250305 GB |  250305 GB |
|       from small pool |    3550 KB |       9 MB |     225 GB |     225 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  154147 GB |  154147 GB |
|       from large pool |  244672 KB |  473024 KB |  153890 GB |  153889 GB |
|       from small pool |    2594 KB |    4843 KB |     257 GB |     257 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10692 K  |   10692 K  |
|       from large pool |      36    |      77    |    5170 K  |    5170 K  |
|       from small pool |     187    |     225    |    5522 K  |    5522 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10692 K  |   10692 K  |
|       from large pool |      36    |      77    |    5170 K  |    5170 K  |
|       from small pool |     187    |     225    |    5522 K  |    5522 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    5415 K  |    5415 K  |
|       from large pool |      10    |      11    |    2141 K  |    2141 K  |
|       from small pool |       9    |      23    |    3274 K  |    3274 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 167, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 168 [256/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [2560/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [3072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [3328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [4096/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 168 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [5120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [5632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [6144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [6656/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 168 [6912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [7168/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 168 [7424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [7680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [8704/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [9728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [10496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [10752/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [11008/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [11264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [11520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [12288/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [13312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [13568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [14080/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 168 [14336/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 168 [14592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [14848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [15104/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 168 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [16384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [17152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [17664/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [18176/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [18688/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [18944/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [19968/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [21248/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [21760/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [22272/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 168 [22528/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [23552/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [24320/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [24576/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [24832/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [25088/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [25344/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 168 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [25856/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [26624/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [26880/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [27136/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [27392/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [27904/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [28672/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [29184/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [29440/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [29952/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [30208/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [30720/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [30976/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [32000/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [32768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [33024/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [33536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [33792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [34816/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [35072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [35584/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [35840/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [36096/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [36608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [36864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [37376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [37632/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [37888/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [38400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [38912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 168 [39168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [40448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [40704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [41216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [41472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [43776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [44032/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [44800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [45056/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [45824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [46080/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [46336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 168 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 168 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [47872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 168 [48128/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 168 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 168 [49664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 168 [49920/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 168 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 168 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  252031 GB |  252031 GB |
|       from large pool |  400448 KB |    1770 MB |  251804 GB |  251804 GB |
|       from small pool |    3550 KB |       9 MB |     226 GB |     226 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  252031 GB |  252031 GB |
|       from large pool |  400448 KB |    1770 MB |  251804 GB |  251804 GB |
|       from small pool |    3550 KB |       9 MB |     226 GB |     226 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  155070 GB |  155070 GB |
|       from large pool |  244672 KB |  473024 KB |  154811 GB |  154811 GB |
|       from small pool |    2594 KB |    4843 KB |     259 GB |     259 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10756 K  |   10756 K  |
|       from large pool |      36    |      77    |    5201 K  |    5201 K  |
|       from small pool |     187    |     225    |    5555 K  |    5555 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10756 K  |   10756 K  |
|       from large pool |      36    |      77    |    5201 K  |    5201 K  |
|       from small pool |     187    |     225    |    5555 K  |    5555 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5448 K  |    5448 K  |
|       from large pool |      10    |      11    |    2154 K  |    2154 K  |
|       from small pool |      11    |      23    |    3294 K  |    3294 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 168, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 169 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [2048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [4096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [4864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [5888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [6144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [6912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [8192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [9728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [9984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [10752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [11776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [12032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [12288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [12800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [13568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [14080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [14336/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 169 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [15360/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [15616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [17408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [17664/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [18432/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [18688/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [18944/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [19456/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [19712/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [19968/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [20480/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [20992/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 169 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [21760/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [22272/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [22528/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [23552/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [24320/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [24832/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [25088/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [25600/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [26624/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [27392/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [27648/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [28160/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [28416/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [28928/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [29184/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [29696/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [29952/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [30208/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [30720/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 169 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [31232/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [31744/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [32000/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [32256/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [32512/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 169 [32768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [33536/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [34048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [34560/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [34816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [35072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [35328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [35584/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 169 [35840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [36096/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 169 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [36608/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [37376/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [38144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [38400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [38912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [39168/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 169 [39424/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [41728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 169 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [43008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [43520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [43776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [44288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [44800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [45312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [45568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [45824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [46336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [46592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 169 [46848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [47360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 169 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [48896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 169 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 169 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 169 [50000/50000]	Loss: 4.6052	LR: 0.008000
epoch 169 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  253531 GB |  253531 GB |
|       from large pool |  400448 KB |    1770 MB |  253303 GB |  253303 GB |
|       from small pool |    3550 KB |       9 MB |     228 GB |     228 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  253531 GB |  253531 GB |
|       from large pool |  400448 KB |    1770 MB |  253303 GB |  253303 GB |
|       from small pool |    3550 KB |       9 MB |     228 GB |     228 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  155993 GB |  155993 GB |
|       from large pool |  244672 KB |  473024 KB |  155733 GB |  155732 GB |
|       from small pool |    2594 KB |    4843 KB |     260 GB |     260 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10820 K  |   10820 K  |
|       from large pool |      36    |      77    |    5232 K  |    5232 K  |
|       from small pool |     187    |     225    |    5588 K  |    5588 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10820 K  |   10820 K  |
|       from large pool |      36    |      77    |    5232 K  |    5232 K  |
|       from small pool |     187    |     225    |    5588 K  |    5588 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5480 K  |    5480 K  |
|       from large pool |      10    |      11    |    2167 K  |    2167 K  |
|       from small pool |      10    |      23    |    3313 K  |    3313 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 169, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 170 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [512/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [1024/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [1536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [3328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [5120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [6144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [6912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [7424/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 170 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [7936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [9216/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [11520/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [12288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [12800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [13056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [14080/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [14336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [14592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [15104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [15872/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 170 [16128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [17152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [17664/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [18176/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [18432/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [18944/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [19200/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [19456/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [19712/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [20224/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [20480/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 170 [20736/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 170 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [22016/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [22272/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [22784/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [23296/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [23552/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [23808/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [24064/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [24320/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [24576/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [24832/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [25344/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [26880/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [27136/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [27392/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [28416/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [28928/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 170 [29184/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [29440/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [29696/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [29952/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [30208/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [30464/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 170 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [31232/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [31488/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [33024/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [33280/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [33536/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [34048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [34304/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [34560/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [35072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [35840/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 170 [36096/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [36352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [37120/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [37632/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 170 [37888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 170 [38144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [38912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [39680/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 170 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [40448/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 170 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [41472/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [41984/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [45056/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 170 [45312/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 170 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [46592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [47104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [47360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 170 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [48384/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 170 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 170 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 170 [49664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 170 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 170 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 170 training time consumed: 21.70s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  255031 GB |  255031 GB |
|       from large pool |  400448 KB |    1770 MB |  254802 GB |  254801 GB |
|       from small pool |    3550 KB |       9 MB |     229 GB |     229 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  255031 GB |  255031 GB |
|       from large pool |  400448 KB |    1770 MB |  254802 GB |  254801 GB |
|       from small pool |    3550 KB |       9 MB |     229 GB |     229 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  156916 GB |  156916 GB |
|       from large pool |  244672 KB |  473024 KB |  156654 GB |  156654 GB |
|       from small pool |    2594 KB |    4843 KB |     262 GB |     262 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10884 K  |   10884 K  |
|       from large pool |      36    |      77    |    5263 K  |    5263 K  |
|       from small pool |     187    |     225    |    5621 K  |    5621 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10884 K  |   10884 K  |
|       from large pool |      36    |      77    |    5263 K  |    5263 K  |
|       from small pool |     187    |     225    |    5621 K  |    5621 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5511 K  |    5511 K  |
|       from large pool |      10    |      11    |    2179 K  |    2179 K  |
|       from small pool |      10    |      23    |    3331 K  |    3331 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 170, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-170-regular.pth
Training Epoch: 171 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [1280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [3072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [3584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [4608/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 171 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [6656/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [7168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [7424/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [8192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [8448/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [8704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [8960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [9216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [10240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [10496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [11264/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [11520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [12032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [12288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [13056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [13568/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [15360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [15616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [15872/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [16896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [17408/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 171 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [18176/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [18944/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [19200/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [19456/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [19712/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [20480/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 171 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [21248/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [21504/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [22016/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [22272/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [22528/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [22784/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [23296/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [23552/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [23808/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [24320/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [25088/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 171 [25344/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [26112/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [26624/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [26880/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [27136/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [27392/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [27904/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [28416/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 171 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [28928/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [29440/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [29696/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [29952/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 171 [30208/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [30464/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [30720/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [30976/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [31232/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [31488/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [31744/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [32000/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 171 [32512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [32768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [33024/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [33536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [34304/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [35840/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [36352/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 171 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [37120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [37376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [37632/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [38144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [38400/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 171 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [38912/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [39680/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 171 [39936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [40192/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 171 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [40704/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 171 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [41472/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 171 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [42752/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 171 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [43520/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 171 [43776/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 171 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [44800/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 171 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [48128/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 171 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 171 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 171 [49152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 171 [49664/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 171 [49920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 171 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 171 training time consumed: 21.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  256531 GB |  256531 GB |
|       from large pool |  400448 KB |    1770 MB |  256301 GB |  256300 GB |
|       from small pool |    3550 KB |       9 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  256531 GB |  256531 GB |
|       from large pool |  400448 KB |    1770 MB |  256301 GB |  256300 GB |
|       from small pool |    3550 KB |       9 MB |     230 GB |     230 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  157839 GB |  157839 GB |
|       from large pool |  244672 KB |  473024 KB |  157576 GB |  157575 GB |
|       from small pool |    2594 KB |    4843 KB |     263 GB |     263 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   10948 K  |   10948 K  |
|       from large pool |      36    |      77    |    5294 K  |    5294 K  |
|       from small pool |     187    |     225    |    5654 K  |    5654 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   10948 K  |   10948 K  |
|       from large pool |      36    |      77    |    5294 K  |    5294 K  |
|       from small pool |     187    |     225    |    5654 K  |    5654 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    5544 K  |    5544 K  |
|       from large pool |      10    |      11    |    2192 K  |    2192 K  |
|       from small pool |      13    |      23    |    3351 K  |    3351 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 171, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.49s

Training Epoch: 172 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [3584/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [4096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [5120/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [5632/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 172 [5888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [6144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [6400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [6656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [8192/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 172 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [8960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [9216/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [9472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [10240/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [11008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [11520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [12544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [13056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [13568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [14080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [15360/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 172 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [15872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [16896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [17152/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 172 [17408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [18176/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [18432/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [18688/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [18944/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [19456/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [19712/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [19968/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [21504/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [22016/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [22272/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 172 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [23040/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 172 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [23552/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [23808/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [24064/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 172 [24320/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [25088/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [25344/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 172 [25600/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [26624/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [26880/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 172 [27136/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [27392/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [28672/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [28928/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 172 [29184/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 172 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [29952/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [30464/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [30720/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [31488/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [32256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [33536/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 172 [33792/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [34304/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 172 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [34816/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [35328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [35584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [37120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [37376/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [37888/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [38144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [38400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [38656/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [39424/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [40192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [41216/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [41472/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 172 [41728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [42496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [42752/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [44032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [45056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 172 [45312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [45824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [46848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 172 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [48384/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 172 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 172 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 172 [49664/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 172 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 172 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 172 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  258032 GB |  258031 GB |
|       from large pool |  400448 KB |    1770 MB |  257799 GB |  257799 GB |
|       from small pool |    3550 KB |       9 MB |     232 GB |     232 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  258032 GB |  258031 GB |
|       from large pool |  400448 KB |    1770 MB |  257799 GB |  257799 GB |
|       from small pool |    3550 KB |       9 MB |     232 GB |     232 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  158762 GB |  158762 GB |
|       from large pool |  244672 KB |  473024 KB |  158497 GB |  158497 GB |
|       from small pool |    2594 KB |    4843 KB |     265 GB |     265 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11012 K  |   11012 K  |
|       from large pool |      36    |      77    |    5325 K  |    5324 K  |
|       from small pool |     187    |     225    |    5687 K  |    5687 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11012 K  |   11012 K  |
|       from large pool |      36    |      77    |    5325 K  |    5324 K  |
|       from small pool |     187    |     225    |    5687 K  |    5687 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5576 K  |    5576 K  |
|       from large pool |      10    |      11    |    2205 K  |    2205 K  |
|       from small pool |      11    |      23    |    3371 K  |    3371 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 172, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 173 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [2048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [3072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [4096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [6656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [7424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [8704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [9216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [10240/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [10496/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [11264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [11520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [11776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [12032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [12544/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [13824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [14080/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [14336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [14592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [15360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [15872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [16128/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [16640/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [17664/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [18176/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [18432/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [18688/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [18944/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [19456/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [19712/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [20736/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [21248/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [21760/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [22016/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [22272/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [22528/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [23040/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [23296/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [23552/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [23808/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [24320/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [24576/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [25088/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [25600/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [26112/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [27136/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 173 [27392/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [27648/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [27904/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [28416/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [28672/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [28928/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [29440/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [29696/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [29952/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [30208/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [31488/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [32000/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [32256/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [32512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [33024/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [33792/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 173 [34048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [34816/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [35072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 173 [35584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [35840/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 173 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [36352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [37632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [37888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [38912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 173 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [39936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [40960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [41472/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [41728/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [42752/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [43520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [44544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [44800/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [45056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [45312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [46336/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 173 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 173 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [48128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 173 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 173 [49664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 173 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 173 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 173 training time consumed: 21.70s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  259532 GB |  259531 GB |
|       from large pool |  400448 KB |    1770 MB |  259298 GB |  259298 GB |
|       from small pool |    3550 KB |       9 MB |     233 GB |     233 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  259532 GB |  259531 GB |
|       from large pool |  400448 KB |    1770 MB |  259298 GB |  259298 GB |
|       from small pool |    3550 KB |       9 MB |     233 GB |     233 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  159686 GB |  159685 GB |
|       from large pool |  244672 KB |  473024 KB |  159419 GB |  159418 GB |
|       from small pool |    2594 KB |    4843 KB |     266 GB |     266 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11076 K  |   11076 K  |
|       from large pool |      36    |      77    |    5355 K  |    5355 K  |
|       from small pool |     187    |     225    |    5720 K  |    5720 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11076 K  |   11076 K  |
|       from large pool |      36    |      77    |    5355 K  |    5355 K  |
|       from small pool |     187    |     225    |    5720 K  |    5720 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    5609 K  |    5609 K  |
|       from large pool |      10    |      11    |    2218 K  |    2218 K  |
|       from small pool |      13    |      23    |    3390 K  |    3390 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 173, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 174 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [2816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [4352/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [5632/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [5888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [6144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [6656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [8448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [8960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [9472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [11264/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [12800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [13568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [14592/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 174 [14848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [15360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [15616/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [17152/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [17408/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 174 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [17920/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [18176/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [18432/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [18688/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [18944/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [19456/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [19712/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [19968/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [20224/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 174 [20480/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [20736/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [20992/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 174 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [21760/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 174 [22016/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [22272/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [22528/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [23040/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [23296/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [23552/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [23808/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [24320/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [24576/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [25344/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [25856/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [26368/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [26880/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [27392/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [28416/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 174 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [29440/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [29696/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 174 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [30208/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [30720/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 174 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [31488/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [31744/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [32256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [32768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [33024/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [33280/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 174 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [34048/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 174 [34304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [35328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [36096/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 174 [36352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [37120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [37376/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 174 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [37888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [38144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [38400/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [38656/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [39168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [39424/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [39680/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [39936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 174 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [40448/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [40960/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 174 [41216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 174 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [42240/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 174 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [43264/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 174 [43520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [45056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [45312/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 174 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [47104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [47360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [47616/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 174 [47872/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 174 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 174 [48640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 174 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [49152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 174 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 174 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 174 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  261032 GB |  261032 GB |
|       from large pool |  400448 KB |    1770 MB |  260797 GB |  260797 GB |
|       from small pool |    3550 KB |       9 MB |     234 GB |     234 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  261032 GB |  261032 GB |
|       from large pool |  400448 KB |    1770 MB |  260797 GB |  260797 GB |
|       from small pool |    3550 KB |       9 MB |     234 GB |     234 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  160609 GB |  160608 GB |
|       from large pool |  244672 KB |  473024 KB |  160340 GB |  160340 GB |
|       from small pool |    2594 KB |    4843 KB |     268 GB |     268 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11140 K  |   11140 K  |
|       from large pool |      36    |      77    |    5386 K  |    5386 K  |
|       from small pool |     187    |     225    |    5753 K  |    5753 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11140 K  |   11140 K  |
|       from large pool |      36    |      77    |    5386 K  |    5386 K  |
|       from small pool |     187    |     225    |    5753 K  |    5753 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    5641 K  |    5641 K  |
|       from large pool |      10    |      11    |    2231 K  |    2231 K  |
|       from small pool |      12    |      23    |    3409 K  |    3409 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 174, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 175 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [3072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [4352/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [4608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [4864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [5888/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [6144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [7424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [7936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [8192/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [8704/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [8960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [9216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [10240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [11008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [11520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [13056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [14336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [14848/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [15360/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [16128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [16384/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [16640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [16896/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [17408/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [18176/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [18432/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [18688/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [18944/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [19200/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [19456/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [20224/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [20736/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [21504/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [21760/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [22016/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [22272/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [23040/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [23296/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [23552/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [23808/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [24064/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 175 [24320/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [24576/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [24832/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [25088/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [25856/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [26368/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [26624/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [26880/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [27136/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [27392/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [27648/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [27904/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 175 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [28416/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [28672/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [28928/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [29184/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [30464/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [30976/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 175 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [31488/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [32000/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [32512/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 175 [32768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [33024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [33536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [33792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [34048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [34304/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [34560/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [35328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 175 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [35840/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 175 [36096/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [36608/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [37120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [38656/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [38912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [39168/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 175 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [40704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [40960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [43008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [44288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 175 [44544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [45056/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 175 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [45824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 175 [46080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [46336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 175 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [47872/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 175 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [49408/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 175 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 175 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 175 [50000/50000]	Loss: 4.6056	LR: 0.008000
epoch 175 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  262532 GB |  262532 GB |
|       from large pool |  400448 KB |    1770 MB |  262296 GB |  262296 GB |
|       from small pool |    3550 KB |       9 MB |     236 GB |     236 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  262532 GB |  262532 GB |
|       from large pool |  400448 KB |    1770 MB |  262296 GB |  262296 GB |
|       from small pool |    3550 KB |       9 MB |     236 GB |     236 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  161532 GB |  161531 GB |
|       from large pool |  244672 KB |  473024 KB |  161261 GB |  161261 GB |
|       from small pool |    2594 KB |    4843 KB |     270 GB |     270 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11204 K  |   11204 K  |
|       from large pool |      36    |      77    |    5417 K  |    5417 K  |
|       from small pool |     187    |     225    |    5786 K  |    5786 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11204 K  |   11204 K  |
|       from large pool |      36    |      77    |    5417 K  |    5417 K  |
|       from small pool |     187    |     225    |    5786 K  |    5786 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5673 K  |    5673 K  |
|       from large pool |      10    |      11    |    2244 K  |    2244 K  |
|       from small pool |      10    |      23    |    3429 K  |    3429 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 175, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 176 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [2304/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [3072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [6656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [7936/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 176 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [8960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [9984/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [10240/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 176 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [10752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [11264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [11520/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [12032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [12800/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [13824/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [14336/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 176 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [14848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [15104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [16128/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [16384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [17664/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [17920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [18176/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [18432/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [18944/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 176 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [19456/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [19968/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [20224/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [20480/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [22528/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [23552/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [23808/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [24320/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [24832/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [25088/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [25600/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [25856/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [26368/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [26624/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [26880/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [27136/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [27392/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [27904/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [28160/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [28416/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [28928/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [29184/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [30208/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [30464/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [32000/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [32512/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [33024/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [33536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [34048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [34304/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 176 [34560/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [35840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [36096/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [36608/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [36864/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 176 [37120/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [37632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [37888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [38656/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [38912/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [39168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [39424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [39680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 176 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [40448/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [40960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [41216/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [41472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 176 [41728/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [42496/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [42752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [43008/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 176 [43264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [43520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [44800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [45056/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 176 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [45824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [46080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 176 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 176 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 176 [49408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 176 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 176 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 176 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  264032 GB |  264032 GB |
|       from large pool |  400448 KB |    1770 MB |  263795 GB |  263794 GB |
|       from small pool |    3550 KB |       9 MB |     237 GB |     237 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  264032 GB |  264032 GB |
|       from large pool |  400448 KB |    1770 MB |  263795 GB |  263794 GB |
|       from small pool |    3550 KB |       9 MB |     237 GB |     237 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  162455 GB |  162454 GB |
|       from large pool |  244672 KB |  473024 KB |  162183 GB |  162183 GB |
|       from small pool |    2594 KB |    4843 KB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11268 K  |   11268 K  |
|       from large pool |      36    |      77    |    5448 K  |    5448 K  |
|       from small pool |     187    |     225    |    5819 K  |    5819 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11268 K  |   11268 K  |
|       from large pool |      36    |      77    |    5448 K  |    5448 K  |
|       from small pool |     187    |     225    |    5819 K  |    5819 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5706 K  |    5706 K  |
|       from large pool |      10    |      11    |    2256 K  |    2256 K  |
|       from small pool |      11    |      23    |    3449 K  |    3449 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 176, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 177 [256/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [2048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [5888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [7424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [8192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [8448/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 177 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [9216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [9472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [10752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [11264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [11520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [11776/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [12032/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [12800/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [13056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [13312/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [13824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [14336/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [14848/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 177 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [15360/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 177 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [15872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [17408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [18176/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [18432/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [18688/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [18944/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [19200/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [19456/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [19968/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [20736/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [21248/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [21760/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [22016/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [22528/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 177 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [23552/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [24064/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [24320/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [24576/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [25088/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [25344/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [26368/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [26880/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 177 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [27904/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [28160/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [28416/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [28928/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [29440/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [30208/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [30720/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [30976/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [31744/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [32000/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [32512/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [33024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [33536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [33792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [34048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [34304/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [34560/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 177 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [35072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [35328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [36352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [37632/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [37888/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 177 [38144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [38400/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 177 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [39424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [40192/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [40704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [41984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [42240/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [43008/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 177 [43264/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 177 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [43776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 177 [44032/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [44800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [45824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [46080/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [46848/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 177 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [48384/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 177 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [48896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 177 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 177 [49408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [49664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 177 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 177 [50000/50000]	Loss: 4.6056	LR: 0.008000
epoch 177 training time consumed: 21.79s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  265533 GB |  265532 GB |
|       from large pool |  400448 KB |    1770 MB |  265294 GB |  265293 GB |
|       from small pool |    3550 KB |       9 MB |     238 GB |     238 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  265533 GB |  265532 GB |
|       from large pool |  400448 KB |    1770 MB |  265294 GB |  265293 GB |
|       from small pool |    3550 KB |       9 MB |     238 GB |     238 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  163378 GB |  163377 GB |
|       from large pool |  244672 KB |  473024 KB |  163104 GB |  163104 GB |
|       from small pool |    2594 KB |    4843 KB |     273 GB |     273 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11332 K  |   11332 K  |
|       from large pool |      36    |      77    |    5479 K  |    5479 K  |
|       from small pool |     187    |     225    |    5852 K  |    5852 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11332 K  |   11332 K  |
|       from large pool |      36    |      77    |    5479 K  |    5479 K  |
|       from small pool |     187    |     225    |    5852 K  |    5852 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    5737 K  |    5737 K  |
|       from large pool |      10    |      11    |    2269 K  |    2269 K  |
|       from small pool |       9    |      23    |    3467 K  |    3467 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 177, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.47s

Training Epoch: 178 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [2048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [3840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [5120/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [5888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [6144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [6656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [7680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [8448/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [8960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [9216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [9728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [9984/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [10496/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 178 [10752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [11008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [11264/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 178 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [11776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [12032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [13568/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [15104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [15872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [16128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [16384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [16896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [17664/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [18432/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [19968/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [20480/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [22528/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [22784/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [23296/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [23552/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [24320/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [25088/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [25344/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [25600/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [25856/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [26112/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [26368/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [26880/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [27136/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [27392/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [27904/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 178 [28160/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [28672/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [28928/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [29952/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 178 [30208/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [30464/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 178 [30720/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 178 [30976/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [31488/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [31744/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [32000/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [32256/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [32512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [32768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [33024/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 178 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [33536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [34048/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [34304/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [34560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [35328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [36608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [36864/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 178 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [37376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [38144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [38912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [39168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [39680/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [40448/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [40704/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 178 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [41216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [41728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 178 [41984/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [42240/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [42752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [43264/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 178 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [43776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [44544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [44800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [45056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [46080/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [46848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 178 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 178 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 178 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 178 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 178 [50000/50000]	Loss: 4.6053	LR: 0.008000
epoch 178 training time consumed: 22.00s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  267033 GB |  267032 GB |
|       from large pool |  400448 KB |    1770 MB |  266792 GB |  266792 GB |
|       from small pool |    3550 KB |       9 MB |     240 GB |     240 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  267033 GB |  267032 GB |
|       from large pool |  400448 KB |    1770 MB |  266792 GB |  266792 GB |
|       from small pool |    3550 KB |       9 MB |     240 GB |     240 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  164301 GB |  164300 GB |
|       from large pool |  244672 KB |  473024 KB |  164026 GB |  164026 GB |
|       from small pool |    2594 KB |    4843 KB |     274 GB |     274 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11396 K  |   11396 K  |
|       from large pool |      36    |      77    |    5510 K  |    5510 K  |
|       from small pool |     187    |     225    |    5886 K  |    5885 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11396 K  |   11396 K  |
|       from large pool |      36    |      77    |    5510 K  |    5510 K  |
|       from small pool |     187    |     225    |    5886 K  |    5885 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    5768 K  |    5768 K  |
|       from large pool |      10    |      11    |    2282 K  |    2282 K  |
|       from small pool |       9    |      23    |    3485 K  |    3485 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 178, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 179 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [1280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [3328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [5120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [6656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [7168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [7680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [8192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [8704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [9216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [9472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [10240/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [10752/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [11008/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 179 [11264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [11520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [13568/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 179 [13824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [14336/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [14848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [15616/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [15872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [16640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [16896/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 179 [17152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [17408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [18176/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [18944/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [19456/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 179 [19712/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [20224/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [21504/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [22272/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [22528/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [23040/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [23296/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [23808/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [24064/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [24320/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [24576/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 179 [24832/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 179 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [25600/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [25856/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [26368/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [26624/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [26880/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [27904/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 179 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [28416/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [28928/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [29184/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 179 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [30208/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [30464/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [30720/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [31488/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [31744/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [32000/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [32256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [32512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [32768/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [33024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [33536/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 179 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [34048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [34304/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 179 [34560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [34816/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [35328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [35840/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [36096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [36864/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 179 [37120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [37632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [37888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [38144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [38400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [39424/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [40192/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 179 [40448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [41216/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 179 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [41728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [42752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [43008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [43520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 179 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [44032/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [44288/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 179 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [44800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [45056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [45568/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 179 [45824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 179 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 179 [48896/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 179 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 179 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 179 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 179 training time consumed: 21.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  268533 GB |  268533 GB |
|       from large pool |  400448 KB |    1770 MB |  268291 GB |  268291 GB |
|       from small pool |    3550 KB |       9 MB |     241 GB |     241 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  268533 GB |  268533 GB |
|       from large pool |  400448 KB |    1770 MB |  268291 GB |  268291 GB |
|       from small pool |    3550 KB |       9 MB |     241 GB |     241 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  165224 GB |  165223 GB |
|       from large pool |  244672 KB |  473024 KB |  164947 GB |  164947 GB |
|       from small pool |    2594 KB |    4843 KB |     276 GB |     276 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11460 K  |   11460 K  |
|       from large pool |      36    |      77    |    5541 K  |    5541 K  |
|       from small pool |     187    |     225    |    5919 K  |    5918 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11460 K  |   11460 K  |
|       from large pool |      36    |      77    |    5541 K  |    5541 K  |
|       from small pool |     187    |     225    |    5919 K  |    5918 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5800 K  |    5800 K  |
|       from large pool |      10    |      11    |    2295 K  |    2295 K  |
|       from small pool |      11    |      23    |    3505 K  |    3505 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 179, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 180 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [4352/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 180 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [5120/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 180 [5376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [6400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [7168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [7424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [9216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [9472/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 180 [9728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [10752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [11008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [11264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [11520/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 180 [11776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [13568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [15360/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 180 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [15872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [16384/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [17408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [17664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [18688/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 180 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [19200/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 180 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [19968/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 180 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [20480/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [20992/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [21760/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [22016/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [22272/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 180 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [23040/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [23808/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [24064/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [24320/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [24576/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 180 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [25088/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [25856/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [26880/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [27136/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 180 [27392/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [28416/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [28672/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [29184/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [29952/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [30208/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [30464/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [30720/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [31744/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [32512/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [34048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [34560/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 180 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [35072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [35328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [35584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [35840/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [36096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [36352/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [37376/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [38912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [39168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [39424/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [40448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [40960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [41216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [42240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [42752/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [43520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 180 [43776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [44544/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 180 [44800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 180 [45056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [45312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [45824/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 180 [46080/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 180 [46336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [47360/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 180 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [48384/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 180 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 180 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 180 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 180 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  270033 GB |  270033 GB |
|       from large pool |  400448 KB |    1770 MB |  269790 GB |  269790 GB |
|       from small pool |    3550 KB |       9 MB |     242 GB |     242 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  270033 GB |  270033 GB |
|       from large pool |  400448 KB |    1770 MB |  269790 GB |  269790 GB |
|       from small pool |    3550 KB |       9 MB |     242 GB |     242 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  166147 GB |  166146 GB |
|       from large pool |  244672 KB |  473024 KB |  165869 GB |  165869 GB |
|       from small pool |    2594 KB |    4843 KB |     277 GB |     277 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11524 K  |   11524 K  |
|       from large pool |      36    |      77    |    5572 K  |    5572 K  |
|       from small pool |     187    |     225    |    5952 K  |    5951 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11524 K  |   11524 K  |
|       from large pool |      36    |      77    |    5572 K  |    5572 K  |
|       from small pool |     187    |     225    |    5952 K  |    5951 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5832 K  |    5832 K  |
|       from large pool |      10    |      11    |    2308 K  |    2308 K  |
|       from small pool |      10    |      23    |    3524 K  |    3524 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 180, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.48s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-180-regular.pth
Training Epoch: 181 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [3584/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [4608/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [5120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [6912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [7168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [7936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [8704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [9216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [10752/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [11008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [11520/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 181 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [12800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [13056/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [13568/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [14336/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 181 [14592/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [14848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [15360/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [16896/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 181 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [17664/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [18176/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [18432/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [18944/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [19200/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [19456/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [19968/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 181 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [20480/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 181 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [21504/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [22016/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [22272/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 181 [22528/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [22784/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [23808/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [24320/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [24576/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [25344/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [26624/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [26880/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [28160/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [28416/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [28928/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [29184/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 181 [29440/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [29696/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [29952/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [30208/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [30976/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [32256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [33024/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [33280/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [33536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [33792/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [34048/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [35328/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 181 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [36608/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 181 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [37120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [37376/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [37888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 181 [38144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [38656/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [38912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [39168/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 181 [39424/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [39936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [40192/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [40704/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [41472/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [41728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [41984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [42240/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [43520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [43776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [44800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [45056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 181 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [46848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [47360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 181 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [47872/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 181 [48128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 181 [48384/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 181 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [49152/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 181 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 181 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 181 [49920/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 181 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 181 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  271533 GB |  271533 GB |
|       from large pool |  400448 KB |    1770 MB |  271289 GB |  271289 GB |
|       from small pool |    3550 KB |       9 MB |     244 GB |     244 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  271533 GB |  271533 GB |
|       from large pool |  400448 KB |    1770 MB |  271289 GB |  271289 GB |
|       from small pool |    3550 KB |       9 MB |     244 GB |     244 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  167070 GB |  167069 GB |
|       from large pool |  244672 KB |  473024 KB |  166790 GB |  166790 GB |
|       from small pool |    2594 KB |    4843 KB |     279 GB |     279 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11588 K  |   11588 K  |
|       from large pool |      36    |      77    |    5603 K  |    5603 K  |
|       from small pool |     187    |     225    |    5985 K  |    5985 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11588 K  |   11588 K  |
|       from large pool |      36    |      77    |    5603 K  |    5603 K  |
|       from small pool |     187    |     225    |    5985 K  |    5985 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    5864 K  |    5864 K  |
|       from large pool |      10    |      11    |    2320 K  |    2320 K  |
|       from small pool |      10    |      23    |    3543 K  |    3543 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 181, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 182 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [1280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [3328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [4096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [5888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [6144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [6400/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [7424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [7680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [8960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [10240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [10752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [12288/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [12800/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [13056/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 182 [13312/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 182 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [14336/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 182 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [15104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [15616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [15872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [16128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [16384/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 182 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [17152/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [17408/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [17664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [17920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [18176/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [18432/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 182 [18688/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [18944/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [19456/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [19712/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [19968/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [20480/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 182 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [20992/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [22272/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [22528/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [23296/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [23552/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [23808/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [24064/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [24320/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [25600/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [25856/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [26368/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [27648/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [29184/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [29440/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [30208/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [30464/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [30976/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [31232/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [31488/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [32512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [32768/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [33024/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [34048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [34304/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [35072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [35840/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [36352/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [36608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [36864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [37120/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [37376/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 182 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [37888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [38144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [38400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [38912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [39168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [39424/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [39680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [39936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [40192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [40704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 182 [40960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [41472/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [42240/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [43008/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 182 [43264/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [43520/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 182 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [44032/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 182 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [44544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [45312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [45824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 182 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 182 [46336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [47104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [48640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 182 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 182 [49920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 182 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 182 training time consumed: 21.71s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  273033 GB |  273033 GB |
|       from large pool |  400448 KB |    1770 MB |  272788 GB |  272787 GB |
|       from small pool |    3550 KB |       9 MB |     245 GB |     245 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  273033 GB |  273033 GB |
|       from large pool |  400448 KB |    1770 MB |  272788 GB |  272787 GB |
|       from small pool |    3550 KB |       9 MB |     245 GB |     245 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  167993 GB |  167993 GB |
|       from large pool |  244672 KB |  473024 KB |  167712 GB |  167712 GB |
|       from small pool |    2594 KB |    4843 KB |     280 GB |     280 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11652 K  |   11652 K  |
|       from large pool |      36    |      77    |    5634 K  |    5634 K  |
|       from small pool |     187    |     225    |    6018 K  |    6018 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11652 K  |   11652 K  |
|       from large pool |      36    |      77    |    5634 K  |    5634 K  |
|       from small pool |     187    |     225    |    6018 K  |    6018 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    5896 K  |    5896 K  |
|       from large pool |      10    |      11    |    2333 K  |    2333 K  |
|       from small pool |      13    |      23    |    3562 K  |    3562 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 182, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 183 [256/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 183 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [3072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 183 [3328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [3584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [4096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 183 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [5888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [7424/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 183 [7680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 183 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [8448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [8960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [10496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [11008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [11264/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [11520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [12032/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 183 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [13568/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [13824/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 183 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [14336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [14592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [15872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [16640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [16896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [17152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [17408/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [17664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [18432/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [18688/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [19456/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [20480/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [20736/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [20992/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [21504/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 183 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [22528/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [23040/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [23296/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [23552/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 183 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [24320/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 183 [24576/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [24832/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [25344/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [25856/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [26112/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [26880/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [28160/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [28416/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [28928/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [29184/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [29952/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 183 [30208/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [30464/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [30976/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [31232/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [31488/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [32000/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [32768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [33536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [34048/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 183 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [34560/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [35328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [35584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [35840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [37120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [37376/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 183 [37632/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 183 [37888/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 183 [38144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [39168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [39424/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 183 [39680/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 183 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [40448/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [40960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [41216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [41472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 183 [41728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [41984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [42752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 183 [43008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [43520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [44032/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 183 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 183 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [44800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [45056/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 183 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 183 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 183 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 183 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 183 training time consumed: 21.88s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  274534 GB |  274533 GB |
|       from large pool |  400448 KB |    1770 MB |  274287 GB |  274286 GB |
|       from small pool |    3550 KB |       9 MB |     246 GB |     246 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  274534 GB |  274533 GB |
|       from large pool |  400448 KB |    1770 MB |  274287 GB |  274286 GB |
|       from small pool |    3550 KB |       9 MB |     246 GB |     246 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  168916 GB |  168916 GB |
|       from large pool |  244672 KB |  473024 KB |  168633 GB |  168633 GB |
|       from small pool |    2594 KB |    4843 KB |     282 GB |     282 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11716 K  |   11716 K  |
|       from large pool |      36    |      77    |    5665 K  |    5665 K  |
|       from small pool |     187    |     225    |    6051 K  |    6051 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11716 K  |   11716 K  |
|       from large pool |      36    |      77    |    5665 K  |    5665 K  |
|       from small pool |     187    |     225    |    6051 K  |    6051 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    5929 K  |    5929 K  |
|       from large pool |      10    |      11    |    2346 K  |    2346 K  |
|       from small pool |      11    |      23    |    3582 K  |    3582 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 183, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 184 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [1024/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [4096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [6656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [8704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [9216/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [9472/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 184 [9728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [9984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [10752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [12032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [13568/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [13824/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [14080/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [14336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [15104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [16640/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 184 [16896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [17152/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [17408/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [17664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [18176/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [18432/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [18944/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [19456/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [19712/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [20736/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 184 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [21248/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [21760/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [22272/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [22528/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [23040/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [23296/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [23552/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 184 [23808/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [24064/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [24320/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [24576/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [25344/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [25600/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 184 [25856/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [26624/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [26880/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [27392/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [28160/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [28672/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [28928/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [29440/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [30720/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [31744/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [32000/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [32256/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [32512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [33536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [33792/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 184 [34048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [34560/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [35584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [36608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [37376/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [37632/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 184 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [38144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [38400/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 184 [38656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [39168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [39424/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 184 [39680/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 184 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [40448/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [41216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [41728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [43264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [44800/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 184 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [45824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [46080/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 184 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 184 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 184 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [48384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [48896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 184 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [49408/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 184 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 184 [50000/50000]	Loss: 4.6057	LR: 0.008000
epoch 184 training time consumed: 21.81s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  276034 GB |  276033 GB |
|       from large pool |  400448 KB |    1770 MB |  275785 GB |  275785 GB |
|       from small pool |    3550 KB |       9 MB |     248 GB |     248 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  276034 GB |  276033 GB |
|       from large pool |  400448 KB |    1770 MB |  275785 GB |  275785 GB |
|       from small pool |    3550 KB |       9 MB |     248 GB |     248 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  169839 GB |  169839 GB |
|       from large pool |  244672 KB |  473024 KB |  169555 GB |  169555 GB |
|       from small pool |    2594 KB |    4843 KB |     283 GB |     283 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11780 K  |   11780 K  |
|       from large pool |      36    |      77    |    5696 K  |    5696 K  |
|       from small pool |     187    |     225    |    6084 K  |    6084 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11780 K  |   11780 K  |
|       from large pool |      36    |      77    |    5696 K  |    5696 K  |
|       from small pool |     187    |     225    |    6084 K  |    6084 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    5961 K  |    5961 K  |
|       from large pool |      10    |      11    |    2359 K  |    2359 K  |
|       from small pool |      13    |      23    |    3602 K  |    3602 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 184, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 185 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [2560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [3328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [3584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [9472/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [10496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [10752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [11008/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [11520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [12032/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [12800/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 185 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [13312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [13568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [14336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [14592/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [14848/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [15104/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [15360/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [15872/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 185 [16128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [16640/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [17152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [18176/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 185 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [18944/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [19200/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [19456/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [20480/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [20736/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [21504/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [22016/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [22272/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [22528/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [23040/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [23552/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [24064/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [24320/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 185 [24576/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [24832/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [25344/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [25600/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [25856/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 185 [26112/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [26368/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [26624/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [27904/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 185 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [28672/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [29696/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [29952/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 185 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [30464/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [30720/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [32000/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [32256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [33536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [33792/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [34048/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [34304/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 185 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [35072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 185 [35328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [35584/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [35840/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 185 [36096/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 185 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [36608/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 185 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [37120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [37632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [38144/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [38656/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [38912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [39168/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 185 [39424/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [39680/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 185 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [40192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 185 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [40704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [40960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [41728/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 185 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [42240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [42496/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 185 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [43520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [43776/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 185 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [44288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [44544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [45312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 185 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [46336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [47104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [47872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 185 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 185 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 185 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 185 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  277534 GB |  277534 GB |
|       from large pool |  400448 KB |    1770 MB |  277284 GB |  277284 GB |
|       from small pool |    3550 KB |       9 MB |     249 GB |     249 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  277534 GB |  277534 GB |
|       from large pool |  400448 KB |    1770 MB |  277284 GB |  277284 GB |
|       from small pool |    3550 KB |       9 MB |     249 GB |     249 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  170762 GB |  170762 GB |
|       from large pool |  244672 KB |  473024 KB |  170476 GB |  170476 GB |
|       from small pool |    2594 KB |    4843 KB |     285 GB |     285 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11844 K  |   11844 K  |
|       from large pool |      36    |      77    |    5727 K  |    5727 K  |
|       from small pool |     187    |     225    |    6117 K  |    6117 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11844 K  |   11844 K  |
|       from large pool |      36    |      77    |    5727 K  |    5727 K  |
|       from small pool |     187    |     225    |    6117 K  |    6117 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    5993 K  |    5993 K  |
|       from large pool |      10    |      11    |    2372 K  |    2372 K  |
|       from small pool |      12    |      23    |    3621 K  |    3621 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 185, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 186 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [1280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [3072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 186 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [4608/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 186 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [7168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [8448/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 186 [8704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [9472/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [9728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 186 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [11008/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 186 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [11520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [12032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [12288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [12544/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [13568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [14336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [14848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [15360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [16896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [17408/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [17664/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 186 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [18176/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [18432/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [18944/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [19456/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [19712/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [19968/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [20736/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [20992/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [21248/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 186 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [22016/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [22272/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [22528/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [22784/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [23040/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [23808/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [24320/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [24576/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [24832/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [25344/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [25600/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 186 [25856/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 186 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [26368/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [26624/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [26880/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [27392/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 186 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [27904/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [28416/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [28672/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [29184/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 186 [29440/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [29696/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [29952/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [30208/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [30464/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [30976/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [31744/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [32512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [33024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [33536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [33792/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [34048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [34560/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [35328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [35840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [37120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [37376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [37632/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [37888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [38144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [38912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [39424/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [40192/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 186 [40448/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [41728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [41984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [42240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [42496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [42752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 186 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [43264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [43776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [44032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [45824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 186 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [46848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [48640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [48896/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 186 [49152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 186 [49408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 186 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 186 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 186 training time consumed: 21.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  279034 GB |  279034 GB |
|       from large pool |  400448 KB |    1770 MB |  278783 GB |  278783 GB |
|       from small pool |    3550 KB |       9 MB |     251 GB |     251 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  279034 GB |  279034 GB |
|       from large pool |  400448 KB |    1770 MB |  278783 GB |  278783 GB |
|       from small pool |    3550 KB |       9 MB |     251 GB |     251 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  171685 GB |  171685 GB |
|       from large pool |  244672 KB |  473024 KB |  171398 GB |  171398 GB |
|       from small pool |    2594 KB |    4843 KB |     287 GB |     287 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11908 K  |   11908 K  |
|       from large pool |      36    |      77    |    5758 K  |    5758 K  |
|       from small pool |     187    |     225    |    6150 K  |    6150 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11908 K  |   11908 K  |
|       from large pool |      36    |      77    |    5758 K  |    5758 K  |
|       from small pool |     187    |     225    |    6150 K  |    6150 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    6025 K  |    6025 K  |
|       from large pool |      10    |      11    |    2385 K  |    2385 K  |
|       from small pool |      10    |      23    |    3640 K  |    3640 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 186, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 187 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [3072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [3584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [3840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [5120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [5888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [6400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [9216/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [9728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [9984/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 187 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [10496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [11520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [12032/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [12288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [13056/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 187 [13312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [14336/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 187 [14592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [15616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [16640/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [17920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [18176/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [18432/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [19200/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [19456/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [20736/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [20992/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [21760/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [22016/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [22272/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [22528/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [23296/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [23552/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [24064/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 187 [24320/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [24576/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [25088/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 187 [25344/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 187 [25600/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [26112/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [26368/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [26880/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [27648/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [28160/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 187 [28416/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [29184/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 187 [29440/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [30208/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [30720/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [30976/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [31488/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 187 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [32000/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [32256/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [32512/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 187 [32768/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [33536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [33792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [34304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [34560/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [35072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [36608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [36864/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 187 [37120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [37376/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [37632/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [37888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [38144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [38400/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [38912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [39168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [39680/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [40192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [40448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [41216/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [42240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [42496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 187 [42752/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 187 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [44032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [45312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [46848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [47104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [47360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [48384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 187 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [49152/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 187 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 187 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 187 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 187 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 187 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  280534 GB |  280534 GB |
|       from large pool |  400448 KB |    1770 MB |  280282 GB |  280282 GB |
|       from small pool |    3550 KB |       9 MB |     252 GB |     252 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  280534 GB |  280534 GB |
|       from large pool |  400448 KB |    1770 MB |  280282 GB |  280282 GB |
|       from small pool |    3550 KB |       9 MB |     252 GB |     252 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  172608 GB |  172608 GB |
|       from large pool |  244672 KB |  473024 KB |  172319 GB |  172319 GB |
|       from small pool |    2594 KB |    4843 KB |     288 GB |     288 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   11973 K  |   11972 K  |
|       from large pool |      36    |      77    |    5789 K  |    5789 K  |
|       from small pool |     187    |     225    |    6183 K  |    6183 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   11973 K  |   11972 K  |
|       from large pool |      36    |      77    |    5789 K  |    5789 K  |
|       from small pool |     187    |     225    |    6183 K  |    6183 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    6058 K  |    6058 K  |
|       from large pool |      10    |      11    |    2397 K  |    2397 K  |
|       from small pool |      11    |      23    |    3660 K  |    3660 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 187, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 188 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [2048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [2816/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [3584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [6912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [7424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [8192/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [8704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [9216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [10496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [11264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [12288/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [14080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [14336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [14848/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 188 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [15360/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [16896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [17408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [18176/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [18432/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [18688/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [18944/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 188 [19200/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [19456/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [19968/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [20480/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [20736/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [20992/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [21248/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [22528/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [22784/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [23296/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [23552/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [23808/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [24064/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [24320/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [24576/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [25088/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [25344/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [25600/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [26880/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [27136/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [28160/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [28416/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [28928/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [29440/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [30464/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [30720/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [31232/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 188 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [31744/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [32256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [32512/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [32768/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [33792/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 188 [34048/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 188 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [34560/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [34816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [35328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [35840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [36352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [36864/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [37632/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [38144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [38400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [39168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [39680/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 188 [39936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [40192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [40448/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [40704/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 188 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [41984/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [42240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [43008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [43264/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 188 [43520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [44544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [47360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 188 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [48128/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 188 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [49408/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 188 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 188 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 188 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 188 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  282035 GB |  282034 GB |
|       from large pool |  400448 KB |    1770 MB |  281781 GB |  281780 GB |
|       from small pool |    3550 KB |       9 MB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  282035 GB |  282034 GB |
|       from large pool |  400448 KB |    1770 MB |  281781 GB |  281780 GB |
|       from small pool |    3550 KB |       9 MB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  173531 GB |  173531 GB |
|       from large pool |  244672 KB |  473024 KB |  173241 GB |  173241 GB |
|       from small pool |    2594 KB |    4843 KB |     290 GB |     290 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12037 K  |   12036 K  |
|       from large pool |      36    |      77    |    5820 K  |    5820 K  |
|       from small pool |     187    |     225    |    6216 K  |    6216 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12037 K  |   12036 K  |
|       from large pool |      36    |      77    |    5820 K  |    5820 K  |
|       from small pool |     187    |     225    |    6216 K  |    6216 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    6089 K  |    6089 K  |
|       from large pool |      10    |      11    |    2410 K  |    2410 K  |
|       from small pool |       9    |      23    |    3679 K  |    3679 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 188, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 189 [256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [1024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [3072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [6144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [6656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [6912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [7424/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [8192/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [9216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [9728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [9984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [10496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [11008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [11264/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [12032/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 189 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [12544/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [13568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [14336/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 189 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [14848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [15872/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [16640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [16896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [17920/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [18432/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [18688/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [18944/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [19200/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [19456/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [19712/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [19968/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [20224/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [20992/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 189 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [21504/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [21760/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 189 [22016/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [22272/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [22528/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 189 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [23296/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [23808/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [24064/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [24320/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [24576/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [25088/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [25344/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [25600/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [26368/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [26624/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 189 [26880/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [28160/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [28416/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [28928/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [29184/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [29440/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [30208/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [30464/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [30720/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [32512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [32768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [33280/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [33536/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [34048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [34560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [34816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [35072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [35328/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [35840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [36096/50000]	Loss: 4.6061	LR: 0.008000
Training Epoch: 189 [36352/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [36608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [36864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [37120/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [37376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [37632/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [37888/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [38144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [38400/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [38656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 189 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [39168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [39424/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 189 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [40192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 189 [40448/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [41984/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [42240/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [43776/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [45056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [45312/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [45568/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [45824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [46080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [46336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [46592/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 189 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [47104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 189 [47360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 189 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 189 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 189 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 189 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 189 training time consumed: 21.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  283535 GB |  283534 GB |
|       from large pool |  400448 KB |    1770 MB |  283280 GB |  283279 GB |
|       from small pool |    3550 KB |       9 MB |     255 GB |     255 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  283535 GB |  283534 GB |
|       from large pool |  400448 KB |    1770 MB |  283280 GB |  283279 GB |
|       from small pool |    3550 KB |       9 MB |     255 GB |     255 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  174454 GB |  174454 GB |
|       from large pool |  244672 KB |  473024 KB |  174162 GB |  174162 GB |
|       from small pool |    2594 KB |    4843 KB |     291 GB |     291 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12101 K  |   12100 K  |
|       from large pool |      36    |      77    |    5851 K  |    5851 K  |
|       from small pool |     187    |     225    |    6249 K  |    6249 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12101 K  |   12100 K  |
|       from large pool |      36    |      77    |    5851 K  |    5851 K  |
|       from small pool |     187    |     225    |    6249 K  |    6249 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    6120 K  |    6120 K  |
|       from large pool |      10    |      11    |    2423 K  |    2423 K  |
|       from small pool |       9    |      23    |    3696 K  |    3696 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 189, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.50s

Training Epoch: 190 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [1024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [1536/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [1792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [4352/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [5888/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [6144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [6400/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [6656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [8448/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [8704/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [9216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [9984/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [10496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [10752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [11008/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [11520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [11776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [12032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [12544/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [12800/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [13056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [13568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [13824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [14336/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [14848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [15104/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [15616/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 190 [15872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [16384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [18176/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [19200/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [19712/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [19968/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 190 [20224/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [20480/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [20736/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [21504/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [22016/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [22528/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [23296/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [23552/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 190 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [24320/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [24832/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 190 [25088/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [25344/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [25856/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 190 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [26368/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [26880/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [27904/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 190 [28160/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 190 [28416/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [29440/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [29952/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [30208/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [30464/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [30720/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [31744/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [32000/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [32256/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [32768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [33536/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [33792/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [34048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [34304/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [34560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [34816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [35072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [35328/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [35584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [36096/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [36608/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [37376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [38144/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 190 [38400/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [38656/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [38912/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [39168/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [39680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [40192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [40448/50000]	Loss: 4.6060	LR: 0.008000
Training Epoch: 190 [40704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 190 [40960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [41728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [42752/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 190 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [43520/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 190 [43776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [44800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [45568/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [45824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [46592/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [47872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 190 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 190 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 190 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 190 [49664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [49920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 190 [50000/50000]	Loss: 4.6057	LR: 0.008000
epoch 190 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  285035 GB |  285035 GB |
|       from large pool |  400448 KB |    1770 MB |  284778 GB |  284778 GB |
|       from small pool |    3550 KB |       9 MB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  285035 GB |  285035 GB |
|       from large pool |  400448 KB |    1770 MB |  284778 GB |  284778 GB |
|       from small pool |    3550 KB |       9 MB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  175377 GB |  175377 GB |
|       from large pool |  244672 KB |  473024 KB |  175084 GB |  175084 GB |
|       from small pool |    2594 KB |    4843 KB |     293 GB |     293 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12165 K  |   12164 K  |
|       from large pool |      36    |      77    |    5882 K  |    5882 K  |
|       from small pool |     187    |     225    |    6282 K  |    6282 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12165 K  |   12164 K  |
|       from large pool |      36    |      77    |    5882 K  |    5882 K  |
|       from small pool |     187    |     225    |    6282 K  |    6282 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    6153 K  |    6153 K  |
|       from large pool |      10    |      11    |    2436 K  |    2436 K  |
|       from small pool |      11    |      23    |    3716 K  |    3716 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 190, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-190-regular.pth
Training Epoch: 191 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [1280/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [2816/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [3584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [4096/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [4352/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 191 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [4864/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [5376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [5888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [6144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [7424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [7936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [8448/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [9216/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [10496/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 191 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [11008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [11520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [12800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [13568/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [14080/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [14336/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [15360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [16896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [17408/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [17664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [18176/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 191 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [18688/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [18944/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [19200/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [19456/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [20224/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [20736/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [21504/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [21760/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [22016/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [22784/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [23040/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 191 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [24064/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [24320/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [24576/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [25088/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [25856/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [26112/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [26368/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [26880/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [27136/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [27392/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [27648/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [27904/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [28416/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [29184/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [30208/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 191 [30464/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [32000/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [32256/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [32512/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 191 [32768/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [33280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [33536/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 191 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [34048/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [34560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [34816/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [35072/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [35328/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [35584/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [35840/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [36352/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [36864/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [37120/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [37632/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [37888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [38144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [38912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [39168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 191 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [39680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [40192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [40704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [41472/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [41728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [41984/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [42752/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [43264/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [43776/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [44544/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [44800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [45056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 191 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [45824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [46080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [46592/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 191 [46848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 191 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 191 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 191 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [49920/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 191 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 191 training time consumed: 21.77s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  286535 GB |  286535 GB |
|       from large pool |  400448 KB |    1770 MB |  286277 GB |  286277 GB |
|       from small pool |    3550 KB |       9 MB |     257 GB |     257 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  286535 GB |  286535 GB |
|       from large pool |  400448 KB |    1770 MB |  286277 GB |  286277 GB |
|       from small pool |    3550 KB |       9 MB |     257 GB |     257 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  176300 GB |  176300 GB |
|       from large pool |  244672 KB |  473024 KB |  176005 GB |  176005 GB |
|       from small pool |    2594 KB |    4843 KB |     294 GB |     294 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12229 K  |   12228 K  |
|       from large pool |      36    |      77    |    5913 K  |    5913 K  |
|       from small pool |     187    |     225    |    6315 K  |    6315 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12229 K  |   12228 K  |
|       from large pool |      36    |      77    |    5913 K  |    5913 K  |
|       from small pool |     187    |     225    |    6315 K  |    6315 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    6185 K  |    6185 K  |
|       from large pool |      10    |      11    |    2449 K  |    2449 K  |
|       from small pool |      10    |      23    |    3736 K  |    3736 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 191, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.41s

Training Epoch: 192 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [2816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [3584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [5376/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [5888/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [6144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [6656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [6912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [8192/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [10240/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [10496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [10752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [11264/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [11520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [11776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [12800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [13568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [13824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [14080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [14336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [15104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [15360/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [15872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [16128/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [16384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [16640/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [17408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [17920/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [18688/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [18944/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [19200/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 192 [19456/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [19968/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [20224/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [20736/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [22528/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [22784/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [23040/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [23296/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [23552/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [24064/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [24320/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [24576/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [24832/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [25344/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [25600/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [25856/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [26112/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [26368/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [26624/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [26880/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [27648/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [27904/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [28160/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 192 [28416/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [28672/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [28928/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [29184/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [29696/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [30208/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [30720/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [31232/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [31744/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [32512/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [33792/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [34304/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [34560/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [35072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 192 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [35840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [36096/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [36352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [36608/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [36864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [37120/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [37376/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 192 [37632/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [37888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [38144/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [38656/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [39168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [39424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [39936/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [40192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [40960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [41728/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [42240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 192 [42496/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [43264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [43520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [44032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 192 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [44544/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [45312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [45568/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [46080/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [46336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [47360/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 192 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [48384/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 192 [48640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 192 [48896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [49408/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 192 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 192 [50000/50000]	Loss: 4.6058	LR: 0.008000
epoch 192 training time consumed: 21.75s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  288035 GB |  288035 GB |
|       from large pool |  400448 KB |    1770 MB |  287776 GB |  287776 GB |
|       from small pool |    3550 KB |       9 MB |     259 GB |     259 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  288035 GB |  288035 GB |
|       from large pool |  400448 KB |    1770 MB |  287776 GB |  287776 GB |
|       from small pool |    3550 KB |       9 MB |     259 GB |     259 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  177223 GB |  177223 GB |
|       from large pool |  244672 KB |  473024 KB |  176927 GB |  176926 GB |
|       from small pool |    2594 KB |    4843 KB |     296 GB |     296 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12293 K  |   12292 K  |
|       from large pool |      36    |      77    |    5944 K  |    5944 K  |
|       from small pool |     187    |     225    |    6348 K  |    6348 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12293 K  |   12292 K  |
|       from large pool |      36    |      77    |    5944 K  |    5944 K  |
|       from small pool |     187    |     225    |    6348 K  |    6348 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    6216 K  |    6216 K  |
|       from large pool |      10    |      11    |    2462 K  |    2462 K  |
|       from small pool |      10    |      23    |    3754 K  |    3754 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 192, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.43s

Training Epoch: 193 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [1024/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [1536/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [3840/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [4096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [5888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [6144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [6656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [7168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [7424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [7680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [8960/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 193 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [9728/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [10240/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [11264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [11520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [11776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [12800/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [14080/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [14336/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 193 [14592/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [14848/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [15104/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [15616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [15872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [16384/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [16640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [16896/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [17664/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [17920/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [18176/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [18432/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [18688/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [18944/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [19712/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 193 [19968/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [20224/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [20480/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [20992/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [21504/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 193 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [22016/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 193 [22272/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [22528/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [23040/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 193 [23296/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [23552/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [24064/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [24320/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [24576/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [24832/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [25600/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [26112/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [26368/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [26624/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [26880/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [27648/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 193 [27904/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [28160/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 193 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [28928/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [29184/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 193 [29440/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [29696/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 193 [29952/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [30208/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [32000/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [32256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [32512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [32768/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [33536/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [33792/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [34304/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [35328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [35584/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 193 [35840/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [36096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [36608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [36864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [37120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 193 [37376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [37632/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [37888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [38144/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 193 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [38656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [39680/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 193 [39936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [40192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [40704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [40960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 193 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [41728/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 193 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [42240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [42496/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [45824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 193 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [47104/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [47360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [47872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [48384/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 193 [48640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [49152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 193 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 193 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 193 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 193 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  289535 GB |  289535 GB |
|       from large pool |  400448 KB |    1770 MB |  289275 GB |  289275 GB |
|       from small pool |    3550 KB |       9 MB |     260 GB |     260 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  289535 GB |  289535 GB |
|       from large pool |  400448 KB |    1770 MB |  289275 GB |  289275 GB |
|       from small pool |    3550 KB |       9 MB |     260 GB |     260 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  178146 GB |  178146 GB |
|       from large pool |  244672 KB |  473024 KB |  177848 GB |  177848 GB |
|       from small pool |    2594 KB |    4843 KB |     297 GB |     297 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12357 K  |   12356 K  |
|       from large pool |      36    |      77    |    5975 K  |    5975 K  |
|       from small pool |     187    |     225    |    6381 K  |    6381 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12357 K  |   12356 K  |
|       from large pool |      36    |      77    |    5975 K  |    5975 K  |
|       from small pool |     187    |     225    |    6381 K  |    6381 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    6248 K  |    6248 K  |
|       from large pool |      10    |      11    |    2474 K  |    2474 K  |
|       from small pool |      13    |      23    |    3773 K  |    3773 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 193, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 194 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [2048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [3072/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [5120/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [5376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [6144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [6400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [7168/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [7424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [7680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [8704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [9472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [9984/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 194 [10240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [11264/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [11520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [12288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [12544/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [12800/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 194 [13056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [13312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [13568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [13824/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [14336/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [14848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [15872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [16384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [16640/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 194 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [17664/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [18432/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [18688/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [19456/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [19712/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [20224/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [20736/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [20992/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [21248/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [21504/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [21760/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [22528/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 194 [22784/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [23040/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [23296/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [23808/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [24064/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [24320/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [25600/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [25856/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [26368/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 194 [26624/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [27136/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [27392/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [27648/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [28160/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [28416/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [28672/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [28928/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [29440/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [29696/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [29952/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [30208/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [30464/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [30976/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [31488/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [31744/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [32000/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 194 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [32768/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [33024/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [33280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [33792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [34816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [35072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 194 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [35840/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [36352/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [36608/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [37376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [37888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [38144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [38400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [38912/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [39680/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [39936/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 194 [40192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [40704/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [40960/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [41728/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [42752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [43008/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 194 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [43520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [43776/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 194 [44032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [44544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [45056/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [45824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 194 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [46592/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [47360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [48128/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 194 [48640/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 194 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 194 [49408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 194 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 194 [49920/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 194 [50000/50000]	Loss: 4.6053	LR: 0.008000
epoch 194 training time consumed: 21.74s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  291036 GB |  291035 GB |
|       from large pool |  400448 KB |    1770 MB |  290774 GB |  290773 GB |
|       from small pool |    3550 KB |       9 MB |     261 GB |     261 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  291036 GB |  291035 GB |
|       from large pool |  400448 KB |    1770 MB |  290774 GB |  290773 GB |
|       from small pool |    3550 KB |       9 MB |     261 GB |     261 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  179069 GB |  179069 GB |
|       from large pool |  244672 KB |  473024 KB |  178770 GB |  178769 GB |
|       from small pool |    2594 KB |    4843 KB |     299 GB |     299 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12421 K  |   12420 K  |
|       from large pool |      36    |      77    |    6006 K  |    6006 K  |
|       from small pool |     187    |     225    |    6415 K  |    6414 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12421 K  |   12420 K  |
|       from large pool |      36    |      77    |    6006 K  |    6006 K  |
|       from small pool |     187    |     225    |    6415 K  |    6414 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    6281 K  |    6281 K  |
|       from large pool |      10    |      11    |    2487 K  |    2487 K  |
|       from small pool |      11    |      23    |    3793 K  |    3793 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 194, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 195 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [1280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [2560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [3328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [4096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [4864/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [5376/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [5632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [5888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [6144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [6400/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [7168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [7424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [8192/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [8960/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [9216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [10496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [10752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [11008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [11264/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [11520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [12032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [12288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [12544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [12800/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [13056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [13568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [14080/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [14336/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [14592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [15104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [15360/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 195 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [15872/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 195 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [16384/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [16640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [16896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [17152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [17920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [18176/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [18432/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [18944/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [19712/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [19968/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [20480/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [20992/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [21248/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [21504/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [21760/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [22016/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [22272/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [22528/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [22784/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [23296/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [23552/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [24064/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [24320/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 195 [24576/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [24832/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [25344/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [25600/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [25856/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [26112/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [26624/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [27392/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [28160/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [28672/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [29184/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [30208/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [30464/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [30720/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [30976/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [31488/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [31744/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [32000/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [32256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [32512/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 195 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [33280/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [33536/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 195 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [34048/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [34560/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [34816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [35072/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [35328/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [35584/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 195 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [36352/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 195 [36608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [37120/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [38144/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [38400/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [38656/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 195 [38912/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 195 [39168/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [39424/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [39936/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 195 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [40448/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [40704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [40960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [41216/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [41984/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 195 [42240/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [43264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [44032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 195 [44288/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [45568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [45824/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [46080/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 195 [46336/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [47104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 195 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [48128/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 195 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 195 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [49664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 195 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 195 training time consumed: 21.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  292536 GB |  292535 GB |
|       from large pool |  400448 KB |    1770 MB |  292273 GB |  292272 GB |
|       from small pool |    3550 KB |       9 MB |     263 GB |     263 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  292536 GB |  292535 GB |
|       from large pool |  400448 KB |    1770 MB |  292273 GB |  292272 GB |
|       from small pool |    3550 KB |       9 MB |     263 GB |     263 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  179992 GB |  179992 GB |
|       from large pool |  244672 KB |  473024 KB |  179691 GB |  179691 GB |
|       from small pool |    2594 KB |    4843 KB |     300 GB |     300 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12485 K  |   12484 K  |
|       from large pool |      36    |      77    |    6037 K  |    6037 K  |
|       from small pool |     187    |     225    |    6448 K  |    6447 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12485 K  |   12484 K  |
|       from large pool |      36    |      77    |    6037 K  |    6037 K  |
|       from small pool |     187    |     225    |    6448 K  |    6447 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      23    |      33    |    6314 K  |    6314 K  |
|       from large pool |      10    |      11    |    2500 K  |    2500 K  |
|       from small pool |      13    |      23    |    3813 K  |    3813 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 195, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 196 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [2048/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [2816/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [3072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [3328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [3584/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [3840/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [4096/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [4352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [4608/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [4864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [5888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [6144/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 196 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [6912/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [7680/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [7936/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [8704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [8960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [9216/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [9984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [10240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [11520/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [12032/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [12288/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [12544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [12800/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [13056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [13824/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [14080/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [14336/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [14848/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [15104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [15360/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [15616/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 196 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [16128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [16384/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 196 [16640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [16896/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [17152/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [17408/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [17664/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [17920/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [18176/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [18432/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [18688/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [19200/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [19456/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [19968/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [20224/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 196 [20480/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [20736/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [20992/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [21248/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [21760/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [22272/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 196 [22528/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [23040/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [23296/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [23552/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [23808/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [24320/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [24576/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 196 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [25856/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [26112/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 196 [26368/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [26624/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [26880/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [27136/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [27392/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [27648/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [27904/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [28160/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [28416/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [29952/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [30464/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [30720/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [30976/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [31232/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [31488/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [31744/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 196 [32000/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 196 [32256/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [32512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [32768/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [34048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [34304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [34560/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 196 [34816/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [35072/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [35328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [36096/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 196 [36352/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [36608/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [36864/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [37632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [37888/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [38144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [38400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [38912/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [39168/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [39424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [40192/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [40448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [41216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [41472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [41728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [41984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [42240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [42496/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [42752/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [43008/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [43520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [43776/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [44032/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 196 [44288/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 196 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [44800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [45312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 196 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [45824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 196 [46080/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [46592/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [46848/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [47104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [47616/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [48640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [48896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 196 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 196 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [49664/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 196 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 196 [50000/50000]	Loss: 4.6058	LR: 0.008000
epoch 196 training time consumed: 21.96s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  294036 GB |  294036 GB |
|       from large pool |  400448 KB |    1770 MB |  293771 GB |  293771 GB |
|       from small pool |    3550 KB |       9 MB |     264 GB |     264 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  294036 GB |  294036 GB |
|       from large pool |  400448 KB |    1770 MB |  293771 GB |  293771 GB |
|       from small pool |    3550 KB |       9 MB |     264 GB |     264 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  180915 GB |  180915 GB |
|       from large pool |  244672 KB |  473024 KB |  180613 GB |  180612 GB |
|       from small pool |    2594 KB |    4843 KB |     302 GB |     302 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12549 K  |   12549 K  |
|       from large pool |      36    |      77    |    6068 K  |    6068 K  |
|       from small pool |     187    |     225    |    6481 K  |    6481 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12549 K  |   12549 K  |
|       from large pool |      36    |      77    |    6068 K  |    6068 K  |
|       from small pool |     187    |     225    |    6481 K  |    6481 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      22    |      33    |    6345 K  |    6345 K  |
|       from large pool |      10    |      11    |    2513 K  |    2513 K  |
|       from small pool |      12    |      23    |    3832 K  |    3832 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 196, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

Training Epoch: 197 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [3840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [4352/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [4864/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [5120/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [5376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [6400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [6656/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [7424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 197 [7680/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [8704/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [9216/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [9984/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 197 [10240/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [10496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [10752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [11008/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [11520/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [12032/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [12288/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 197 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [13312/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [13568/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [13824/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [14080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [14336/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [14848/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [15104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [15360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [15616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [15872/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [16384/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 197 [16640/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [16896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [17920/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [18432/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [18688/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 197 [18944/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [19200/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [19456/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [19968/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [20224/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [20736/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [20992/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [21504/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [21760/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [22528/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [22784/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [23296/50000]	Loss: 4.6046	LR: 0.008000
Training Epoch: 197 [23552/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 197 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [24064/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [24320/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [24576/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [24832/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [25088/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [25344/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [25600/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [25856/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [26368/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 197 [26624/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [26880/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [27136/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [27392/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [27648/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [28160/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [28928/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [29184/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [29696/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [29952/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [30208/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [30720/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [30976/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [31488/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [31744/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [32000/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [32256/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [32512/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [32768/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [33024/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [33536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [34048/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [34560/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [34816/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [35072/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 197 [35328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [35840/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [36608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [36864/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [37376/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 197 [37632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [38144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [38400/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [38912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [39424/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [40192/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [40448/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [40704/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [41216/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [41472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [41728/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 197 [41984/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [42496/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [42752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [43008/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [43264/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [43520/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 197 [43776/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 197 [44032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [44288/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [44800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [45056/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [45312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 197 [45568/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [46080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [46336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [47360/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 197 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [48384/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [48640/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [48896/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [49152/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 197 [49408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 197 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 197 [50000/50000]	Loss: 4.6055	LR: 0.008000
epoch 197 training time consumed: 21.72s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  295536 GB |  295536 GB |
|       from large pool |  400448 KB |    1770 MB |  295270 GB |  295270 GB |
|       from small pool |    3550 KB |       9 MB |     265 GB |     265 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  295536 GB |  295536 GB |
|       from large pool |  400448 KB |    1770 MB |  295270 GB |  295270 GB |
|       from small pool |    3550 KB |       9 MB |     265 GB |     265 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  181838 GB |  181838 GB |
|       from large pool |  244672 KB |  473024 KB |  181534 GB |  181534 GB |
|       from small pool |    2594 KB |    4843 KB |     304 GB |     304 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12613 K  |   12613 K  |
|       from large pool |      36    |      77    |    6098 K  |    6098 K  |
|       from small pool |     187    |     225    |    6514 K  |    6514 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12613 K  |   12613 K  |
|       from large pool |      36    |      77    |    6098 K  |    6098 K  |
|       from small pool |     187    |     225    |    6514 K  |    6514 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      20    |      33    |    6378 K  |    6378 K  |
|       from large pool |      10    |      11    |    2526 K  |    2526 K  |
|       from small pool |      10    |      23    |    3851 K  |    3851 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 197, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.44s

Training Epoch: 198 [256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [1280/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [1536/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [1792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [2304/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [2816/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [3328/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [4096/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [4352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [4608/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [5632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [5888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [6144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [6656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [6912/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [7424/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [7680/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [7936/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [8192/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [8448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [8704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [9984/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [10240/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 198 [10496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [10752/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [11008/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [11264/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [11520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [11776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [12032/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [12544/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [13056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [13312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [13568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [13824/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [14080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [14336/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 198 [14592/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [15616/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [15872/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 198 [16128/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [16384/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [16640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [16896/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 198 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [17408/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [17664/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [18176/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [18432/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [18688/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [18944/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [19456/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [19712/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [19968/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [20224/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [21248/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [21504/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [21760/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [22016/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [22272/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [22528/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [22784/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [23040/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [23552/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [23808/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [24064/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [24320/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [24576/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [24832/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [25088/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [25344/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [25600/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [25856/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [26112/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [26368/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [26624/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [26880/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [27136/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [27392/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [27648/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [27904/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [28160/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [28416/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 198 [28672/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [29184/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [29440/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [29696/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [29952/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 198 [30208/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [30464/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [30976/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [31232/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [31488/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [31744/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [32000/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [32256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [32512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [32768/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [33280/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [33536/50000]	Loss: 4.6059	LR: 0.008000
Training Epoch: 198 [33792/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [34048/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 198 [34304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [34560/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [34816/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [35072/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [35328/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [35840/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [36352/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [36608/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [36864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [37120/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 198 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [37632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [37888/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [38144/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [38400/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [39168/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [39680/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [39936/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [40192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [40960/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 198 [41216/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 198 [41472/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [41728/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [41984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [42240/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [42496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [42752/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [43264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [43520/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [43776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [44288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 198 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [44800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [45056/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [45312/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 198 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [46080/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [46336/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [46592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [47616/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [47872/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [48128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [48384/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [48640/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [48896/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 198 [49152/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 198 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 198 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 198 [50000/50000]	Loss: 4.6054	LR: 0.008000
epoch 198 training time consumed: 21.83s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  297036 GB |  297036 GB |
|       from large pool |  400448 KB |    1770 MB |  296769 GB |  296769 GB |
|       from small pool |    3550 KB |       9 MB |     267 GB |     267 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  297036 GB |  297036 GB |
|       from large pool |  400448 KB |    1770 MB |  296769 GB |  296769 GB |
|       from small pool |    3550 KB |       9 MB |     267 GB |     267 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  182761 GB |  182761 GB |
|       from large pool |  244672 KB |  473024 KB |  182456 GB |  182455 GB |
|       from small pool |    2594 KB |    4843 KB |     305 GB |     305 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12677 K  |   12677 K  |
|       from large pool |      36    |      77    |    6129 K  |    6129 K  |
|       from small pool |     187    |     225    |    6547 K  |    6547 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12677 K  |   12677 K  |
|       from large pool |      36    |      77    |    6129 K  |    6129 K  |
|       from small pool |     187    |     225    |    6547 K  |    6547 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      33    |    6411 K  |    6411 K  |
|       from large pool |      10    |      11    |    2538 K  |    2538 K  |
|       from small pool |      11    |      23    |    3872 K  |    3872 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 198, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.45s

Training Epoch: 199 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [512/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [768/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [1536/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [2048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [2304/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [3328/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [3584/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [3840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [4096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [4352/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [4608/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [4864/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [6144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [6400/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 199 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [6912/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [7168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [7424/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [7680/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [7936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [8192/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [8448/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [8704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [8960/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [9216/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [9472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [9728/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [9984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [10240/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [10496/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [10752/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [11008/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [11264/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [11520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [11776/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [12032/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [12288/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [12544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [12800/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [13056/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [14080/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [14336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [14592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [14848/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [15104/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [15360/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [15616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [15872/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [16128/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [16384/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [16640/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [16896/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [17152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [17408/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [17920/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [18176/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 199 [18432/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 199 [18688/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [18944/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [19200/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [19456/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [19712/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [19968/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [20224/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [20480/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [20992/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 199 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [21504/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [21760/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [22016/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [22272/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [22528/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [23040/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [23296/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [23552/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [23808/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [24064/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [24320/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [24576/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [24832/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [25088/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [25344/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [25600/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [25856/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [26112/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [26368/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [26880/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [27136/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [27392/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [27648/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [27904/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [28160/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [28416/50000]	Loss: 4.6047	LR: 0.008000
Training Epoch: 199 [28672/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [28928/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [29184/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [29440/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [29696/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [29952/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [30208/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [30720/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [30976/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [31232/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [31488/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [31744/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [32256/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 199 [32512/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [32768/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [33024/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [33280/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [33536/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 199 [33792/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [34048/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [34304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [34560/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [34816/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [35072/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [35328/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [35584/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [35840/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [36096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [36352/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [36608/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 199 [36864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [37120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [37376/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [37632/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [37888/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [38144/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [38400/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [38656/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [38912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [39168/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 199 [39424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [39680/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [39936/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [40192/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 199 [40448/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [40704/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [40960/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [41216/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 199 [41472/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [41728/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 199 [41984/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [42496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [42752/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [43008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [43264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [43776/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [44032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [44544/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [44800/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [45056/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [45312/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [45568/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [46080/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 199 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [46592/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [47360/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [47616/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [47872/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [48128/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 199 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 199 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [49152/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 199 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [49920/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 199 [50000/50000]	Loss: 4.6052	LR: 0.008000
epoch 199 training time consumed: 21.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  298537 GB |  298536 GB |
|       from large pool |  400448 KB |    1770 MB |  298268 GB |  298268 GB |
|       from small pool |    3550 KB |       9 MB |     268 GB |     268 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  298537 GB |  298536 GB |
|       from large pool |  400448 KB |    1770 MB |  298268 GB |  298268 GB |
|       from small pool |    3550 KB |       9 MB |     268 GB |     268 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  183684 GB |  183684 GB |
|       from large pool |  244672 KB |  473024 KB |  183377 GB |  183377 GB |
|       from small pool |    2594 KB |    4843 KB |     307 GB |     307 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12741 K  |   12741 K  |
|       from large pool |      36    |      77    |    6160 K  |    6160 K  |
|       from small pool |     187    |     225    |    6580 K  |    6580 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12741 K  |   12741 K  |
|       from large pool |      36    |      77    |    6160 K  |    6160 K  |
|       from small pool |     187    |     225    |    6580 K  |    6580 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    6442 K  |    6442 K  |
|       from large pool |      10    |      11    |    2551 K  |    2551 K  |
|       from small pool |       9    |      23    |    3890 K  |    3890 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 199, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.46s

Training Epoch: 200 [256/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [512/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [768/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [1024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [1280/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [1536/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [1792/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [2048/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [2304/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [2560/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [2816/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [3072/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [3328/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [3584/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [3840/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [4096/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [4352/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [4608/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [4864/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [5120/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [5376/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [5632/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [5888/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [6144/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [6400/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [6656/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [6912/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [7168/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [7424/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [7680/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [7936/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [8192/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [8448/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [8704/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [8960/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [9216/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [9472/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [9728/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [9984/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [10240/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [10496/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [10752/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [11008/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [11264/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [11520/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [11776/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [12032/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [12288/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [12544/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [12800/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [13056/50000]	Loss: 4.6048	LR: 0.008000
Training Epoch: 200 [13312/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [13568/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [13824/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [14080/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [14336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [14592/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [14848/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [15104/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [15360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [15616/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [15872/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [16128/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [16384/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [16640/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [16896/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [17152/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [17408/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [17664/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [17920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [18176/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [18432/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [18688/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [18944/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [19200/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [19456/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [19712/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [19968/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [20224/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [20480/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [20736/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [20992/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [21248/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [21504/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [21760/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 200 [22016/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [22272/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [22528/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [22784/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [23040/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [23296/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [23552/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [23808/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [24064/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [24320/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [24576/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [24832/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [25088/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [25344/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [25600/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [25856/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [26112/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [26368/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [26624/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [26880/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [27136/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [27392/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [27648/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [27904/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [28160/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [28416/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [28672/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [28928/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [29184/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [29440/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [29696/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 200 [29952/50000]	Loss: 4.6049	LR: 0.008000
Training Epoch: 200 [30208/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [30464/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [30720/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [30976/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [31232/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [31488/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [31744/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [32000/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [32256/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [32512/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [32768/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [33024/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [33280/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [33536/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [33792/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [34048/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [34304/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [34560/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [34816/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 200 [35072/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [35328/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [35584/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [35840/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [36096/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [36352/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [36608/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 200 [36864/50000]	Loss: 4.6050	LR: 0.008000
Training Epoch: 200 [37120/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [37376/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [37632/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [37888/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [38144/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [38400/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [38656/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [38912/50000]	Loss: 4.6051	LR: 0.008000
Training Epoch: 200 [39168/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [39424/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [39680/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [39936/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [40192/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [40448/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [40704/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [40960/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [41216/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 200 [41472/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [41728/50000]	Loss: 4.6057	LR: 0.008000
Training Epoch: 200 [41984/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [42240/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [42496/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [42752/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [43008/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [43264/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [43520/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [43776/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [44032/50000]	Loss: 4.6058	LR: 0.008000
Training Epoch: 200 [44288/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [44544/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [44800/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [45056/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [45312/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [45568/50000]	Loss: 4.6052	LR: 0.008000
Training Epoch: 200 [45824/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [46080/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [46336/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [46592/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [46848/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [47104/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [47360/50000]	Loss: 4.6056	LR: 0.008000
Training Epoch: 200 [47616/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [47872/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [48128/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [48384/50000]	Loss: 4.6053	LR: 0.008000
Training Epoch: 200 [48640/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [48896/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [49152/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [49408/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [49664/50000]	Loss: 4.6054	LR: 0.008000
Training Epoch: 200 [49920/50000]	Loss: 4.6055	LR: 0.008000
Training Epoch: 200 [50000/50000]	Loss: 4.6056	LR: 0.008000
epoch 200 training time consumed: 21.69s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  403998 KB |    1773 MB |  300037 GB |  300036 GB |
|       from large pool |  400448 KB |    1770 MB |  299767 GB |  299766 GB |
|       from small pool |    3550 KB |       9 MB |     269 GB |     269 GB |
|---------------------------------------------------------------------------|
| Active memory         |  403998 KB |    1773 MB |  300037 GB |  300036 GB |
|       from large pool |  400448 KB |    1770 MB |  299767 GB |  299766 GB |
|       from small pool |    3550 KB |       9 MB |     269 GB |     269 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3378 MB |    3378 MB |    3378 MB |       0 B  |
|       from large pool |    3366 MB |    3366 MB |    3366 MB |       0 B  |
|       from small pool |      12 MB |      12 MB |      12 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  247266 KB |  477764 KB |  184607 GB |  184607 GB |
|       from large pool |  244672 KB |  473024 KB |  184299 GB |  184298 GB |
|       from small pool |    2594 KB |    4843 KB |     308 GB |     308 GB |
|---------------------------------------------------------------------------|
| Allocations           |     223    |     293    |   12805 K  |   12805 K  |
|       from large pool |      36    |      77    |    6191 K  |    6191 K  |
|       from small pool |     187    |     225    |    6613 K  |    6613 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     223    |     293    |   12805 K  |   12805 K  |
|       from large pool |      36    |      77    |    6191 K  |    6191 K  |
|       from small pool |     187    |     225    |    6613 K  |    6613 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      21    |      21    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       6    |       6    |       6    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      19    |      33    |    6472 K  |    6472 K  |
|       from large pool |      10    |      11    |    2564 K  |    2564 K  |
|       from small pool |       9    |      23    |    3908 K  |    3908 K  |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 200, Average loss: 0.0184, Accuracy: 0.0100, Time consumed:1.42s

saving weights file to checkpoint/vgg16/Tuesday_16_March_2021_18h_07m_25s/vgg16-200-regular.pth
